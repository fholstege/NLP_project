<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Generalized Robust Conjoint Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
							<email>theodoros.evgeniou@insead.edu</email>
						</author>
						<author>
							<persName><forename type="first">Constantinos</forename><surname>Boussios</surname></persName>
							<email>boussios@openratings.com</email>
						</author>
						<author>
							<persName><forename type="first">Giorgos</forename><surname>Zacharia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Technology Management</orgName>
								<orgName type="institution">INSEAD, Boulevard de Constance</orgName>
								<address>
									<postCode>77300</postCode>
									<settlement>Fontainebleau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>200 West Street</addrLine>
									<postCode>02451</postCode>
									<settlement>Waltham</settlement>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">MIT, Cambridge</orgName>
								<orgName type="laboratory">Laboratory for Information and Decision Systems</orgName>
								<address>
									<postCode>02139</postCode>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>200 West Street</addrLine>
									<postCode>02451</postCode>
									<settlement>Waltham</settlement>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Center for Biological and Computational Learning, MIT, Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Generalized Robust Conjoint Estimation</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 e 1526-548X 05 2403 0415</idno>
					</monogr>
					<idno type="DOI">10.1287/mksc.1040.0100</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-13T12:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>choice models</term>
					<term>data mining</term>
					<term>econometric models</term>
					<term>hierarchical Bayes analysis</term>
					<term>marketing tools</term>
					<term>regression and other statistical techniques</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The amount of data capturing preferences of people for particular products, services, and information sources, has been dramatically increasing in recent years largely due, for example, to electronic commerce. Traditional preference modeling methods such as conjoint analysis <ref type="bibr" target="#b15">(Carroll and Green 1995</ref><ref type="bibr" target="#b28">, Green and Srinivasan 1978</ref><ref type="bibr" target="#b29">, Green and Srinivasan 1990</ref>) have been used for many preference modeling applications <ref type="bibr" target="#b61">(Wittink and Cattin 1989)</ref> typically with data gathered under controlled conditions such as through questionnaires. However, much of the available information today about choices of people, such as scanner or clickstream data, is not gathered in such a controlled way and therefore is more noisy <ref type="bibr" target="#b17">(Cooley et al. 1997</ref><ref type="bibr" target="#b33">, Kohavi 2001</ref>. It is therefore important to develop new preference modeling methods that are (a) highly accurate, (b) robust to noise, and (c) computationally efficient in order to handle the large amounts of choice data available.</p><p>In this paper we present a family of preference models, from simple linear ones like existing ones (Ben-Akiva and Lerman 1985, <ref type="bibr" target="#b28">Srinivasan 1978, Srinivasan and</ref><ref type="bibr" target="#b50">Shocker 1973)</ref> to highly nonlinear ones that are robust to noise. They are developed based on the well-founded field of statistical learning theory and are shown to be almost equivalent to support vector machines (SVM) <ref type="bibr" target="#b58">(Vapnik 1998)</ref>, therefore bringing a number of new theories and tools to the preference modeling research community as also done by the recent work of <ref type="bibr" target="#b19">(Cui and Curry 2005)</ref>. Their estimation involves solving a quadratic programming optimization problem with simple box constraints, which is computationally efficient and leads to a unique optimal solution.</p><p>We compare our methods with logistic regression <ref type="bibr">Lerman 1985, Louviere et al. 2000)</ref>, hierarchical Bayes (HB) (DeSarbo and Ansari 1997, <ref type="bibr" target="#b1">Allenby et al. 1998</ref>, and the polyhedral estimation methods of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> using simulations as in <ref type="bibr" target="#b5">Arora and Huber (2001)</ref>, and <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>. Our methods are shown experimentally to be more robust to noise than both logistic regression and the polyhedral methods, to either significantly outperform or never be worse than both logistic regression and the polyhedral methods, and <ref type="bibr">Evgeniou, Boussios, and Zacharia: Generalized Robust</ref> </p><note type="other">Conjoint Estimation 416</note><p>Marketing Science 24(3), pp. <ref type="bibr">415-429, © 2005 INFORMS</ref> to estimate nonlinear utility models faster and better than all methods including HB.</p><p>In this paper we do not address the issue of designing questionnaires as typically done in conjoint analysis. We plan to explore this issue in the future within the framework discussed here. We focus only on the utility estimation problem. We also deal only with full-profile preference data-full product comparisons-for the case of choice-based conjoint analysis (CBC) <ref type="bibr" target="#b36">(Louviere et al. 2000)</ref> instead of other metric based ones. Extensions to the latter are possible like in the case of SVM regression <ref type="bibr" target="#b58">(Vapnik 1998)</ref>.</p><p>A number of practical issues arise when modeling preferences from unconstrained observations of choices. For example there are typically problems of taste heterogeneity <ref type="bibr">Ansari 1997, Jedidi et al. 2003)</ref> among the subjects providing the data, or unobservable choice sets and incomplete information about choices <ref type="bibr" target="#b37">(Manski 1977)</ref> which may also dynamically change over time <ref type="bibr">(Pauwels 2005)</ref>. These make the estimated models biased. In this paper we do not consider the latter issues, which is typical for conjoint analysis methods. In terms of handling heterogeneity across many individuals we only explore a simple ad hoc extension of the proposed methods to handle this issue, which shows promising results relative to HB. Extensions of the proposed methods to handle heterogeneity are part of future work.</p><p>Traditional conjoint estimation methods, such as logistic regression and HB, are developed assuming a particular probabilistic model of the data and the noise. Unlike those, our approach, like that of <ref type="bibr" target="#b55">Toubia et al. (2003)</ref> and <ref type="bibr" target="#b19">Cui and Curry (2005)</ref>, is based on formulating the problem of preference modeling as an optimization problem where an appropriate cost function is minimized without assuming a particular probabilistic model for the data. The cost function is motivated by statistical arguments, namely by statistical learning theory, an empiricist's approach to developing models from data. We briefly discuss the statistical motivation of the optimization models. It is important to note that it is possible to justify the optimization models presented here using Bayesian arguments and reformulating the cost functions in terms of likelihood maximization <ref type="bibr" target="#b22">(Evgeniou et al. 2000a)</ref>. This is beyond the scope of this paper and we focus on providing methods and mathematical tools that can supplement existing ones that assume particular probabilistic models.</p><p>The work we present does not aim to replace existing methods for preference modeling, but instead to contribute to the field new tools and methods that can complement existing ones. One of the goals of this work is to bring to the field of conjoint analysis ideas from statistical learning theory and SVM which have been successfully used for other data analysis problems <ref type="bibr">(Evgeniou et al. 2000a, b;</ref><ref type="bibr" target="#b58">Vapnik 1998)</ref>.</p><p>The paper is organized as follows. In §2 we present our approach to modeling preferences. For simplicity we only show the basic linear model and briefly discuss some properties and the extension to nonlinear preference modeling. The latter are presented in more detail in the online appendix available at the Marketing Science website (http://mktsci.pubs.informs.org). In §3 experiments comparing the methods with other conjoint analysis methods, namely logistic regression, HB, and the estimation method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>, are shown. Finally §4 is a summary and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Previous Work</head><p>The market research community has traditionally approached utility estimation problems through function estimation. Conjoint analysis is one of the main methods for modeling preferences from data <ref type="bibr" target="#b15">(Carroll and</ref><ref type="bibr" target="#b15">Green 1995, Green and</ref><ref type="bibr" target="#b28">Srinivasan 1978)</ref>. A number of conjoint analysis methods have been proposed-see, for example, the Sawtooth Software website. Since the early 1970s conjoint analysis has been a very popular approach with hundreds of commercial applications per year <ref type="bibr" target="#b61">(Wittink and Cattin 1989)</ref>. In conjoint analysis designing questionnaires is a central issue <ref type="bibr" target="#b5">(Arora and Huber 2001</ref><ref type="bibr" target="#b34">, Kuhfeld et al. 1994</ref><ref type="bibr" target="#b41">, Oppewal et al. 1994</ref><ref type="bibr" target="#b47">, Segal 1982</ref>, which, as mentioned above, we do not address here.</p><p>Within the discrete choice analysis area users' preferences are modeled as random variables of logit models <ref type="bibr" target="#b8">(Ben-Akiva and Lerman 1985;</ref><ref type="bibr" target="#b9">Ben-Akiva et al. 1997;</ref><ref type="bibr" target="#b38">McFadden 1974</ref><ref type="bibr" target="#b39">McFadden , 1986</ref>. Both conjoint analysis and discrete choice methods have always faced the tradeoff between model (multinomial logit models) complexity and computational ease as well as predictive performance of the estimated model. This tradeoff is linked to the well-known "curse of dimensionality" <ref type="bibr" target="#b52">(Stone 1985)</ref>: as the number of dimensions increases an exponential increase in the number of data is needed to maintain reliable model estimation. The method we present in this paper can handle this issue, as already shown for other applications <ref type="bibr">(Evgeniou et al. 2002</ref><ref type="bibr" target="#b58">, Vapnik 1998</ref>.</p><p>A different approach was implemented by <ref type="bibr" target="#b30">Herbrich et al. (1999)</ref> who instead of trying to apply regression techniques for utility function estimation, they reformulated the problem as an ordinal regression estimation and used SVM to predict transitive ranking boundaries. More recently, <ref type="bibr" target="#b19">Cui and Curry (2005)</ref> used SVM directly for predicting choices of consumers. Our methods are similar with those in <ref type="bibr" target="#b30">Herbrich et al. (1999)</ref> and <ref type="bibr" target="#b19">Cui and Curry (2005)</ref>: in particular they are almost equivalent to SVM. Unlike <ref type="bibr" target="#b30">Herbrich et al. (1999)</ref> and <ref type="bibr" target="#b19">Cui and Curry (2005)</ref>, we focus here on choice-based conjoint analysis and on the comparison with logistic  <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>.</p><p>Finally, recent work by <ref type="bibr" target="#b55">Toubia et al. (2003</ref><ref type="bibr" target="#b54">Toubia et al. ( , 2004</ref> addresses the problem of designing questionnaires and estimating preference models through solving polyhedral optimization problems which are similar to our methods as we discuss below. They develop methods for both metric <ref type="bibr" target="#b55">(Toubia et al. 2003</ref>) and choicebased <ref type="bibr" target="#b54">(Toubia et al. 2004</ref>) conjoint analysis. Our work, like that of <ref type="bibr" target="#b55">Toubia et al. (2003</ref><ref type="bibr" target="#b54">Toubia et al. ( , 2004</ref>, also aims at exploring the direction of developing new methods for preference modeling that are based on polyhedral optimization. A main difference from <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> is that they focus more on the design of individual-specific questionnaires while we focus on the estimation of a utility function from data. In the experiments below we only use the utility function estimation method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> and not the questionnaire design method they propose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A Theoretical Framework for</head><p>Modeling Preferences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Setup and Notation</head><p>We consider the standard (i.e., <ref type="bibr" target="#b36">Louviere et al. 2000)</ref> problem of estimating a utility function from a set of examples of past choices all coming from a single individual-so from a single true underlying utility function.</p><p>In the experiments we only deal with heterogeneity using a simple ad hoc approach described at the end of this section. Formally we have data from n choices where, without loss of generality, the ith choice is among two products (or services, bids, etc.)</p><formula xml:id="formula_0">x 1 i x 2 i .</formula><p>To simplify notation we assume that for each i the first product x 1 i is the preferred one-we can rename the products otherwise. All products are fully characterized by m-dimensional vectors, where m is the number of attributes describing the products. We represent the jth product for choice i as</p><formula xml:id="formula_1">x j i = x j i 1 x j i 2</formula><p>x j i m (notice that we use bold letters for vectors). So the ith choice is among a pair of m-dimensional vectors. We are now looking for a utility function that is in agreement with the data, namely a function that assigns higher utility value to the first product-the preferred one-for each pair of choices. This is the standard setup of choice-based conjoint analysis <ref type="bibr" target="#b36">(Louviere et al. 2000)</ref>. Variations of this setup (i.e., cases where we know pairwise relative preferences with intensities) can be modeled in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">A Robust Method for Linear Utility</head><p>Function Estimation We first make the standard assumption <ref type="bibr">Lerman 1985, Srinivasan and</ref><ref type="bibr" target="#b50">Shocker 1973)</ref> that the utility function is a linear function of the values (or logarithms of the values, without loss of generality) of the product attributes: the utility of a product</p><formula xml:id="formula_2">x = x 1 x 2 x m is U x = w 1 • x 1 + w 2 • x 2 + • • • + w m • x m .</formula><p>We are looking for a utility function with parameters w 1 w 2 w m that agrees with our data; that is we are looking for w 1 w m such that ∀ i ∈ 1 n ,</p><formula xml:id="formula_3">w 1 • x 1 i 1 + w 2 • x 1 i 2 + • • • + w m • x 1 i m ≥ w 1 • x 2 i 1 + w 2 • x 2 i 2 + • • • + w m • x 2 i m (1)</formula><p>Clearly there may be no w f that satisfy all n constraints, since in practice the true utility function does not have to be linear and generally there are a lot of inconsistencies in data describing preferences of people. To allow for errors/inconsistencies we use slack variables, a standard approach for optimization methods <ref type="bibr">(Bertsimas and Tsitsiklis 1997)</ref>. For each of the n inequality constraints (1) we introduce a positive slack variable i which effectively measures how much inconsistency/error there is for choice i, as in <ref type="bibr" target="#b50">Srinivasan and Shocker (1973)</ref>. So we are now looking for a set of parameters w 1 w 2 w m so that we minimize the error i=1 n i where i ≥ 0 and satisfy</p><formula xml:id="formula_4">∀ i ∈ 1 2 n , w 1 • x 1 i 1 + w 2 • x 1 i 2 + • • • + w m • x 1 i m ≥ w 1 • x 2 i 1 + w 2 • x 2 i 2 + • • • + w m • x 2 i m − i (2)</formula><p>Notice that one may require minimizing the L 0 norm of the slack variables i so that what is penalized is the number of errors/inconsistencies and not the "amount" of it. In that case the optimization problem becomes an integer programming problem which is hard to solve. So in this simple model we are looking for a linear utility function that minimizes the amount of errors/inconsistencies on the estimation data. This, however, may lead to models that overfit the current data, are sensitive to noise, and can suffer from the curse of dimensionality and are, therefore, are less accurate and cannot handle well choice data that involve a large number of attributes m and are noisy <ref type="bibr" target="#b58">(Vapnik 1998)</ref>. It is therefore important to augment this model to avoid overfitting, hence improve accuracy performance and handle noise better.</p><p>Statistical learning theory suggests that this can be achieved by controlling the complexity of the model estimated: a very complex model (i.e., a polynomial of very high degree) may fit the estimation data perfectly but has the danger of overfitting and being sensitive to noise, while a very simple model may not be powerful enough to capture the relations in the data. Therefore one needs to control the complexity of the model in some way. Appropriate measures of complexity that are not necessarily related to the Marketing Science 24(3), pp. 415-429, © 2005 INFORMS number of parameters estimated have been defined in the past <ref type="bibr" target="#b59">(Vapnik and Chervonenkis 1971</ref><ref type="bibr" target="#b3">, Alon et al. 1993</ref><ref type="bibr" target="#b58">, Vapnik 1998</ref>. Discussing them is beyond the scope of this paper, and we refer the reader to <ref type="bibr" target="#b59">Vapnik and Chervonenkis (1971)</ref> and Tapnik (1998) for more information. The main point to emphasize is that it is necessary to include a complexity control to avoid overfitting and handle noise and a large number of attributes better <ref type="bibr" target="#b58">(Vapnik 1998)</ref>.</p><p>We now present a way to do this as in the case of SVM, a method for classification and regression developed within the framework of statistical learning theory and widely used with a lot of success for other data analysis problems <ref type="bibr" target="#b58">(Vapnik 1998)</ref>. We briefly describe SVM in the Technical Appendix available at the Marketing Science website (http://mktsci.pubs.informs.org), and we refer the reader for more information on this rich area of research to <ref type="bibr" target="#b58">(Vapnik 1998)</ref> and to www.kernelmachines.org.</p><p>As a final note, existing methods for preference modeling such as logistic regression fit the data without controlling for the complexity of the model; therefore, as we see in the experiments section below, they are less accurate and tend to be more sensitive to noise than the methods we discuss here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Robust Preference Modeling Methods</head><p>Choosing the appropriate complexity control is an important subject of ongoing research. In our case the main question is what are the characteristics of a utility function that make it complex? For example, in the case of regression, a standard measure of complexity of a model/function is how "smooth" the function isformally the L 2 integral of its first derivative <ref type="bibr" target="#b60">(Wahba 1990</ref><ref type="bibr" target="#b53">, Tikhonov and Arsenin 1977</ref><ref type="bibr" target="#b58">, Vapnik 1998</ref><ref type="bibr" target="#b27">, Girosi et al. 1995</ref>. This is in a sense a "natural" measure of complexity for regression. However, it is not clear if having a "smooth" utility function is necessarily "natural." We note that the dimensionality of the space where a function is estimated (i.e., the number of attributes m of the products) or the number of parameters used to represent the utility function can be, but do not need to be, measures of complexity <ref type="bibr" target="#b58">(Vapnik 1998)</ref>.</p><p>We use a model complexity control that is standard for other data analysis methods <ref type="bibr" target="#b58">(Vapnik 1998</ref><ref type="bibr" target="#b60">, Wahba 1990</ref><ref type="bibr" target="#b27">, Girosi et al. 1995</ref>, such as for SVM (see the Technical Appendix at http://mktsci.pubs.informs.org). Intuitively, we require that constraints (1) hold (when they are feasible) with some "confidence margin." We would like to find a function that assigns to the preferred products utility that is larger than that assigned to the nonpreferred products by as high an amount as possible. The idea of requiring a margin is analogous to the method proposed by <ref type="bibr" target="#b48">Srinivasan (1998)</ref>. Geometrically the intuition is as follows. Consider first the case where the feasible area defined by the comparison constraints ( <ref type="formula">1</ref>) is nonempty-so the 's in (2) are all 0. If we represent each constraint (1) as a hyperplane in the space of parameters w f , as shown in Figure <ref type="figure" target="#fig_0">1</ref>, then the feasible area is a polyhedron in that space. If we search among all functions that fit the data perfectly, then any solution w in this feasible area will do. Instead we choose the solution point in the feasible area that is the furthest from the constraints, therefore satisfying the hardest comparison constraints the most. This is the center of the largest inscribed sphere in the feasible area <ref type="bibr" target="#b58">(Vapnik 1998)</ref> shown in Figure <ref type="figure" target="#fig_0">1</ref>. It can be shown that the "margin" with which the constraints are satisfied, which is also the radius of the largest inscribed sphere, is equal to 1/ w 2 <ref type="bibr" target="#b58">(Vapnik 1998)</ref>; hence by minimizing w 2 we can maximize the "margin" with which the chosen products are preferred by the other ones. This is what we do below. In the case where empirical errors exist (there are nonzero slack variables i ) the intuition is similar: we want to minimize the amount of error while satisfying the correct comparisons "as much as possible."</p><p>We also refer the reader to <ref type="bibr" target="#b10">Bennett and Bredensteiner (2000)</ref> for another intuitive geometric interpretation of both the case where the feasible area is nonempty and the case where it is empty.</p><p>The proposed method is to simultaneously minimize the error we make on the example data via minimizing the slack variables i and maximize the margin with which the solution satisfies the constraints. As in the case of SVM it can be shown <ref type="bibr" target="#b58">(Vapnik 1998</ref>) that this is achieved through the following optimization problem. (For simplicity we omit the mathematical Notes. The four lines shown here correspond to four comparisons-four constraints. The solution of the proposed method is the center of the largest inscribed sphere to this polyhedron. In this case the sphere (circle) touches 3 of the hyperplanes: these correspond to the hardest choices.</p><p>derivation and we refer the reader to Vapnik 1998 for it.) min</p><formula xml:id="formula_5">w 1 w m i i=1 n i + f =1 m w 2 f</formula><p>subject to:</p><formula xml:id="formula_6">w 1 • x 1 i 1 + w 2 • x 1 i 2 + • • • + w m • x 1 i m ≥ w 1 • x 2 i 1 + w 2 • x 2 i 2 + • • • + w m • x 2 i m + 1 − i for ∀ i ∈ 1 n and i ≥ 0 (3)</formula><p>Notice the following:</p><p>(1) The role of the constant 1 (clearly any other constant would also work, since the solution w is defined up to a scale factor, as long as we also appropriately change parameter ) at the constraints: slack variables i are nonzero (hence we pay a cost) both for constraints corresponding to comparisons that are not satisfied by the estimated utility function, and for those satisfied but with "confidence margin" less than 1. For the scaling of the solution we found <ref type="bibr" target="#b58">(Vapnik 1998)</ref>.</p><p>(2) In the case where all are zero, the radius of the inscribed sphere in Figure <ref type="figure" target="#fig_0">1</ref> is equal to 1/ w 2 , so, effectively, the smaller w 2 , the larger the radius, which is the reason we minimize w 2 in (3) <ref type="bibr" target="#b58">(Vapnik 1998)</ref>. It turns out that the smaller w 2 is, the smaller the complexity of the estimated model is <ref type="bibr" target="#b58">(Vapnik 1998)</ref>.</p><p>(3) Parameter controls the trade off between fitting the data ( i i ) and the complexity of the model ( w 2 ). There are a number of ways to choose parameter <ref type="bibr" target="#b60">(Wahba 1990</ref><ref type="bibr" target="#b58">, Vapnik 1998</ref>. For example it can be chosen so that the prediction error in a small validation set is minimized or through cross validation (also called leave-one-out error) <ref type="bibr" target="#b60">(Wahba 1990</ref>). Briefly, the latter is done as follows.</p><p>For a given parameter we measure its leave-oneout error as follows: for each of the n choice data, we estimate a utility function using (3) only with the remaining n − 1 data and test if the estimated function correctly chooses the right product for the choice data point not used (left out). We then count the number of errors in the n choice points when they were left out. This is the cross validation (leave-one-out) error for the parameter . We then choose the parameter with the smallest cross validation error.</p><p>Cross validation is used when we can assume that the future data (choices) come from the same distribution as the data used for estimation <ref type="bibr" target="#b58">(Vapnik 1998)</ref>. However, in conjoint analysis this may not always be the case. Such is the situation, for example, when the estimation data come from an orthogonal design: the orthogonal design is not a sample from the probability distribution of future choices. So cross validation cannot be formally used with an orthogonal design, but in practice one can still use it, as we also do in the experiments below. Instead, a validation set approach can, for example, be used in such cases. We need to assume that the validation data come from the same probability distribution as the future data.</p><p>In the experiments below we tuned using cross validation. We chose, using line search, a lambda between 0.001 and 100 (samples every order of magnitude only). Because we have a few data for each individual we use the same for all individuals that we choose using the average cross validation error across the individuals.</p><p>The idea of finding a central point in the feasible area defined by the data-imposed constraints is not new <ref type="bibr" target="#b50">(Srinivasan and Shocker 1973;</ref><ref type="bibr" target="#b55">Toubia et al. 2003</ref><ref type="bibr" target="#b54">Toubia et al. , 2004</ref>. For example, <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> choose the analytic center of the polyhedron described above slightly modified to take into account other constraints. We believe that both choices lead to models that are robust to noise, as also shown by the experiments below. A key difference of (3) from the method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> is that in our case we optimize both the error on the data and the complexity of the solution w 2 simultaneously using the tradeoff parameter . <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> do not handle this trade off between error and complexity through a simultaneous optimization. Other than including a complexity control, the accuracy performance as well as the robustness of a method to noise depends on how this trade off between error on the data and complexity is handled <ref type="bibr" target="#b58">(Vapnik 1998)</ref>. We conjecture that the difference in performance between our method and the method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> shown in the experiments below is due to the difference between the way these two methods handle this trade off.</p><p>In our case we incorporate a trade off parameter that controls "how much" the constraints need to be satisfied, and we develop a family of methods that are almost equivalent to the well-known method of SVM classification (see the Technical Appendix) with similar useful characteristics, namely:</p><p>• the estimation is done through fast quadratic programming optimization with box constraints, namely constraints that give only upper and lower bounds to the parameters to be estimated;</p><p>• the estimated utility function turns out to depend only on certain data-the "hard choices" that are the hyperplanes touching the inscribed sphere in Figure <ref type="figure" target="#fig_0">1</ref>-that are automatically detected;</p><p>• the generalization to highly nonlinear utility functions-that turn out to be linear in parameters <ref type="bibr" target="#b58">(Vapnik 1998)</ref>-is straightforward and computationally efficient;</p><p>• probabilistic guarantees on the future performance of the estimated model can be given under Marketing Science 24(3), pp. 415-429, © 2005 INFORMS certain assumptions about the probability distribution of the data. In particular, it can be shown that the predictive performance of the estimated models-that is, how often the estimated utility assigns higher utility to the correct product for future choices-increases as w 2 (which controls the confidence margin on the estimation data as discussed above) and the error i i decrease and as the number of data n increases <ref type="bibr" target="#b58">(Vapnik 1998</ref>). <ref type="bibr">1</ref> Next, we discuss these briefly and we refer the reader to the Technical Appendix for more details on each of these points.</p><p>Finally, we note that the solution w of (3) need not be positive. In practice we may want to impose the constraint that parameters w f are positive or (equivalently) to incorporate prior knowledge about the base level for each product attribute when we use levels to describe the product attributes <ref type="bibr" target="#b55">(Toubia et al. 2003)</ref>. We show in Appendix A below how to augment model (3) using virtual examples <ref type="bibr" target="#b46">(Scholkopf et al. 1996</ref>) to include such positivity constraints or to incorporate knowledge about the base level of an attribute. In the experiments below we added for all methods of positivity constraints capturing prior knowledge about the base level of the product attributes, since the polyhedral method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> requires the use of such constraints. </p><formula xml:id="formula_7">U x = w • x = n i=1 * i x 1 i − x 2 i • x (5)</formula><p>where * i are the dual parameters <ref type="bibr">(Bertsimas and Tsitsiklis 1997)</ref> corresponding to the dual optimization problem of (3). The dual problem is a quadratic <ref type="bibr">1</ref> The following theorem is well known for SVM <ref type="bibr" target="#b58">(Vapnik 1998</ref><ref type="bibr" target="#b22">, Evgeniou et al. 2000a</ref>): With probability 1 − , the probability that a future point is misclassified by a support vector machine classification solution that makes k misclassifications on n example data and has margin w 2 on this data is bounded by:</p><formula xml:id="formula_8">&lt; k n + w 2 n (4)</formula><p>where is decreasing with n and and increasing with w 2 . One can also replace k with i i and use a different . For simplicity we do not give the form of here and refer the reader for example to <ref type="bibr" target="#b58">Vapnik (1998)</ref> and <ref type="bibr" target="#b22">Evgeniou et al. (2000a)</ref>. We note that this theorem holds only if the n data (product differences) are i.i.d., which is not necessarily the case for the preference modeling setup.</p><p>It is an open question how to extend this theorem to the conjoint estimation case. This theorem currently provides only an informal motivation for the proposed approach.</p><p>programming problem with box constraints that has a unique optimal solution and is quickly solved in practice. See the Technical Appendix (http://mktsci. pubs.informs.org) and <ref type="bibr" target="#b16">Cortes and Vapnik (1995)</ref>. It can be shown <ref type="bibr" target="#b58">(Vapnik 1998</ref>) that for the optimal solution (5), and for SVM in general, only a few of the coefficients * i are nonzero. These are the coefficients i that correspond to the pairs of products x 1 i x 2 i hard to choose from. In other words the utility function model developed from a set of choices is specified only by the "hard" choices, which are automatically found by the method. This is in agreement with the intuition that preferences are shaped by the hard choices one has to make. Moreover, although we do not deal with this issue here, intuitively one could also use this characteristic of the proposed method to design questionnaires in the spirit of <ref type="bibr" target="#b55">Toubia et al. (2003)</ref>. For example there has been work in the area of active learning see for example <ref type="bibr" target="#b54">(Tong and Koller 2000)</ref>-that can be used for this problem. It is interesting to note that the questionnaire design approach of <ref type="bibr" target="#b55">Toubia et al. (2003)</ref> is similar in spirit with the active learning methods in the literature. We plan to explore this direction in future work.</p><p>2.4.2. Nonlinear Models. By estimating the utility function in its dual form (5), one can also estimate nonlinear functions efficiently even if the number of primal parameters w f is very large even if it is infinite <ref type="bibr" target="#b58">(Vapnik 1998</ref><ref type="bibr" target="#b60">, Wahba 1990</ref>. This is done by solving the dual optimization problem of (3), therefore always optimizing for the n free parameters i of (5) independent of the dimensionality of the "data" x. Consider for example the case where we model the utility using a polynomial of degree 2, and assume that we only have 2-attribute products. The utility of a product x 1 x 2 is therefore w 1 x 1 2 + w 2 x 2 2 + w 3 x 1 x 2 + w 4 x 1 + w 5 x 2 . Notice that we include the interaction of the attributes. Instead of estimating the 5 parameters w f , we estimate the n dual parameters i , where n is the number of data (comparisons). The utility function is then of the form:</p><formula xml:id="formula_9">U x = n i=1 * i x 1 i 1 2 − x 2 i 1 2 x 1 i 2 2 − x 2 i 2 2 x 1 i 1 x 1 i 2 − x 2 i 1 x 2 i 2 x 1 i 1 − x 2 i 1 x 1 i 2 − x 2 i 2 • x 1 2 x 2 2 x 1 x 2 x 1 x 2</formula><p>The dual formulation is always a quadratic programming optimization problem with box constraints and number of variables ( i ) equal to n, the number of constraints in (3) <ref type="bibr" target="#b58">(Vapnik 1998)</ref>. We discuss this further in the Technical Appendix available at the Marketing Science website (http://mktsci.pubs.informs. org). So the number of variables w f in the primal formulation (3) is not important <ref type="bibr" target="#b58">(Vapnik 1998)</ref>: products with a very large number m of attributes, as well as highly nonlinear utility functions that, for example, include all high order interactions among attributes can be computationally efficiently estimated in a robust way.</p><p>2.4.3. Handling Heterogeneity. The method discussed so far assumes that the data come from a single true underlying utility function and we estimate one utility function. In practice the data may come from many individuals and therefore from different underlying utility functions. A state-of-the-art approach to handling such data is by assuming a priori that all utility functions come from a probability distribution, for example a (unknown) Gaussian, and then estimating all utility functions simultaneously through also estimating the parameters of this distribution, as it is done in the case of hierarchical Bayes <ref type="bibr" target="#b35">(Lenk et al. 1996</ref><ref type="bibr" target="#b20">, DeSarbo et al. 1997</ref>.</p><p>Developing methods along the lines of the ones presented here that can be used to simultaneously estimate many utility functions that are assumed to be related in some way (i.e., all come from the same, unknown, Gaussian distribution) is a direction for future research. Some possible directions can be, for example, along the lines of boosting <ref type="bibr" target="#b25">(Freund and</ref><ref type="bibr">Schapire 1997, Friedman et al. 1998)</ref> or learning with heterogeneous kernels <ref type="bibr" target="#b11">(Bennett et al. 2002)</ref>. The issue is an open one also in the area of statistical learning theory.</p><p>In this paper we compare our approach with hierarchical Bayes even though we simply estimate one utility function for each individual independently: Hence HB has a relative advantage in the experiments since it combines information across all individuals. We also investigate along the direction of combining the models estimated for each individual following a simple ad hoc approach as in <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> briefly as follows.</p><p>First we estimate one model, for example one linear utility function w k , for each individual k independently. We then take the mean of the estimated models w = 1/N k w k , where N is the number of individuals. Finally, for each individual we replace w k with k w k + 1 − k w. Parameters k are between 0 and 1 and we estimate them by minimizing the mean square error of k w k + 1 − k w from the true utility function of each individual k. This gives an upper bound on the performance that can be achieved if we were to estimate k using only the available data as should be done in practice (in practice a validation set can be used to set the parameters k as for the case of parameter discussed above). Although this is a very simple and ad hoc approach to handling heterogeneity, the experiments show that the proposed direction is promising. We plan to explore this direction in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We run Monte Carlo simulations to study the performance of the methods under varying conditions. Simulations have often been used in the past to study preference modeling methods (i.e., <ref type="bibr" target="#b14">Carmone and Jain 1978</ref><ref type="bibr" target="#b55">, Toubia et al. 2003</ref><ref type="bibr" target="#b4">, Andrews et al. 2002</ref>. They are useful, for example, in exploring various domains in order to identify strengths and weaknesses of methods. Below we explore domains that vary according to noise (magnitude) and respondent heterogeneity. We used simulations to compare our methods with logistic regression, the recently proposed polyhedral estimation method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>, and with HB for heterogeneous data, considered a state-of-theart approach. It is important to note that in all cases we generated the data in a way that gives an advantage to logistic regression and HB-that is, the data were generated according to the probability distributions assumed by these methods. Moreover, the comparison with HB is not well defined since our methods are for individual utility estimation while HB uses information across many individuals. The simple ad hoc extension to handle heterogeneity that we described above, labeled as "SVM Mix" below, is the only method that can be directly compared with HB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Design of Simulations</head><p>For easy comparison with other work in the literature we followed the basic simulation design used by other researchers in the past. In particular we simply replicated the experimental setup of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>, which in turn was based on the simulation studies of <ref type="bibr" target="#b5">Arora and Huber (2001)</ref>. For completeness we briefly describe that setup.</p><p>We generated data describing products with 4 attributes, each attribute having 4 levels. Each question consisted of 4 products to choose from. The question design we used was either orthogonal or randomly generated. For the orthogonal design to be well defined we used 16 questions per individual as in <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>. The random design is a closer simulation for data that are not from questionnaires, such as more unconstrained consumer choice data.</p><p>We simulated 100 individuals. The partworths for each individual were generated randomly from a Gaussian with mean − − 1 3 1 3 for each attribute. Parameter is the magnitude that controls the noise (response accuracy). As in <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> we used = 3 for high magnitude (low noise) and = 0 5 for low magnitude (high noise). We modeled heterogeneity among the 100 individuals by varying the variance 2 of the Gaussian from which the partworths were generated. The covariance matrix of the Gaussian was a diagonal matrix with all diagonal elements being 2 . We modeled high heterogeneity using 2 = 3 , and low heterogeneity using 2 = 0 5 , as in <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>. As discussed in <ref type="bibr" target="#b5">Arora and Huber (2001)</ref> and <ref type="bibr" target="#b55">Toubia et al. (2003)</ref> these parameters are chosen so that the range of average partworths and heterogeneity found in practice is covered.</p><p>Notice that for each of the four attributes the mean partworths are the smallest for the first level and the largest for the fourth level-in increasing order. Because the method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> requires that constraints about the relative order of the actual partworths for each level relative to the lowest level are added (in the form of positivity constraints- <ref type="bibr" target="#b54">Toubia et al. 2004</ref>), we incorporated this information to all other methods. For our method and for logistic regression this was done using the virtual examples approach discussed in Appendix A. For the case of HB this was done by simply constraining the sampling from the posterior during the HB estimation iterations to be such that we only use partworth samples for which the lowest levels are the same ones as the actual lowest levels. Adding constraints to HB can be done in other ways, too, as discussed in Sawtooth Software (see, for example, the Sawtooth Software website), but none of them is standard. Notice that the relative order may be changing as we sample the partworths for the four levels: We incorporated constraints about the actual lowest levels and not the lowest levels of the mean partworths.</p><p>Finally, all experiments were repeated five times, so a total of 500 individual utilities were estimated, and the average performance is reported. Notes. The true utilities are linear and linear utility models are estimated. Bold indicates best or not significantly different than best at p &lt; 0 05 among analytic center, SVM, and logistic regression-the first three columns only.</p><p>With a * we indicate the best among all columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Results</head><p>We compare the methods using the RMSE of the estimated partworths. Both estimated and true partworths were always normalized for comparability. In particular, as in <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>, each attribute is made such that the sum of the levels is 0, and the utility vector is then normalized such as the sum of the absolute values is 1. We also measured the predictive performance (hit rate) of the estimated models by generating 100 new random questions for each individual and testing how often the estimated utility functions predict the correct winning product. In the table below we report the hit rates below the RMSE errors.</p><p>Table <ref type="table" target="#tab_2">1</ref> shows the results. The format of the table is the same as that of <ref type="bibr" target="#b5">Arora and Huber (2001)</ref> and <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>. We label our method as "SVM" since it is very similar to SVM classification. The polyhedral method of Toubia et al. ( <ref type="formula">2004</ref>) is labeled as "Analytic"-the method is called Analytic Center in <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>. We also tested the simple method discussed in §2.4.3 to take into account information across the 100 individuals. The results are shown in the last column of Table <ref type="table" target="#tab_2">1</ref> with label "SVM Mix."</p><p>We performed two significance tests: (a) One to compare only the analytic center method, logistic regression, and the method proposed here-the three methods that do not combine information across individuals. The best of the first three columns is reported in bold. (b) One to find the best among all columns (including HB and SVM Mix) which we report with a " * ."</p><p>From Table <ref type="table" target="#tab_2">1</ref> we observe the following:</p><p>• SVM significantly outperforms both the analytic center method and logistic regression, the latter for the random designs and when there is noise. It is never worse than logistic regression or the analytic center method.</p><p>• Both SVM and SVM Mix are relatively better for the random design. For example SVM is similar to logistic regression in all orthogonal design cases. We believe this is partly due to the problem with choosing parameter for the orthogonal design, as discussed above, and because in general the future data come from a different probability distribution than the estimation data. This limitation also indicates that it may be important to combine the proposed method with a similar method for designing questionnaires. As shown by <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> such an extension to questionnaire design can lead to significant improvements. We leave this as part of future work.</p><p>• The proposed method significantly outperforms both logistic regression and the analytic center method when there is noise for the random design. The performance drop from high magnitude to low magnitude, namely when noise increases, is significantly lower for SVM than for both logistic regression and the analytic center for the random design. It is significantly lower than logistic regression for the orthogonal design but larger than the analytic center method in that case. However the latter is always significantly worse than the proposed method. The proposed method is therefore overall more robust to noise than the other methods. We also note that for our method the performance drop from low to high noise is influenced by the relative s used since different s are used for the high and low magnitudes (chosen using cross-validation).</p><p>• Heterogeneity: the simple extension (SVM Mix) shows promising results. For the random design, HB is better only in the case of low magnitude and high heterogeneity, while in all other cases HB and SVM Mix perform similarly. This, coupled with the fact that the proposed method is computationally efficient while HB is not (Sawtooth Software), indicates the potential of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Estimation of Nonlinear Models</head><p>The next set of experiments considers the case where the true underlying utility function of each individual (respondent) is nonlinear. In order to account for the nonlinear effect, we estimate nonlinear models as described in §2.4.2. and in the online appendix. A typical nonlinear effect in consumer preferences is simple interaction between two different product attributes (i.e., price and brand). In our case, adding interactions among all product attribute levels (all 16 dimensions) would lead to a large number of parameters to estimate (15 * 16/2 = 120) which would be computationally intractable for HB. Therefore we only added the interactions between the first two attributes. Since each attribute has 4 levels we added an extra 4 × 4 = 16 dimensions capturing all interactions among the 4 levels of the first attribute and the 4 levels of the second one. Thus, the utility function of each individual consisted of the original 16 parameters generated as before, plus 16 new parameters capturing the interactions among the levels of the first two attributes (clearly, without loss of generality, other choices could be made).</p><p>These new 16 parameters were generated from a Gaussian with mean 0 and standard deviation nl . The size of nl controls the size of the interaction parameters of the underlying utility functions. We assume that we do not know the sign of the interaction coefficients other than for the method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>. The method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> requires prior knowledge of the least-desired level of each feature, so we incorporated this information (in the form of positivity constraints for the interaction coefficients) for the analytic center, effectively giving that method an advantage relative to the other ones. In practice we may not know the sign of the interaction coefficients, so we did not add this information to the other three methods.</p><p>Our objective is to experiment with two different levels of nonlinearity: low nonlinearity and high nonlinearity. Our definition of "level of nonlinearity" is given below in the form of a short sequence of computations. For a given nl :</p><p>(1). Draw a random population of 1,000 utility functions (1,000 individuals);</p><p>(2). Generate the set of all 4 × 4 × 4 × 4 = 256 possible 4-attribute products;</p><p>(3). For each individual and each product, compute the absolute values of the nonlinear and linear parts of the utility of the product separately;</p><p>(4). For each individual, add all 256 absolute values of the nonlinear parts and the linear parts separately, and take the ratio between the sum-absolutenonlinear and the sum-absolute-linear;</p><p>(5). Compute the average of this ratio over the 1,000 individuals.</p><p>We use the average ratio computed in the last step as a characterization of the relative size of the underlying nonlinear (interaction) effect in the simulated population. In the sequel, we present experiments for the cases where the average-ratio is 25% (low nonlinearity) and 75% (high nonlinearity). In other words, over all possible products the average nonlinear part of the utility is about 25% (low nonlinearity) or 75% (high nonlinearity) of the linear part. The values of nl that result in the specified levels of nonlinearity are:</p><p>• for low magnitude and high heterogeneity: 0.61 and 1.84 (low and high nonlinearity, respectively);</p><p>• for high magnitude and low heterogeneity: 1.26 and 3.80 (low and high nonlinearity, respectively).</p><p>Marketing Science 24(3), pp. <ref type="bibr">415-429, © 2005 INFORMS</ref> To estimate the nonlinear utilities using logistic regression and HB we represented the data using 32 dimensional vectors (16 linear plus 16 nonlinear). For the method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> we followed the suggestion in <ref type="bibr" target="#b55">Toubia et al. (2003)</ref>: we introduced an additional feature with 16 levels corresponding to the 16 nonlinear interaction parameters. Therefore the three methods (other than SVM which as we discussed always estimates the n dual parameters i ) estimated 32 parameters for each individual. Notice that HB can hardly handle even this low dimensional nonlinear case (see, for example, the Sawtooth Software website), which in contrast is a computationally mild case for the polyhedral method, SVM, and logistic regression. For computational reasons (for HB) and to avoid cluttering, we only did experiments in two cases:</p><p>• high magnitude and low heterogeneity-the "easiest" case in practice;</p><p>• low magnitude and high heterogeneity-the "hardest" case in practice, and also the case where our method has the least advantage relative to HB as shown in Table <ref type="table" target="#tab_2">1</ref>.</p><p>For computational reasons we also simulated 100 individuals only once (instead of 5 times in the linear experiments case) for these experiments.</p><p>In Table <ref type="table" target="#tab_3">2</ref> we compare only SVM Mix and HB, since the conclusions about the comparison of the polyhedral method, SVM, and logistic regression are similar as for the linear utility experiments. We show the performances of the logistic, SVM, and polyhedral methods in Appendix B. Although the actual utilities are nonlinear, we also estimated linear models to see if it is even worth estimating nonlinear models to begin with. To compare the linear and nonlinear models we use hit rates: the percentage of correct prediction of 100 out-of-sample choices. In Appendix B we report other RMSE errors. In Table <ref type="table" target="#tab_3">2</ref> we also report the RMSE of the nonlinear parts of the utility functions, which captures the accuracy with which the 16 interaction coefficients are estimated. Therefore in Table <ref type="table" target="#tab_3">2</ref> we show the hit rates of linear SVM Mix with the Notes. Hit rates and the RMSE of the estimated interaction coefficients in parenthesis are reported. Bold indicates best or not significantly different than best at p &lt; 0 05 across all columns.</p><p>mixture parameter estimated as in the linear experiments; nonlinear SVM Mix where now we estimated two mixture parameters l and nl for the linear and nonlinear parts of the estimated utility using again the method outlined in the linear experiments; linear HB; and nonlinear HB. In parenthesis, for the nonlinear models, we report the RMSE of the interaction coefficients (the 16 coefficients for the nonlinear part of the utility function).</p><p>The results show the following:</p><p>• When the nonlinearity is low the linear models are generally better than the nonlinear ones.</p><p>• When nonlinearity is high, it is generally better to estimate nonlinear models both for HB and for SVM Mix.</p><p>• The best (among linear and nonlinear) HB outperforms the best (among linear and nonlinear) SVM Mix in the cases it outperformed it in the linear experiments (Table <ref type="table" target="#tab_2">1</ref>). However, the relative differences of the hit rates decrease as the amount of nonlinearity increases. For example, in the high nonlinearity case the nonlinear SVM Mix is similar to HB in three out of the four cases (Low-High or High-Low for random and orthogonal), while in Table <ref type="table" target="#tab_2">1</ref>, SVM Mix is similar to HB only in one out of the four cases. This indicates that the proposed approach has a relative advantage when there are nonlinearities.</p><p>• When we estimate nonlinear models, the RMSE of the nonlinear part of the estimated function is smaller for SVM Mix than for HB. In other words, the proposed method captures the nonlinear interactions better than HB. In Appendix B we show that a simple SVM (not "Mix") is also on average better than any other method in terms of capturing the nonlinear effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>Preference modeling has been a central problem in the marketing community and is becoming increasingly important in other business areas such as in supply chain and procurement where the procurement processes are automated and data describing past choices are captured. At the same time, the "democratization" of data, in the sense that data is captured everywhere and under any conditions, implies that companies often need to use preference modeling tools that do not assume the data is generated in a controlled environment, i.e., through questionnaires. As the conditions under which preference data are captured vary, and as more and more applications arise, there is an increasing need for new tools and approaches to the problem of preference modeling that are computationally efficient, have high accuracy, and can handle noise and high (multiattribute products) dimensional data. The work presented here aims at opening a direction of research in the area of preference modeling that can lead to such new approaches and tools. We did not discuss here issues such as how to use the proposed framework, for example, for designing questionnaires: We believe this is possible, as we briefly discussed and as is indicated by the work of <ref type="bibr" target="#b55">Toubia et al. (2003)</ref>, and we leave this for future research. Instead we focused on laying the foundations for methods and tools to solve a variety of preference modeling problems.</p><p>In this paper we presented a framework for developing computationally efficient preference models that have high accuracy and can handle noisy and large dimensional data. The framework is based on the well-founded field of statistical learning theory <ref type="bibr" target="#b58">(Vapnik 1998)</ref>. Highly nonlinear conjoint estimation models can also be computationally efficiently estimated. The models estimated depend only on a few data points, the ones that correspond to "hard choices." This can provide useful insights to managers by focusing their attention only on those choices. Moreover, this characteristic can be used to design individual specific questionnaires along the lines of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>.</p><p>The experiments showed that:</p><p>• The proposed approach significantly outperforms both the method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> and standard logistic regression, the latter when there is noise and for the random design. It is never worse than the best among these three methods.</p><p>• The proposed approach is less sensitive to noise-high response error-than both logistic regression and the method of <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>. It is therefore more robust to noise.</p><p>• The proposed approach is relatively weaker when data from an orthogonal design are used. This limitation indicates that it may be important to combine the proposed method with a method similar in spirit for designing questionnaires. As shown by <ref type="bibr" target="#b54">Toubia et al. (2004)</ref>, such an extension to questionnaire design can lead to significant improvements. We leave this as part of future work.</p><p>• A simple method for handling heterogeneity lead to promising results with performance often similar to that of HB;</p><p>• When the true underlying utility function is nonlinear (for example there are interaction effects between the product attributes) it is better to estimate nonlinear models when the nonlinearity is high. Moreover the proposed method estimates the interaction coefficients significantly better than all other methods.</p><p>Estimation is also computationally efficient, so, for example, large datasets for products with large numbers of attributes can also be used, unlike the case of HB.</p><p>A number of extensions are possible within this framework for preference modeling. A clear direction for future work is to incorporate to the individualspecific models cross-respondent information in the case of heterogeneity. The experiments show that even a simple ad-hoc method for handling heterogeneity is already promising. Another important direction is to develop other preference modeling methods using the principles of Statistical Learning Theory: in particular, there is evidence (see for example Rifkin 2002) that the most important part of the proposed approach is the incorporation of the complexity control in the estimation process. It may be the case that logistic regression with complexity control, for example along the lines of <ref type="bibr" target="#b62">Zhu and Hastie (2001)</ref>, is a more appropriate approach than the one we tested here, since it may better capture the noise model of the data typically assumed in conjoint analysis.</p><p>The machinery developed for SVM as well as statistical learning theory can be used for solving problems in the field of conjoint analysis in new ways. For example, one can extend the use of virtual examples we used here (discussed in Appendix A) for adding positivity constraints on the utility function. Empirical evidence shows that if the original data used to estimate a model are extended to include virtual examples, then the performance of the estimated models improves <ref type="bibr" target="#b46">(Scholkopf et al. 1996)</ref>. Generally, virtual examples are data that are either added to the estimation data by the user because of prior knowledge about them, or are generated from the existing data using transformations that the user knows a priori do not alter their key characteristic (i.e., which product is the preferred one) <ref type="bibr" target="#b46">(Scholkopf et al. 1996)</ref>. Furthermore, models for metric-based conjoint analysis <ref type="bibr" target="#b55">(Toubia et al. 2003)</ref> can also be developed within the framework in this paper, for example in the spirit of SVM regression instead of classification <ref type="bibr" target="#b58">(Vapnik 1998)</ref>. Finally, another direction of research is to develop active learning <ref type="bibr" target="#b54">(Tong and Koller 2000)</ref> type methods for the problem of adaptively designing questionnaires, as for example in <ref type="bibr" target="#b55">Toubia et al. (2003)</ref>.</p><p>Marketing Science 24(3), pp. <ref type="bibr">415-429, © 2005 INFORMS</ref> Once the problem of preference modeling is seen within the framework of statistical learning theory and SVM, a number of new methods can be developed for the conjoint analysis field. The work in this paper does not aim, by any means, to replace existing methods of preference modeling, but instead to contribute to the field new tools and frameworks that can be complementary to existing ones for solving preference modeling problems. Finally, the experiments presented here are by no means exhaustive: more experiments by other researchers will be needed to establish the relative strengths and weaknesses of the proposed approach, as is always the case with any newly developed method. 1 0 0 0 1 0 0 0 0 1 These difference vectors correspond to pairs of products that have all attributes the same apart from one: the product with a higher value (by 1) for the one different attribute is preferred. Formally this modifies problem (3) as follows: min</p><formula xml:id="formula_10">w 1 w m i n i=1 i + m f =1 f + f =1 m w 2 f</formula><p>subject to:</p><formula xml:id="formula_11">w 1 • x 1 i 1 + w 2 • x 1 i 2 + • • • + w m • x 1 i m ≥ w 1 • x 2 i 1 + w 2 • x 2 i 2 + • • • + w m • x 2 i m + 1 − i for ∀ i ∈ 1 n w f ≥ 1 − f ∀ f = 1 m f ≥ 0 i ≥ 0 (7)</formula><p>Notice that m constraints of the form w f ≥ 1 − f , m new slack variables f , and m constraints f ≥ 0 have been added. The slack variables f push the optimal w f to be positive. Notice that we can further tune the proposed method by putting a different weight C on the f in the cost function so that we can have w f being more or less pushed towards positivity. For example, if the cost function is min</p><formula xml:id="formula_12">w 1 w m i n i=1 i + C m f =1 f + f =1 m w 2 f (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>for a very large C, then all w f will become positive (if there is a positive feasible solution for w f ). This way one can control the requirement that w f be positive. In the experiments we have used the simple method where C = 1 so equal weight is put on all slack variables. Parameter C can in practice also be tuned using cross-validation or a validation set. In the nonlinear case-using kernels-the use of the virtual examples will not force only the linear effects of attribute f to be positive, but the overall effects of this attribute to be positive. For example, for a polynomial kernel of degree 2, the virtual examples will force w f + w 2 f ≥ 1 − f . So in the general nonlinear case the virtual examples as used here will imply that for two products "all else being equal, more of a particular attribute by 1 is better," and this requirement can still be relaxed/controlled by the use of C for the slack variables f .</p><p>The experiments were designed similar to these in <ref type="bibr" target="#b54">Toubia et al. (2004)</ref> where the positivity of the underlying utility function is used to capture the assumption that we know for each product attribute which level has the lowest partworth. One can remove that level and assume that all other partworths are positive. If, instead, the products are represented as binary vectors with each attribute corresponding to a number of dimensions equal to the number of levels for that attribute with a 1 at the location of the present level and a 0 elsewhere-often used in practice <ref type="bibr">Huber 2001, Toubia et al. 2004</ref>) and also in our experiments-then the virtual example corresponding to the prior knowledge that the partworth of a level is the smallest one would be, for example, of the form 1 0 0 −1 0 0 0 0 0 0 0 0 0 0 0 0 in the case of 4 attributes with 4 levels each for which we know that for the first attribute the fourth level has the smallest partworth-smaller than the first level in this case. This is the representation we used in the experiments for our method and for logistic regression. Finally we note that one can add other types of prior knowledge to constrain the estimation of the utility function through the use of virtual examples <ref type="bibr" target="#b46">(Scholkopf et al. 1996)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1Each of the Constraints (1) Defined by a Comparison is a Hyperplane in the Space of Parameters w f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Dual Parameters and Hard Choices. It turns out that as in the case of SVM (see Technical Appendix at http://mktsci.pubs.informs.org) the utility function estimated through (3) can be written in the form:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Evgeniou, Boussios, and Zacharia: Generalized Robust Conjoint Estimation    </figDesc><table><row><cell>Marketing Science 24(3), pp. 415-429, © 2005 INFORMS</cell><cell>417</cell></row><row><cell>regression, HB, and the estimation method of</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Comparison of Methods Using RMSE and Hit Rates in Parenthesis</figDesc><table><row><cell>Mag</cell><cell>Het</cell><cell>Design</cell><cell>Analytic</cell><cell>SVM</cell><cell>Logistic</cell><cell>HB</cell><cell>SVM Mix</cell></row><row><cell>L</cell><cell>H</cell><cell>Random</cell><cell>0 92</cell><cell>0 69</cell><cell>0 77</cell><cell>0 60  *</cell><cell>0 64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>79 1%</cell><cell>81 8%</cell><cell>81 1%</cell><cell>84 5%</cell><cell>83 1%</cell></row><row><cell>L</cell><cell>H</cell><cell>Orthogonal</cell><cell>0 75</cell><cell>0 66</cell><cell>0 67</cell><cell>0 56  *</cell><cell>0 61</cell></row><row><cell></cell><cell></cell><cell></cell><cell>81 2%</cell><cell>82 7%</cell><cell>82 7%</cell><cell>85 5%</cell><cell>83 9%</cell></row><row><cell>L</cell><cell>L</cell><cell>Random</cell><cell>1 15</cell><cell>0 86</cell><cell>1 00</cell><cell>0 66  *</cell><cell>0 69  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell>74 5%</cell><cell>77 4%</cell><cell>75 7%</cell><cell>82 6%</cell><cell>81 7%</cell></row><row><cell>L</cell><cell>L</cell><cell>Orthogonal</cell><cell>0 89</cell><cell>0 81</cell><cell>0 83</cell><cell>0 62  *</cell><cell>0 67</cell></row><row><cell></cell><cell></cell><cell></cell><cell>76 9%</cell><cell>78 6%</cell><cell>78 3%</cell><cell>83 8%</cell><cell>82 3%</cell></row><row><cell>H</cell><cell>H</cell><cell>Random</cell><cell>0 67</cell><cell>0 53</cell><cell>0 52</cell><cell>0 46  *</cell><cell>0 48  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell>84 0%</cell><cell>85 9%</cell><cell>86 9%</cell><cell>88 2%</cell><cell>87 2%</cell></row><row><cell>H</cell><cell>H</cell><cell>Orthogonal</cell><cell>0 81</cell><cell>0 61</cell><cell>0 59</cell><cell>0 49  *</cell><cell>0 51  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell>80 4%</cell><cell>84 1%</cell><cell>84 9%</cell><cell>87 3%</cell><cell>86 3%</cell></row><row><cell>H</cell><cell>L</cell><cell>Random</cell><cell>0 65</cell><cell>0 52</cell><cell>0 53</cell><cell>0 35  *</cell><cell>0 37  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell>83 3%</cell><cell>86 0%</cell><cell>85 8%</cell><cell>90 3%</cell><cell>89 5%</cell></row><row><cell>H</cell><cell>L</cell><cell>Orthogonal</cell><cell>0 81</cell><cell>0 68</cell><cell>0 65</cell><cell>0 34  *</cell><cell>0 53</cell></row><row><cell></cell><cell></cell><cell></cell><cell>79 2%</cell><cell>81 3%</cell><cell>82 8%</cell><cell>90 6%</cell><cell>85 8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>The True Utilities are Nonlinear</figDesc><table><row><cell>Mag</cell><cell>Het</cell><cell>NL</cell><cell>Des</cell><cell>SVM Mix Lin</cell><cell>SVM Mix NL</cell><cell>HB Lin</cell><cell>HB NL</cell></row><row><cell>L</cell><cell>H</cell><cell>L</cell><cell>Rand</cell><cell>81.6%</cell><cell>81.1% (1.43)</cell><cell>82.7%</cell><cell>81.5% (1.56)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Orth</cell><cell>81.7%</cell><cell>81.0% (1.49)</cell><cell>82.7%</cell><cell>80.7% (1.61)</cell></row><row><cell>L</cell><cell>H</cell><cell>H</cell><cell>Rand</cell><cell>75.3%</cell><cell>78.1% (1.15)</cell><cell>76.2%</cell><cell>78.6% (1.33)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Orth</cell><cell>75.2%</cell><cell>76.6% (1.30)</cell><cell>75.4%</cell><cell>76.1% (1.50)</cell></row><row><cell>H</cell><cell>L</cell><cell>L</cell><cell>Rand</cell><cell>87.9%</cell><cell>87.6% (1.48)</cell><cell>88.2%</cell><cell>88.4% (1.57)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Orth</cell><cell>85.5%</cell><cell>84.1% (1.48)</cell><cell>89.0%</cell><cell>88.3% (1.61)</cell></row><row><cell>H</cell><cell>L</cell><cell>H</cell><cell>Rand</cell><cell>78.6%</cell><cell>82.6% (1.14)</cell><cell>79.8%</cell><cell>83.2% (1.31)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Orth</cell><cell>77.5%</cell><cell>78.6% (1.27)</cell><cell>79.8%</cell><cell>81.1% (1.41)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank John Hauser, Duncan Simester, and Olivier Toubia for all the support and useful conversations during this work and also thank Olivier Toubia for making available the software programs used for the experiments with the method proposed by <ref type="bibr" target="#b55">Toubia et al. (2003)</ref> and with hierarchical Bayes.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Adding Positivity Constraints</head><p>The coefficients of a utility function, w f , in problem (3), are sometimes assumed to be positive; if not, the values of the corresponding attributes can often be negative and have the corresponding coefficients be positive <ref type="bibr" target="#b55">(Toubia et al. 2003)</ref>. Therefore in practice it is often (but not always) important to add such constraints to the estimation of the utility function. To do so, the estimation method (for example in the simple linear case) should be modified by adding to (3) the extra constraints:</p><p>However, such a modification makes the generalization of the method to the nonlinear case using kernels impossible <ref type="bibr" target="#b58">(Vapnik 1998</ref>). It is therefore not possible to add such constraints directly and still be able to efficiently estimate highly nonlinear models <ref type="bibr" target="#b58">(Vapnik 1998)</ref>.</p><p>To avoid this problem, we use virtual examples <ref type="bibr" target="#b40">(Niyogi et al. 1998</ref><ref type="bibr" target="#b46">, Scholkopf et al. 1996</ref>. In particular, the positivity of the m parameters w f is incorporated in the models by adding m (virtual) example difference vectors</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Nonlinear Experiments: Detailed Results</head><p>We show all the results of the nonlinear experiments in Table <ref type="table">3</ref>. In each cell we report five performances: (a) the  From the results we observe the following:</p><p>• Nonlinear part estimation: The key result is that SVM Mix estimates the nonlinear parts of the utility functions better than all other methods, including HB. SVM, without combining information across individuals, is also better than both the logistic regression and HB. It should be noted that all methods have large RMSE as compared to the linear estimations. We attribute this to the fact that each 32-dimensional vector describing a product includes just a single nonzero element out of the total 16 nonlinear elements (since only one of the four levels of the two attributes involved for the nonlinearity is nonzero for each product). In contrast, there are 4 nonzero elements out of the 16 linear ones. Effectively, there is little information about the nonlinear part of the utility functions.</p><p>• Linear part estimation: For the linear parts of the estimated utility function the comparison of SVM, logistic, and polyhedral is qualitatively similar as in the linear experiments (Table <ref type="table">1</ref>).</p><p>• Linear part estimation comparison with HB: The difference between HB and "SVM Mix" for the linear parts of the utility function is relatively smaller than in the linear utility experiments (Table <ref type="table">1</ref>). Our method is therefore less influenced by nonlinearities than HB.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marketing models of consumer heterogeneity</title>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econometrics</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="57" to="78" />
			<date type="published" when="1999-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the heterogeneity of demand</title>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="384" to="389" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scale-sensitive dimensions, uniform convergence, and learnability. 34th IEEE Sympos</title>
		<author>
			<persName><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations Comput. Sci</title>
		<imprint>
			<date type="published" when="1993-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical Bayes versus finite mixture conjoint analysis models: a comparison of fit, prediction, and partworth recovery</title>
		<author>
			<persName><forename type="first">Rick</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Currim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving parameter estimates and model prediction by aggregate customization in choice experiments</title>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="273" to="283" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hierarchical Bayes model of primary and secondary demand</title>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discrete choice models with latent choice sets</title>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Boccara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Res. Marketing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="9" to="24" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Discrete Choice Analysis: Theory and Application to Travel Demand</title>
		<author>
			<persName><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">R</forename><surname>Moshe</surname></persName>
		</author>
		<author>
			<persName><surname>Lerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling methods for discrete choice analysis</title>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcfadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Bockenholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Bolduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatra</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vithala</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Revelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Steinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Duality and geometry in SVM classifiers. Pat Langley</title>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Bredensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventeenth Internat. Conf. Machine Learning</title>
				<meeting>Seventeenth Internat. Conf. Machine Learning<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MARK: a boosting algorithm for heterogeneous kernel models</title>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michinari</forename><surname>Momma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Embrechts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGKDD Internat. Conf. Knowledge Discovery and Data Mining</title>
				<meeting>SIGKDD Internat. Conf. Knowledge Discovery and Data Mining<address><addrLine>Edmonton, Alberta, Canada; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introduction to Linear Optimization</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tsitsikilis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multinomial probit formulation for large choice sets</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Bolduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Ben-Akiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Sixth IATBR Conf</title>
				<meeting>Sixth IATBR Conf<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robustness of conjoint analysis: some Monte Carlo results</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Carmone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="300" to="303" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Psychometric methods in marketing research: Part I, Conjoint analysis</title>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="385" to="391" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Web mining: Information and pattern discovery on the World Wide Web</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mobasher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m">Proc. 9th IEEE Internat. Conf. Tools with Artificial Intelligence (ICTAI&apos;97)</title>
				<meeting>9th IEEE Internat. Conf. Tools with Artificial Intelligence (ICTAI&apos;97)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prediction in marketing using the support vector machines</title>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Curry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci. Forthcoming</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representing heterogeneity in consumer response models</title>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Desarbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Ansari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="348" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Probabilistic Theory of Pattern Recognition</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laszlo</forename><surname>Györfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Applications of Mathematics</title>
		<imprint>
			<biblScope unit="issue">31</biblScope>
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularization networks and support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical learning theory: a primer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="13" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image representations and feature selection for multimedia database search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge Data Engrg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="911" to="920" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of online learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. System Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting</title>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="407" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regularization theory and neural networks architectures</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="219" to="269" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conjoint analysis in consumer research: issues and outlook</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Consumer Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="123" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conjoint analysis in marketing: new developments with implications for research and practice</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large margin rank boundaries for ordinal regression</title>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
				<editor>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="29" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Measuring heterogeneous reservation prices for product bundles</title>
		<author>
			<persName><surname>Jedidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Jagpal</surname></persName>
		</author>
		<author>
			<persName><surname>Manchanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="130" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient distribution-free learning of probabilistic concepts</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Systems Sci</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="464" to="497" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining E-commerce data: the good, the bad, and the ugly</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh ACM SIGKDD Internat. Conf. Knowledge Discovery Data Mining</title>
				<meeting>Seventh ACM SIGKDD Internat. Conf. Knowledge Discovery Data Mining<address><addrLine>San Francisco, California; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient experimental design with marketing research applications</title>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">F</forename><surname>Kuhfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName><surname>Garratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hierarchical Bayes conjoint analysis: recovery of partworth heterogeneity from reduced experimental designs. Marketing Sci</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Desarbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="173" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">J</forename><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Hensher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joffre</surname></persName>
		</author>
		<author>
			<persName><surname>Swait</surname></persName>
		</author>
		<title level="m">Stated Choice Methods: Analysis and Applications</title>
				<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The structure of random utility models</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Manski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Decision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="254" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Conditional logit analysis of qualitative choice behavior. Paul Zarembka, ed. Frontiers in Econometrics</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcfadden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="105" to="142" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The choice theory approach to marketing research</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcfadden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="297" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Incorporating prior information in machine learning by creating virtual examples</title>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Proc. Intelligent Signal Processing</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2196" to="2209" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling hierarchical conjoint processes with integrated choice experiments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Oppewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Timmermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How dynamic consumer response, competitor response, company support, and company inertia shape longterm marketing effectiveness</title>
		<author>
			<persName><forename type="first">Koen</forename><surname>Pauwels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="596" to="610" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Properties of support vector machines</title>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="955" to="974" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Everything old is new again: a fresh look at historical approaches in machine learning</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rifkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><surname>Sawtooth Software</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<author>
			<persName><surname>Hb-Reg</surname></persName>
		</author>
		<ptr target="http://www.sawtoothsoftware.com/hbreg.shtml" />
		<title level="m">Hierarchical Bayes regression</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Incorporating invariances in support vector learning machines. C. von der Malsburg</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks, ICANN&apos;96</title>
		<title level="s">Lecture Notes in Comput. Sci.</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Seelen</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Vorbrüggen</surname></persName>
			<persName><forename type="first">B</forename><surname>Sendhoff</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1112</biblScope>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reliability of conjoint analysis: contrasting data collection procedures</title>
		<author>
			<persName><forename type="first">Madhav</forename><forename type="middle">N</forename><surname>Segal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="139" to="143" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A strict paired comparison linear programming approach to nonmetric conjoint analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boussios</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zacharia</forename></persName>
		</author>
		<idno>INFORMS 429</idno>
	</analytic>
	<monogr>
		<title level="j">Generalized Robust Conjoint Estimation Marketing Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Zionts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res. Methods, Models and Applications. Quorum Books</title>
		<imprint>
			<biblScope unit="page" from="97" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Linear programming techniques for multidimensional analysis of preferences</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><surname>Shocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrica</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="369" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving the predictive power of conjoint analysis by constrained parameter estimation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naresh</forename><surname>Malhotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="433" to="438" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Additive regression and other nonparametric models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="689" to="705" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Solutions of Ill-Posed Problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Arsenin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>W. H. Winston</publisher>
			<pubPlace>Washington, D.C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><forename type="middle">I</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><surname>Simester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventeenth Internat. Conf. Machine Learning</title>
				<meeting>Seventeenth Internat. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="41" to="116" />
		</imprint>
	</monogr>
	<note>Polyhedral methods for adaptive choice-based conjoint analysis</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast polyhedral adaptive conjoint estimation</title>
		<author>
			<persName><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><forename type="middle">I</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ely</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><surname>Dahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="303" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Judgment under uncertainty: heuristics and biases</title>
		<author>
			<persName><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Product Design and Development</title>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">T</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><surname>Eppinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>McGraw-Hill, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequences of events to their probabilities</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Probab. Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Splines Models for Observational Data</title>
		<author>
			<persName><forename type="first">Grace</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Series in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Commercial use of conjoint analysis: an update</title>
		<author>
			<persName><forename type="first">Dick</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Cattin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="91" to="96" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Kernel logistic regression and the import vector machine</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS2001</title>
				<meeting>NIPS2001<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
