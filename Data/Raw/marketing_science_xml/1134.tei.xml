<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Reputation Management: Estimating the Impact of Management Responses on Consumer Reviews</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08-18">August 18, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Davide</forename><surname>Proserpio</surname></persName>
							<email>proserpi@marshall.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Marshall School of Business</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georgios</forename><surname>Zervas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Questrom School of Business</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Reputation Management: Estimating the Impact of Management Responses on Consumer Reviews</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2017-08-18">August 18, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2017.1043</idno>
					<note type="submission">Received: September 29, 2015 Revised: April 13, 2016; August 16, 2016 Accepted: September 19, 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-13T12:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>online reviews • reputation management</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the relationship between a firm's use of management responses and its online reputation. We focus on the hotel industry and present several findings. First, hotels are likely to start responding following a negative shock to their ratings. Second, hotels respond to positive, negative, and neutral reviews at roughly the same rate. Third, by exploiting variation in the rate with which hotels respond on different review platforms and variation in the likelihood with which consumers are exposed to management responses, we find a 0.12-star increase in ratings and a 12% increase in review volume for responding hotels. Interestingly, when hotels start responding, they receive fewer but longer negative reviews. To explain this finding, we argue that unsatisfied consumers become less likely to leave short indefensible reviews when hotels are likely to scrutinize them. Our results highlight an interesting trade-off for managers considering responding: fewer negative ratings at the cost of longer and more detailed negative feedback.</p><p>History: K. Sudhir served as the editor-in-chief and Duncan Simester served as associate editor for this article.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>User-generated online reviews have been continuously gaining credibility in the eyes of consumers, and today they are an essential component of the consumer decision-making process <ref type="bibr">Mayzlin 2006, Luca 2011)</ref>. With the popularity and reach of online review platforms growing rapidly, firms are under increasing pressure to maintain a flawless online reputation. While investing in improved products and services can result in better ratings, inevitably firms experience failures that lead to negative reviews. Dealing with negative reviews is challenging because, unlike offline word of mouth, they persist online and firms can neither selectively delete them, nor opt out from being reviewed altogether. To manage unfavorable reviews, firms often resort to questionable practices like review fraud <ref type="bibr">(Mayzlin et al. 2014, Luca and</ref><ref type="bibr" target="#b18">Zervas 2016)</ref>, soliciting positive reviews in exchange for perks, threatening legal action against negative reviewers, and using nondisparagement clauses in sales contracts that stipulate fines if consumers write negative reviews. At the same time, technological advances in detecting fake reviews, enforcement of false advertising regulations against those who commit review fraud, and emerging legislation aiming to protect consumer free speech online have created an environment where these activities carry significant legal and financial risk for dubious reward.</p><p>In this climate, the practice of publicly responding to consumer reviews has emerged as an alternative reputation management strategy that is legal, endorsed by review platforms, and widely adopted by managers. A management response is an open-ended piece of text that is permanently displayed beneath the review it addresses. Unlike the review itself, the response does not carry a rating, and it does not affect the responding firm's average rating. While review platforms ensure that responses meet basic standards (such as avoiding offensive language), they allow any firm to respond to any reviewer. Most major review platforms, including TripAdvisor and Yelp, allow firms to respond. Yet, despite management responses now being commonplace, their efficacy in recovering a firm's reputation remains an open question.</p><p>In this paper, we estimate the impact of management responses on TripAdvisor hotel ratings. We show that, on average, responding hotels see a consistent increase of 0.12 stars in their ratings after they start using management responses. While this gain appears modest when evaluated against the usual 5-star scale, in practice, most ratings are concentrated to a narrower range. The standard deviation of hotel ratings in our data is 0.8 stars. Furthermore, because TripAdvisor and other Proserpio and Zervas: Online Reputation Management</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>646</head><p>Marketing <ref type="bibr">Science, 2017</ref><ref type="bibr">, vol. 36, no. 5, pp. 645-665, © 2017</ref> review platforms round average ratings to the nearest half star, small changes can have a material impact. For example, if a 4.24-star hotel can cross the 4.25-star threshold, it will see its rating jump by half a star. In our data, 27% of responding hotels increased their rounded ratings by at least half a star within six months of their first management response.</p><p>Several selection issues need to be considered before ascribing a causal interpretation to our results. First, hotels select into treatment, i.e., responding to reviews. Second, hotels choose which reviews to respond to and how to respond to them. If unaccounted for, these nonrandom choices can bias estimation of an average treatment effect (ATE). For instance, our estimate could be biased upward if we do not account for the possibility that hotels that are "better" at responding are also more likely to respond. Controlling for these choices is difficult outside of an experimental context.</p><p>Thus, instead of estimating an ATE, our goal is to consistently estimate an average treatment effect on the treated (ATT). The ATT can be consistently estimated when treatment assignment is nonrandom, and in particular when there is a correlation between treatment and potential outcomes, e.g., if hotels decide to respond based on an expectation that responding will increase their ratings. The ATT measures the impact of management responses conditional on the hotels that self-selected into treatment, the reviews they decided to respond to, and the manner in which they responded. The ATT will be biased if a hotel's decision to respond is driven by unobserved factors that also affect the hotel's ratings. For instance, a hotel's decision to respond may be prompted by (unobserved by us) service improvements and renovations Notes. The "Pre" and "Post" data sets respectively indicate reviews submitted prior to and following each hotel's first management response. All effect sizes reported are corrected for Ashenfelter's dip and are statistically significant at least at the 5% level.</p><p>that the hotel made to avoid further negative reviews. <ref type="bibr">1</ref> Therefore, increased ratings following a management response can simply reflect an effort by hotel management to fix the problem that was causing the negative reviews in the first place, rather than any direct impact of the management responses themselves. We approach this identification challenge in various ways requiring different assumptions from the data. Table <ref type="table" target="#tab_0">1</ref> summarizes our identification strategies and robustness checks, which we describe in detail next.</p><p>Our first identification strategy uses Expedia ratings to control for changes in hotel quality. This approach is motivated by a difference in managerial practice between TripAdvisor and Expedia: while hotels frequently respond to TripAdvisor reviews, they almost never do so on Expedia. We build on this observation to estimate an ATT using a difference-in-differences (DD) identification strategy. Intuitively, the DD estimator compares changes in the TripAdvisor ratings of any given hotel following its decision to begin responding against a baseline of changes in the same hotel's Expedia ratings over the same period of time. The key assumption needed for the DD estimate to be consistent is that differences between TripAdvisor and Expedia ratings would have been constant in the absence of treatment. To defend this assumption, we need to understand why hotels respond on one platform but not the other.</p><p>Is the choice to only respond on TripAdvisor exogenously determined, or is it driven by changes in hotel quality? One explanation for solely responding on TripAdvisor that is compatible with our identification assumptions is that reviews are less salient on Expedia. Unlike TripAdvisor, which is in the business of collecting and disseminating reviews, Expedia is an online travel agency <ref type="bibr" target="#b19">(Mayzlin et al. 2014</ref> make the same point). Comparing how the two sites present information highlights this distinction: while TripAdvisor prominently displays a hotel's reviews, Expedia displays a booking form, prices for various room types, and the hotel's average rating-individual reviews and responses are only available on a secondary page. In addition to being displayed less prominently, Expedia reviews are much shorter, and they arrive at nearly twice the rate they do on TripAdvisor. Therefore, hotels may be less inclined to respond to them because they are less substantive and are quickly superseded by fresher information. Another motivation for hotels to respond more frequently on TripAdvisor is that, unlike Expedia, TripAdvisor allows nonverified hotel guests to submits reviews. Therefore, hotels may be more likely to closely monitor TripAdvisor and respond to negative reviews they perceive as unfair or fake.</p><p>Cross-platform DD estimation will be biased if hotels take other actions that affect their TripAdvisor ratings relative to Expedia at the same time they start responding. For instance, if hotels make renovations specifically valued by TripAdvisor users, which they then announce by responding to TripAdvisor reviews, the ATT we estimate will be likely biased upward. We perform several robustness checks to show that our results are unlikely to be driven by TripAdvisorspecific improvements. First, we show that for a long period preceding each hotel's first management response, TripAdvisor and Expedia ratings moved in parallel. Therefore, at least prior to treatment, ratings on the two review platforms are consistent with Trip-Advisor and Expedia users valuing changes in hotel quality equally. Second, we show that management responses on TripAdvisor had no impact on the same hotel's Expedia ratings. Therefore, for our estimate to be biased it would have to be the case that Expedia users have no value whatsoever for hotel improvements targeted at TripAdvisor users. Third, consider the possibility that hotels make TripAdvisor-specific improvements by targeting a traveler segment that is overrepresented on TripAdvisor compared to Expedia. For example, if business travelers strongly prefer Trip-Advisor and hotels make improvements specifically valued by business travelers, TripAdvisor ratings will rise relative to Expedia. We argue that this is unlikely to be the case because our results hold even when we compare TripAdvisor and Expedia travelers belonging to the same segments. Fourth, we show that the impact of management responses is larger for reviewers that are more likely to have read them. A reviewer's propensity to read management responses is outside a hotel's control, and is therefore unlikely to be correlated with unobserved actions the hotel took to improve its ratings.</p><p>A related concern arises if hotels simultaneously adopt multiple reputation management strategies. For instance, some hotels may start posting fake reviews at the same time they start responding <ref type="bibr">(Mayzlin et al. 2014, Luca and</ref><ref type="bibr" target="#b18">Zervas 2016)</ref>. This is particularly problematic in our setting because posting fake reviews is easier on TripAdvisor than it is on Expedia. To ensure that the ATT we estimate is not driven by review fraud, we show that our results hold for hotels that are unlikely to commit review fraud in the first place.</p><p>To avoid bias due to cross-platform differences, we develop a second identification strategy that relies only on TripAdvisor ratings. The basic idea behind this strategy is that any difference in the ratings of two guests who stayed at the same hotel at the same time is unlikely to be due to unobserved hotel improvements. Thus, we estimate the impact of management responses by comparing the ratings of guests who left a review before a hotel began responding with the ratings of guests who stayed at the same hotel at the same time but left a review after the hotel began responding. This estimate is nearly identical to our cross-platform estimate.</p><p>Finally, in Section 5, we turn our attention to understanding the mechanism underlying our findings. We argue that management responses result in better ratings because they change the cost of leaving a review in two ways. First, we argue that management responses decrease the cost of leaving a positive review because consumers have a positive utility for hotel managers taking note of their feedback. Conversely, consumers may choose not to leave a positive review, if they are unsure hotel managers will read it. Second, we argue that management responses increase the cost of leaving a negative review because reviewers know that their feedback will be scrutinized.</p><p>We provide evidence for this mechanism by investigating the impact of management responses on two additional outcomes managers care about: review volume and review length. First, we examine the argument that consumers are more willing to leave a review if managers are likely to note their feedback. To do this, we show that review volume increases following the adoption of management responses. Furthermore, we show that after hotels start responding, they attract more reviewers who are more positive in their evaluations even when they review nonresponding businesses, suggesting that these positive reviewers see management responses as an incentive to leave a review. Next, we examine the argument that management responses increase the cost of leaving a negative review. We show that when hotels respond, even though negative reviews become more infrequent, they also become longer. Meanwhile, the length of positive reviews remains the same. This suggests that when hotel guests have a poor experience they may opt out <ref type="bibr">Marketing Science, 2017</ref><ref type="bibr">, vol. 36, no. 5, pp. 645-665, © 2017</ref> of leaving a review unless they are willing to invest the extra effort required to write a defensible complaint. While some reviewers will choose to expend this extra effort, others will not. Thus, when hotels start responding, they attract fewer but longer negative reviews. On one hand, these longer negative reviews may alarm hotel managers considering responding. On the other hand, they are in fact a natural side effect of the mechanism driving the overall increase in positive ratings. This highlights an interesting trade-off in using management responses: better ratings at the cost of fewer but longer negative reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Empirical Strategy</head><p>Our goal is to estimate the impact of management responses on the ratings of hotels that respond to reviews. This quantity is an average treatment effect on the treated, and it is defined only for hotels that have elected to respond to TripAdvisor reviewers. Therefore, it is not necessarily equal to the average treatment effect, which is the effect management responses would have had on the TripAdvisor ratings of a randomly chosen hotel. To motivate our empirical strategy, we consider an exogenous intervention that would allow us to estimate the ATT. With access to the TripAdvisor platform, we would randomly assign TripAdvisor visitors into one of two conditions: a treatment group exposed to a version of the site that displays management responses (i.e., the current TripAdvisor site) and a control group exposed to a version of TripAdvisor modified to omit management responses, but otherwise identical. Then, using counterfactual notation, for any responding hotel i, the ATT is given by</p><formula xml:id="formula_0">E(Y i1 − Y i0 | D 1),</formula><p>where Y i1 is a TripAdvisor rating for hotel i from the treatment condition, Y i0 is a TripAdvisor rating from the control condition, and D 1 indicates that hotel i is among those that are treated, i.e., among those that post management responses.</p><p>The key challenge arising from our lack of experimental data is that we do not observe the counterfactual ratings Y i0 that consumers would have submitted had they not been exposed to management responses. To address this identification challenge, we need to construct an appropriate control group out of our nonexperimental data to stand in for Y i0 .</p><p>Before describing our identification strategy for the ATT, we highlight some difficulties inherent in estimating an ATE even with a randomized controlled trial. Unlike the hypothetical ATT experiment that randomly exposes some users to management responses, to estimate an ATE, we would have to instruct a randomly chosen set of hotels to start responding. We would also have to instruct these hotels on which reviews to respond to. While this could also be done at random, it is hard to argue that this strategy is close to what hotels might do in practice. Next, we would have to randomize the types of responses treated hotels post. For example, should hotels respond in an antagonistic or in a conciliatory manner? Should hotels respond in depth, or briefly? The space of treatments (i.e., response strategies) seems so large that, unless we want to estimate the ATE of a specific strategy, focusing on the impact of management responses given the way hotels currently respond (i.e., the ATT) seems more sensible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Cross-Platform Identification Strategy</head><p>A first solution, which exploits the panel nature of our data, is to use the ratings of hotel i submitted prior to its first management response as a control group. Using the superscripts pre and post for ratings submitted before and after hotel i began responding, the required assumption to identify the ATT is E(Y</p><formula xml:id="formula_1">pre i0 | D 1) E(Y post i0 | D 1). 2</formula><p>This assumption is unlikely to hold, leading to endogeneity in our estimation. The key threat to validity is that hotels often use management responses to advertise improvements they have made following a poor review, and therefore increased ratings following a management response can be the result of these improvements, rather than the outcome of consumer exposure to the management response itself.</p><p>A second solution to the identification challenge is based on the observation that most hotels that respond to their TripAdvisor reviews do not respond to their reviews on Expedia. Therefore, in principle, we could use the Expedia ratings of hotel i in place of the unobserved counterfactual ratings Y i0 . Denoting Expedia ratings by Z, the necessary identification condition is E(Y i0 | D 1) E(Z i0 | D 1), and it is also unlikely to hold. The endogeneity issue arising in this case is that TripAdvisor and Expedia reviewers are likely to differ in unobservable ways that determine their ratings. For example, in Table <ref type="table" target="#tab_1">2</ref>, we show that the average hotel rating on TripAdvisor is 0.3 stars lower than on Expedia; i.e., Expedia reviewers report greater levels of satisfaction.</p><p>In this paper, we combine the above two approaches in a DD identification strategy, which requires weaker assumptions. We proceed in two steps: first, we construct a matched control for each hotel's TripAdvisor ratings using the same hotel's ratings on Expedia; then, we compare posttreatment differences in the hotel's TripAdvisor ratings against a baseline of posttreatment differences in the same hotel's Expedia ratings. Formally stated, our main identification assumption is</p><formula xml:id="formula_2">E(Y post i0 − Y pre i0 | D 1, X) E(Z post i0 − Z pre i0 | D 0, X). (1)</formula><p>This is the so-called parallel-trends assumption of DD models, and it is weaker than both assumptions stated above. It states that, conditional on observed characteristics X, differences in (potential) outcomes do not depend on whether a unit was treated or not. DD allows both for platform-independent transient shocks to hotel ratings and time-invariant cross-platform differences in hotel ratings. We can partially test the parallel-trends assumption by comparing the pretreatment rating trends of treated and control units. We return to this point in Section 4.1, where we show that pretreatment trends are indeed parallel, thereby providing evidence in support of our main identification assumption. This is our preferred identification strategy, and we will refer to it as a cross-platform DD to highlight its use of hotel ratings from both TripAdvisor and Expedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Triple Differences.</head><p>As a robustness check, we also estimate the effect of management responses using a difference-in-difference-in-differences (DDD) design, which allows us to simultaneously control for crossplatform and cross-hotel confounders. To implement Renovates DDD, we first need to identify a control group of hotels that should have been unaffected by treatment on either review platform. We again rely on the natural hotel matching available to us and use all nonresponding TripAdvisor hotels and their corresponding one-to-one matched Expedia units. Conceptually, DDD takes place in two DD steps. First, we compute a cross-platform DD for responding hotels, similar to Equation (1). Then, we adjust this DD for unobserved cross-platform differences by subtracting from it the cross-platform DD for nonresponding hotels. Formally stated, the DDD identification assumption is</p><formula xml:id="formula_3">E((Y t+1 i0 − Y t i0 ) − (Z t+1 i0 − Z t i0 ) | D 1, X) E((Y t+1 i0 − Y t i0 ) − (Z t+1 i0 − Z t i0 ) | D 0, X).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Within-Platform Identification Strategy</head><p>Our cross-platform DD identification strategy is robust to review-platform-independent, transitory shocks to hotel ratings. However, unobserved platform-specific shocks to hotel ratings whose timing is correlated with management responses can bias our estimation. In this section, we describe an identification strategy to mitigate this concern. Our approach exploits the fact that most (over 98%) TripAdvisor reviewers indicate in their reviews when they stayed in a hotel. The insight motivating this identification strategy is that any difference in the ratings of two TripAdvisor reviewers who stayed at the same hotel at the same time is unlikely to be driven by unobserved hotel renovations. This model only relies on variation in the ratings of guests who stayed at the same hotel in the same month to identify the impact of management responses. Figure <ref type="figure" target="#fig_0">1</ref> illustrates how this identification strategy solves the problem of unobserved hotel renovations. Within-platform identification of the impact of management responses conditional on guests' dates of stay relies on the difference between reviews A1 and A2 but not B1 and B2. Hotel A's unobserved renovation is not a concern because guests A1 and A2 stayed at the hotel at the same time. By contrast, a comparison of reviews B1 and B2 could result in bias when estimating the impact of management responses because guest B2 experienced hotel renovations that guest B1 did not.</p><p>Marketing <ref type="bibr">Science, 2017</ref><ref type="bibr">, vol. 36, no. 5, pp. 645-665, © 2017</ref> However, the within-platform identification strategy does not take into account the difference between reviews B1 and B2 to estimate the ATT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head><p>To study the effect of management review responses on hotel reputation, we combine information collected from various sources. In this section, we describe the various data sets we collected, and then we explain how we merged them to obtain the sample we use in our analyses.</p><p>The two major sources of data we use are Trip-Advisor and Expedia reviews for Texas hotels. Trip-Advisor is a major travel review platform that contains more than 150 million reviews for millions of accommodations, restaurants, and attractions. TripAdvisor reached over 260 million consumers per month during 2013, a fact that signifies its influence on traveler decision making. We collected the entire review history of the 5,356 Texas hotels that are listed on TripAdvisor. In total, our TripAdvisor sample contains 314,776 reviews, with the oldest review being from August 2001 and the most recent from December 2013. Each review in our data set is associated with a star rating, text content, the date it was submitted, and a unique identifier for the reviewer who submitted it. If the review received a management response, we record the date the response was posted, which typically differs from the date the review was submitted, and the content of the response. Of the 5,356 hotels in our Trip-Advisor sample, 4,603 received at least one review, and 2,590 left at least one management response.</p><p>Expedia is an online travel agent that provides services like airline and hotel reservations and car rentals. Similar to TripAdvisor, consumers can review the Expedia services they purchase. We collected the entire review history of the 3,845 Texas hotels listed on Expedia, for a total of 519,962 reviews. <ref type="bibr">3</ref> The earliest Expedia review is from September 2004 and the most recent is from December 2013. Our Expedia review sample contains the same review attributes as our TripAdvisor sample. Of the 3,845 hotels in our Expedia sample, 3,356 were reviewed, and 587 left at least one management response.</p><p>Having collected TripAdvisor and Expedia reviews, our next step is to link these review samples together by hotel. To do so, we exploit a feature of the Expedia website: Expedia provides a link to each hotel's TripAdvisor page if such a page exists on TripAdvisor. This allows us to accurately match nearly every hotel's Expedia and TripAdvisor reviews. To verify the accuracy of the Expedia-provided links, we randomly sampled 100 Expedia-TripAdvisor pairs, and manually verified that they correspond to the same hotel by checking the hotel's name and address. We found no discrepancies. Using this information, we are able to match 3,681 out of 3,845 Expedia hotels (96% of the Expedia hotel sample). After matching each hotel across the two review platforms, we further balance our estimation sample by limiting ourselves to hotels that have been reviewed on both sites. Of the 3,681 matched hotels, 3,264 are reviewed on both sites. This way, our data include TripAdvisor and Expedia ratings for every hotel, and thus allow us to identify our treatment effect from only within-hotel, cross-platform variation. After limiting our sample to hotels that have been reviewed on both review platforms, we are left with a total of 806,342 reviews, of which 291,119 are from TripAdvisor and 515,223 are from Expedia. Finally, since in part of our analyses we use Expedia ratings as a control group, we also create a subset of data that excludes any hotels that have posted management responses on Expedia. This leaves us with 2,697 matched hotels and 552,051 reviews, of which 203,068 are from TripAdvisor, and 348,983 are from Expedia. Table <ref type="table" target="#tab_2">3</ref> describes the various estimation samples we use in our analyses. The matched set of Trip-Advisor and Expedia ratings for hotels that have been reviewed on both platforms, excluding hotels that have ever responded on Expedia, constitutes our main estimation sample. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">User Review Histories</head><p>In Section 5, we use the entire TripAdvisor review history of every user who reviewed a Texas hotel on Trip-Advisor. For every user that reviewed a hotel in our  Reviews with a response (%)</p><p>TripAdvisor Expedia</p><p>TripAdvisor sample, we collected their entire review history, for a total of 3,047,428 reviews from 214,141 users. We were not able to obtain the review histories of a small fraction of users (2.2%) either because they left anonymous reviews on TripAdvisor (the username associated with such reviews is "A TripAdvisor Member") or because they closed their TripAdvisor accounts and therefore their user profiles do not exist anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Descriptive Statistics</head><p>A key difference between TripAdvisor and Expedia, which we exploit in our analysis, is that hotels often post management responses on TripAdvisor, but they rarely do so on Expedia. Figure <ref type="figure" target="#fig_1">2</ref> illustrates this difference: we plot the cumulative percentage of reviews that have received a management response by year. We find that by 2013, 31.5% of TripAdvisor reviews had received a management response, compared to only 2.3% for Expedia, highlighting the difference in the rate of management response adoption across the two review platforms.</p><p>Having established that management responses are infrequent on Expedia, we next turn our attention to investigating the adoption patterns of management responses on TripAdvisor. An interesting aspect underlying the increasing adoption trend of management responses on TripAdvisor is the elapsed time between a review being posted and receiving a management response. Figure <ref type="figure" target="#fig_2">3</ref> plots the average lag (measured in days) between reviews and management responses by review submission year. On average, TripAdvisor reviews submitted in 2013 received a response 25 days later, while reviews posted in 2009 received a response almost 10 months later. How can we explain the managerial practice of responding to old reviews? A possible interpretation is that hotel managers are concerned that even old reviews can be read by and affect the decision-making process of future TripAdvisor visitors. By responding to these old reviews, hotel managers are potentially attempting to steer the behavior of future TripAdvisor visitors who might stumble on them.</p><p>Next, we turn our attention to analyzing the frequency with which hotels respond to reviews on Trip-Advisor. Figure <ref type="figure" target="#fig_3">4</ref> plots the fraction of TripAdvisor reviews that received a response by star rating. While a priori we might expect negative reviews to be more likely to receive a response, we find that in our data this is not the case. In fact, five-star reviews are among the most likely to receive a response, and negative reviews are almost as likely to receive a response as positive reviews. While reviews with different ratings eventually receive responses at approximately the same rate, managers tend to respond to negative reviews first. We demonstrate this in Figure <ref type="figure">5</ref>. The figure plots the average ratings of reviews that received management responses, in chronological order. We see that while the first response goes to a review with an average Chronological response rank Star-rating rating of approximately three stars, the rating associated with the 20th response is nearly four stars. This pattern of responding causes a transient endogeneity problem: because managers tend to respond to negative reviews first, ratings following the adoption of management responses are likely to be higher than ratings submitted just before a manager's first response regardless of any effect management responses may have on ratings. What are the characteristics of hotels that use management responses? Table <ref type="table" target="#tab_1">2</ref> compares hotels by their adoption of management responses on TripAdvisor. We find that responding hotels have higher average ratings both on TripAdvisor and Expedia. The mean difference between the star ratings of responding and nonresponding hotels is 0.5 stars. Table <ref type="table" target="#tab_1">2</ref> also highlights an interesting cross-platform difference: while on average Texas hotels have more reviews on Expedia than they do on TripAdvisor, the length of the text associated with the average Expedia review is only one-third the length of the average TripAdvisor review. The average Expedia review is 201 characters long, which is slightly longer than a tweet. This difference may further explain the reason behind the lower rate of adoption of management responses on Expedia: consumers do not write long, descriptive Expedia reviews that merit response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section we present the results of regression analyses we carried out to estimate the causal effect of management responses on hotel reputation. These analyses are based on the three identification strategies we described above. In addition to these findings, we provide empirical evidence in support of the identification assumptions underlying our causal claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cross-Platform DD</head><p>Cross-platform DD, which is our preferred specification, estimates changes to the TripAdvisor ratings of any given hotel after it starts responding, relative to before and adjusted for any change over the same period to its Expedia ratings. The identifying assumption that allows a causal interpretation of our findings is that TripAdvisor and Expedia ratings would have evolved in parallel in the absence of treatment. While this assumption is not fully testable, the panel nature of our data generates some testable hypotheses that we can use to reinforce the plausibility of our causal claims. Specifically, given our long observation period, we can test for differences in trends between the two platforms prior to treatment.</p><p>To compare pretreatment trends, we partition time around the day each hotel started responding in 30day intervals, taking the offset of the first response to be 0. Then, for example, [0, 30) is the 30-day interval starting on the day the hotel began responding, and [−30, 0) is the 30-day interval just before. We focus our trend analysis on the two-year period centered on each hotel's first response, resulting in the definition of 24 distinct intervals. Since hotels began responding at different times, these intervals correspond to different calendar dates for different hotels. Next, we associate each TripAdvisor and Expedia rating in our estimation sample with a dummy variable indicating the interval that contains it. Finally, we estimate the following DD regression:</p><formula xml:id="formula_4">Stars i jt β 1 After i jt + β 2 TripAdvisor j + γInterval i jt × TripAdvisor i j + X i jt γ + α j + τ t + i jt ,<label>(3)</label></formula><p>where Stars i jt is the star rating of review i for hotel j in calendar month t, After i jt is an indicator for reviews (on either platform) submitted after hotel j started responding, TripAdvisor i j is an indicator for Trip-Advisor ratings, and Interval i jt is the set of 30-daylong treatment clock dummies we described above.</p><p>The coefficient for After i jt captures differences in ratings between treatment and nontreatment periods, the coefficient for TripAdvisor i j captures differences in ratings across platforms, and γ, the vector of interaction coefficients associated with each interval, is the DD estimate of interest. As is common in DD analyses, we include review-platform-specific quadratic time trends in X i jt as an additional safeguard against nonparallel trends. Finally, our model includes calendar-month fixed effects τ t to control for transient shocks in ratings that are common across review platforms. While we could estimate this model by pooling ratings from different hotels together, we choose to include a matched-pair fixed effect α j , i.e., a shared fixed effect for reviews of the same hotel from either review platform. The use of matched-pair fixed effects enables identification from only within-hotel variation. <ref type="bibr">5</ref> We estimate the model in Equation ( <ref type="formula" target="#formula_4">3</ref>) using ordinary least squares (OLS). To account for serial </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difference between</head><p>TripAdvisor and Expedia ratings</p><formula xml:id="formula_5">[ -3 3 0 , -3 0 0 ) [ -2 7 0 , -2 4 0 ) [ -2 1 0 , -1 8 0 ) [ -1 5 0 , -1 2 0 ) [ -9 0 , -6 0 ) [ -3 0 , 0 ) [ 3 0 , 6 0 ) [ 9 0 , 1 2 0 ) [ 1 5 0 , 1 8 0 ) [ 2 1 0 , 2 4 0 ) [ 2 7 0 , 3 0 0 ) [ 3 3 0 , 3 6 0 )</formula><p>30-day intervals around initial response 95% CI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coefficent</head><p>Note. The solid line plots the γ-coefficient estimates from Equation (3), and the dashed lines their respective 95% confidence intervals.</p><p>correlation in our dependent variable, we cluster errors at the hotel level <ref type="bibr" target="#b8">(Donald and</ref><ref type="bibr">Lang 2007, Bertrand et al. 2004)</ref>. We choose to normalize the coefficient for the [−60, −30) interval to 0. While choosing a different baseline would have yielded identical conclusions, our particular choice eases presentation, as will become evident shortly. The coefficients of the remaining intervals can be interpreted as differences between the Trip-Advisor and Expedia ratings over time with respect to the [−60, 30) baseline. We present a graphical analysis of our estimates in Figure <ref type="figure" target="#fig_4">6</ref>. The figure plots the estimated values of the interval coefficients γ, together with their 95% confidence intervals.</p><p>The figure reveals several distinctive features of hotel rating dynamics prior to and following the adoption of management responses. First, visual inspection of pretreatment trends suggests that they are parallel with the exception of the 30-day interval immediately preceding the treatment period. To back this claim statistically, we perform a Wald test, which fails to reject (p &lt; 0.43) the hypothesis of joint equality among pretreatment intervals excluding [−30, 0). Second, the figure reveals a negative outlier at [−30, 0), which is caused by the fact that managers tend to respond to negative reviews first. While, on average, the adoption of management responses is preceded by a substantive negative shock to their TripAdvisor ratings, we do not know whether this association is causal. This negative shock to TripAdvisor ratings prior to adopting management responses is reminiscent of Ashenfelter's dip <ref type="bibr" target="#b0">(Ashenfelter and Card 1985)</ref>, an empirical regularity first observed in the context of job training programs, where program participants tended to experience an earnings drop just prior to enrolling in them.</p><p>Ashenfelter's dip can be a sign of transient or persistent endogeneity.</p><p>The presence of Ashenfelter's dip can overstate our DD estimates because hotel ratings-just like employee earnings-are likely to mean revert following an out-of-the-ordinary negative period, regardless of any intervention by hotel management. Following common practice (see, e.g., <ref type="bibr" target="#b12">Heckman and Smith 1999</ref><ref type="bibr" target="#b14">, Jepsen et al. 2014</ref><ref type="bibr" target="#b16">, Li et al. 2011</ref>, we correct for transient endogeneity caused by Ashenfelter's dip by computing long-run differences, where we symmetrically exclude a number of periods around the adoption of management responses. Our final observation regards the posttreatment period, and it foreshadows our main result. Following the adoption of management responses, we see a sustained increase in ratings. In fact, hotel ratings not only recover following the adoption of management responses, but they consistently exceed their prior levels by over 0.1 stars.</p><p>Given the graphical evidence in support of the parallel-trends assumption underlying our identification strategy, we next estimate the causal impact of management responses on hotel ratings. The following model implements our cross-platform DD identification strategy:</p><formula xml:id="formula_6">Stars i jt β 1 After i jt + β 2 TripAdvisor i j + δAfter i jt × TripAdvisor i j + X i jt γ + α j + τ t + i jt ,<label>(4)</label></formula><p>where the variables are as in Equation ( <ref type="formula" target="#formula_4">3</ref>), except that we replace the variable Interval i jt with the variable After i jt . The matched-hotel fixed effects α j ensure that our identification relies only on within-hotel variation, i.e., comparing the ratings of any given hotel on Trip-Advisor with the ratings of the same hotel on Expedia. The primary coefficient of interest is δ, which measures the causal impact of management responses on hotel ratings. We first estimate Equation (4) on the sample of responding hotels using OLS with standard errors clustered at the hotel level. We present our results in the first column of Table <ref type="table" target="#tab_3">4</ref>. The estimated coefficient for the interaction term After i jt × TripAdvisor i j is 0.15 stars, and it is statistically significant. Next, to correct for Ashenfelter's dip, we repeat our estimation excluding ratings submitted anywhere between 30 days prior and 30 days following a hotel's first management response. <ref type="bibr">6</ref> We present these results in the second column of Table <ref type="table" target="#tab_3">4</ref>. As expected, our adjusted estimate for δ is slightly smaller. However, even after accounting for transient negative shocks to hotel ratings prior to the response period, we find that management responses cause subsequent hotel ratings to rise by an average of 0.12 stars.</p><p>The coefficient for After i jt , which measures changes in the ratings of Expedia reviewers over the same time period is also of interest as it can be seen as a treatment effect on the nontreated. We estimate its value to be statistically indistinguishable from zero, suggesting that Expedia reviewers were unaffected by management responses on TripAdvisor. This is as we would have hoped for and provides additional evidence in support of the parallel-trends identification assumption. If ratings for the control group had changed following treatment, it would be harder to argue that controlling for these changes completely eliminates bias. Moreover, the observation that the ratings of Expedia reviewers were unaffected by treatment indicates that it is highly unlikely that increased ratings after adopting management responses were the outcome of unobserved hotel improvements to avoid further negative reviews-unless one is willing to argue that only Trip-Advisor's reviewers experienced these improvements, and Expedia users did not see any change whatsoever. We perform additional robustness checks against this type of concern in Section 4.2.</p><p>Overall, our analysis suggests that responding hotels were able to significantly increase their future Trip-Advisor ratings solely by responding to their past reviews. These findings indicate that management responses are a powerful reputation management tool that can improve consumer ratings and, in turn, financial performance. In Section 4.2, we perform robustness checks to verify that our results hold when accounting for various forms of endogeneity that cross-platform DD cannot address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Robustness Checks for Cross-Platform DD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Differences in Cross-Platform Traveler Demographics and TripAdvisor-Specific Improvements.</head><p>A key implication of the assumption underlying crossplatform DD identification is that TripAdvisor and Expedia users do not differentially value certain hotel improvements that happen to coincide with the adoption of management responses. If this assumption fails, cross-platform DD will lead to upward-biased estimates. To exemplify this concern, suppose that the dominant demographic on TripAdvisor is business travelers, while there are few or no Expedia users who belong to this travel segment. Then, a hotel manager monitoring TripAdvisor reviews might simultaneously react in two ways. First, the manager might ensure that the concerns raised in the reviews of business travelers are addressed (e.g., by making improvements to the hotel's business center). Second, the manager may respond to the TripAdvisor reviews that raised these concerns. Under these circumstances, the manager's action could result in a TripAdvisor-specific increase in ratings, thereby inducing bias in our estimation.</p><p>How likely is this type of bias in our setting? Recall that previously we found that Expedia ratings do not change at all following the adoption of management responses on TripAdvisor (the coefficient for After i jt is statistically indistinguishable from zero). Therefore, if the effect we measure is due to unobserved hotel improvements, then Expedia users do not value these improvements at all. Even though it is plausible that Expedia users have different tastes than TripAdvisor users, and, indeed, that they value TripAdvisorspecific improvements less than TripAdvisor users, it is less likely that Expedia users' tastes are so widely different that they do not value TripAdvisor-specific improvements at all. Nevertheless, we cannot rule out that Expedia users have zero value for TripAdvisorspecific improvements and hotels target their improvements at traveler segments that are overrepresented by a wide margin on TripAdvisor and that these TripAdvisor-specific improvements coincide with the adoption of management responses. In this section, we perform additional robustness checks to guard against this type of concern.</p><p>Our robustness checks rely on the fact that both TripAdvisor and Expedia ask reviewers about the purpose of their trip at the review submission time. This information is voluntarily provided by reviewers, and therefore not all reviews carry such a designation. Moreover, in our sample, Expedia appears to have started collecting this information in 2010, whereas TripAdvisor started collecting this information as early as 2003. Nevertheless, the number of reviews carrying this label is substantial: considering post-2009 reviews, 48% of Expedia reviews and 89% of TripAdvisor reviews are associated with a particular traveler segment. The four most popular traveler segments, on both platforms, are "business," "couples," "families," and "friends." Expedia allows users to select among other less popular choices (such as "golfing trip" and "students") that do not exist as options on TripAdvisor. We focus our analysis on the four segments that exist on both platforms, which comprise the majority of labeled reviews. We then repeat our cross-platform DD estimation by traveler segment. The motivation for this robustness check is that by separately analyzing each traveler segment we lower the probability of bias arising from cross-platform reviewer heterogeneity. We present these results in Table <ref type="table" target="#tab_4">5</ref>. We find that our results are robust to conditioning on the traveler segment. Management responses have a positive (and, interestingly, similar in magnitude) impact on the ratings of the different traveler types. Taken together these by-segment regressions suggest that our results are unlikely to be due to TripAdvisor-specific improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Changes in the Review Environment and Reviewer Selection.</head><p>A different type of concern with our results is that we have not accounted for changes in the review environment other than the adoption of management responses. <ref type="bibr">7</ref> A number of papers, including <ref type="bibr" target="#b10">Godes and Silva (2012)</ref> and <ref type="bibr" target="#b21">Moe and Schweidel (2012)</ref>, discuss the role of the review environment consumers encounter in both the decision to leave a review and the review's valence. If the timing of the adoption of management responses happens to coincide with changes in the review environment that result in increased ratings, our estimates will be biased. In fact, as we have seen in Figure <ref type="figure" target="#fig_4">6</ref>, hotels do adopt management responses following an unusually large negative shock in their ratings, i.e., a change in their review environment. Given the dynamic nature of changes in the review environment, the Ashenfelter's dip correction we have used so far may not fully correct for this type of bias. For instance, consider the following hypothetical scenario. After a hotel receives a string of bad reviews, two things happen: (a) the hotel starts responding, and (b) hotel guests who had a positive experience start inflating their ratings to compensate for what they perceive as inaccurately low prior ratings. In this case, it would be these "activist" reviewers causing the increase in ratings, not the management responses. <ref type="bibr">8</ref> To test the robustness of our results to changes in the review environment dynamics, we include two salient characteristics of the review environment as controls in our cross-platform DD specification: for each review, we compute (the log of) the number of TripAdvisor reviews preceding it and the average rating of these prior reviews.</p><p>We report these results in the third column of Table <ref type="table" target="#tab_3">4</ref>. The impact of management responses on ratings remains robust to the inclusion of review environment controls. However, some care is needed in interpreting the estimated coefficient for the treatment effect (After i jt × TripAdvisor i j ). While in some cases (like the one described in the previous paragraph) the inclusion of review environment controls will correct for unobserved bias, in other cases, including review environment controls could in fact introduce bias rather than correct for it. Specifically, the ATT will be downward biased if the average rating of prior reviews positively affects future ratings. Prior empirical studies (e.g., <ref type="bibr" target="#b15">Li and Hitt 2008)</ref> find a positive association between an average rating and subsequent reviews. This association can cause a feedback loop: a hotel manager responds to a review; in turn, this results in a subsequent positive review, which increases the hotel's average rating; and finally, the increased average rating itself raises the incidence of positive reviews. In this case, the average rating of prior reviews mediates the relationship between management responses and ratings. More generally, this type of bias arises when management responses cause changes in the review environment, which then cause increases in ratings. However, even in such cases, there is a useful way to interpret the difference in the coefficients for the ATT in the presence and absence of the review environment controls (columns (2) and (3) of Table <ref type="table" target="#tab_3">4</ref>): their difference captures the indirect effect of management responses on ratings through their positive impact on a hotel's average rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Management Response Visibility as a Treatment</head><p>Indicator. Our analyses so far used management response adoption as a treatment indicator. Under this treatment scheme, all TripAdvisor reviews left after a hotel's first management response were part of the treatment group, while TripAdvisor reviews left prior to a hotel's first response were part of the control group. Then, we estimated an ATT by taking the difference in ratings between the treatment and control groups. If hotels took other unobserved actions that specifically affected their TripAdvisor ratings at the same time they started responding, then this estimate could be biased. Consider, for instance, the case <ref type="bibr">Marketing Science, 2017</ref><ref type="bibr">, vol. 36, no. 5, pp. 645-665, © 2017</ref> of TripAdvisor-specific hotel improvements: if hotels make improvements that are specifically appealing to TripAdvisor users at the same time they start responding, an ATT estimated as above will reflect the impact of both management responses and the impact of these improvements. Here, we explicitly guard against this endogeneity concern by identifying a control group of TripAdvisor users who were unlikely to be affected by management responses even though they reviewed hotels after they had started responding (and were thus affected by TripAdvisor-specific improvements or other unobserved hotel actions coinciding with the adoption of management responses).</p><p>While we cannot precisely know which reviewers were exposed to management responses, we can exploit the fact that TripAdvisor orders reviews by date and displays 10 reviews per page to construct a proxy. As an example, which we illustrate in Figure <ref type="figure" target="#fig_5">7</ref>, consider a hotel that has 10 reviews and that has responded to only the first review it received. Then, consider what the hotel's next two reviewers, whom we label "reviewer 11" and "reviewer 12," see. When reviewer 11 arrives to leave a review (as shown in the left column of Figure <ref type="figure" target="#fig_5">7</ref>), the management response is still visible on the hotel's first page of reviews. After reviewer 11 leaves a review (as shown in the right What Reviewer 11 sees prior to leaving a review What Reviewer 12 sees prior to leaving a review EMPTY Notes. Reviewer 11 is more likely to read the management response to review 1 than reviewer 12 is. By the time reviewer 12 arrives to leave a review, the management response is displayed on page 2, and is thus less likely to be read. column of Figure <ref type="figure" target="#fig_5">7</ref>), the review carrying the management response will be relegated to the hotel's second page of reviews. Therefore, reviewer 12 will be less likely to read the response than reviewer 11. Because the effect of management responses should be larger for reviewers who are more likely to have read them, we can use reviewers like reviewer 12 as a control group.</p><p>Concretely, since reviewers are more likely to read the first page of reviews than they are to click through and also read the second page of reviews, we construct the variable Pct. page 1 responded i jt , which measures the fraction of the 10 most recent reviews (i.e., the reviews on page 1) prior to review i that carried a response. We then interact this proxy variable with After i jt × TripAdvisor i j and reestimate our model. We report these results in the first column of Table <ref type="table" target="#tab_5">6</ref>. We find a positive and significant interaction effect for Pct. page 1 responded i jt . This suggests that reviewers who are more likely to read a management response are more likely to be affected by them. Following the same logic, we construct the variable Pct. page 2 responded i jt , which denotes the fraction of reviews on page 2 that carried a management response at the time review i was posted. We reestimate the cross-platform DD model including interactions for both the page 1 and page 2 proxies. We report these results in the second column of Table <ref type="table" target="#tab_5">6</ref>. The estimate of the page 2 proxy is smaller and not statistically significant, coinciding with our intuition that users are less likely to be affected by management responses on the second page of a hotel's reviews.</p><p>Finally, to reinforce the point that identification using management response visibility as a treatment indicator is not vulnerable to endogenous changes in  ratings happening at the time hotels start responding, we estimate the same two specifications as in the previous paragraph using only reviews submitted following each hotel's first response. The intuition behind this analysis is that if a hotel starts responding when it renovates, then all subsequent reviewers experience these renovations. Therefore, while the difference between a rating submitted prior to a hotel's first response and a rating submitted after a hotel's response could be driven by unobserved TripAdvisor-specific improvements, it is harder to argue the same for the difference between two ratings that were both submitted after a hotel began responding. The results of these analyses, which we display in Table <ref type="table" target="#tab_6">7</ref>, are similar to our estimates using the entire data set of reviews. These robustness checks suggest that the effect we measure is due to management responses. Specifically, our results indicate that the impact of responding is higher in situations where management responses are more likely to have been read. By contrast, in situations where management responses are not displayed prominently (e.g., on the second page of a hotel's TripAdvisor reviews), their impact is smaller. Furthermore, these results are unlikely to be explained by hotel renovations. While renovations are likely to drive increased ratings, we have less reason to believe that renovations will differentially impact hotel guests depending on their likelihood of reading a management response after their stay. One limitation of the analyses in the section is that our response visibility proxy is almost certainly measured with error: some reviewers will not note management responses on the first page of a hotel's reviews, while other reviewers will note management responses buried in a hotel's last page of reviews. Such measurement error will attenuate the ATT we estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Management Responses and Review</head><p>Fraud. An identification concern arises if hotels that adopt management responses simultaneously adopt other reputation management strategies such as posting fake reviews. In this case, we may mistake increases in ratings due to review fraud for increases in ratings due to management responses, resulting in a positive bias in the ATT we estimate. Interestingly, the sign of such bias can also be negative. If hotels choose to stop posting fraudulent reviews when the option of directly responding to consumers becomes available to them, the ATT we estimate will be biased downward. Therefore, while this type of bias is a concern, its direction will depend on whether management responses and review fraud are substitutes or complements. Whether management responses encourage or discourage review fraud activity is an interesting open question with implications for the design of review platforms. The cross-platform DD strategy is especially susceptible to review fraud biases because posting fake reviews is easier on TripAdvisor than it is on Expedia: while any traveler can leave a review on TripAdvisor, Expedia requires that users have paid and stayed. <ref type="bibr">9</ref> We perform two robustness checks to mitigate concerns arising from review fraud. Both checks rely on the fact that some firms have higher incentives to commit review fraud than others. If firms predisposed to review fraud are the ones that benefit from management responses, we might worry that review fraud is biasing our results.</p><p>For our first robustness check, we leverage the fact that review fraud incentives vary by hotel organizational form. Specifically, prior work <ref type="bibr">(Mayzlin et al. 2014, Luca and</ref><ref type="bibr" target="#b18">Zervas 2016)</ref> has shown that chain-affiliated firms are less likely to post fake reviews than independent firms. This difference in review fraud incentives arises for two reasons. First, because chain hotels benefit less from consumer reviews <ref type="bibr" target="#b17">(Luca 2011)</ref>, they have weaker incentives to commit review fraud in the first place. Second, if a chain hotel is caught committing review fraud, there can be negative spillover effects on the reputation of the brand it is affiliated with. For this reason, as <ref type="bibr" target="#b19">Mayzlin et al. (2014)</ref> point out, some chains have adopted social media policies that prohibit anyone other than their guests (e.g., the chain's employees) from posting reviews. Based on this observation, we repeat our analysis separately for independent and chain-affiliated hotels. We report these results in Table <ref type="table" target="#tab_7">8</ref>. Looking at chain hotels, which are unlikely to commit review fraud, we find that the impact of management responses on their ratings is positive, significant, and of similar magnitude to our previous estimates (0.11, p &lt; 0.001). This result suggests that the ATT we estimate is unlikely to be inflated because of review fraud. Intriguingly, we estimate a larger ATT (0.19) for nonchains. While it is tempting to interpret this result as evidence of independent hotel review fraud coinciding with the adoption of management responses, it could also be the case that management responses have a stronger impact on the reputation of independent hotels than the reputation of chains.</p><p>Our second robustness check relies on evidence from the literature suggesting that hotels with fewer reviews are more likely to commit review fraud to enhance their reputations <ref type="bibr" target="#b18">(Luca and Zervas 2016)</ref>. At the same time, there is little reason to believe that hotels with fewer reviews should benefit more from management responses than hotels with more reviews. Therefore, if hotels with fewer reviews see greater increases in their ratings after they start responding, we might worry about confounding arising from review fraud. <ref type="bibr">10</ref> To test whether the benefits from responding vary by pretreatment review volume, we augment Equation (1) with an interaction term between treatment and the number of pretreatment reviews for each hotel (i.e., the number of reviews the hotel had just prior to its first response). We report these results in the third column of Table <ref type="table" target="#tab_7">8</ref>. The interaction term is statistically indistinguishable from zero, suggesting that the impact of management responses is independent from the number of reviews a hotel had when it decided to start responding. <ref type="bibr">11</ref> This robustness check provides additional evidence that benefits from responding do not vary by a hotel's incentives to commit review fraud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5.">Difference-in-Difference-in-Differences.</head><p>As a final robustness check, we replicate our results using DDD, which is more stringent than the double differencing methods we have used thus far. Our estimation sample now comprises all responding and nonresponding hotels on TripAdvisor, and their oneto-one matched controls on Expedia. Then, the DDD estimate compares posttreatment changes in Trip-Advisor ratings for responding hotels against the baseline of matched Expedia ratings over the same period of time, and then adjusts this estimate for unobservable platform trends by differencing out cross-platform changes in the ratings for nonresponding hotels over the same period of time. In other words, the DDD estimator is the difference between the cross-platform DD for responding and nonresponding hotels DDD DD responding cross-platform − DD nonresponding cross-platform .</p><p>The following model implements our DDD estimator:</p><formula xml:id="formula_7">Stars i jt β 1 Responding j + β 2 TripAdvisor i j + β 3 Responding j × TripAdvisor i j + β 3 Responding j × τ t + β 3 TripAdvisor i j × τ t + δAfter i jt × Responding j × TripAdvisor i j + X i jt γ + α j + τ t + i jt .<label>(5)</label></formula><p>The variables Responding j × τ t , and TripAdvisor i j × τ t are a full set of review-platform-and treatment-statusspecific time fixed effects. The DDD estimate is δ.</p><p>Because we can match TripAdvisor to Expedia ratings, we use matched-pair fixed effects α j , which subsume the coefficient for Responding j . We report our results, first without and then with Ashenfelter's dip correction, in Table <ref type="table" target="#tab_8">9</ref>. The DDD estimate (0.08 stars, p &lt; 0.01) for the impact of management responses on subsequent ratings, which controls for both cross-hotel and cross-platform unobservable trends as well as Ashenfelter's dip, supports our results so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6.">Sensitivity Analysis Using Rosenbaum Bounds.</head><p>Our cross-platform DD and DDD identification strategies use a one-to-one matched sample of treated and untreated units to identify the impact of management responses on hotel ratings. While matching the reviews of the same hotel across different platforms ensures compatibility in terms of observables, it does not mitigate the problem of selection on (time-varying) unobservables. Thus far, we dealt with selection on unobservables by performing case-specific robustness checks against hidden biases such as TripAdvisorspecific improvements and review fraud. Now, we assess the overall sensitivity of our estimates to any kind of hidden bias using Rosenbaum bounds <ref type="bibr" target="#b24">(Rosenbaum 2002)</ref>. One benefit of using Rosenbaum bounds is that we can assess the sensitivity of our results to hidden bias without having to specify how such bias might arise in practice. Specifically, suppose that treatment assignment (conditional on observables) is biased such that the odds of treatment of a unit and its matched control differ by a multiplier Γ, where Γ 1 corresponds to the case of random treatment assignment. It is helpful to conceptualize such bias as the result of an unobserved covariate that both affects selection into treatment by a factor Γ and that is highly predictive of the outcome we are measuring. Because of this double requirement on the unobservable, Rosenbaum bounds are considered worst case analyses <ref type="bibr" target="#b7">(DiPrete and Gangl 2004)</ref>. Using Rosenbaum's methods, we can compute an upper bound on the p-value associated with the treatment effect assuming selection on unobservables of magnitude Γ.</p><p>We compute Rosenbaum bounds at various levels of Γ to examine how biases of different size would affect the significance level of the ATT. Because in our setting treatment is assigned to clusters (hotel platforms) rather than individuals, we adjust our bounds for clustered treatment assignment <ref type="bibr" target="#b11">(Hansen et al. 2014)</ref>. Not accounting for clustering would exaggerate our effective sample size in a manner similar to using nonclustered standard errors. 1.0 0.000 1.5 0.000 2.0 0.001 2.5 0.004 3.0 0.010 3.5 0.020 4.0 0.034 4.5 0.051 5.0 0.070 5.5 0.091 6.0 0.114 (p max ) on the p-value associated with the ATT at different levels of the sensitivity parameter Γ. We find that the minimum value of Γ at which the treatment effect we estimate becomes statistically insignificant at the 5% level is just below 4.5. The literature typically interprets values of Γ &gt; 2 as evidence for robustness to large biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results for Within-Platform Identification</head><p>Arguably, the key concern with cross-platform identification is that differencing does not completely eliminate bias arising from unobserved differences between TripAdvisor and Expedia that may be correlated with the adoption of management responses and changes in hotel ratings. Here, we use the within-platform identification strategy described in Section 2.2 to estimate the impact of management responses. We implement this identification strategy with the following model:</p><formula xml:id="formula_8">Stars i jt β 1 Responding j + δAfter i jt × Responding j + X i jt γ + η j × Year-Month Stayed i jt + τ t + i jt ,<label>(6)</label></formula><p>where the interactions η j × Year-Month Stayed i jt are hotel-year-month-of-stay fixed effects. The precision of these fixed effects is at the year-month level because TripAdvisor does not disclose exact dates of travel, likely to protect user privacy. In total, our model contains over 110,000 such fixed effects in addition to time fixed effects and linear time trends by treatment status.</p><p>(To our surprise, some variation remains in our data after we introduce all of these controls.) The effect of management responses is identified by variation in the difference between the ratings of TripAdvisor reviewers who left a review prior to a hotel's adoption of management responses and the ratings of TripAdvisor reviewers who stayed at the same hotel during the same year-month but left a review following a hotel's adoption of management responses. While this identification strategy mitigates the concern of unobserved hotel renovations, bias can arise if the elapsed time between staying at a hotel and reviewing it is correlated with the guest's rating. To account for endogeneity arising from review timing, we include as controls the time elapsed between a review and a stay, and the square of the same variable (to allow for nonlinear effects). We report these results in the first column of Table <ref type="table" target="#tab_0">11</ref>. In the second column, we also correct for Ashenfelter's dip to account for the fact that hotels tend to start responding when they experience negative shocks to their ratings. We find a positive and significant effect for responding whose magnitude is similar to our previous results.</p><p>A concern with using a flexible polynomial trend to absorb correlation between how long guests wait to leave a review and how enjoyable their stay was is <ref type="bibr">Marketing Science, 2017</ref><ref type="bibr">, vol. 36, no. 5, pp. 645-665, © 2017</ref>  that the relationship between the two variables may be more complex. To avoid parametric assumptions about the relationship between rating and elapsed time, we would like the elapsed time covariate to be balanced between the treatment and control groups, i.e., we would like to have P(Treated | Elapsed time between staying and reviewing) P(Treated). Using management response visibility as the treatment indicator achieves this goal. A Kolmogorov-Smirnov test fails to reject the null hypothesis that treated and control reviewers have different distributions of elapsed times between staying and reviewing. Table <ref type="table" target="#tab_0">12</ref> reports our within-platform estimates using management response visibility as a treatment indicator. As before, we confirm that the impact of management responses is stronger for reviewers who are more likely to have read them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Robustness to Alternative Functional Forms</head><p>In our analysis so far, we have modeled an ordered discrete outcome (the 1-5 star rating associated with each review) using a continuous linear model. While this modeling choice is common in the literature, it misrepresents the data generation process and can lead to bias. In this section, we repeat our analysis using a generalized ordered probit specification, which reflects our data generating process more accurately. We begin by briefly describing the generalized ordered probit model-for a complete description see <ref type="bibr" target="#b27">Terza (1985)</ref>.</p><p>The model posits that the cumulative probabilities of the discrete outcomes (the star ratings) are given by</p><formula xml:id="formula_9">Pr[Stars i jt ≤ s | x i jk , z i jk ] Φ(κ s + x i jk β s + z i jk γ) s 1 to 4, (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where Φ is the cumulative normal distribution. Compared to the standard ordered probit, the generalized model allows some of its coefficients (the β s ) to vary by outcome. This generalization relaxes the parallelregressions assumption of the standard ordered probit model and allows the effect of covariates to vary across outcomes. We begin by estimating the generalized ordered probit model on the TripAdvisor ratings of responding hotels. In the set of threshold-varying controls, we include an indicator After i jt , denoting the postresponse period. In addition, to flexibly control for unobserved time trends, we also include a set of year dummies and linear time trends (whose coefficients do not vary by outcome to avoid introducing too many parameters in the model). We estimate the model using maximum likelihood estimation and compute standard errors clustered at the hotel level with a nonparametric bootstrap. We report our results in the first column of Table <ref type="table" target="#tab_0">13</ref>. While these estimates are not as easily interpretable as in the linear case, in general, a set of positive and significant coefficients (as we find here) suggests an increase in the probability of higher ratings. To arrive at more interpretable estimates, we also compute average marginal probability effects (MPEs) as described in <ref type="bibr" target="#b2">Boes and Winkelmann (2006)</ref>. Omitting irrelevant subscripts for simplicity, marginal probability effects are defined as</p><formula xml:id="formula_11">MPE sl (x) ∂ Pr[Stars ≤ s | x, z]/∂β (l) s φ(κ s + x β s )β (l) σ − φ(κ s−1 + x β s )β (l) s−1 , (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where</p><formula xml:id="formula_13">β (l)</formula><p>s denotes lth item of the vector β s . Then, the average MPEs are defined as E x [MPE sl (x)], and they should be interpreted as average probability changes given a marginal change in the covariate of interest. Average MPEs can be consistently estimated using the estimated model parameters in place of the true parameters. We report average MPEs and bootstrap standard errors (clustered at the hotel level) for After i jt in the first column of Table <ref type="table" target="#tab_0">14</ref>. We find that the likelihood of receiving a five-star review increases by approximately 7% following the adoption of management responses. Meanwhile, the probability of a one-star rating decreases by nearly 2%. These results are in line with our previous DD estimates using a linear model. In the spirit of DD, we also perform a falsification check. Specifically, we reestimate the same generalized ordered probit model on the Expedia reviews of these same hotels that respond on TripAdvisor. Here, we set the variable After i jt to 1 for all Expedia reviews following each hotel's first management response on Trip-Advisor. We report these estimates and their associated average MPEs in the second columns of Tables <ref type="table" target="#tab_0">13  and 14</ref>. As expected, we find no change in the Expedia ratings of responding hotels following their adoption of management responses on TripAdvisor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Why Do Management Responses Affect Hotel Ratings?</head><p>In this section, we investigate the mechanism underlying our findings. We argue that management responses can improve hotel ratings because they increase the cost of leaving a negative review while making it more worthwhile to leave a positive one. Intuitively, the cost of negative reviews increases because when hotels respond, consumers feel that their reviews will be closely scrutinized. Therefore, consumers become less likely to submit low-quality negative reviews. On the other hand, consumers considering leaving a positive review likely appreciate the hotel reading their review and responding to them. Therefore, hotel guests are more likely to submit a positive review when hotels take note of their feedback.</p><p>To empirically support this argument, we analyze the impact of management responses on review volume, review length, and the types of reviewers a hotel attracts. Beyond helping us understand the mechanism underlying our findings, these analyses yield insights on managerially relevant variables other than star ratings.</p><p>Our first finding is that the length of negative reviews tends to increase after hotels begin responding. To arrive at this result, we employ the same crossplatform DD strategy used in Section 4.1. Thus, we estimate Equation (1), but using the review length (measured in characters) as the dependent variable. Negative reviews on TripAdvisor are, on average, longer than positive reviews. Therefore, we separately estimate the impact of management responses on review length for each star rating and report these results in columns (2)-( <ref type="formula" target="#formula_8">6</ref>) of Table <ref type="table" target="#tab_0">15</ref>. Because the average TripAdvisor rating of responding hotels is 3.8 stars, we define negative reviews as those with 1, 2, or 3 stars, and positive reviews as those with 4 or 5 stars. We find that reviewers leave 1-and 2-star reviews that are approximately 10% longer after hotels begin responding. The impact on 3-star reviews is smaller, while the length of positive reviews remains unchanged. Thus, we find that hotel managers who consider responding to reviews face an interesting trade-off: by responding they can increase their average star rating at the cost of receiving longer, and therefore more detailed, negative reviews.</p><p>This finding can also help us explain why management responses increase hotel ratings. Hotel guests feel <ref type="bibr">Marketing Science, 2017</ref><ref type="bibr">, vol. 36, no. 5, pp. 645-665, © 2017</ref> the need to leave longer and more detailed reviews when they believe that hotel managers will scrutinize their comments and publicly respond. For some guests, writing a longer and more detailed negative review will be worth the time and effort. Others, however, will not be motivated to expend this extra effort, and instead will opt for not leaving any review at all. In other words, management responses increase the cost of writing a negative review.</p><p>Second, we find that following a hotel's decision to begin responding, total review volume increases. Since, on average, ratings also increase, these extra reviews are mostly positive. Again, we estimate the impact on review volume using the cross-platform DD strategy (Equation ( <ref type="formula">1</ref>)). Specifically, we estimate the percentage change in the number of reviews a hotel receives after it begins responding on TripAdvisor, relative to percentage increases on Expedia over the same period of time. To do so, we first aggregate our data at the hotel-month level. Then, our dependent variable is log(Review count jt) ), i.e., the logarithm of the number of reviews hotel j received in month t. As before, we cluster errors at the hotel level. We report these results in the first column of Table <ref type="table" target="#tab_0">15</ref>. We find that the number of reviews a hotel receives increases by 12% following its decision to begin responding. 12 Why does review volume increase? We believe that positive reviewers who might have otherwise not left a review are more willing to provide feedback when the hotel has signaled that it is listening. We also point out that, all else equal, an increased number of reviews is a desirable outcome because it is often interpreted as a sign of hotel popularity and, thus, quality.</p><p>Third, we argue that if there is an increased benefit of leaving positive reviews when hotels respond, then reviewers who are inherently more positive should  <ref type="formula">1</ref>) is the log of the number of reviews of hotel j at time t. The dependent variable in columns ( <ref type="formula" target="#formula_3">2</ref>)-( <ref type="formula" target="#formula_8">6</ref>) is the length of review i of hotel j at time t. Cluster-robust t-statistics (at the individual hotel level) are shown in parentheses. All specifications include time fixed effects and platform-specific linear time trends. * p &lt; 0.1; * * * p &lt; 0.01.</p><p>review the hotel more often. We define an inherently positive reviewer as someone who tends to leave more positive reviews than the average TripAdvisor reviewer, whether a firm responds or not. To show that responding hotels attract more inherently positive reviewers, we begin with the observation that ratings can be decomposed into three components: a reviewer fixed effect θ k that captures how positive a reviewer is on average; a hotel fixed effect η j that captures the average quality of hotel j; and an idiosyncratic shock, jk . <ref type="bibr">13</ref> Then, the rating of reviewer k for business j is given by</p><formula xml:id="formula_14">Stars jk θ k + η j + jk .<label>(9)</label></formula><p>We estimate the reviewer fixed effects θ k based on a holdout set of reviews that contains each reviewer's entire TripAdvisor review history excluding reviews for responding hotels.</p><p>Then, to test the hypothesis that when hotels start responding they attract reviewers who are inherently more positive, we estimate the following model using the TripAdvisor reviews of both responding and nonresponding hotels:</p><formula xml:id="formula_15">Reviewer type i jt βAfter i jt + η j + τ t + jk .<label>(10)</label></formula><p>Here, the dependent variable Reviewer type i jt is the value of θ k associated with the reviewer who wrote review i for hotel j (as estimated using Equation ( <ref type="formula" target="#formula_14">9</ref>)). The variable After i jt is an indicator for reviews submitted after hotel j starts responding. The coefficient of interest, β, captures changes in reviewer positivity after hotels start responding. To further limit the influence of unobserved transient factors that could affect reviewer selection, we limit our estimation sample to one year before and one year after the treatment, since any two reviewers are more likely to be comparable in their unobserved characteristics if their reviews are closer in time. We present our results in Table <ref type="table" target="#tab_0">16</ref>. We find that reviewers who leave reviews after hotels start responding are, on average, 0.04 stars more positive than reviewers who left reviews prior to the adoption of management responses. A robustness check using a six-month bandwidth (BW), shown in the second column of Table <ref type="table" target="#tab_0">16</ref>, yields similar results. This finding further supports the idea that management responses directly affect reviewer selection: once hotels start responding, they attract reviewers who are inherently more positive in their evaluations, regardless of whether hotels respond or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Management Responses and</head><p>Retaliatory Reviewing We briefly highlight a theoretical connection between our results and the literature on retaliation in bilateral review platforms. A number of field and lab studies <ref type="bibr" target="#b23">(Resnick and Zeckhauser 2002</ref><ref type="bibr" target="#b6">, Dellarocas and Wood 2008</ref><ref type="bibr" target="#b3">, Bolton et al. 2013</ref> have shown that in settings where agents can sequentially rate each other, negative ratings are underreported because of a fear of retaliation. The primary example of this phenomenon is eBay. Up to 2008, during which time eBay buyers and sellers could rate each other, buyers with a poor experience would often avoid leaving a negative review for a seller for fear that the seller would also follow up with a negative review. When eBay introduced new rules that removed the option for sellers to leave negative feedback for buyers, sellers started receiving an increased number of negative reviews <ref type="bibr" target="#b13">(Hui et al. 2016)</ref>. More recently, Airbnb has faced similar issues <ref type="bibr" target="#b9">(Fradkin et al. 2014</ref><ref type="bibr" target="#b28">, Zervas et al. 2015</ref>.</p><p>Here, we draw a parallel between management responses and bilateral reviewing: hotels can "retaliate" negative reviews by disputing a reviewer's claims in a management response, which in turn may discourage future guests with a negative experience from leaving a review altogether. This behavior can shift reviewer selection toward reviewers with higher ratings and, on average, improve the ratings of responding hotels. A limitation of using the retaliation theory to explain our findings is that, unlike in a bilateral review platform, TripAdvisor does not allow hotels to rate their guests, which would visibly harm the guests' online reputation. Thus, the main risk a reviewer faces in leaving a negative TripAdvisor review is primarily psychological. While the direct economic consequences of an antagonistic management response are not clear, some existing research <ref type="bibr" target="#b22">(Ockenfels et al. 2012)</ref> suggests that consumers place more value on their online reputation than economic incentives alone would predict. For instance, the threat of an antagonistic management response may incur social and emotional costs that can affect a reviewer's decision to leave a negative review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Other Mechanisms to Explain Our Findings</head><p>A change in reviewing costs is not the only potential explanation for our results. Here, we briefly discuss a second mechanism that could in principle explain our findings, but find limited evidence to back it up. Drawing from the service failure and recovery literature (e.g., <ref type="bibr" target="#b26">Tax et al. 1998</ref><ref type="bibr" target="#b20">, McCollough et al. 2000</ref>, we hypothesize that management responses encourage consumers who left negative reviews to return, give hotels a second try, and possibly leave a fresh, positive review. We find some limited evidence for this hypothesis in our data, which we present in detail in the online appendix. However, the number of reviews by returning consumers is too small (1.3% of all TripAdvisor reviews) to adequately explain the increase in ratings of responding hotels. As the number of reviews by returning consumers grows, this will be a hypothesis worth revisiting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Managerial Implications and Conclusion</head><p>In this paper, we show that management responses are an effective way for firms to improve their online reputation. We study the Texas hotel industry, and we show that, on average, responding hotels see a 0.12star increase in their TripAdvisor ratings when they begin responding to reviewers. To explain this finding, we hypothesize that management responses increase the cost of leaving a negative review, while decreasing the cost of leaving a positive one. We empirically support this hypothesis by showing that following the adoption of management responses, negative reviews become longer (i.e., costlier to produce), overall review volume increases, and hotels attract reviewers who are inherently more positive in their evaluations.</p><p>Our findings have economic and managerial implications for hotels, consumers, and review platforms. As far as hotels are concerned, our results indicate that management responses are an effective reputation management strategy. Furthermore, this strategy is sanctioned by review platforms, and it can <ref type="bibr">Marketing Science, 2017</ref><ref type="bibr">, vol. 36, no. 5, pp. 645-665, © 2017</ref> directly impact the financial performance of firms that use it <ref type="bibr" target="#b17">(Luca 2011)</ref>. One downside of responding is that hotels are more likely to attract fewer but more detailed negative reviews from guests who are trying harder to substantiate their complaints, knowing that hotels will scrutinize their feedback. This highlights an interesting trade-off for managers. Our own experience as consumers, often focusing on reading negative reviews first, suggests that the risks in longer negative reviews may in some instances outweigh the benefits of increased ratings. Quantifying these trade-offs is an interesting area for future research.</p><p>A limitation to the conclusion that management responses can help firms improve their ratings is that in our work, we do not estimate the impact of using management responses for a randomly chosen hotel; i.e., we estimate an ATT instead of an ATE. Despite this limitation, we see two significant implications that we can draw from the ATT. First, our work informs hotels that are currently responding to reviews about the effects of management responses on their reputations, an effect they may not have been aware of. Second, even though the treatment effect could be significantly different for hotels that do not currently respond, we speculate that this is unlikely to be the case. Our analysis indicates that the primary driver of improved reputation is a change in reviewer behavior rather than any particular hotel characteristic. Furthermore, in many instances, responding and nonresponding hotels are highly similar: we can match approximately 25% of nonresponding chains to a responding chain with the same affiliation in the same city. For instance, while Americas Best Value Inn at 3243 Merrifield Avenue, Dallas currently responds, the Americas Best Value Inn at 4154 Preferred Place, Dallas does not. This is an example where we might expect the impact of management responses to be similar for the two hotels. Therefore, even though our results should not be taken as a definite prescription for improving a firm's online reputation, we think that management responses are a promising reputation management strategy even for hotels that are not currently responding.</p><p>The benefits of management responses for consumers and review platforms are less obvious. On one hand, by opening up a communication channel to consumers, review platforms encourage hotels to engage with their guests, to inform future visitors of steps they have taken to correct issues reported in prior reviews, and to create a richer information environment that should in principle help consumers make better choices. Furthermore, as we have shown, management responses encourage review creation. Therefore, management responses can help review platforms grow the size of their review collections, which is a metric review platform commonly used to evaluate their success. On the other hand, our work shows that management responses have the undesired consequence of negative review underreporting, which positively biases the ratings of responding hotels. This is a downside for review platforms striving to maintain unbiased ratings and for consumers who might be misled.</p><p>Our results also have implications for review platforms that do not allow responding, or for platforms like Expedia on which hotels tend not to respond. As we have shown, management responses lead to more reviews. Yet, where do these reviews come from? One possibility is that reviewers who would not have otherwise left a review now choose to leave one. A more intriguing hypothesis is that management responses result in cross-platform substitution: reviewers migrate from platforms that do now allow management responses to platforms that allow management responses because their reviews are more likely to have an impact on the latter. Fully understanding the mechanism that drives review volume increases following the adoption of management responses is an interesting open question.</p><p>Taken together, our results highlight an information design problem: how can review platforms enable the interaction of firms and consumers without introducing reporting bias? While it is beyond the scope of this work to provide an exhaustive list of alternative designs, other practical schemes to consider include responding to consumers privately and management responses that are not attached to specific reviews.</p><p>Our results can be extended in various ways. For instance, managers who are considering responding to consumer reviews face a complex decision problem that involves choosing which reviews to respond to, when to respond to them, and how to respond. Future work can combine econometric methods with natural language-processing techniques to estimate heterogeneous treatment effects arising from the various ways businesses handle praise and complaints. Such analyses can yield prescriptive guidelines for managers looking to communicate with consumers in different customer service scenarios. A randomized field experiment to measure differences between the ATT and the ATE would be another interesting extension of our work. Such an experiment would help us understand if management responses work better for some firms than they do for others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Within-Platform Identification Relies on the Reviews of Hotel A But Not Hotel B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The Cumulative Percentage of Reviews with a Response by Year</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Average Lag (in Days) Between a TripAdvisor Review and Its Management Response by Review Submission Year</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure</head><label>4</label><figDesc>Figure 4. The Fraction of TripAdvisor Reviews That Carry a Response by Star Rating</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The Evolution of Treatment Effects: Differences in Hotel Ratings Between Expedia and TripAdvisor, as a Function of a Hotel's Decision to Begin Responding to Reviews</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Identifying the Impact of Management Responses by Exploiting Variation in the Likelihood of Reading a Management Response</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overview of the Main Identification Strategies and Robustness Checks We Perform</figDesc><table><row><cell>Data used</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Hotel Summary Statistics</figDesc><table><row><cell></cell><cell>TripAdvisor</cell><cell>Expedia</cell></row><row><cell>Matched hotels</cell><cell></cell><cell></cell></row><row><cell>Avg. hotel rating</cell><cell>3.6</cell><cell>3.9</cell></row><row><cell>Reviews per hotel</cell><cell>84.3</cell><cell>157.8</cell></row><row><cell>Responses per hotel</cell><cell>27.8</cell><cell>3.6</cell></row><row><cell>Avg. review length</cell><cell>617.0</cell><cell>201.0</cell></row><row><cell>Avg. response length</cell><cell>439.2</cell><cell>306.5</cell></row><row><cell>Matched hotels that respond</cell><cell></cell><cell></cell></row><row><cell>on TripAdvisor</cell><cell></cell><cell></cell></row><row><cell>Avg. hotel rating</cell><cell>3.8</cell><cell>4.1</cell></row><row><cell>Reviews per hotel</cell><cell>107.4</cell><cell>183.7</cell></row><row><cell>Responses per hotel</cell><cell>40.9</cell><cell>5.0</cell></row><row><cell>Avg. review length</cell><cell>624.3</cell><cell>200.2</cell></row><row><cell>Avg. response length</cell><cell>439.2</cell><cell>307.2</cell></row><row><cell>Matched hotels that do not respond</cell><cell></cell><cell></cell></row><row><cell>on TripAdvisor</cell><cell></cell><cell></cell></row><row><cell>Avg. hotel rating</cell><cell>3.3</cell><cell>3.6</cell></row><row><cell>Reviews per hotel</cell><cell>35.4</cell><cell>95.7</cell></row><row><cell>Responses per hotel</cell><cell>-</cell><cell>0.2</cell></row><row><cell>Avg. review length</cell><cell>601.3</cell><cell>203.0</cell></row><row><cell>Avg. response length</cell><cell>-</cell><cell>291.6</cell></row><row><cell cols="3">Note. A matched hotel is one that exists on both TripAdvisor and</cell></row><row><cell>Expedia.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Data Set Description</figDesc><table><row><cell>TripAdvisor Expedia</cell></row></table><note>a Matched responding hotels that are reviewed on both platforms, excluding hotels that respond on Expedia.b Matched hotels that are reviewed on both platforms, excluding hotels that respond on Expedia.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Cross-Platform DD    </figDesc><table><row><cell></cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell></row><row><cell>After × TripAdvisor</cell><cell>0.149  *  *  *</cell><cell>0.123  *  *  *</cell><cell>0.097  *  *  *</cell></row><row><cell></cell><cell>(7.21)</cell><cell>(5.49)</cell><cell>(5.20)</cell></row><row><cell>TripAdvisor</cell><cell>−1.006  *  *  *</cell><cell>−1.027  *  *  *</cell><cell>−0.803  *  *  *</cell></row><row><cell></cell><cell>(−20.38)</cell><cell>(−20.21)</cell><cell>(−18.31)</cell></row><row><cell>After</cell><cell>−0.005</cell><cell>−0.012</cell><cell>−0.003</cell></row><row><cell></cell><cell>(−0.45)</cell><cell>(−0.91)</cell><cell>(−0.24)</cell></row><row><cell>Avg. rating</cell><cell></cell><cell></cell><cell>0.288  *  *  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(26.53)</cell></row><row><cell>Log review count</cell><cell></cell><cell></cell><cell>−0.003</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(−0.69)</cell></row><row><cell>Ashenfelter's dip correction</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>429,956</cell><cell>415,361</cell><cell>411,993</cell></row><row><cell>R 2 within</cell><cell>0.020</cell><cell>0.020</cell><cell>0.024</cell></row></table><note>Notes. The dependent variable is rating i of hotel j at time t. Cluster-robust t-statistics (at the individual hotel level) are shown in parentheses. All specifications include time fixed effects and platform-specific linear time trends.* * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Cross-Platform DD by Traveler Segment The dependent variable is rating i of hotel j at time t. Clusterrobust t-statistics (at the individual hotel level) are shown in parentheses. All specifications include time fixed effects and platformspecific linear time trends.</figDesc><table><row><cell></cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell></row><row><cell></cell><cell>Business</cell><cell>Couples</cell><cell>Families</cell><cell>Friends</cell></row><row><cell>After × TripAdvisor</cell><cell>0.093  *  *</cell><cell>0.176  *  *  *</cell><cell>0.104  *  *  *</cell><cell>0.111  *</cell></row><row><cell></cell><cell>(2.43)</cell><cell>(4.54)</cell><cell>(2.71)</cell><cell>(1.74)</cell></row><row><cell>TripAdvisor</cell><cell>−0.846  *  *  *</cell><cell>−0.520  *  *  *</cell><cell>−1.223  *  *  *</cell><cell>−0.695</cell></row><row><cell></cell><cell>(−4.19)</cell><cell>(−3.56)</cell><cell>(−7.99)</cell><cell>(−1.28)</cell></row><row><cell>After</cell><cell>0.005</cell><cell>−0.066  *</cell><cell>−0.025</cell><cell>0.019</cell></row><row><cell></cell><cell>(0.15)</cell><cell>(−1.88)</cell><cell>(−0.78)</cell><cell>(0.30)</cell></row><row><cell>Ashenfelter's dip</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>correction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N</cell><cell>59,886</cell><cell>41,126</cell><cell>62,282</cell><cell>14,787</cell></row><row><cell>R 2 within</cell><cell>0.0068</cell><cell>0.016</cell><cell>0.017</cell><cell>0.021</cell></row><row><cell>Notes.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* p &lt; 0.1; * * p &lt; 0.05; * * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Cross</figDesc><table><row><cell cols="3">-Platform DD Using Management Response</cell></row><row><cell>Visibility as a Treatment Indicator</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(1)</cell><cell>(2)</cell></row><row><cell>After × TripAdvisor</cell><cell>0.101  *  *  *</cell><cell>0.100  *  *  *</cell></row><row><cell></cell><cell>(4.00)</cell><cell>(3.92)</cell></row><row><cell>After × TripAdvisor</cell><cell>0.084  *  *  *</cell><cell>0.062  *  *  *</cell></row><row><cell>× Pct. page 1 responded</cell><cell>(4.08)</cell><cell>(2.76)</cell></row><row><cell>After × TripAdvisor</cell><cell></cell><cell>0.013</cell></row><row><cell>× Pct. page 2 responded</cell><cell></cell><cell>(1.34)</cell></row><row><cell>TripAdvisor</cell><cell>−1.014  *  *  *</cell><cell>−1.012  *  *  *</cell></row><row><cell cols="2">(−19.95)</cell><cell>(−19.91)</cell></row><row><cell>After</cell><cell>−0.013</cell><cell>−0.012</cell></row><row><cell cols="2">(−0.99)</cell><cell>(−0.97)</cell></row><row><cell>Ashenfelter's dip correction</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>415,361</cell><cell>415,361</cell></row><row><cell>R 2 within</cell><cell>0.020</cell><cell>0.020</cell></row></table><note>Notes. The dependent variable is rating i of hotel j at time t. Clusterrobust t-statistics (at the individual hotel level) are shown in parentheses. All specifications include time fixed effects and platformspecific linear time trends.* * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Cross-Platform DD Using Management Response Visibility as a Treatment Indicator Only with Reviews Submitted After Each Hotel's First Management Response</figDesc><table><row><cell></cell><cell>(1)</cell><cell>(2)</cell></row><row><cell>TripAdvisor</cell><cell>−0.789  *  *  *</cell><cell>−0.786  *  *  *</cell></row><row><cell></cell><cell>(−7.71)</cell><cell>(−7.69)</cell></row><row><cell>TripAdvisor × Pct. page 1 responded</cell><cell>0.071  *  *  *</cell><cell>0.056  *  *</cell></row><row><cell></cell><cell>(3.66)</cell><cell>(2.52)</cell></row><row><cell>TripAdvisor × Pct. page 2 responded</cell><cell></cell><cell>0.009</cell></row><row><cell></cell><cell></cell><cell>(0.95)</cell></row><row><cell>Ashenfelter's dip correction</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>274,200</cell><cell>274,200</cell></row><row><cell>R 2 within</cell><cell>0.0097</cell><cell>0.0097</cell></row><row><cell cols="3">Notes. The dependent variable is rating i of hotel j at time t. Cluster-</cell></row><row><cell cols="3">robust t-statistics (at the individual hotel level) are shown in paren-</cell></row><row><cell cols="3">theses. All specifications include time fixed effects and platform-</cell></row><row><cell>specific linear time trends.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>* * p &lt; 0.05; * * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Cross-Platform DD Robustness Checks for Fake Reviews: ATT by Hotel Affiliation and Pretreatment Review Volume</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>(3)</cell></row><row><cell></cell><cell>(1)</cell><cell>(2)</cell><cell>By review</cell></row><row><cell></cell><cell>Nonchain</cell><cell>Chain</cell><cell>volume</cell></row><row><cell>After × TripAdvisor</cell><cell>0.195  *  *  *</cell><cell>0.104  *  *  *</cell><cell>0.126  *  *  *</cell></row><row><cell></cell><cell>(2.65)</cell><cell>(5.29)</cell><cell>(5.33)</cell></row><row><cell>TripAdvisor</cell><cell>−1.016  *  *  *</cell><cell>−1.043  *  *  *</cell><cell>−1.026  *  *  *</cell></row><row><cell></cell><cell>(−7.61)</cell><cell>(−21.10)</cell><cell>(−19.70)</cell></row><row><cell>After</cell><cell>−0.032</cell><cell>−0.009</cell><cell>−0.011</cell></row><row><cell></cell><cell>(−0.74)</cell><cell>(−0.68)</cell><cell>(−0.86)</cell></row><row><cell>After × TripAdvisor</cell><cell></cell><cell></cell><cell>−0.000</cell></row><row><cell>× Pretreatment</cell><cell></cell><cell></cell><cell>(−0.69)</cell></row><row><cell>num. reviews</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ashenfelter's dip correction</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>65,902</cell><cell>349,459</cell><cell>404,231</cell></row><row><cell>R 2 within</cell><cell>0.020</cell><cell>0.020</cell><cell>0.020</cell></row></table><note>Notes. The dependent variable is rating i of hotel j at time t. Clusterrobust t-statistics (at the individual hotel level) are shown in parentheses. All specifications include time fixed effects and platformspecific linear time trends.* * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Cross-Platform DDD    </figDesc><table><row><cell>(1)</cell><cell>(2)</cell></row></table><note>Notes. The dependent variable is rating i of hotel j at time t. Clusterrobust t-statistics (at the individual hotel level) are shown in parentheses. All specifications include time fixed effects and platformspecific linear time trends.* * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc>displays upper bounds</figDesc><table><row><cell cols="2">Table 10. Rosenbaum Bounds for</cell></row><row><cell>Cross-Platform DD</cell><cell></cell></row><row><cell>Sensitivity parameter</cell><cell>Maximum significance level</cell></row><row><cell>Γ</cell><cell>p max</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc>Within-Platform Identification: Comparing the TripAdvisor Ratings of Travelers Who Stayed at the Same Hotel in the Same Month</figDesc><table><row><cell></cell><cell>(1)</cell><cell>(2)</cell></row><row><cell>After</cell><cell>0.276  *  *  *</cell><cell>0.121  *  *</cell></row><row><cell></cell><cell>(8.02)</cell><cell>(1.97)</cell></row><row><cell>Time between review and stay</cell><cell>0.037  *  *  *</cell><cell>0.037  *  *  *</cell></row><row><cell></cell><cell>(4.56)</cell><cell>(4.54)</cell></row><row><cell>Time between review and stay 2</cell><cell>−0.001  *  *  *</cell><cell>−0.001  *  *  *</cell></row><row><cell></cell><cell>(−4.15)</cell><cell>(−4.26)</cell></row><row><cell>Ashenfelter's dip correction</cell><cell>No</cell><cell>Yes</cell></row><row><cell>N</cell><cell>308,261</cell><cell>299,295</cell></row><row><cell>R 2 within</cell><cell>0.0029</cell><cell>0.0025</cell></row><row><cell cols="3">Notes. The dependent variable is rating i of hotel j at time t. Cluster-</cell></row><row><cell cols="3">robust t-statistics (at the individual hotel-month level) are shown</cell></row><row><cell cols="3">in parentheses. All specifications include hotel-month-of-stay fixed</cell></row><row><cell cols="3">effects, time fixed effects, and treatment-status-specific linear time</cell></row><row><cell>trends.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>* * p &lt; 0.05; * * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Within</figDesc><table><row><cell cols="2">-Platform Identification Using</cell><cell></cell></row><row><cell cols="3">Management Response Visibility as the Treatment Indicator</cell></row><row><cell></cell><cell>(1)</cell><cell>(2)</cell></row><row><cell>After</cell><cell>0.101</cell><cell>0.099</cell></row><row><cell></cell><cell>(1.64)</cell><cell>(1.61)</cell></row><row><cell>After × Pct. page 1 responded</cell><cell>0.067  *  *  *</cell><cell>0.056  *  *  *</cell></row><row><cell></cell><cell>(3.89)</cell><cell>(2.63)</cell></row><row><cell>After × Pct. page 2 responded</cell><cell></cell><cell>0.010</cell></row><row><cell></cell><cell></cell><cell>(0.89)</cell></row><row><cell>Time between review and stay</cell><cell>0.038  *  *  *</cell><cell>0.038  *  *  *</cell></row><row><cell></cell><cell>(4.55)</cell><cell>(4.56)</cell></row><row><cell>Time between review and stay 2</cell><cell>−0.001  *  *  *</cell><cell>−0.001  *  *  *</cell></row><row><cell></cell><cell>(−4.27)</cell><cell>(−4.27)</cell></row><row><cell>Ashenfelter's dip correction</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>299,295</cell><cell>299,295</cell></row><row><cell>R 2 within</cell><cell>0.0026</cell><cell>0.0026</cell></row><row><cell cols="3">Notes. The dependent variable is rating i of hotel j at time t. Cluster-</cell></row><row><cell cols="3">robust t-statistics (at the individual hotel-month level) are shown</cell></row><row><cell cols="3">in parentheses. All specifications include hotel-month-of-stay fixed</cell></row><row><cell cols="3">effects, time fixed effects, and treatment-status-specific linear time</cell></row><row><cell>trends.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>* * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>Generalized Ordered Probit The dependent variable is rating i of hotel j at time t. Bootstrap standard errors are shown in parentheses. All specifications include year fixed effects and linear time trends.</figDesc><table><row><cell></cell><cell>(1)</cell><cell>(2)</cell></row><row><cell></cell><cell>TripAdvisor</cell><cell>Expedia</cell></row><row><cell>Threshold 1 | 2</cell><cell></cell><cell></cell></row><row><cell>After</cell><cell>0.168  *  *  *</cell><cell>0.041</cell></row><row><cell></cell><cell>(3.76)</cell><cell>(1.26)</cell></row><row><cell>Threshold 2 | 3</cell><cell></cell><cell></cell></row><row><cell>After</cell><cell>0.141  *  *  *</cell><cell>0.033</cell></row><row><cell></cell><cell>(3.46)</cell><cell>(0.99)</cell></row><row><cell>Threshold 3 | 4</cell><cell></cell><cell></cell></row><row><cell>After</cell><cell>0.145  *  *  *</cell><cell>0.022</cell></row><row><cell></cell><cell>(3.24)</cell><cell>(0.63)</cell></row><row><cell>Threshold 4 | 5</cell><cell></cell><cell></cell></row><row><cell>After</cell><cell>0.165  *  *  *</cell><cell>0.019</cell></row><row><cell></cell><cell>(3.57)</cell><cell>(0.69)</cell></row><row><cell>Ashenfelter's dip correction</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>159,772</cell><cell>255,589</cell></row><row><cell>Notes.</cell><cell></cell><cell></cell></row></table><note>* * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Average Marginal Probability Effects of</figDesc><table><row><cell>Generalized Ordered Probit</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(1)</cell><cell>(2)</cell></row><row><cell></cell><cell>TripAdvisor</cell><cell>Expedia</cell></row><row><cell>1 star</cell><cell>−0.022  *  *  *</cell><cell>−0.004</cell></row><row><cell></cell><cell>(−4.04)</cell><cell>(−1.27)</cell></row><row><cell>2 stars</cell><cell>−0.009  *  *</cell><cell>−0.002</cell></row><row><cell></cell><cell>(−2.39)</cell><cell>(−0.67)</cell></row><row><cell>3 stars</cell><cell>−0.016  *  *  *</cell><cell>−0.000</cell></row><row><cell></cell><cell>(−2.62)</cell><cell>(−0.07)</cell></row><row><cell>4 stars</cell><cell>−0.018  *  *  *</cell><cell>−0.001</cell></row><row><cell></cell><cell>(−3.03)</cell><cell>(−0.29)</cell></row><row><cell>5 stars</cell><cell>0.065  *  *  *</cell><cell>0.008</cell></row><row><cell></cell><cell>(3.57)</cell><cell>(0.69)</cell></row><row><cell>Ashenfelter's dip correction</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>159,772</cell><cell>255,589</cell></row><row><cell cols="3">Notes. Bootstrap standard errors are shown in parentheses. All spec-</cell></row><row><cell cols="2">ifications include year fixed effects and linear time trends.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>* * p &lt; 0.05; * * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 .</head><label>15</label><figDesc>The Impact of Management Responses on Reviewing Activity and Review Length</figDesc><table><row><cell></cell><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell><cell>(5)</cell><cell>(6)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Review length</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Num. reviews</cell><cell>1 star</cell><cell>2 stars</cell><cell>3 stars</cell><cell>4 stars</cell><cell>5 stars</cell></row><row><cell>After × TripAdvisor</cell><cell>0.12  *  *  *</cell><cell>88.35  *  *  *</cell><cell>93.24  *  *  *</cell><cell>47.92  *  *  *</cell><cell>19.60</cell><cell>8.28</cell></row><row><cell></cell><cell>(4.57)</cell><cell>(3.88)</cell><cell>(3.83)</cell><cell>(2.79)</cell><cell>(1.51)</cell><cell>(0.51)</cell></row><row><cell>TripAdvisor</cell><cell>−0.68  *  *  *</cell><cell>849.81  *  *  *</cell><cell>1,021.08  *  *  *</cell><cell>981.80  *  *  *</cell><cell>890.98  *  *  *</cell><cell>717.68  *  *  *</cell></row><row><cell></cell><cell>(−14.78)</cell><cell>(17.57)</cell><cell>(21.72)</cell><cell>(24.64)</cell><cell>(27.62)</cell><cell>(17.64)</cell></row><row><cell>After</cell><cell>0.01</cell><cell>−0.50</cell><cell>−10.84</cell><cell>−13.63</cell><cell>−6.13</cell><cell>−10.84  *</cell></row><row><cell></cell><cell>(0.70)</cell><cell>(−0.03)</cell><cell>(−0.87)</cell><cell>(−1.62)</cell><cell>(−1.18)</cell><cell>(−1.73)</cell></row><row><cell>Ashenfelter's dip correction</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>N</cell><cell>122,350</cell><cell>22,754</cell><cell>28,427</cell><cell>51,300</cell><cell>120,319</cell><cell>192,561</cell></row><row><cell>R 2 within</cell><cell>0.24</cell><cell>0.16</cell><cell>0.18</cell><cell>0.19</cell><cell>0.21</cell><cell>0.21</cell></row></table><note>Notes. The dependent variable in column (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 .</head><label>16</label><figDesc>Change in Reviewer Types Following a Hotel's Decision to Begin Responding The dependent variable is the reviewer type θ k associated with the consumer k who reviewed hotel j at time t. Cluster-robust t-statistics (at the individual hotel level) are shown in parentheses. All specifications include hotel fixed effects.</figDesc><table><row><cell></cell><cell>(1)</cell><cell>(2)</cell></row><row><cell></cell><cell>BW ±12 months</cell><cell>BW ±6 months</cell></row><row><cell>After</cell><cell>0.040  *  *  *</cell><cell>0.033  *  *  *</cell></row><row><cell></cell><cell>(4.47)</cell><cell>(3.06)</cell></row><row><cell>N</cell><cell>59,710</cell><cell>33,284</cell></row><row><cell>R 2 within</cell><cell>0.00061</cell><cell>0.00043</cell></row><row><cell>Notes.</cell><cell></cell><cell></cell></row></table><note>* * * p &lt; 0.01.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Frederic Brunel, John Byers, Sharon Goldberg, Michael Luca, Tim Simcoe, and Greg Stoddard for helpful comments and discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Endnotes <ref type="bibr">1</ref> A recent New York Times article suggests that hotels commonly use online reviews as a guide for renovations. See http://www .nytimes.com/2014/09/23/business/hotels-use-online-reviews-as -blueprint-for-renovations.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proserpio and Zervas: Online Reputation Management</head><p>Marketing <ref type="bibr">Science, 2017</ref><ref type="bibr">Science, , vol. 36, no. 5, pp. 645-665, © 2017</ref> INFORMS 665 2 For ease of presentation, we describe our identification strategy in terms of two periods, before and after treatment, but its extension to a setting with multiple pre and post periods is straightforward.</p><p>3 Because Expedia and Hotels.com merged their review databases in June 2013, our Expedia sample includes reviews from both sites. However, all our analyses are robust to the exclusion of Hotels.com reviews. <ref type="bibr">4</ref> We conducted separate analyses with estimation samples that include the ratings of hotels that respond on Expedia up to the point they begin responding, as well as the ratings of hotels that have only been reviewed on one of the two review platforms. Our results are not sensitive to these alternative choices of estimation sample. <ref type="bibr">5</ref> The results of a pooled regression are not meaningfully different.</p><p>6 Sensitivity tests excluding longer periods did not yield meaningfully different results. <ref type="bibr">7</ref> We thank an anonymous reviewer for this suggestion.</p><p>8 A priori, while this behavior is plausible, we think it is unlikely to persist over long periods. Presumably, once the "correction" happens, reviewers will stop inflating their ratings. 9 Even though TripAdvisor allows anyone to post a review, it tries to ensure the integrity of the content that it publishes. For more, see http://www.tripadvisor.com/vpages/review_mod_fraud_detect.html. Therefore, not every fake review that is submitted to TripAdvisor will end up being published. Similarly, even though Expedia requires that consumers have paid and stayed, review fraud is still possible: a hotel can create a fake reservation to allow it to post a fake review. <ref type="bibr">10</ref> We thank the editor-in-chief for suggesting this robustness check.</p><p>11 Interacting with the log of pretreatment responses also yields a zero coefficient. 12 A fixed-effects Poisson model gives a similar estimate.</p><p>13 <ref type="bibr" target="#b5">Dai et al. (2012)</ref> take a similar approach in deconstructing consumer ratings and demonstrate how it provides a more accurate prediction of a business' true quality.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using the longitudinal structure of earnings to estimate the effect of training programs</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename><surname>Ashenfelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Econom. Statist</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="648" to="660" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How much should we trust differences-in-differences estimates? Quart</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econom</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="275" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ordered response models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Winkelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modern Econometric Analysis</title>
				<editor>
			<persName><forename type="first">O</forename><surname>Hubler</surname></persName>
			<persName><forename type="first">J</forename><surname>Frohn</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="167" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Engineering trust: Reciprocity in the production of reputation information</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ockenfels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="285" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The effect of word of mouth on sales: Online book reviews</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mayzlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="354" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimal aggregation of consumer ratings: An application to Yelp</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Bureau of Economic Research</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The sound of silence in online feedback: Estimating trading risks in the presence of reporting bias</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dellarocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="460" to="476" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Assessing bias in the estimation of causal effects: Rosenbaum bounds on matching estimators and instrumental variables estimation with imperfect instruments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Diprete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociol. Methodology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="271" to="310" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inference with difference-in-differences and other panel data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Econom. Statist</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Fradkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Holtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pearson</surname></persName>
		</author>
		<title level="m">Reporting bias and reciprocity in online reviews: Evidence from field experiments on Airbnb. Working paper, Massachusetts Institute of Technology</title>
				<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequential and temporal dynamics of online opinion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Godes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="473" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clustered treatment assignments and sensitivity to unmeasured biases in observational studies</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">505</biblScope>
			<biblScope unit="page" from="133" to="144" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pre-programme earnings dip and the determinants of participation in a social programme. Implications for simple programme evaluation strategies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econom. J</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">457</biblScope>
			<biblScope unit="page" from="313" to="348" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reputation and regulations: Evidence from eBay</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3604" to="3616" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The labor-market returns to community college degrees, diplomas, and certificates</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jepsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Troske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Labor Econom</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="121" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-selection and information role of online product reviews</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Hitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Systems Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="456" to="474" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The welfare impact of microcredit on rural households in</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">China. J. Socio-Econom</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="404" to="411" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reviews, reputation, and revenue: The case of Yelp</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Harvard Business School; Boston</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fake it till you make it: Reputation, competition, and Yelp review fraud</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zervas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3412" to="3427" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Promotional reviews: An empirical investigation of online review manipulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mayzlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chevalier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Econom. Rev</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2421" to="2455" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An empirical investigation of customer satisfaction after service failure and recovery</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mccollough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Yadav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Service Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="137" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online product opinions: Incidence, evaluation, and evolution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Moe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schweidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="386" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Negotiating reputations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ockenfels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Croson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Oxford Handbook of Economic Conflict Resolution</title>
				<editor>
			<persName><forename type="first">G</forename><surname>Bolton</surname></persName>
			<persName><forename type="first">R</forename><surname>Croson</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="223" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Trust among strangers in Internet transactions: Empirical analysis of eBay&apos;s reputation system. Baye MR, ed. The Economics of the Internet and E-commerce</title>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zeckhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Applied Microeconomics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="127" to="157" />
			<date type="published" when="2002" />
			<publisher>Emerald Group Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<title level="m">Observational Studies</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A model of customer satisfaction with service encounters involving failure and recovery</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="356" to="372" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Customer evaluations of service complaint experiences: Implications for relationship marketing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandrashekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="76" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordinal probit: A generalization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Terza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Statist.-Theory Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A first look at online reputation on Airbnb, where every stay is above average. Working paper</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zervas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Proserpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Boston University; Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
