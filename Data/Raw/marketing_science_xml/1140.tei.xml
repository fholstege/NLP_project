<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Data Fusion with Selection Correction: An Application to Customer Base Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-02">February 2, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">Minh</forename><surname>Mccarthy</surname></persName>
							<email>daniel.mccarthy@emory.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Marketing</orgName>
								<orgName type="institution">Emory University</orgName>
								<address>
									<postCode>30322</postCode>
									<settlement>Atlanta</settlement>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elliot</forename><forename type="middle">Shin</forename><surname>Oblander</surname></persName>
							<email>eoblander23@gsb.columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Marketing Division</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Data Fusion with Selection Correction: An Application to Customer Base Analysis</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2021-02-02">February 2, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2020.1259</idno>
					<note type="submission">Received: July 23, 2019 Revised: March 17, 2020; May 17, 2020; May 28, 2020 Accepted: June 5, 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-13T12:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>data fusion</term>
					<term>selection correction</term>
					<term>customer relationship management</term>
					<term>marketing-finance interface</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The diversity of data sources has increased significantly in recent years. At last count, the website Pro-grammableWeb provides searchable access to approximately 23,000 application programming interfaces (APIs), facilitating the use of a broad array of granular data sets. Third-party data companies sell access to an equally diverse collection of data sets. Although household purchase and viewership data from companies such as Kantar, Comscore, Nielsen, and IRI are relatively popular in the academic marketing literature, there has been a groundswell of other more modern, less wellknown data sets, including credit card panel data (Second Measure), geolocation data (PlaceIQ, Mogean), clickstream data (jumpshot), email receipt data (Rakuten Intelligence), and more. These data sources supplement traditional census-level data sources, including aggregate data collected by and/or filed with governmental institutions and industry groups (e.g., the Securities and Exchange Commission (SEC), Census Bureau, and National Retail Federation).</p><p>As a result, it is increasingly common that there is more than one data set available covering a particular population of interest. Modelers may naturally want to perform analysis using a fusion of data sources, but doing so creates new methodological challenges because of the difficulty of incorporating differing granularities of data and selection bias. We propose a methodology to solve these challenges, allowing the fusing together of (1) aggregated data about the population as a whole with (2) granular data for a possibly nonrepresentative subsample of that population, when the size of the population may be very large. When a representative sample of granular data is not available, training a model on aggregated population-level data and granular data from a possibly nonrepresentative sample could allow modelers to derive the benefits of both sources-representativeness and rich individual-level visibility, respectivelywhile ameliorating their limitations.</p><p>This underlying data structure-representative but limited aggregated data and detailed but possibly nonrepresentative granular data-is an increasingly common one in marketing, economics, and finance. As mentioned previously, a large and growing number of data sets are now available to marketing researchers, increasing the range of questions these researchers can answer, subject to the availability of suitable methods.</p><p>The aggregate-disaggregate data fusion problem has previously been studied by <ref type="bibr" target="#b8">Feit et al. (2013)</ref>, who build off of prior work on Bayesian estimation of individual-level models of consumer choice using only aggregated data <ref type="bibr" target="#b5">(Chen and Yang 2007)</ref>. Similar data fusion problems have arisen in a variety of fields outside of marketing, such as fusing aggregate product market shares with household-level survey data in economics <ref type="bibr" target="#b1">(Berry et al. 2004)</ref> or fusing aggregate census demographic data with disaggregate triplevel data in transportation research <ref type="bibr" target="#b6">(Dias et al. 2019)</ref>. Despite the prevalence of this data structure across disciplines, extant literature proposing generally applicable methodologies for aggregate-disaggregate data fusion is limited; what methods have been proposed assume that there is no selection bias in the disaggregate data, do not scale to large populations, and/or require potentially arbitrary aggregation of granular data into summary statistics.</p><p>We propose a computationally scalable estimator for aggregate-disaggregate data fusion that is able to correct for selection bias in the disaggregate data. We achieve scalability by asymptotically approximating the likelihood of the observable data with a proxy likelihood function that is efficient to compute. The intuition behind our approximation is as follows. The exact likelihood of the aggregate data is generally not feasible to compute directly, making it difficult to incorporate aggregate data into a likelihood-based estimation procedure. However, for large target populations, many types of aggregate summary statisticsincluding sample averages and transformations of them-will converge to a normal distribution. Thus, we can approximate the likelihood of the aggregated data using a normal likelihood. This allows for fast approximation of the likelihood function, because the normal approximation requires only the first two moments of the aggregate statistics (analogous to common moment-based estimators), making it feasible to approximately maximize the joint likelihood of the aggregate and disaggregate data. <ref type="bibr">1</ref> Within this computational procedure, we correct for selection bias by allowing the distribution of heterogeneous characteristics (e.g. individual-level parameters in a mixture model) to differ between the individuals underlying the disaggregate data and the overall population of interest.</p><p>In sum, our contribution is to propose a general methodology for estimating a model using representative but limited aggregate data and disaggregate but possibly nonrepresentative panel data at scale to obtain the best of both worlds: richer inferences and more accurate predictions than if we had used either data source on its own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Scope and Limitations</head><p>Before discussing our specific customer base analysis use case, we first explicitly delineate the general applicability, benefits, and limitations of our method. In doing so, we illustrate not only the usefulness of the methodology beyond our specific empirical application but also the boundary conditions under which it may not be beneficial.</p><p>The specific aggregate-disaggregate data fusion problem to which our method is applicable is the estimation of an individual-level micro-statistical model for a population of interest (e.g., all households in the United States), 2 for which researchers have at their disposal a combination of (1) aggregate macro data that are representative of the entire population of interest and (2) disaggregate micro data that cover a possibly nonrepresentative subsample of that population. The aggregate data may include any summary statistic that is asymptotically normal, including but not limited to sample moments (under the central limit theorem), nonlinear transformations thereof (by the delta method), and sample quantiles. Our methodology allows for statistically and computationally efficient estimation in such settings, using both data sources jointly, while correcting for potential selection bias in the disaggregate data source.</p><p>Performing this type of data fusion can confer several advantages to researchers relative to estimating a model on only one of the two data sources:</p><p>• Compared with estimation using only limited aggregated data (e.g., customer base models estimated on aggregate customer base statistics reported to the SEC), incorporating disaggregate data provides two distinct benefits: first, for a given model specification, the added information can improve statistical precision, leading to more accurate inferences and predictions; second, additional visibility into individual-level behavior can enable researchers to estimate more complex models that would not be identifiable with the available aggregate data alone, leading to richer insights into the underlying individuallevel processes <ref type="bibr" target="#b1">(Berry et al. 2004</ref>).</p><p>• Compared with estimation using only nonrepresentative disaggregate data (e.g., consumer choice models estimated on scanner panel data), incorporating the representative aggregate data allows researchers to generalize from the disaggregate data sample to the population of interest as a whole, achieving external validity in their estimates.</p><p>Although our proposed method enables researchers to reap these benefits in many application areas, there are cases where our method may not be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</head><p>Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref> Our method introduces the ancillary problem of estimating a model of selection into the disaggregate data; this entails a tradeoff between the added information about the population conveyed through the disaggregate data and the added estimation variance introduced by the selection model. When there is severe selection bias, the size of the disaggregate data is small, and/or the aggregate data are already so rich that the model is well identified using the aggregate data alone, the incremental information gained through the disaggregate data may be small, so the costs could outweigh the benefits. Thus, our method is most likely to be beneficial in cases where models would only be weakly identified through aggregate data alone and where the disaggregate data are at least somewhat representative of the population of interest. Although our proposed method may improve identification relatively speaking, it does not guarantee strong identification in an absolute sense; indeed, in our empirical application in Section 6, model performance improves when incorporating disaggregate data, but some of the resulting standard errors are still fairly large.</p><p>Having provided an application-agnostic view of the strengths, weaknesses, and applicability of the proposed method, we narrow our focus in the next section to the specific setting to which we apply our method in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Application to Modeling Subscriber Acquisition and Retention</head><p>To frame the discussions of our model and methodology in Sections 2 and 3, we briefly describe and motivate our empirical application and the specific data structure that we encounter in it. We apply the proposed data fusion methodology to a customer base analysis problem: modeling customer acquisition and churn behavior at a subscription-based firm as if we were an external stakeholder. We analyze the quality and quantity of the customer base of the music streaming service Spotify, and how it has evolved over time.</p><p>This modeling exercise is arguably the most important step in the process of linking customer-level activity to the overall financial valuation of firms, commonly referred to as customer-based corporate valuation (CBCV). <ref type="bibr" target="#b11">Gupta et al. (2004)</ref> provided the first proof of concept for how CBCV could be implemented for publicly traded firms. A large number of papers in marketing have built on this seminal work, studying the valuation implications of firm capital structure <ref type="bibr" target="#b23">(Schulze et al. 2012)</ref>, heterogeneous customer retention <ref type="bibr" target="#b19">(McCarthy et al. 2017)</ref>, business type <ref type="bibr" target="#b18">(McCarthy and Fader 2018)</ref>, and more. Other disciplines have written papers on this topic as well, including finance <ref type="bibr" target="#b10">(Gourio and Rudanko 2014)</ref> and accounting <ref type="bibr" target="#b4">(Bonacchi et al. 2015)</ref>.</p><p>As in prior literature, we model customer acquisition and retention through a series of hazard models governing (1) the duration of time until customers are acquired and (2) how much time elapses after that before they churn. We assume that the analyst is an external stakeholder (e.g., an investor), and as such, only has access to external data sources (e.g., company data publicly disclosed through SEC filings and data from third-party providers) and not internal ones (e.g., internal customer relationship management system information). This outside-in perspective facilitates the valuation of market-based assets for investors, who ultimately determine the value of such assets through the financial markets. That said, similar data structures could arise in inside-out analyses, as we will discuss in Section 7.</p><p>There are two research gaps within extant CBCV literature that we address through this empirical application. The first gap is the range of the input data used in CBCV models. All the aforementioned papers only use aggregated customer data summaries disclosed by the companies themselves (e.g., the total number of customers acquired in a particular quarter). In addition to limiting the richness of the models we can specify, this limits analyses to firms that voluntarily disclose customer metrics on a regular basis, because public customer data disclosure is not mandatory <ref type="bibr" target="#b0">(Bayer et al. 2017)</ref>.</p><p>The second gap is the treatment of repeat acquisition and churn. Although customers who churn from a firm may be reacquired in future periods (and then churn again), none of the aforementioned papers separately model initial and repeat behaviors, because the resulting models would be difficult to identify from aggregated data alone. Repeat customer behavior is important for long-run firm outcomes, which are a primary driver of corporate valuation. As a company matures and the composition of its customer base shifts toward reacquired customers, its overall retention curve will shift from its initial retention curve to its repeat retention curve. This dynamic makes it important to know how the churn profile for newly acquired customers differs from that of reacquired customers. For instance, we show in our empirical example that Spotify's repeat retention curve is substantially higher than its initial retention curve, implying improving retention as Spotify matures. Despite the importance of capturing repeat behavior, all previous papers have been unable to separate out initial and repeat behaviors because of limited data.</p><p>As discussed in Section 1.1, these gaps are precisely those that are likely to be improved by data fusion: By supplementing limited aggregate data with rich disaggregate data, we can identify acquisition and churn models for more companies and can separate out initial and repeat behavior. Accordingly, we estimate our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</head><p>Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref> model using two data sources instead of one. As with extant literature, our first data source is a collection of aggregate summary statistics disclosed by Spotify itself through SEC filings and investor reports about its customer base. Our second data source is a credit card panel from alternative data firm Second Measure. Through this panel data, we can see monthly credit card spends at Spotify for each of approximately 3 million credit card panel members starting in January 2015. Although Second Measure's data set is large and granular, it is a nonrepresentative subsample of the overall population and covers only a part of Spotify's tenure.</p><p>Beyond Spotify, this data structure is very applicable to the CBCV use case. The aggregate summary statistics that Spotify disclosed are also disclosed by scores of other publicly traded companies, including those analyzed in prior literature. Furthermore, the credit card panel has company names associated with each purchase, making it a useful data source for all business-to-consumer companies. As such, the proposed methodology and data sources could be used for many other companies. We further discuss how this approach could be applied to other problems, both in CBCV and beyond, in Section 7.</p><p>The rest of the paper is organized as follows. We specify a model of customer acquisition and retention in Section 2. We describe our proposed methodology, with which we estimate the proposed model using aggregate and disaggregate data, in Section 3. We discuss identification of our model in Section 4 and then run a simulation study to understand the performance of the proposed methodology relative to extant approaches in Section 5. We provide an empirical analysis that applies the proposed estimation procedure to data from Spotify in Section 6 and then close with a discussion in Section 7. We include proofs of asymptotic properties, other derivations, and data in the online appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Modeling Customer Acquisition and Retention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Individual-Level Model</head><p>To model customer dynamics in a subscription-based setting, we must specify the processes by which prospective customers (prospects) are acquired and the process by which they churn. Furthermore, a single individual can cancel a subscription and subsequently resubscribe, sometimes several times, and as such, it is important to model the process by which previously churned customers are reacquired. We specify these processes at the individual level in this section and then discuss how we account for unobserved heterogeneity in the next section. For notational simplicity, we denote the initial acquisition, initial churn, repeat acquisition, and repeat churn processes as IA, IC, RA, and RC, respectively. We model time-to-acquisition and time-to-churn using a proportional hazards model with Weibull baseline hazard, a widely used duration model in customer base analysis and beyond <ref type="bibr" target="#b25">(Schweidel et al. 2008b</ref><ref type="bibr" target="#b19">, McCarthy et al. 2017</ref>. In our empirical application, the customer payment cycle is monthly, so we discretize to the monthly level. Assuming that the value of a covariate is constant within each time period, the probability of not yet having been acquired m months after becoming a prospect (analogously, not yet having churned m months after becoming a customer) is</p><formula xml:id="formula_0">S m|λ, c, β, x 1:m ( ) exp −λB m|c, β, x 1:m ( ) ( ) ,</formula><p>for m 1, 2, . . .</p><p>where</p><formula xml:id="formula_2">B m|c, β, x 1:m ( ) ∑ m t 1 t c − t − 1 ( ) c [ ] exp β x t ( )<label>(2)</label></formula><p>and x a:b indicates the set of all variables x with indices in the range a, a + 1, . . . , b (e.g., x 1:3 represents {x 1 , x 2 , x 3 }). Here, λ &gt; 0 is a scale parameter, c &gt; 0 is a shape parameter, β is a vector of regression coefficients, and x t is a vector of covariates for month t. <ref type="bibr">3</ref> We allow each process to have distinct parameters: For example, there are four rate parameters λ (IA) , λ <ref type="bibr">(IC)</ref> , λ <ref type="bibr">(RA)</ref> , and λ (RC) (likewise for β and c). In our empirical application, we use a vector of quarterly dummies as x t for each of the four processes to account for seasonality. Following prior literature <ref type="bibr" target="#b11">(Gupta et al. 2004</ref><ref type="bibr" target="#b19">, McCarthy et al. 2017</ref>, we also allow for zero-inflation in the IA and RA processes by introducing intermediate Bernoulli filters: A prospect will only ever be acquired at all with probability π (IA) , and after churning, a prospect will only ever be reacquired with probability π <ref type="bibr">(RA)</ref> . As in previous CBCV literature, we assume the size of the prospect pools are known in advance as they would not be separately identifiable from π (IA) and π <ref type="bibr">(RA)</ref> .</p><p>As discussed in Section 1, prior customer base analysis literature based on aggregated data has not separately modeled initial and repeat customer behavior. Incorporating granular panel data through data fusion allows us to separate out the two processes to understand how the behavioral patterns of repeat customers differ from those of first-time customers. We summarize the full individual-level model visually in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Parameter Heterogeneity</head><p>Consistent with prior literature, we incorporate unobserved heterogeneity by allowing the rate parameter λ in each process to vary across individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</head><p>Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref> Oftentimes a gamma heterogeneity distribution is used because of its conjugacy with the Weibull distribution <ref type="bibr" target="#b25">(Schweidel et al. 2008b</ref>). In our model, we have a four-dimensional vector of rate parameters</p><formula xml:id="formula_3">λ i (λ (IA) i , λ (IC) i , λ (RA) i , λ (RC) i</formula><p>) . Although we could use independent gamma distributions for each parameter to retain conjugacy, we would also like to capture possible correlations between parameters: For instance, a positive correlation between λ (IA) and λ <ref type="bibr">(IC)</ref> would indicate that early adopters also tend to be early abandoners, a pattern that cannot be captured by independent gamma distributions. Instead, we assume that each individual's rate parameter vector λ i is drawn from a multivariate lognormal distribution 4 :</p><formula xml:id="formula_4">log λ i ( ) ∼ N log λ 0 ( ), Σ λ ( ) .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Panel Selection</head><p>There may be selection bias that skews the distribution of behavioral patterns in the panel. We do not know the mechanism through which each individual i is selected into the panel, but without loss of generality, we can denote it by P(Z i 1|ξ i ) f (ξ i ) for some function f , where ξ i is a vector of individual-level (possibly unobserved) characteristics on which individuals are selected into the panel. If the selection mechanism and the acquisition/churn processes are dependent (i.e., ξ i ⊥ ⊥ Y i , where Y i is a random vector representing an individual i's acquisition and churn outcomes), then the selection process is nonignorable and must be corrected, or else it will skew parameter estimates and cause the inferences from the panel to not generalize to the population <ref type="bibr" target="#b15">(Little and Rubin 2019)</ref>. We model the selection mechanism jointly with Y i to correct for this bias in the panel.</p><p>In our setting, it is reasonable to assume that ξ i includes only ex ante heterogeneous characteristics, such that individuals are not directly selected into the credit card panel by their ex post behavior Y i ; by definition, selection occurs before any granular behavior is observed in the panel. Hence, we assume the conditional independence ξ i ⊥ ⊥ Y i |λ i holds. This assumption allows for the possibility that, for instance, wealthier customers may be more likely to be selected into a credit card panel (higher P(Z i 1)) and thus be more likely to sign up for Spotify (higher λ (IA) i ), but assumes that their selection is not based on whether they actually sign up for Spotify (Y i ). Thus, we model the probability that individual i is selected into the panel as a logistic regression on log(λ i ):</p><formula xml:id="formula_5">f λ i ( ) : P Z i 1|λ i ( ) Logit −1 β Z ( ) 0 + β Z ( ) ( ) log λ i (<label>)</label></formula><formula xml:id="formula_6">( ) .<label>(4)</label></formula><p>Our assumed specification,f (λ i ), can be seen as a reduced-form approximation to the true selection mechanism f (ξ i ): ξ i is unobserved but only introduces nonignorable selection bias via dependency with λ i ; thus, modeling the selection mechanism throughf (λ i ) controls for selection bias by indirectly controlling for dependency between ξ i and Y i . The estimates of β <ref type="bibr">(Z)</ref> allow us to infer whether, for example, panel members are more or less prone to churning than members of the target population as a whole. In our setting, we do not have any observed covariates about panel members, but it would be straightforward to extend the selection function to also include data on observed characteristics. <ref type="bibr">5</ref> It is not immediately obvious that a model incorporating selection based on latent variables would be empirically identified. A key property in our context that allows for identification without needing to rely on distributional assumptions is that we have two sources of data about Y i : We have panel data, which may be contaminated by sample selection bias, and aggregate data, which covers the full target population. Intuitively, identifying the selection mechanism amounts to identifying the discrepancies between the panel and aggregate data in the periods they overlap. We formalize this intuition in Section 4.</p><p>Our approach for correcting for selection bias is analogous to the approaches used by <ref type="bibr" target="#b17">Manchanda et al. (2004)</ref>, <ref type="bibr" target="#b29">Van Diepen et al. (2009)</ref>, and Schweidel and Notes. Weibull + PH(λ, c, β) is shorthand notation for a proportional hazards model with a Weibull(λ, c) baseline hazard and β is a vector of covariate coefficients, discretized to the monthly level via differencing of the cumulative distribution function. The Bernoulli processes determine whether a customer is ever (re)acquired, whereas the respective Weibull processes determine time until (re)acquisition, given that (re)acquisition occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</head><p>Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref><ref type="bibr">INFORMS Knox (2013</ref>, who correct for nonrandom targeting of direct marketing by modeling targeting as a function of unobserved response heterogeneity. <ref type="bibr" target="#b27">Schweidel and Moe (2014)</ref> use a similar approach to model consumer self-selection into posting on different online platforms.</p><p>The individual-level model, the heterogeneity distribution, and the panel selection mechanism jointly form our model specification; the full data generating process underlying our model specification is summarized in Online Appendix 1. The model parameters are <ref type="bibr">RA)</ref> , and c (p) , β (p) for p ∈ {IA, IC, RA, RC}. <ref type="bibr">6</ref> We will refer to the concatenation of all of these parameters as θ.</p><formula xml:id="formula_7">λ 0 , Σ λ , β (Z) 0 , β (Z) , π (IA) , π<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Estimation Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Observable Data</head><p>As discussed in Section 1.2, we do not observe all individual-level acquisition and churn data in our setting; instead, we observe some summary statistics aggregated across all individuals i and individual-level data for all panel members. Here, we formally define the observable panel data and aggregated summary statistics.</p><p>For this exposition, it is convenient to re-encode the acquisition and churn time outcomes into a series of binary random variables. Define IA im ∈ {0, 1} as a binary random variable equal to 1 if individual i is initially acquired in month m and takes value 0 otherwise. Define IC im , RA im , and RC im analogously. Then, for a company that has been in commercial operations for M months, each individual's outcome up to month M can be represented as a 4M-length vector of binary random variables, which we will call Y i :</p><formula xml:id="formula_8">Y i IA i1 , . . . , IA iM , IC i1 , . . . , IC iM , RA i1 , . . . , RA iM , ( RC i1 , . . . , RC iM ).</formula><p>(5)</p><p>First, we characterize the panel data. Through it, we implicitly observe for each individual a binary variable Z i equal to 1 if individual i was selected into the panel and 0 if not. Conditional on individual i being in the panel, we observe a left-truncated version of Y i : that is, we observe all activity for individuals who are initially acquired after the panel is established, but do not observe any activity for individuals who are initially acquired before the panel is established.</p><p>Denoting m * as the starting month of panel data, the observable panel data, which we will callỸ i , for an individual i who is in the panel, is as follows:</p><formula xml:id="formula_9">Y i |Z i 1 ( ) IA im * , . . . , IA iM , IC im * , . . . , IC iM , ( RA im * , . . . , RA iM , RC im * , . . . , RC iM ) .<label>(6)</label></formula><p>As such, the length of the observation period in the panel data are M − m * +1 months. If individual i was not selected into the panel, then we observe no panel data for that individual: that is, (Ỹ i |Z i 0) ∅. In our empirical context, observations begin for all credit card panel members at the same time, but this exposition could easily be generalized to imbalanced panels where m * is individual-specific. Next, we characterize the aggregate data. The three summary statistics disclosed by Spotify (hereafter referred to interchangeably as disclosures) in our empirical application are the gross number of subscribers added and lost in quarter q (which we call ADD q and LOSS q , respectively), and the total count of active subscribers at the end of quarter q (END q ). These summary statistics are also the most commonly disclosed by publicly traded subscription-based companies <ref type="bibr" target="#b19">(McCarthy et al. 2017)</ref> and can be expressed as aggregations of linear combinations of the elements of Y i :</p><formula xml:id="formula_10">ADD q ∑ N i 1 ∑ 3q m 3q−2 IA im + RA im LOSS q ∑ N i 1 ∑ 3q m 3q−2 IC im + RC im END q ∑ q q * 1 ADD q * − LOSS q * ,<label>(7)</label></formula><p>where N is the total size of the target population. The random vector of aggregate data observations, which we will call D N , is the concatenation of all observed values of ADD q , LOSS q , and END q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Proxy Likelihood Function</head><p>A seemingly natural approach to estimate our model with this observable data would be to use likelihoodbased methods, such as maximum likelihood estimation. The full log-likelihood of all observed data can be decomposed as follows:</p><formula xml:id="formula_11">θ|z 1:N ,ỹ 1:N , d ( ) ∑ N i 1 log P θ Z i z i ( ) ( ) ⏟̅̅̅̅̅̅̅⏞⏞̅̅̅̅̅̅̅⏟ selection outcomes + ∑ i|z i 1 { } log P θỸi ỹ i |Z i z i ( ) ( ) ⏟̅̅̅̅̅̅̅̅̅̅̅̅⏞⏞̅̅̅̅̅̅̅̅̅̅̅̅⏟ panel data + log P θ D N d|Z 1:N z 1:N , Y 1:N ỹ 1:N ( ) ( ) ⏟̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅⏞⏞̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅⏟ aggregate data .</formula><p>(8) Selection bias is accounted for in the panel log-likelihood and aggregate data log-likelihood by conditioning on Z i in the second and third terms of this equation, and the aggregate data avoids double-counting the panel data by conditioning onỸ i .</p><p>Conditional upon the individual-level vectors λ 1:N , the first two terms in Equation ( <ref type="formula">8</ref>) are simple to compute: The first term is the panel selection likelihood,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</head><p>Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref> which is the likelihood of a logistic regression model; the second term is the panel data likelihood, which consists of duration probabilities computed directly from our model. The third term, however, requires an N-fold convolution over Y 1:N and is not tractable to compute. As such, typical likelihood-based estimators are infeasible. Alternately, we could consider simply using moment-based methods such as nonlinear least squares, which are often relatively simple to implement and have been used extensively in prior CBCV literature <ref type="bibr" target="#b11">(Gupta et al. 2004</ref><ref type="bibr" target="#b19">, McCarthy et al. 2017</ref><ref type="bibr" target="#b18">, McCarthy and Fader 2018</ref>. However, this would require summarizing the granular data {Z 1:N ,Ỹ 1:N } into aggregate moments, while only relatively simple models admit sufficient statistics that allow summarization without information loss. This is the approach taken by <ref type="bibr" target="#b1">Berry et al. (2004)</ref>, but it requires the potentially arbitrary choice of which moments to include, possibly hurting statistical efficiency. In our setting, it is unclear what summary statistics would adequately summarize the panel data with minimal information loss. Instead, our proposed estimation procedure incorporates both the aggregate and panel data as-is, without information loss; this sharpens our model estimates and improves the portability of our method to other domains in which the disaggregate data may be difficult to aggregate into summary statistics.</p><p>To the best of our knowledge, the primary prior work that has proposed an estimator applicable to our setting is that of <ref type="bibr" target="#b8">Feit et al. (2013)</ref>, who use Bayesian imputation, building off prior work on Bayesian estimation of individual-level models of consumer choice using only aggregated data <ref type="bibr" target="#b5">(Chen and Yang 2007;</ref><ref type="bibr" target="#b20">Musalem et al. 2008</ref><ref type="bibr" target="#b21">Musalem et al. , 2009</ref>. In the Bayesian imputation approach, the missing observations in Y 1:N are treated as parameters to be estimated along with all model parameters; augmented data setsŶ 1:N representing possible values of Y 1:N are simulated, with proposed augmented data sets accepted only if the resulting aggregate data pointsD N are equal to the observed aggregate data points d.</p><p>This method performs well when the overall population N is not large. However, it is computationally infeasible to scale to large-data settings such as ours: A Y i vector must be imputed for all N population members, and when the vector of summary statistics d is high-dimensional, there will be a large number of equality constraints that proposed data setsŶ 1:N will be unlikely to satisfy, resulting in low acceptance ratios and thus poor mixing and slow convergence. In our empirical application, N is six orders of magnitude larger than in <ref type="bibr" target="#b8">Feit et al. (2013)</ref>, making this approach infeasible.</p><p>To ameliorate the scalability issues, subsampling the data (e.g., scaling down the population size and aggregate count data by a multiplicative factor) has been proposed in previous work <ref type="bibr" target="#b21">(Musalem et al. 2009</ref>). This approach, although getting around scalability issues, results in poor statistical efficiency, because subsampling the data inflates estimation variance. For instance, scaling down the aggregate data by 1,000 times would result in standard errors that are ̅̅̅̅̅̅̅ ̅ 1,000 √ ≈ 31.6 times larger than when using the full data.</p><p>The approaches of <ref type="bibr" target="#b1">Berry et al. (2004)</ref> and <ref type="bibr" target="#b8">Feit et al. (2013)</ref> could be modified to correct for selection bias by computing moments and probabilities conditional on panel selection outcomes as in Equation ( <ref type="formula">8</ref>). However, because of the previous issues of summarizing the panel data (in the former case) and scalability (in the latter case), we instead propose model estimation by maximizing a proxy likelihood function, which replaces the third term in Equation ( <ref type="formula">8</ref>) by a computationally tractable approximation. Recall that this term is the log-likelihood of D N , which is the aggregation of (a linear transformation of) N individual-level outcomes Y i . Thus, under mild regularity conditions, the central limit theorem states that the distribution of D N is asymptotically well approximated by a normal distribution. Hence, we approximate the likelihood of D N using its asymptotic distribution. <ref type="bibr">7</ref> In particular, define the finite sample conditional mean and variance of D N as follows:</p><formula xml:id="formula_12">µ N θ ( ) E θ D N |z 1:N ,ỹ 1:N [ ] Σ N θ ( ) Var θ D N |z 1:N ,ỹ 1:N [ ] . (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>The distribution MVN(µ N (θ), Σ N (θ)) is an asymptotic approximation to the distribution of D N given the panel data, so we can approximate the true likelihood function from Equation ( <ref type="formula">8</ref>) by replacing the last term with the log-density of this multivariate normal distribution. Thus, we have replaced the computationally prohibitive task of computing the full distribution of D N by the much more manageable task of computing its first two moments, similar to what would be required for other moment-based procedures such as two-stage generalized method of moments <ref type="bibr" target="#b12">(Hansen 1982)</ref>. A typical moment-based estimator would require only the unconditional moments of D N , whereas here we condition on the disaggregate data to avoid doublecounting the panel members (i.e., computing the likelihood of the panel members directly through their panel activity and again indirectly through their representation within the aggregate data); however, we could easily replace µ N and Σ N with their unconditional analogues without fundamentally altering the properties of our estimator when the conditional moments are infeasible to compute. The second moment Σ N may be computationally expensive to obtain, because the number of covariance elements to compute will be very large when D N</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</head><p>Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref> is high-dimensional. <ref type="bibr">8</ref> In contrast, the first moment µ N (θ) is relatively easy to compute, because it scales linearly with the dimension of D N (and would also be required for any typical moment-based estimator such as nonlinear least squares). As such, we consider an estimator where only µ N (θ) is computed in the optimization, and the covariance matrix is fixed at some positive definite matrixΣ N instead of being continuously updated (we will discuss in Section 3.4 the matter of choosing an appropriate matrixΣ N ). That is, dropping the conditioning on z 1:N ,ỹ 1:N , d for notational brevity, the proxy likelihood˜ is as follows (up to additive constants):</p><formula xml:id="formula_14">N θ|Σ N ( ) ∑ N i 1 log P θ Z i z i ( ) ( ) + ∑ i|z i 1 { } log P θỸi ỹ i |Z i z i ( ) ( ) − 1 2 d − µ N θ ( ) ( ) Σ N ( ) −1 d − µ N θ ( ) ( ) .<label>(10)</label></formula><p>The third term in Equation ( <ref type="formula" target="#formula_14">10</ref>) is the multivariate normal approximation to the log-likelihood of d. We can also see this third term as a quadratic form of moment conditions, which is equivalent to the objective function of the generalized method of moments, up to normalization by N <ref type="bibr" target="#b12">(Hansen 1982)</ref>. Equation ( <ref type="formula" target="#formula_14">10</ref>) as a whole is the objective function of our estimation procedure. In Section 3.3, we describe how to compute the proxy likelihood for our empirical specification and then discuss our proposed estimation procedure in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computing the Proxy Likelihood</head><p>Although we have reduced the computational problem of the aggregate data likelihood down to computing the first two moments of the aggregate summary statistics as defined in Equation ( <ref type="formula" target="#formula_12">9</ref>), there remains the issue of how to compute these moments. As noted in Section 3.1, all summary statistics we observe are simply aggregations of linear transformations of Y i , which makes it straightforward to compute these quantities as a function of the conditional mean vector and covariance matrix of Y i |Z i ,Ỹ i . Thus, we just need to compute the moments of Y i |Z i ,Ỹ i to derive the moments of the summary statistics. <ref type="bibr">9</ref> Because Y i is a vector of Bernoulli random variables, computing its first two moments requires computing all marginal and pairwise event probabilities. These probabilities are not straightforward to compute, because a single customer may engage in repeat behaviors (adoption and/or churn) more than once: For instance, naively computing probabilities such as P(RA im 1) would require marginalizing over all possible sequences of acquisitions and churns that the customer could have taken to arrive at reacquisition in month m, which is computationally prohibitive even for short time horizons. To make the mar-ginalization feasible, we construct a recursive belief propagation algorithm that exploits the Markovian structure of our model, substantially reducing computational burden <ref type="bibr" target="#b22">(Pearl 1988)</ref>.</p><p>Finally, we must also marginalize the individuallevel parameters λ 1:N out of the objective function. As mentioned in Section 2.2, our heterogeneity distribution is nonconjugate; as such, we approximate the integrals numerically via simulation using Halton sequences, which have been used successfully in empirical applications similar to ours <ref type="bibr" target="#b2">(Bhat 2001</ref><ref type="bibr" target="#b28">, Train 2009</ref>. In essence, this amounts to simulating K draws from the mixing distribution λ (k) ∼ g(λ) using a Halton sequence, computing each probability and expectation in Equation ( <ref type="formula" target="#formula_14">10</ref>) conditional upon λ (k) , and then averaging over the K draws before plugging the results into the proxy likelihood equation. Because the second and third terms of Equation ( <ref type="formula" target="#formula_14">10</ref>) involve probabilities and expectations conditional upon the panel selection outcome Z i and the panel dataỸ i , for these terms, we integrate over the posteriors g(λ i |Z i ) and g(λ i |Z i ,Ỹ i ) using importance sampling, weighting each λ <ref type="bibr">(k)</ref> by the probability of the data being conditioned on. Pseudocode and a step-by-step procedure for how to compute the moments µ N and Σ N , including a derivation of the aforementioned belief propagation algorithm, are provided in Online Appendix 2.</p><p>These computational ingredients provide all the tools needed to efficiently compute the proxy likeli-hood˜ , such that we can use our method in practice. The full procedure for computing˜ is summarized in Algorithm 1. With our objective function in hand, we now discuss our estimation procedure. </p><formula xml:id="formula_15">( ) ∼ N log λ 0 ( ), Σ λ ( )<label>)</label></formula><p>Compute the probability of selection into the panel given λ (k) (Equation (4)):</p><formula xml:id="formula_16">p k : P θ Z 1|λ k ( ) ( ) Logit −1 β Z ( ) 0 + β Z ( ) ( ) log λ k ( ) ( ) ( )</formula><p>Compute the panel selection log-likelihood (first term of Equation ( <ref type="formula" target="#formula_14">10</ref>)) by averaging over the p k s:</p><p>z :</p><formula xml:id="formula_17">∑ N i 1 z i log 1 K ∑ K k 1 p k ( ) + 1 − z i ( )log 1 − 1 K ∑ K k 1 p k ( )</formula><p>.</p><p>Compute the panel data log-likelihood (second term of Equation ( <ref type="formula" target="#formula_14">10</ref>)) by averaging each panel member's likelihood over the simulated λ (k) s and summing the marginal log-likelihood over panel members, where the p k s serve as posterior importance sampling weights, as described in Online Appendix 2.3:</p><formula xml:id="formula_18">y : ∑ i|z i 1 { } log 1 ∑ K k 1 p k ∑ K k 1 p k P θỸi ỹ i ⃒ ⃒ ⃒Z i z i ,λ i λ k ( ) ( )<label>(</label></formula><p>)</p><p>.</p><p>Compute the aggregate proxy likelihood (third term of Equation ( <ref type="formula" target="#formula_14">10</ref>)) by first computing the mean function µ N (θ) using Algorithm 2 in the online appendix and then plugging this into the multivariate normal logdensity formula for aggregate data d, yielding (up to additive constants):</p><formula xml:id="formula_19">d : − 1 2 d − µ N θ ( ) ( ) Σ N ( ) −1 d − µ N θ ( )<label>(</label></formula><p>) .</p><p>Sum the three terms to compute the total proxy loglikelihood for the parameters θ:</p><formula xml:id="formula_20">: z + y + d return˜ 3.4.</formula><p>Maximum Proxy Likelihood Estimation Section 3.2 introduced the proxy likelihood function˜ and Section 3.3 described how to compute it. With this objective function, we can construct a one-stage estimator as follows:</p><formula xml:id="formula_21">θ 1 ( ) N z 1:N ,ỹ 1:N , d|Σ N ( ) arg max θ˜ N θ|Σ N ( )</formula><p>for some q × q positive definite matrixΣ N , where q is the dimension of d. However, we must also consider the practical matter of choosing the covariance ma-trixΣ N to ensure that the normal approximation is accurate. As such, we propose a two-stage estimator θ (2) N (z 1:N ,ỹ 1:N , d|Σ N ), which updatesΣ N , analogous to the two-stage generalized method of moments procedure for weight matrix selection <ref type="bibr" target="#b12">(Hansen 1982</ref>):</p><p>1. InitializeΣ N to some q × q positive-definite matrix (e.g., the identity matrix or the true covariance matrix at a heuristic estimate of θ).</p><p>2. Obtain an initial parameter estimateθ by maximizing the proxy likelihood givenΣ N :</p><formula xml:id="formula_22">θ ←θ 1 ( ) N z 1:N ,ỹ 1:N , d|Σ N ( )<label>.</label></formula><p>3. Update covariance matrixΣ N to the true covariance matrix at the initial estimateθ (Algorithm 3 in the online appendix):</p><formula xml:id="formula_23">Σ N ← Σ Nθ ( ) .</formula><p>4. Updateθ using the updated covariance ma-trixΣ N :θ</p><formula xml:id="formula_24">←θ 1 ( ) N z 1:N ,ỹ 1:N , d|Σ N (</formula><p>) .</p><p>5. Return the updatedθ as the final parameter estimateθ <ref type="bibr">(2)</ref> N . Under mild regularity conditions analogous to those of maximum likelihood and generalized method of moments,θ <ref type="bibr">(1)</ref> N andθ (2) N are consistent, converging at rate O p (N −1/2 ) and achieving asymptotic normality (derivations are available in Online Appendix 3). <ref type="bibr">10</ref> We will refer to these estimation methods collectively as maximum proxy likelihood (MPL). Although we use the two-stage estimatorθ <ref type="bibr">(2)</ref> N in all our simulations and in our empirical example for its statistical efficiency, the single-stage estimatorθ <ref type="bibr">(1)</ref> N may still be useful for models where computing the second moment Σ N (θ) is infeasible. To calculate the standard errors associated with this estimate, we derive its asymptotic variance in Online Appendix 3. <ref type="bibr">11</ref> We now have a computationally feasible and statistically efficient method for estimating our model. In the following sections, we demonstrate the validity of our method in several ways: first through a discussion of model identification in Section 4, then through a simulation study in Section 5, and finally through our empirical example of Spotify in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Identification</head><p>We discuss how changes in the model parameters produce distinct patterns in the aggregate and disaggregate data, identifying the model. We first discuss identification of all parameters that are homogeneous across customers and then the parameters that govern cross-sectional heterogeneity in customers' propensities, before concluding with an analogous discussion of the parameters and functional form of the selection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Homogeneous Parameters</head><p>The homogeneous parameters of our specification are the process-specific covariate coefficients β <ref type="bibr">(p)</ref> and duration dependence parameter c <ref type="bibr">(p)</ref> for the four processes p ∈ {IA, IC, RA, RC}, as well as the proportion of the population who will ever be acquired π <ref type="bibr">(IA)</ref> , and the proportion of churned customers who will ever be reacquired, π <ref type="bibr">(RA)</ref> .</p><p>First, we consider β <ref type="bibr">(p)</ref> . In our empirical application, x t is a vector of quarterly dummies. In our panel data, we can directly observe quarterly seasonality in initial and repeat acquisition and churn behaviors, separately identifying seasonality for the four processes. These parameters are further identified through the aggregate data by quarterly acquisitions and churns (ADD q and LOSS q , respectively). Although our aggregate data do not distinguish between first-time and repeat activity, they can still facilitate separate identification of initial and repeat behaviors through changes in seasonality over time: As a company matures, an increasing proportion of its acquisitions and churns will be attributable to repeat behaviors; therefore, long-term trends in the strength of seasonality distinguish between initial and repeat processes. The effects of other timevarying covariates can be identified similarly.</p><p>Identification of the duration dependence parameters c (p) follows from observing long-term trends in acquisition and churn counts in the panel data, which trace out the baseline shape of the empirical hazard function for each process. Unobserved heterogeneity in the scale parameter λ (p) i results in a decreasing hazard function because of survivorship bias, which can be difficult to separate out from duration dependence. In general, duration dependence and heterogeneity can only be separately identified under restrictions on the separatibility and/or parametric form of the hazard function <ref type="bibr" target="#b13">(Heckman 1991)</ref>. These conditions are satisfied under our model, enabling identification; that being said, identification may be sensitive to violations of model assumptions that are difficult to verify, and some degree of misspecification is inevitable in practice. As such, although our model is formally identifiable, we should nevertheless interpret the duration dependence estimates with some caution.</p><p>The parameters π (IA) and π <ref type="bibr">(RA)</ref> are identified by observing the asymptote of where the initial and repeat acquisition curves flatten out: For instance, in the panel data, we can directly observe the proportion of panel members who have been initially acquired and the proportion of previously churned panel members who have been reacquired. With a long enough panel horizon, we can infer the asymptotes of the cumulative acquisition curves that determine π (IA) and π <ref type="bibr">(RA)</ref> . Identification of these parameters will naturally be weaker for companies that are still rapidly growing or have very long acquisition cycles, because it will be more difficult to infer the asymptotes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Heterogeneity Distribution Parameters</head><p>The parameters governing the heterogeneity distribution of λ i are the log-mean vector λ 0 and covariance matrix Σ λ . We first discuss how the aggregate data identifies the log-mean and variance of λ i . We then discuss how the panel data identifies the distribution of λ i , up to distortion by selection bias.</p><p>Although the acquisition and retention curves for each process are not directly observable in the aggregate data, ADD q and LOSS q are nevertheless informative about the distribution of λ i . The log-means of the distributions, λ 0 , are readily identified by the scale of the ADD q and LOSS q curves: Earlier periods, in which most customers are first-timers, identify λ (IA) 0 and λ <ref type="bibr">(IC)</ref> 0 ; later periods, in which most customers are repeaters, identify λ (RA) 0 and λ (RC) 0 . The variance of λ <ref type="bibr">(IA)</ref> i is identified by the shape of the ADD q curve in early periods: Low variance suggests near-exponentially declining curves (augmented by duration dependence), whereas high variance suggests a steep initial drop off followed by a quick flattening out because of survivorship bias. The distribution of λ <ref type="bibr">(IC)</ref> i is identified by observing sequential correlations between the LOSS q and the ADD q curves: For instance, if in the quarter after a spike of high acquisitions (e.g., because of seasonality), there is a corresponding spike in LOSS q , this suggests high heterogeneity variance; conversely, if LOSS q does not spike sharply immediately after spikes in ADD q , this suggests low heterogeneity variance. The variance of λ <ref type="bibr">(RA)</ref> i and λ <ref type="bibr">(RC)</ref> i are identified through analogous patterns in later periods.</p><p>Next, we consider identification of the heterogeneity parameters through the panel data. The following arguments imply identification of the heterogeneity distribution of the panel members and not the population as a whole. As we will discuss in the next section, this distinction will be key to identifying the selection parameters.</p><p>The panel-level marginal distribution of λ (p) i for each process is identified from the shape of the initial/ repeat acquisition and retention curves, analogous to identification of the distribution of λ (IA) i from the aggregate ADD q curve.</p><p>Additionally, the correlations between the different process λ (p) i s (i.e., the off-diagonal elements of Σ λ ) are identified directly by joint observations in the panel data: For instance, if panel members who are first acquired early on tend to churn more quickly than panel members acquired later on, this suggests a positive correlation between λ (IA) i and λ <ref type="bibr">(IC)</ref> i <ref type="bibr" target="#b24">(Schweidel et al. 2008a)</ref>. It is less obvious how such correlations would manifest in the aggregate data, where we cannot observe the joint distribution of acquisition and churn times. For these parameters, the panel data are most informative.</p><p>Although we use a lognormal heterogeneity specification, a similar identification argument applies to other distributional forms: The presence of high λ (p) i s reflects mostly at the beginning of a process, whereas the presence of low λ (p) i s reflects later on, and dependencies between the different processes can be inferred directly by the joint distribution of acquisition and churn times in the panel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Selection Parameters and Functional Form</head><p>The identification of the selection function intuitively boils down to observing discrepancies between the acquisition and churn trends in the panel data relative to the aggregate data. We argued previously that the population mean of λ i is readily identified in the aggregate data, whereas the mean of λ i among panel members is identified through the panel data; accordingly, first-order selection bias is identified by comparing the means implied by the aggregate data versus the panel data. For instance, a positive β <ref type="bibr">(Z)</ref> IA means that the panel population has higher λ (IA) i on average compared with the population as a whole, such that the acquisition rates observed in the panel in early periods will be higher than the acquisition rates implied by the aggregate ADD q figures. Analogously,</p><formula xml:id="formula_25">β (Z) IC , β (Z)</formula><p>RA , and β (Z) RC are identified by discrepancies between panel and aggregate moments that identify the means of the corresponding λ (p) i distributions. Finally, the intercept term β <ref type="bibr">(Z)</ref> 0 simply governs the size of the panel and is identified by the empirical proportion of the total population that is in the panel. <ref type="bibr">12</ref> Selection model specifications other than a logitlinear form can also capture these first-order differences, but the proposed functional form should be a reasonable approximation to smooth, monotonic selection functions, such that bias from misspecification of the functional form should be small. We verify this through simulation studies, available in Online Appendix 5.4, where the true data generating process has a nonlinear selection function, whereas the estimated model assumes a linear selection function. We find that bias and coverage worsen, but the method still mostly recovers the true population parameters.</p><p>Although we use a simple linear specification for our selection function for parsimony, we can also specify more sophisticated nonlinear selection mechanisms that could capture second-order selection effects such as nonmonotonicity. In addition to the mean vector λ 0 , we argued that the overall marginal heterogeneity distributions are identified through the aggregate data. Thus, nonlinearities in the selection function can be identified by discrepancies between the moments that identify different quantiles of the distributions. <ref type="bibr">13</ref> Formally, we show in Online Appendix 4 that as long as the panel data are sufficiently rich, a general selection mechanism P ψ (Z i 1|λ i ), parameterized by vector ψ, is identified as long as there are enough aggregate moments to separate out the discrepancies between the overall population and the panel population implied by each element of ψ; at minimum, there must be as many unique aggregate moments as there are elements of the selection parameter ψ. The intuition for this result is that if the panel data are rich, we can identify all the parameters of the model except ψ on the panel data alone, leveraging all of the aggregate data to estimate ψ by observing the discrepancies between the trends in the aggregate data and those implied by the parameters identified by the panel data.</p><p>In practice, however, there is a bias-variance tradeoff to consider: Allowing for a more flexible selection model reduces bias from misspecification but inflates variance by adding more parameters to estimate. In particular, second-order selection biases are likely to be imprecisely estimated, because the moments needed to identify them will be relatively noisy compared with those needed to identify first-order selection bias. <ref type="bibr">14</ref> Unless we have strong reason to believe that misspecification because of second-order selection biases is severe, a linear selection model is likely to perform better, because it can capture first-order shifts between the panel and aggregate data but can still be relatively precisely estimated. In our empirical application, incorporating panel data with a linear selection mechanism yields better out-of-sample predictions of future aggregate data than using historical aggregate data alone. Although this is in no way a guarantee that our model does not have misspecification, it demonstrates that the linear selection mechanism corrects for selection bias well enough to make the panel data useful in capturing and forecasting populationlevel trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Predictive Accuracy</head><p>We conduct simulation studies to evaluate the performance of the MPL method, comparing it with two other methods commonly used in practice: First, the generalized method of moments with only aggregate data, as in all extant CBCV models (denoted AGG); and second, maximum likelihood with only granular panel data, assuming that there is no selection bias (denoted PAN). We note that MPL requires about as much computing time as AGG and PAN combined and therefore has small incremental cost of implementation in terms of computation. Compute times by method are reported in Online Appendix 5.2.</p><p>We compare AGG and PAN to MPL across a variety of simulation settings to better understand the incremental improvement our method provides as a function of contextual factors. We vary these settings along two groups of dimensions:</p><p>• Data settings: Our baseline data setting has M 60, N 100K, panel size equal to 5% of N, and a moderate degree of selection bias (through β <ref type="bibr">(Z)</ref> ). We then perturb this baseline scenario marginally, considering perturbations (1) M ∈ {36, 84, 108}, (2) N ∈ {20K, 500K, 2.5M}, (3) panel percentages of 1% and 10%,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</head><p>Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref> and (4) selection bias severities of none and high. This results in 11 total data settings.</p><p>• Parameter settings: We separately vary the model parameters governing the initial and repeat acquisition and churn processes along eight dimensions: (1-4) the baseline parameters (λ 0 , c) for these processes, (5 and 6) initial and repeat process variance, and (7 and 8) within-and across-process correlation. We consider low and high values for each dimension that is varied, resulting in 2 8 256 unique sets of parameter values.</p><p>This results in a total of 2,816 simulation settings (see Online Appendix 5.1 for the complete listing of settings). For consistency with the data available in our Spotify application, we assume that END and LOSS data are observed each quarter (which fully determine ADD q , because ADD q END q − END q−1 + LOSS q ). In this section and the next, we evaluate the methods based on predictive accuracy and parameter recovery, respectively, to establish the usefulness of the method for both prediction-oriented use cases (e.g., CBCV) and inference-oriented ones.</p><p>We evaluate predictive accuracy in this section by forecasting quarterly initial and repeat acquisition and churn (QIA, QIC, QRA, QRC), in addition to ADD, LOSS, and END, over a six-quarter holdout period. The former collection of summary statistics separates out initial and repeat behavior, whereas the latter collection pools them. Our error measure is mean absolute percentage error (MAPE) so that error measures are comparable across the disclosures (i.e., summary statistics) despite their differing scales. The true values underlying the MAPE calculations are the actual values of the aggregate statistics in the holdout period. To better understand the overall performance of each method, Table <ref type="table" target="#tab_2">1</ref> shows the MAPE for each method by disclosure, averaging across all parameter and data settings. MPL has the lowest average MAPE figures across all disclosures. Its improvements over the other methods are particularly sizable when predicting the summary statistics that separate out initial and repeat behavior (QIA, QIC, QRA, QRC), because it could better infer these disclosures through the panel data. Its relative advantage remains substantial for ADD, LOSS, and END as well. <ref type="bibr">15</ref> The accuracy of AGG is generally high when predicting disclosures that are observed historically, deteriorating significantly for disclosures that are not. PAN has low accuracy in general, further emphasizing the perils of naively using granular data when it may not be representative of the target population.</p><p>It also important to understand how performance varies as a function of the characteristics of the available data. Figure <ref type="figure" target="#fig_2">2</ref> plots average MAPE figures by method as we vary the four data settings. These figures are averaged across QIA, QIC, QRA, and QRC. Disclosure-specific MAPE data are available in Online Appendix 5.3. MPL consistently outperforms PAN and AGG. The performance gap narrows, particularly for AGG, as N increases, and the gap for PAN narrows as the panel selection bias decreases and the panel size increases. PAN performs worst, except when there is no selection bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Parameter Recovery</head><p>Although predictive accuracy is important for our empirical application, it is also important to evaluate parameter recovery, particularly for other settings where inference may be a primary objective. To this end, we conduct simulations in this section to evaluate MPL's finite sample parameter recovery performance.</p><p>We evaluate MPL using the baseline set of parameter values from the preceding large-scale simulation analysis. <ref type="bibr">16</ref> As a robustness check, we repeat the analysis for another four randomly selected parameter sets from the previous section (the results, which are qualitatively consistent with the results reported here, are provided in Online Appendix 5.4).</p><p>First, we compare the bias and variance of MPL to AGG by computing the median absolute bias and interquartile range (IQR) for each parameter, averaged across 30 replicates. For brevity, we grouped the model parameters into five categories, heterogeneity mean parameters (λ (IA) , λ <ref type="bibr">(IC)</ref> , λ <ref type="bibr">(RA)</ref> , λ (RC) ), heterogeneity variance parameters (σ <ref type="bibr">(IA)</ref> λ , . . . , σ (RC) λ ), heterogeneity correlation parameters (ρ (IA,RA) λ , . . . , ρ (RA,RC) λ ), homogeneous parameters (π (IA) , π (RA) , c (IA) , . . . , c (RC) ), and selection parameters (β <ref type="bibr">(Z)</ref> 0 , β (Z) ), and report each performance measure averaged across all parameters within each category (parameter-by-parameter results are available in Online Appendix 5.4). We compute each parameter's statistics in terms of absolute percentage to account for differing scales and signs of parameters. For example, the median absolute percentage bias for parameter collection c is equal to MAPB c ( )  <ref type="bibr" target="#b30">, 2021</ref><ref type="bibr">, , vol. 40, no. 3, pp. 459-480, © 2021</ref> where n c p is the number of parameters within parameter collection c, θ c(p) denotes the true value of the pth parameter within parameter collection c, and Med(θ c(p) ) represents the sample median of parameter estimateθ c(p) across simulation replicates. We use median and IQR to be robust to outliers, as we found the AGG method yielded very heavy-tailed estimate distributions. The results are shown in Table <ref type="table" target="#tab_3">2</ref>.</p><formula xml:id="formula_26">∑ n c p p 1 |Medθ c p (<label>)</label></formula><formula xml:id="formula_27">( ) − θ c p ( ) ⃒ ⃒ ⃒ ⃒ ⃒ ⃒θ c p ( ) ⃒ ⃒ ⃒ ,</formula><p>Median bias and IQR figures are generally good for MPL for each parameter category: Bias is low and the IQR is generally small. In contrast, median bias and IQR are one to three orders of magnitude larger for AGG than for MPL within each parameter category. Although AGG is asymptotically consistent, it is evidently not empirically identifiable with five years of quarterly data summaries. These results are not sensitive to the true parameter values we select: AGG fails similarly in other parameter settings.</p><p>Next, we focus our study on MPL using more traditional performance measures. In Table <ref type="table" target="#tab_4">3</ref>, we compute the mean absolute bias, coefficient of variation of estimates, and empirical coverage rate of a 95% confidence interval (using the asymptotic variance formula derived in Online Appendix 3 to calculate standard errors). Table <ref type="table" target="#tab_4">3</ref> suggests that, under correct specification, MPL's parameter recovery performance is good. Mean absolute bias as a percentage of true parameter values is less than 7% for all parameter categories. The coefficient of variation (CV) of the parameter estimates was less than 11% across all parameter categories except the heterogeneity correlation parameters, for which the CV was 42%. Empirical coverage is equal to its theoretical target level within simulation error. In Online Appendix 5.4, we also report results when the functional form of the selection model is moderately misspecified; under misspecification, the bias of estimates is inflated, and coverage degrades below the 95% level, but the method is largely still able to recover the correct population-level parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Application to Spotify</head><p>Next, we apply the model to data on paying subscriber activity at Spotify (NYSE: SPOT), the world's largest music streaming platform. The vast majority of Spotify's revenues come from these subscribers, who pay a monthly fee for access to services. <ref type="bibr">17</ref> Spotify publicly discloses END and LOSS data (Equation ( <ref type="formula" target="#formula_10">7</ref>)) in investor presentations and SEC filings. The data are left-and intermediate-censored; whereas commercial operations commenced in October 2008 (i.e., m 0), Spotify began disclosing END data intermittently in Q1 2011, began disclosing END data every quarter in Q1 2015, and began disclosing LOSS data every quarter in Q4 2015. We model these data through Q3 2018 (i.e., the number of months in the calibration period M 120).</p><p>In addition to these public disclosures, Second Measure provided us with a credit card panel data set. This data set consists of the monthly transaction activity data for 3,003,746 panel members from January 2015 (i.e., m * 75) to September 2018. A total of 289,541 of these panel members were initially acquired as Spotify premium subscribers at some point during the observation period. Spotify's publicly disclosed customer data are given in Online Appendix 7, and additional detail regarding the panel data is given in Appendix A.</p><p>We use the same three time-varying covariates in our four submodels: Quarterly dummy variables to capture seasonal fluctuations in the propensity to add and drop service throughout the year. Spotify's service is offered to individuals, and although the company expanded into new geographies in a staged manner, they have operated globally since 2011. Therefore, our unit of analysis is an individual person, and our population is the world population, as this represents everyone who could possibly acquire Spotify's service.</p><p>As in the previous section, we estimate the parameters via maximum proxy likelihood. Each stage of estimation was performed using the R programming language's nlm function, which uses a Newtontype optimization routine, letting the algorithm run until convergence. We initialize the first stage of nlm at an approximate solution obtained using DEoptim, an evolutionary algorithm, so that our starting parameter values for nlm were in a better part of the parameter space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Model Assessment and Comparison</head><p>We first validate the method by examining its in-and out-of-sample performance. To evaluate in-sample performance, we fit the proposed model to all Spotify data and then plot the observed aggregate data-ADD, LOSS, and END-with its corresponding modelbased prediction. The resulting plots are shown in Figure <ref type="figure" target="#fig_3">3</ref>. The in-sample fit of the proposed model is good: Errors are small with no systematic pattern of under-or overprediction.</p><p>Although the in-sample fit of the proposed model is good, it tells us little about the model's predictive validity, whether credit card panel data improves predictive validity, or how the model's forecasting accuracy compares to that of extant CBCV models. Moreover, although Spotify regularly discloses ADD, LOSS, and END, many more companies only regularly disclose END (e.g., Netflix, Blue Apron, Hello-Fresh, and Care.com), so it is informative to assess how predictive validity varies as a function of what data are used for estimation. Predicting ADD and LOSS when only END is observed at the population level also provides us insight into how much the panel data improves our ability to infer measures that are not directly observed in the aggregate data-given the importance of teasing apart initial and repeat behaviors (which are never directly observed in the aggregate data), this is highly important as well. To better understand these questions, we run a rolling holdout analysis. We study the performance of the proposed model as a function of what data are available by varying the observable training data along two dimensions:</p><p>1. Aggregated data: We either train on all aggregated data, on END data alone, or on no aggregated data.</p><p>2. Panel data: We either train on the panel data, or we do not.</p><p>We consider the resulting five nondegenerate data availability scenarios for the proposed model. We compare these proposed model variants to the models in <ref type="bibr" target="#b11">Gupta et al. (2004)</ref>, <ref type="bibr" target="#b23">Schulze et al. (2012), and</ref><ref type="bibr" target="#b19">McCarthy et al. (2017)</ref>, which we refer to hereafter as GLS, SSW, and MFH, respectively. Given the severity of the seasonal fluctuations in the observable data, we enhance the GLS and SSW specifications by incorporating time-varying covariates into them. This allows  <ref type="bibr" target="#b30">, 2021</ref><ref type="bibr">, , vol. 40, no. 3, pp. 459-480, © 2021</ref> us to incorporate the same quarterly seasonal dummy variables into all benchmark models so that no models are penalized for their inability to capture seasonality. Details of the enhanced model specifications are provided in Online Appendix 6.</p><p>For each model, we vary the length of the calibration period M, for M 99, 102, . . . , 117, corresponding to all possible calibration periods from Q4 2016 to Q2 2018. Q4 2016 is the first quarter in which ADD and LOSS data are available for four quarters, identifying the seasonal dummy variables, making it a suitable starting point for the rolling validation.</p><p>In sum, the predictive validity of GLS, SSW, MFH, and the MPL variants are based upon rolling (up to) six-quarter-ahead predictions over seven different calibration periods. For each calibration period, we predict ADD, LOSS, and END, resulting in 81 total rolling predictions. We summarize the predictive performance of these models by computing the MAPE of each models' predictions, averaging across all calibration periods. Table <ref type="table" target="#tab_5">4</ref> provides the resulting MAPE figures.</p><p>The proposed model trained on the panel data alone (row four in Table <ref type="table" target="#tab_5">4</ref>) performs poorly, with MAPE figures in excess of 400% across the board. This suggests that panel selection bias is not ignorable and that naively combining the panel data with the aggregate would make the resulting forecasts worse than if the panel data were simply ignored. When END is observed without any panel data, the proposed model forecasts END reasonably well, but the corresponding MAPE figures for ADD and LOSS exceed 100%, implying that the proposed model cannot separate out the acquisition and churn processes using END disclosures alone.</p><p>We see uniform improvements in predictive accuracy when we add panel data, whether only END is observed (row 6 versus row 5) or all aggregate disclosures are observed (row 8 versus 7). Similarly, predictions uniformly improve when ADD and LOSS Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref> data are observed in addition to END, whether panel data are observed (row 8 versus row 6) or not (row 7 versus row 5).</p><p>Our proposed model using all aggregate and panel data is the best-performing model overall. Although its MAPE is higher than MFH with respect to LOSS, it has the lowest MAPE across all models with respect to ADD and END. END is a particularly important disclosure because it is most directly tied to total revenues.</p><p>The performance of the proposed model is robust to forecasting horizon. In Figure <ref type="figure">4</ref>, we plot the average MAPE with respect to END by forecasting horizon for the proposed model and for MFH. MFH has a smaller MAPE for very short forecasting horizons, but its MAPE grows quickly as the forecasting horizon lengthens, rising to approximately 20% six quarters out. MPL's forecasting error with respect to END is in the single digits over all forecasting horizons. These results are relevant given the importance of long-run forecasting accuracy in CBCV settings.</p><p>Having established the predictive validity of the proposed model, we next turn to insights that can be derived from the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Parameter Estimates and Model Insights</head><p>The parameters of the estimated model trained on all available aggregate and panel data are shown in Table <ref type="table" target="#tab_6">5</ref> (associated standard errors are provided in parentheses, estimated using the asymptotic variance formula derived in Online Appendix 3). The seasonal fluctuations evident in the first two plots of Figure <ref type="figure" target="#fig_3">3</ref> seem to be primarily caused by relatively high propensity of subscribers to be repeat acquired in Q2 and Q4. We observe a strong positive correlation between  <ref type="bibr" target="#b30">, 2021</ref><ref type="bibr">, , vol. 40, no. 3, pp. 459-480, © 2021</ref> the propensity to be initially acquired and the propensity to initially churn, implying that subscribers who join later on are more likely to be loyal customers. This finding is consistent with previous work <ref type="bibr" target="#b24">(Schweidel et al. 2008a)</ref>. Customers with high propensities to be initially acquired also tend to have high propensities to be reacquired, as can be seen from the high value of ρ <ref type="bibr">(IA,RA)</ref> λ . Finally, the values of β <ref type="bibr">(Z)</ref> suggest that panel members have higher propensities to readopt, and marginally lower propensities to rechurn, than the population as a whole. These selection effects may stem from the fact that the panel members are U.S.-based credit card holders and as such are wealthier and more likely to adopt than the average Spotify prospect.</p><p>The standard errors of some parameters are large, particularly those pertaining to heterogeneity and selection bias. Although some of these parameters have innocuous explanations for their standard errors, <ref type="bibr">18</ref> this nonetheless suggests that the empirical identification of our model is not strong, even after performing data fusion. This reflects the fact that our aggregate data are limited: Although the panel is very informative about initial and repeat behaviors, under the presence of selection bias, we are uncertain as to how well this information generalizes to the population as a whole. Hence, despite Table <ref type="table" target="#tab_5">4</ref> demonstrating that accounting for selection bias is essential, the limited time series of ADD and LOSS data available in our context (12 quarters) does not allow for full disentanglement of the different dimensions of selection bias. As a result, the individual selection model parameters are estimated imprecisely; in turn, the populationlevel estimates of the heterogeneity distribution are imprecise. These standard errors accordingly convey our uncertainty in generalizing from panel to population; conversely, ignoring selection bias would yield misleadingly precise estimates. However, the standard errors are narrower than they otherwise would be estimating on the aggregate data alone, because of the additional information gained through the inclusion of a second data source.</p><p>Turning to model insights, as we discussed in Section 1.2, repeat behaviors have significant consequences for Spotify's long-term financial health. Figure <ref type="figure" target="#fig_4">5</ref> plots total quarterly acquisitions, broken down between initial and repeat acquisitions. This figure shows that, although repeat acquisitions had comprised a relatively small proportion of total acquisitions historically, they have been growing significantly over time. Fully 29% of all acquisitions were from repeat acquisitions over the last 12 months of the data.</p><p>The shift in composition of Spotify's subscriber base toward reacquirers is consequential for Spotify's Although most new prospects will not be acquired within one year, approximately 8% of previously churned customers will, and although 40% of new customers will churn within one year of being acquired, the corresponding figure for previously churned customers is only 8%. As Spotify matures, the composition of total acquisitions will continue shifting toward repeat acquisitions. This will stabilize the rate of customer acquisition Notes. The upper panels correspond to the proposed model estimated with the MPL method. The lower panels correspond to the model from <ref type="bibr" target="#b19">McCarthy et al. (2017)</ref>. The first column corresponds to the cumulative acquisition probability for customers who first became a prospect in the final month of the calibration period, whereas the second column corresponds to the retention curve for customers acquired in the final month of the calibration period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</head><p>Marketing <ref type="bibr">Science, 2021</ref><ref type="bibr">, vol. 40, no. 3, pp. 459-480, © 2021</ref> and improve Spotify's overall average retention profile. In contrast, <ref type="bibr" target="#b11">Gupta et al. (2004)</ref> and <ref type="bibr" target="#b23">Schulze et al. (2012)</ref> assume that zero customers will ever be reacquired, making repeat churn irrelevant. <ref type="bibr" target="#b19">McCarthy et al. (2017)</ref> allow for reacquisition but assume that repeat acquisition and repeat churn propensities are identical to the corresponding (worse) initial acquisition and churn propensities. As a result, all three alternative models will understate total reacquisitions and the growth potential of Spotify's customer base as a whole. This is evident from just how much the repeat acquisition and retention curves implied by the proposed model (dotted lines in the upper panels) lie above the corresponding repeat acquisition and retention curves for MFH (lower panels of Figure <ref type="figure">6</ref>). By way of example, the implied 12-month retention rate for reacquired customers is only 42.4% under MFH, well below an implied 91.7% under our proposed model. Without any panel data to identify individual-level customer dynamics, models such as MFH are forced to make simplifying assumptions because, as we have seen through simulations and the rolling validation, aggregated data alone can only accurately model and forecast metrics that are directly historically observed. Although these assumptions are necessary for identification when using only aggregate data, as evidenced here, they can lead to substantial biases in long-term growth projections. By incorporating the panel data into our model through our proposed method, we are able to separate out initial and repeat behaviors, thus overcoming this limitation. Although our empirical estimates are still imprecise for some parameters even after data fusion, the panel data nonetheless improved our ability to make inferences and predictions compared with exclusively using aggregate data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>It is increasingly common for modelers to face situations in which there is more than one data set that is available for a problem at hand. In this paper, we provide a tool for these situations, which allows modelers to use as much data as possible while doing so in a way that accounts for the differing degrees of aggregation and potential selection bias in the underlying data sources. Our proposed estimation method, maximum proxy likelihood (MPL), allows for statistically efficient estimation of models on multiple sources of data while correcting for selection bias, leading to better predictions and inferences than using single sources of data that are either highly aggregated or suffer from selection bias.</p><p>The data fusion methodology proposed here is transferable to many other problems, both in marketing and economics. Within CBCV, an important extension would be to nonsubscription firms, as in <ref type="bibr" target="#b18">McCarthy and Fader (2018)</ref>, where churn behavior is latent. Although CBCV is prediction-focused, in other settings such as discrete choice modeling, the goal may be to infer customer-level sensitivity to price or other marketing variables. For such problems, aggregate market share data could help to generalize inferences beyond the population of household scanner panel members, who may differ from the general population in their sensitivities even after controlling for demographics <ref type="bibr" target="#b16">(Lusk and Brooks 2011)</ref>. The simulations and identification analyses we performed suggest that the proposed methodology would be well-suited to such inference problems.</p><p>Although the model specifications and computations required for these other settings differ from ours, the same approximation and selection correction methods can be used. For models with Markovian structure, belief propagation algorithms such as the one we derive in Online Appendix 2 can be used to efficiently compute the moments required to use MPL.</p><p>Our proposed methodology could also be applied to other data structures. For example, it is often the case that companies only possess detailed internal transactional data for recently acquired cohorts of customers (e.g., because of adoption of a new CRM record system), but possess aggregated statistics summarizing customer activity of previous cohorts. In this case, our method can be used to estimate models for the whole customer base, and in some ways the method would be easier to apply because the selection mechanism determining Z i is known. It could also be the case that multiple partially overlapping panel data sets are available (e.g., a combination of credit card panel data and clickstream data), and/or that nonrepresentative aggregate data are available (e.g., statistics reported by a market research firm). Our method can be further generalized to incorporate several data sources, each of which may have different selection mechanisms.</p><p>Of course, applying our method to more general settings requires careful consideration of model identification. In other data settings with different classes of models, principles similar to our identification arguments in Section 4 still hold: Our method requires disaggregate data rich enough that it helps identify the individual-level processes that are difficult to observe directly in aggregate data, and requires that there is a representative data source that is rich enough to capture population differences from the nonrepresentative data sources along relevant dimensions of the process being modeled. The complexity of the individual-level behavioral model and the selection model required will depend on the context, and the amount of data required to identify the model will vary accordingly; as discussed in Section 4.3, Our  <ref type="bibr" target="#b30">, 2021</ref><ref type="bibr">, , vol. 40, no. 3, pp. 459-480, © 2021</ref> method can be used to estimate models with more complex selection mechanisms, but this in turn requires access to more extensive representative data sources that can tease apart different dimensions of selection bias.</p><p>Finally, the theoretical treatment of panel selection could be further enriched. For example, the degree to which including panel data improves performance relies in part on having nonnegligible overlap between the distribution of the individual-level rate parameters λ for the panel members and the corresponding distribution for the population membersotherwise, inferences could be based on extrapolations from panel members who are outliers relative to the broader population. An important consideration for future work is the development of benchmarks to assess whether there is sufficient overlap between the panel and target population to allow for reliable identification, analogous to methods for assessing the overlap condition in causal inference <ref type="bibr" target="#b14">(Imbens and Rubin 2015)</ref>. In the absence of a formal overlap measure, we advocate thoughtful model validation to empirically assess whether the inclusion of panel data improves performance (e.g., the rolling predictive validation we performed in the Spotify example).</p><p>Looking forward, we hope that this paper encourages analysts to more actively seek out new data sources by arming them with a framework to incorporate these varied data sources into their models. As the diversity of available data sources grows, the need for data fusion methodologies such as the one proposed in this paper will grow with it.</p><p>at Spotify in the panel data set. <ref type="bibr">19</ref> The probability that these customers made a purchase at Spotify before January 2011 is minimal because Spotify acquired very few customers this early on.</p><p>8. Second Measure also provided us with the total size of their panel, whether or not those panel members made purchases during the observation period. As described in Online Appendix 2, we account for the fact that panel members who do not make purchases during the observation period were either inactive before and during the observation period (i.e., made no purchases at all prior to October 2018), or only were acquired before the observation period began (i.e., made their first purchase before January 2015, and thus were excluded from our granular panel data set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Endnotes</head><p>1 Furthermore, our method retains favorable statistical properties even when only the first moment is computable. 2 As such, our method does not apply to macro models, such as time series models for aggregate data. For macro models, researchers should consider other data fusion methods. <ref type="bibr">3</ref> We could add individual-specific covariates x i into this specification as well. Doing so would require knowing the population-level distribution of x i . 4 This specification allows for substantial time dynamics in customer acquisition and retention profiles, because of correlations between dimensions of λ i and separate specification of initial and repeat processes. It could be tempting to further enrich the specification of λ i (e.g., allowing parameters to evolve over time); however, these extensions are likely to be confounded with existing sources of dynamics, which could hurt performance due to poor identification. <ref type="bibr">5</ref> To do so, we would need to know the population distribution of the covariates, because computing the aggregate moments requires us to integrate out the population distribution of covariates. Under the assumption that ξ i ⊥ ⊥ Y i |λ i , including other covariates is unnecessary, because any selection bias will be captured by dependency with λ i ; however, including observed covariates could improve the statistical precision with which the selection function is estimated. <ref type="bibr">6</ref> Although each individual in the population is allowed to have a unique parameter vector λ i , we marginalize the individual-level parameters out in estimation. 7 Under stricter conditions than the central limit theorem, local limit laws further state that the likelihood of the N-fold convolution converges uniformly to the multivariate normal density <ref type="bibr" target="#b3">(Bhattacharya and Rao 1986)</ref>, such that this approximation is asymptotically exact. However, just the regularity conditions of the central limit theorem are sufficient for our estimator to achieve consistency and asymptotic normality. <ref type="bibr">8</ref> In particular, the covariance matrix Σ N has dimension q × q, where q is the dimension of D N ; thus, the number of covariance elements that need to be computed scales quadratically in the dimension of D N . 9 This is true for any summary statistics that are affine transformations of Y i . Although most commonly-disclosed summary statistics in SEC filings are affine transformations of Y i , we could also generalize to other types of summary statistics by computing moments using the delta method or simulation-based approaches <ref type="bibr">(Gourieroux et al. 1993). 10</ref> One could also iterate between estimating θ and updatingΣ N multiple times, but this is asymptotically equivalent to the two-stage procedure and so will have the same theoretical properties. In our parameter recovery simulation study (Section 5.2), we continued estimation for a third stage and found that the mean absolute estimation error did not improve within three significant figures, versus a 7.2% improvement moving from the first stage to the second. <ref type="bibr">11</ref> We can also use the computed asymptotic covariance matrix to construct a prediction interval for our forecasts by iteratively sampling parameter vectors from the asymptotic distribution of the parameter estimates and then sampling realizations of the data from those sampled parameter vectors.</p><p>12 Extensive simulations supporting these arguments are available upon request. <ref type="bibr">13</ref> For instance, if the aggregate data contain spikes in LOSS q after spikes in ADD q in early periods, whereas the panel data do not show as pronounced spikes, this suggests that there is a long tail of λ <ref type="bibr">(IC)</ref> i s in the aggregate data but that the panel distribution has lighter tails, suggesting that people with high λ <ref type="bibr">(IC)</ref> i are underrepresented in the panel. 14 Indeed, in our simulations reported in Section 5.2, we find that when using aggregate data alone, the means of the heterogeneity distributions are much more precisely estimated than the variances. In the absence of second-order selection biases, the panel data can aid in the identification of the population heterogeneity variances. <ref type="bibr">15</ref> Although not part of the formal simulation study, we see the same pattern of relative performance across methods when we assume that only END data are observed historically and are forecasting END versus ADD and LOSS. <ref type="bibr">16</ref> The large-scale simulation study in the previous section had explicit baseline data setting levels (e.g., M 60 and N 100K), so we left those settings as-is for this exercise. The simulation had low and high values for each parameter, so our baseline scenario for this exercise averages these two values for each parameter.</p><p>17 On a trailing 12-month basis, approximately 90% of Spotify's total revenues was generated from premium subscriber fees in Spotify's eight most recent quarters. This proportion has remained relatively constant over time. <ref type="bibr">18</ref> The standard error associated with the β (Z) 0 coefficient is large relative to its point estimate because of uncertainty in the mean of the distribution of λ i . If we were to center the λ i s in the selection equation, the point estimate for β <ref type="bibr">(Z)</ref> 0 in the centered equation would be −10.60 with a standard error of 2.58. Additionally, the pairwise correlation parameters between λ (RC) and the other λ (p) terms have high standard errors because σ (RC) λ is small; there is little variation in λ <ref type="bibr">(RC)</ref> to identify correlations with the other λ <ref type="bibr">(p)</ref> terms. <ref type="bibr">19</ref> This is analogous to the approach used to address the initial condition problem by <ref type="bibr" target="#b7">Erdem and Keane (1996)</ref>, who use the first two years of their data set to approximate the past purchase history of panel members.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Flow Diagram of the Proposed Individual-Level Acquisition and Retention Process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode for Computing Objective Function˜ function PROXYLL(θ, z 1:N ,ỹ 1:N , d,Σ N , K) Simulate K draws of λ and compute conditional panel selection probabilities for each draw: for all k 1, 2, . . . , K do Using the k-th term of a four-dimensional Halton sequence, simulate λ (k) from the mixing distribution log λ k (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (Color online) Simulation Study: Holdout MAPE for QIA, QIC, QRA, and QRC by Method and Data Setting, Averaging across Parameter Settings and Disclosures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Spotify: Quarterly Additions, Losses, and Total Subscribers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (Color online) Spotify: Estimated Quarterly Acquisitions by Form, Initial vs. Repeat</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>McCarthy and Oblander: Scalable Data Fusion with Selection CorrectionMarketingScience, 2021, vol. 40, no. 3, pp. 459-480, © 2021 </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>McCarthy and Oblander: Scalable Data Fusion with Selection CorrectionMarketingScience, 2021, vol. 40, no. 3, pp. 459-480, © 2021 </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Simulation Study: Holdout MAPE by Method and Disclosure, Averaging Across Settings</figDesc><table><row><cell cols="4">McCarthy and Oblander: Scalable Data Fusion with Selection Correction</cell></row><row><cell>Marketing Science</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Disclosure</cell><cell>PAN</cell><cell>AGG</cell><cell>MPL</cell></row><row><cell>QIA</cell><cell>31.0%</cell><cell>23.5%</cell><cell>9.2%</cell></row><row><cell>QIC</cell><cell>10.6%</cell><cell>23.0%</cell><cell>8.0%</cell></row><row><cell>QRA</cell><cell>17.1%</cell><cell>9.1%</cell><cell>3.9%</cell></row><row><cell>QRC</cell><cell>17.2%</cell><cell>10.3%</cell><cell>4.2%</cell></row><row><cell>ADD</cell><cell>17.8%</cell><cell>2.4%</cell><cell>2.0%</cell></row><row><cell>LOSS</cell><cell>14.5%</cell><cell>2.6%</cell><cell>2.1%</cell></row><row><cell>END</cell><cell>84.6%</cell><cell>0.9%</cell><cell>0.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Parameter Recovery Comparison by Parameter Category: Baseline Parameter Setting</figDesc><table><row><cell cols="2">McCarthy and Oblander: Scalable Data Fusion with Selection Correction</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Marketing Science, 2021, vol. 40, no. 3, pp. 459-480, © 2021 INFORMS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MPL</cell><cell></cell><cell>AGG</cell><cell></cell></row><row><cell>Parameters</cell><cell>MAPB (%)</cell><cell>IQR (%)</cell><cell>MAPB (%)</cell><cell>IQR (%)</cell></row><row><cell>Heterogeneity means</cell><cell>0.0%</cell><cell>13.6%</cell><cell>61.6%</cell><cell>188.0%</cell></row><row><cell>Heterogeneity variances</cell><cell>0.7%</cell><cell>7.7%</cell><cell>168.7%</cell><cell>694.7%</cell></row><row><cell>Heterogeneity correlations</cell><cell>8.0%</cell><cell>60.0%</cell><cell>152.0%</cell><cell>709.0%</cell></row><row><cell>Homogeneous parameters</cell><cell>0.6%</cell><cell>3.4%</cell><cell>26.2%</cell><cell>99.9%</cell></row><row><cell>Selection parameters</cell><cell>1.4%</cell><cell>11.2%</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Parameter Recovery Comparison by Parameter Category: Baseline Parameter Setting</figDesc><table><row><cell></cell><cell></cell><cell cols="2">McCarthy and Oblander: Scalable Data Fusion with Selection Correction</cell></row><row><cell></cell><cell></cell><cell>Marketing Science</cell><cell></cell></row><row><cell>Parameters</cell><cell>Mean absolute bias (%)</cell><cell>Coefficient of variation (%)</cell><cell>Coverage (95% confidence interval)</cell></row><row><cell>Heterogeneity means</cell><cell>1.0%</cell><cell>11.0%</cell><cell>95.0%</cell></row><row><cell>Heterogeneity variances</cell><cell>0.6%</cell><cell>6.8%</cell><cell>95.0%</cell></row><row><cell>Heterogeneity correlations</cell><cell>6.8%</cell><cell>42.1%</cell><cell>98.3%</cell></row><row><cell>Homogeneous parameters</cell><cell>0.2%</cell><cell>3.2%</cell><cell>95.0%</cell></row><row><cell>Selection parameters</cell><cell>1.4%</cell><cell>10.9%</cell><cell>98.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Spotify: Average Holdout MAPE for All Disclosures and Models</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Marketing Science</cell></row><row><cell>Model</cell><cell cols="3">Aggregate data Panel data ADD</cell><cell>LOSS</cell><cell>END</cell></row><row><cell>GLS</cell><cell>All</cell><cell>No</cell><cell cols="3">23.4% 31.0% 22.1%</cell></row><row><cell>SSW</cell><cell>All</cell><cell>No</cell><cell cols="2">25.3% 24.3%</cell><cell>6.7%</cell></row><row><cell>MFH</cell><cell>All</cell><cell>No</cell><cell>14.4%</cell><cell>8.9%</cell><cell>7.1%</cell></row><row><cell>Proposed</cell><cell>None</cell><cell>Yes</cell><cell>481%</cell><cell cols="2">628% 860%</cell></row><row><cell></cell><cell>END only</cell><cell>No</cell><cell cols="3">132.8% 211.5% 10.5%</cell></row><row><cell></cell><cell>END only</cell><cell>Yes</cell><cell cols="2">25.3% 41.9%</cell><cell>8.3%</cell></row><row><cell></cell><cell>All</cell><cell>No</cell><cell cols="2">13.1% 13.3%</cell><cell>6.4%</cell></row><row><cell></cell><cell>All</cell><cell>Yes</cell><cell>13.0%</cell><cell>9.8%</cell><cell>4.7%</cell></row><row><cell cols="6">Figure 4. (Color online) Spotify: Average MAPE by Forecasting Horizon for END Disclosures (Proposed Model vs. MFH)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Parameter Estimates (Standard Errors): Spotify</figDesc><table><row><cell cols="2">McCarthy and Oblander: Scalable Data Fusion with Selection Correction</cell><cell></cell><cell></cell></row><row><cell cols="2">Marketing Science, 2021, vol. 40, no. 3, pp. 459-480, © 2021 INFORMS</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Acquisition</cell><cell></cell><cell>Churn</cell></row><row><cell>Parameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Initial behavior</cell><cell></cell><cell></cell><cell></cell></row><row><cell>λ 0</cell><cell>2.616 ×10 −10 (0.490×10 −10 )</cell><cell></cell><cell>0.012 (0.116)</cell></row><row><cell>c</cell><cell>3.369 (0.008)</cell><cell></cell><cell>1.175 (0.595)</cell></row><row><cell>β Q1</cell><cell>0.250 (0.043)</cell><cell></cell><cell>−0.483 (1.714)</cell></row><row><cell>β Q2</cell><cell>0.319 (0.075)</cell><cell></cell><cell>−0.105 (0.605)</cell></row><row><cell>β Q3</cell><cell>0.265 (0.066)</cell><cell></cell><cell>−0.178 (0.872)</cell></row><row><cell>σ λ</cell><cell>2.494 (0.073)</cell><cell></cell><cell>5.517 (22.748)</cell></row><row><cell>π A</cell><cell>0.983 (0.048)</cell><cell></cell><cell></cell></row><row><cell>Repeat behavior</cell><cell></cell><cell></cell><cell></cell></row><row><cell>λ 0</cell><cell>8.362×10 −5 (14.972×10 −5 )</cell><cell></cell><cell>0.031 (0.043)</cell></row><row><cell>c</cell><cell>2.286 (0.128)</cell><cell></cell><cell>0.109 (0.111)</cell></row><row><cell>β Q1</cell><cell>−1.856 (0.106)</cell><cell></cell><cell>2.684 (1.283)</cell></row><row><cell>β Q2</cell><cell>−0.632 (0.277)</cell><cell></cell><cell>0.602 (0.901)</cell></row><row><cell>β Q3</cell><cell>−1.793 (0.806)</cell><cell></cell><cell>1.812 (2.477)</cell></row><row><cell>σ λ</cell><cell>1.174 (1.106)</cell><cell></cell><cell>0.019 (0.035)</cell></row><row><cell>π A</cell><cell>0.996 (0.004)</cell><cell></cell><cell></cell></row><row><cell>Panel selection β (Z) 0</cell><cell>0.514 (41.834)</cell><cell></cell><cell></cell></row><row><cell>β (Z) IA</cell><cell>−0.156 (2.530)</cell><cell>β (Z) IC</cell><cell>−0.600 (2.307)</cell></row><row><cell>β (Z) RA</cell><cell>2.893 (1.587)</cell><cell>β (Z) RC</cell><cell>−2.868 (4.918)</cell></row><row><cell>Correlation ρ (IA,IC) λ</cell><cell>0.516 (0.152)</cell><cell>ρ (IC,RA) λ</cell><cell>0.604 (0.250)</cell></row><row><cell>ρ (IA,RA) λ</cell><cell>0.994 (0.275)</cell><cell>ρ (IC,RC) λ</cell><cell>−0.052 (2.448)</cell></row><row><cell>ρ (IA,RC) λ</cell><cell>0.035 (4.098)</cell><cell>ρ (RA,RC) λ</cell><cell>−0.005 (5.573)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>McCarthy and Oblander: Scalable Data Fusion with Selection Correction</figDesc><table><row><cell>Marketing Science</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Vol. 40, No. 3, May-June 2021, pp. 459-480</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">MarketingScience, 2021, vol. 40, no. 3, pp. 459-480, © 2021 </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors contributed equally to this work. They thank Second Measure for providing access to their credit card panel data; Eric Bradlow and Peter Fader for discussion and guidance throughout the research process; and Shane Jensen, Kinshuk Jerath, Andrey Simonov, Oded Netzer, Olivier Toubia, Asim Ansari, Benjamin Levine, and Matteo Alleman for helpful suggestions. The authors also thank Emory University's Research Computing team for access to and support in the use of computing resources. The authors do not have any financial interest, direct or indirect, in the company studied in the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Spotify Implementation Details</head><p>In this appendix, we provide additional details on our application using Spotify and Second Measure data.</p><p>1. We assume annual formation of new prospect pools and compute the size of each prospect pool based on the world population at and after the time of Spotify's incorporation. That is, we define the size of the initial prospect pool (born at Spotify's time of incorporation in 2008) as the global population in 2008, the prospect pool born in 2009 as the net growth in global population from 2008 to 2009, and so on. We assume that all panel members in the Second Measure data are drawn from the initial 2008 prospect pool.</p><p>2. World population data come from the World Bank (https://data.worldbank.org/indicator/SP.POP.TOTL). This data set contains the world population each year through 2017, whereas the aggregate and disaggregate Spotify data runs through and including Q3 2018, and projections of future customer behavior requires projection of the world population further into the future. As in <ref type="bibr" target="#b19">McCarthy et al. (2017)</ref> and <ref type="bibr" target="#b18">McCarthy and Fader (2018)</ref>, we run a time series regression using world population data (from 1963 to 2017) to forecast the world population in future years. The world population was strictly increasing over this period. Although a simple linear regression of year-on-year percentage change in world population by year does not reject the null hypothesis of the augmented Dickey-Fuller test of nonstationarity, fitting the data to an ARIMA(0,1,0) model via maximum likelihood rejects the null hypothesis that a unit root is present (test statistic: −4.41, p &lt; 0.01). Therefore, we use an ARIMA(0,1,0) specification, which has an R 2 99.0%.</p><p>3. Spotify provides a promotional offer to new customers in the second and fourth quarters of each calendar year, allowing prospects to pay a discounted price upfront to trial the service for the next three months. Virtually all trial amounts are less than $1.30, far below Spotify's regular price of $9.99 per month. As such, there are a number of panel members with a spend amount of $1.30 or less in one month, followed by no payments in the next two months (while the trial offer is still in effect). We assume that subscribers are retained during the promotional period.</p><p>4. The data we obtained from Second Measure do not include any panel members who churned from the panel during the observation period. The proposed approach could be extended to data sets with panel attrition by incorporating an additional timing process for the duration between when panel members enter the panel to when they leave the panel, as long as the date of panel entry was also observed.</p><p>5. There were no new panel members acquired into the data set during the observation period.</p><p>6. The credit card panel data set has a number of so-called skips, or months for which there is no payment, despite there being payments in the month immediately before and after. Second Measure noted that these skips are often because of the timing of when payments are processed versus when they are charged. For this reason, we assume that customers are retained in single-month skips. Accordingly, our model incorporates a one-month lag between when a customer churns and when they are first eligible to be reacquired: In particular, if a customer churns in month m (churn defined as the first month in which there is not a payment), they are reborn as a prospect in month m + 1, and are first eligible to be reacquired in month m + 2. As such, our model specification is in concordance with the assumption that customers are retained in single-month skips: In our specification, a churn entails at least two months of dormancy.</p><p>7. Our panel data set is left-truncated. Second Measure's panel dates back to January 2011, but they provided us with granular data that begins in January 2015 for just the panel members who made no purchases at Spotify from when Second Measure's data began in January 2011 through December 2014 and then registered their first payment in the data set during the observation period. We assume that all active panel members were initially acquired in the first month we observe a payment <ref type="bibr" target="#b30">McCarthy and</ref><ref type="bibr">Oblander: Scalable Data Fusion with Selection Correction Marketing Science, 2021, vol. 40, no. 3, pp. 459-480, © 2021 INFORMS</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do disclosures of customer metrics lower investors&apos; and analysts&apos; uncertainty but hurt firm performance?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Skiera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="259" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Differentiated products demand systems from a combination of micro and macro data: The new car market</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levinsohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pakes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Political Econom</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="105" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quasi-random maximum simulated likelihood estimation of the mixed multinomial logit model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Res. Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="677" to="693" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<title level="m">Normal Approximation and Asymptotic Expansions</title>
				<meeting><address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Customer franchise-A hidden, yet crucial, asset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bonacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contempory Accounting Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1024" to="1049" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating disaggregate models using aggregate data through augmentation of individual choice</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="621" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fusing multiple sources of data to understand ride-hailing use</title>
		<author>
			<persName><forename type="first">Oblander ;</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Lavieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Pendyala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scalable Data Fusion with Selection Correction Marketing Science</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
	<note>Record</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decision-making under uncertainty: Capturing dynamic brand choice processes in turbulent consumer goods markets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Keane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusing aggregate and disaggregate data with an application to multiplatform media consumption</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Feit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="348" to="364" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indirect inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gourieroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renault</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Econometrics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="S85" to="S118" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Customer capital</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gourio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudanko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Econom. Stud</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1102" to="1136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Valuing customers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Stuart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="18" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large sample properties of generalized method of moments estimators</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1029" to="1054" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying the hand of past: Distinguishing state dependence from heterogeneity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Econom. Rev</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="79" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Causal Inference in Statistics, Social, and Biomedical Sciences</title>
				<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Statistical Analysis with Missing Data</title>
				<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">793</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Who participates in household scanning panels?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Agricultural Econom</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="226" to="240" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Response modeling with nonrandom marketing-mix variables</title>
		<author>
			<persName><forename type="first">P</forename><surname>Manchanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chintagunta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="478" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Customer-based corporate valuation for publicly traded noncontractual firms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="617" to="635" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Valuing subscription-based businesses using publicly disclosed customer data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Who&apos;s got the coupon? Estimating consumer preferences and coupon usage from aggregate information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Musalem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Raju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="715" to="730" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian estimation of random-coefficients choice models using aggregate data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Musalem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Raju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Econometrics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="490" to="516" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linking customer and financial metrics to shareholder value: The leverage effect in customerbased valuation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Skiera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A bivariate timing model of customer acquisition and retention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schweidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="829" to="843" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding service retention within and across cohorts using limited information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schweidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="94" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incorporating direct marketing activity into latent attrition models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schweidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Knox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="487" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Listening in on social media: A joint model of sentiment and venue format choice</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schweidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Moe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="387" to="402" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Train</surname></persName>
		</author>
		<title level="m">Discrete Choice Methods with Simulation</title>
				<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic and competitive effects of direct mailings: A charitable giving application</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Diepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Donkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Franses</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="133" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Oblander</forename><surname>Mccarthy</surname></persName>
		</author>
		<title level="m">Scalable Data Fusion with Selection Correction Marketing Science</title>
				<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="459" to="480" />
		</imprint>
	</monogr>
	<note>© 2021 INFORMS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
