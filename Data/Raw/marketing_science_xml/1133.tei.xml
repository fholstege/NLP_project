<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentence-Based Text Analysis for Customer Reviews</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-07-18">July 18, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joachim</forename><surname>Büschken</surname></persName>
							<email>joachim.bueschken@ku.de</email>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
							<email>allenby.1@osu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Catholic University of Eichstätt-Ingolstadt</orgName>
								<address>
									<postCode>85049</postCode>
									<settlement>Ingolstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Fisher College of Business</orgName>
								<orgName type="institution">Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>Ohio</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentence-Based Text Analysis for Customer Reviews</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2016-07-18">July 18, 2016</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2016.0993</idno>
					<note type="submission">Received: August 26, 2013; accepted: October 8, 2015; Preyas Desai served as the editor-in-chief and Peter Fader served as associate editor for this article.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-13T12:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>extended LDA model</term>
					<term>user-generated content</term>
					<term>text data</term>
					<term>unstructured data</term>
					<term>Bayesian analysis</term>
					<term>big data Büschken and Allenby: Sentence-Based Text Analysis for Customer Reviews</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the challenges in understanding consumers is comprehending the language they use to express themselves. Words are difficult to understand because of their varied meaning among people. The word "data" may mean one thing to an analyst and something else to a teenager. Marketing has a long history of devising ways of cutting through the ambiguous use of words by designing questionnaires and experiments in such a way that questions are widely understood and expressed in simple terms. Qualitative interviews and other forms of pretesting are routinely used to identify the best way to query respondents for useful information.</p><p>Despite attempts to make things clear, the analysis of consumer response data continues to be challenged in providing useful insight for marketing analysis. Data collected on fixed-point rating scales, for example, are known to suffer from a multitude of problems such as yea-saying, nay-saying, and scale use tendencies that challenge inference. Moreover, some respondents have the expertise to provide meaningful feedback while others do not, and some provide somewhat independent evaluations about aspects of a product or service, while others tend to halo their responses <ref type="bibr" target="#b5">(Büschken et al. 2013)</ref>. Respondents are also known to substitute answers to questions different than the one being posed <ref type="bibr" target="#b12">(Gal and Rucker 2011)</ref> and exhibit state-dependent responses where item responses carry forward and influence later responses <ref type="bibr" target="#b6">(de Jong et al. 2012</ref>). Conjoint analysis is similarly challenged in getting respondents to make choices that mimic marketplace sensitivities <ref type="bibr" target="#b9">(Ding et al. 2005)</ref>, i.e., to obtain coherent and valid answers to the questions posed.</p><p>The growing availability of text data in the form of unstructured consumer reviews provides the opportunity for consumers to express themselves naturally while not being restricted to the design of a survey in the form of preselected items, available response items, and the forced use of rating scales. They simply say whatever they want to say in a manner and order that seems appropriate to them. The challenge in analyzing text data, as mentioned earlier, is in understanding what the words mean. The use of the word "hot" has a different meaning if it is paired with the word "kettle" as opposed to the word "car." As a result, a simple summary of word counts in text data will likely be confusing unless the analysis relates it to the other words that also appear without assuming an independent process of word choice.</p><p>The model and analysis presented in this paper is based on a class of models that are generally known as "topic" models <ref type="bibr" target="#b4">(Blei et al. 2003</ref><ref type="bibr" target="#b24">, Rosen-Zvi et al. 2004</ref>, where the words contained in a consumer review reflect a latent set of ideas or sentiments,</p><note type="other">954</note><p>Marketing Science 35(6), pp. 953-975, © 2016 INFORMS each of which is expressed with its own vocabulary. A consumer review may provide opinions on different aspects of a product or service, such as its technical features and ease of use, and also on aspects of service and training. The goal of these models is to understand the prevalence of the topics present in the text and to make inferences about the likelihood of the appearance of different words. Words that are likely to appear more often command greater weight in drawing inferences about the latent topic, while the co-occurring words add depth to interpretation.</p><p>Topic models provide a simple, yet powerful way to model high-level interaction of words in speech. The meaning of speech arises from the words jointly used in a sentence or paragraph of a document. Meaning can often not be derived from looking at singular words. This is very much evident in consumer reviews where consumers may use the adjective "great" in conjunction with the noun "experience" or "disappointment." When doing so, they may refer to different attributes of a particular product or service.</p><p>Empirical analysis of high level interaction of variables present unique challenges. Consider the hotel review data that we use in our empirical analysis (see Section 4). These data consist of 1,011 unique terms. An analysis of all two-level interactions, using this data set, implies to consider up to 1,011 2 or 1.02 million variables. It is immediately clear that an analysis of high-level interaction effects using traditional methods such as regression or factor analysis is very hard to conduct. By comparison, topic models do not require prior specification of interaction effects and are capable of capturing the pertinent co-occurring words up to the dimensionality of the whole vocabulary.</p><p>We propose a new variant of the topic model and compare it to existing models using data from online user-generated reviews of hotels and restaurants posted on the Internet. We find that, through a simple model-free analysis of the data, the sentences used in online reviews often pertain to one topic; that is, while a review may be comprised of multiple topics such as location and service, any particular sentence tends to deal with just one. We derive a restricted version of a topic model for predicting consumer reviews that constrains analysis so that each sentence is associated with just one topic, while allowing for the possibility that other sentences can also pertain to the same topic. We find this restriction is statistically supported in the data and leads to more coherent inferences about the hotel and restaurant reviews.</p><p>The remainder of this paper is organized as follows. We review alternative topic models and our proposed extension in the next section. In the appendix, we report on a simulation study that demonstrates the ability of our model to uncover the true data generating mechanism. We then present data on hotel reviews taken from www.expedia.com and restaurant reviews from www.we8there.com and examine the ability of our model to predict customer reviews. A comparison to alternative models is provided in Section 5, followed by concluding comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Topic Models for Customer Reviews</head><p>Text-based analysis of user-generated content (UGC) and consumer reviews has attracted considerable attention in the recent marketing literature. Textual consumer reviews have been used for a variety of purposes in marketing research:</p><p>• Predicting the impact of consumer reviews on sales using the valence of sentences <ref type="bibr" target="#b2">(Berger et al. 2010)</ref> • Determining the relative importance of reviews in comparison to own experience in the learning process of consumers about products <ref type="bibr" target="#b33">(Zhao et al. 2013)</ref> • Analyzing the change in conversion rates as a result of changes in affective content and linguistic style of online reviews <ref type="bibr" target="#b18">(Ludwig et al. 2013)</ref> • Predicting the sales of a product based on review content and sentiment <ref type="bibr" target="#b14">(Godes and Mayzlin 2004</ref><ref type="bibr" target="#b7">, Dellarocas et al. 2007</ref><ref type="bibr" target="#b13">, Ghose et al. 2012</ref> • Eliciting product attributes and consumers preferences for attributes <ref type="bibr">Bradlow 2011, Archak et al. 2011)</ref> • Deriving market structure <ref type="bibr">(Netzer et al. 2012, Lee and</ref><ref type="bibr" target="#b17">Bradlow 2011)</ref> These papers assume that informative aspects of text data are readily observed and can directly serve as covariates and inputs to other analyses. Typically, word counts and frequencies are used as explanatory variables to identify words that are influential in determining customer behavior or in discriminating among outcomes (e.g., satisfied versus unsatisfied experiences).</p><p>Alternatively, one may assume that specific words in UGC are only indicators of latent topics and that these topics are a priori unknown <ref type="bibr" target="#b27">(Tirunillai and Tellis 2014)</ref>. Latent topics are defined by a collection of words with a relatively high probability of usage and not from the prevalence or significance of single words. This is the key idea of latent topic modeling in latent Dirichlet allocation (LDA) <ref type="bibr" target="#b4">(Blei et al. 2003</ref>) and the author-topic model <ref type="bibr" target="#b24">(Rosen-Zvi et al. 2004</ref>) and the idea we are following here. <ref type="bibr" target="#b27">Tirunillai and Tellis (2014)</ref> apply a variant of the LDA model to UGC to capture latent topics and valence in UGC, to analyze topic importance for various industries over time and utilize the emerging topics for brand positioning and market segmentation.</p><p>The goals of our analysis of customer review data are to (i) identify latent topics in customer reviews and assess their predictive performance of satisfaction and (ii) contrast alternative methods of creating inferences about the latent topics. Issues present in both questions are whether simple word choice probabilities are sufficient for establishing meaning in the evaluations and the degree to which topics provide richer insights through the co-occurrence of words in a review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Latent Dirichlet Allocation Model</head><p>A simple model for the analysis of latent topics in text data is the LDA model <ref type="bibr" target="#b4">(Blei et al. 2003</ref>). The LDA model assumes the existence of a fixed number of latent topics that appear across multiple documents, or reviews. Each document is characterized by its own mixture of topics d , and each topic is characterized by a discrete probability distribution over words; that is, the probability that a specific word is present in a text document depends on the presence of a latent topic. It is convenient to think of a dictionary of words that pertain to all reviews, with each topic defined by a unique probability vector of potential word use. Words with high probability are used to characterize the latent topics.</p><p>The nth word appearing in review d, w dn , is thought to be generated by the following process in the LDA model:</p><p>1. Choose a topic z dn ∼ Multinomial d . 2. Choose a word w dn ∼ from p w dn z dn . In the model, d is a document-specific probability vector associated with the topics z dn , and is a matrix of word-topic probabilities m t for word m and topic t, with p w dn = m z dn = t = p w dn = m t . The vector of word probabilities for topic t is thus t .</p><p>Topics z dn and words w dn are viewed as discrete random variables in the LDA model, and both are modeled using a multinomial, or discrete, distribution. The objects of inference are the parameters d and that indicate the probabilities of the topics for each document d and associated words for each topic t. A model involving T topics has dim d = T , and is an M × T matrix of probabilities for the M unique words that appear in the collection of customer reviews. The first element of d is the probability of the first topic in document d, and the first column of is the word probability vector 1 of length M for this first topic.</p><p>The potential advantage of the LDA model is its ability to collect words together that reflect topics of potential interest to marketers. Co-occurring words appearing within a document indicate the presence of a latent topic. These topics introduce a set of word interactions into an analysis so that words with high topic probabilities ( t ) are jointly predicted to be present. Since different topics are associated with different word probabilities, the topics offer a parsimonious way of introducing interaction terms into text analysis. Moreover, the LDA model is not overly restrictive in that it allows each document, or customer review, to be characterized by its own set of topic probabilities ( d ).</p><p>We complete the specification of the standard LDA model by assuming a homogeneous Dirichlet prior for T ). We note that the LDA model does not impose any structure on the data related to the plates; i.e., it assumes that the latent topics z dn can vary from word to word, sometimes referred to as a "bag-ofwords" assumption in the text analysis literature. This assumption differs from the traditional marketing assumption of heterogeneity that exploits the panel structure often found in marketing data where multiple observations are known to be associated with the same unit of analysis. There is a marketing literature on what is known as context-dependent or structural heterogeneity <ref type="bibr" target="#b16">(Kamakura et al. 1996</ref><ref type="bibr" target="#b30">, Yang and Allenby 2000</ref><ref type="bibr" target="#b31">, Yang et al. 2002</ref>) that allows the model likelihood to vary across observations. Restricted versions of the assumption made by the LDA model for observational heterogeneity include models of change points <ref type="bibr" target="#b8">(DeSarbo et al. 2004</ref>) and latent Markov models <ref type="bibr" target="#b10">(Fader et al. 2004</ref><ref type="bibr" target="#b21">, Netzer et al. 2008</ref><ref type="bibr" target="#b20">, Montoya et al. 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sentence-Constrained LDA Model</head><p>We find in the analysis of our data presented below that it is beneficial to constrain the LDA model so that words within a sentence pertain to the same topic. People tend to change topics across sentences, but not within a sentence. The LDA model assumes that the words within a document provide exchangeable information regarding the latent topics of interest, and we note that the data index (n) is simply an index for the word; i.e., it is not related to the authors or the reviews themselves. Our sentence-constrained model moves away from this bag-of-words assumption.</p><p>Figure <ref type="figure">2</ref> displays a plate diagram for our proposed sentence-constrained LDA (SC-LDA) model. A replication plate is added to distinguish the sentences within a review from the words within each sentence. Additional indexing notation is introduced into the model to keep track of the words (n) contained within the sentences (s) within each review (d), w dsn . The latent topic variable z ds is assumed to be the same for all words within the sentence and is displayed outside of the word plate in Figure <ref type="figure">2</ref>. We assume that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documents within corpus</head><p>Words within documents</p><formula xml:id="formula_0">z dn w dn k Topics Corpus d ∈[1,...,D ] K∈[1,...,K ] n∈[1,...,N d ] d</formula><p>the number of sentences in a document (S d ) and the number of words per sentence (N ds ) are determined independently from the topic probabilities ( d ).</p><p>The probability of topic assignment changes because all words within a sentence are used to draw the latent topic assignment, z ds . This requires the estimation algorithm to keep track of the topic assignments by sentence, C SW T mt , as well as the number of words in each sentence, n ds . The appendix describes the estimation algorithm for the SC-LDA model.</p><p>The LDA model has been extended in a variety of ways in the statistics literature, by</p><p>• introducing author information <ref type="bibr" target="#b24">(Rosen-Zvi et al. 2004</ref>) that allows information to be shared across multiple documents by the same author,</p><p>• introducing latent labels for documents <ref type="bibr" target="#b23">(Ramage et al. 2010</ref>) that allow for unobserved associations of documents,</p><p>• incorporating a dynamic topic structure by modeling documents from different periods <ref type="bibr" target="#b3">(Blei and Lafferty 2006)</ref> or assuming that topic assignments are conditional on the previous word <ref type="bibr" target="#b29">(Wallach 2006)</ref> or topic <ref type="bibr" target="#b15">(Gruber et al. 2007</ref>),</p><p>• developing multiple topic layers <ref type="bibr" target="#b28">(Titov and McDonald 2008)</ref> where words in a document may stem either from a document-specific global topic or from the content of the words in the vicinity of a focal word,</p><p>• incorporating the sender-recipient structure of written communication into topic models <ref type="bibr" target="#b19">(McCallum et al. 2005</ref>; in the author-recipient topic model, both the sender and the recipient determine the topic assignment of a word), and</p><p>• incorporating informative word-topic probabilities consistent with domain knowledge through the prior distribution <ref type="bibr" target="#b0">(Andrzejewski et al. 2009)</ref>. Our analysis of text data is designed to uncover latent topics associated with user-generated topics and relate them to product ratings. In marketing, the amount of text available for analysis per review is limited, often having less than 20 words, and multiple reviews for the same author are rare. We therefore do not attempt to develop the LDA model by making it dynamic, having multiple layers of topics, or constraining the prior to reflect prior notions of topics. Instead, we relate user reviews to the topic probabilities with a latent regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sentence-Constrained LDA Model with</head><p>Ratings Data We extend the SC-LDA model with a cut-point model <ref type="bibr" target="#b26">(Rossi et al. 2001</ref><ref type="bibr" target="#b5">, Büschken et al. 2013</ref> to relate the topic probabilities to the ratings data. The advantage of employing a topic model is the ability to collect co-occurring words together as topics, which improves the interpretation of text data. Relating the latent topic probabilities to ratings data is similar to traditional driver analysis, but with UGC that is not constrained to a set of prespecified drivers. Our model offers an alternative to models of ratings data that A cut-point model relates responses on a fixedpoint rating scale to a continuous latent variable and a set of cut points,</p><formula xml:id="formula_1">r d = k if c k−1 ≤ d ≤ c k and d ∼ N d</formula><p>where the cut points c k provide a mechanism for viewing the discrete rating as a censored realization of the latent continuous variable ( d ) that is related to the topic probabilities ( d ) through a regression model. Our regression model is similar to a factor model where are the factor loadings and d are the factor scores.</p><p>The plate diagram for the SC-LDA model with ratings data (SC-LDA-Rating) is provided in Figure <ref type="figure">3</ref>. Our cut-point model is a simplified (i.e., homogenous) version of the model used by <ref type="bibr" target="#b32">Ying et al. (2006)</ref> </p><formula xml:id="formula_2">c = c 1 c 2 c K−1 = c 1 c 1 + 1 c 1 + 2 k=1 k c 1 + K−2 k=1 k</formula><p>where cut points c 0 and c K are − and , respectively, and the are strictly positive cut-point increments. Constraints are needed to identify the SC-LDA-Rating model. For K points in the rating scale, there are traditionally K − 1 free cutoffs if we set c 0 = − and c K = + . Two additional cutoff constraints are needed in our analysis because the regression model is specified with an intercept and error scale, and shifting all of the cutoffs by a constant or scaling all of the cutoffs is redundant with these parameters.</p><p>We also note that the topic probabilities for each document, d , are constrained to sum to one, and as a result the likelihood for the latent regression model is not statistically identified without additional constraints. As discussed in the appendix, we postprocess the draws from the Markov Chain Monte Carlo (MCMC) chain, arbitrarily picking one of the topics to form a contrast for the remaining topics (see <ref type="bibr">Rossi et al. 2005, Chapter 4</ref>). Postprocessing the draws results in inferences based on a statistically identified likelihood. Our proposed model and estimation strategy is discussed in more detail in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">SC-LDA Model with Sticky Topics</head><p>Figure <ref type="figure" target="#fig_4">4</ref> displays a hotel review. The color coding in the display is present to identify different potential topics, which are seen to change across sentences but not within sentences. For example, sentences describing "breakfast" are coded green, and sentences describing the "general experience" are coded blue. We note that in this review topics exhibit stickiness in the sense that the reviewer repeatedly stays with one topic over a number of consecutive sentences. Topic stickiness presents a potential violation of the assumption of independent and identically distributed (i.i.d.) topic assignments in the LDA model and its variants.</p><p>To account for sticky topics, we consider an extension of the SC-LDA-Rating model in which the topic z n−1 , assigned to sentence s n−1 , can exhibit carryover to s n . Stickiness for the purpose of this model is defined as z n = z n−1 . To develop this model, we consider a latent binary variable n that indicates whether the topic assignment to sentence s n is sticky</p><formula xml:id="formula_3">n = 1 z n = z n−1 n = 0 z n ∼ Multinomial d (1)</formula><p>In the SC-LDA model, n = 0 ∀ n, which implies that the SC-LDA with sticky topics can be thought of as a general case of the SC-LDA. We assume to be distributed Binomial with a topic-specific probability t n ∼ Binomial t</p><p>(2) Figure <ref type="figure">5</ref> displays an example of a DAG (Directed Acyclic Graph) for the sticky topic model, given five consecutive sentences in a review. In the upper panel of Figure <ref type="figure">5</ref>, we consider the general case of being unknown. In the lower panel of Figure <ref type="figure">5</ref>, we consider the case of a particular -sequence that reveals sticky and nonsticky topics. In both versions of the DAG, we omit all fixed priors and the assignment of words to the sentences for better readability. In the lower panel of Figure <ref type="figure">5</ref>, for cases of n = 1, relationships between z and are omitted, and the resulting (deterministic) relationships between z n and z n−1 are added to the graph, indicating first-order dependency of the topic assignments. As the DAG in the lower panel of Figure <ref type="figure">5</ref> shows, a value of n = 1 shuts off the relationship between z n and d and establishes a relationship between z n and z n−1 so that z n = z n−1 . This also implies that "observed" topic switches (z n = z n−1 ) are indicative of a topic draw from d . Note that in Figure <ref type="figure">5</ref> we omitted 1 for the first sentence because topic assignments do not carry over between documents. Thus, we fix 1 = 0 and assume z 1 to be generated by d , as no prior topic assignment exists.</p><p>We relate the stickiness of topics to the number of sentences in a review through a regression model</p><formula xml:id="formula_4">d t = e X d t 1 + e X d t</formula><p>(3) where covariate vector X d consists of a baseline and the observed number of sentences in each review, and t is a vector of topic-specific regression coefficients. A priori, it seems reasonable to assume that, as reviews contain more sentences, topics have a tendency to be carried over to the next sentence (see an example in Figure <ref type="figure" target="#fig_4">4</ref>). The approach in Equation ( <ref type="formula">3</ref>) allows for heterogeneity among reviews with respect to topic stickiness. In the appendix, we outline the estimation details for the SC-LDA model with sticky "The hotel was really nice and clean. It was also very quiet. There was a thermostat in each room so you can control the coolness. The bathroom was larger than in most hotels. The breakfast was sausage and scrambled eggs, or waffles you make yourself on a waffle iron. All types of juice, coffee, and cereal available. The breakfast was hot and very good at no extra charge. The only problem was the parking for the car. The parking garage is over a block away. It is $15.00 per day. You don't want to take the car out much because you can't find a place to park in the city, unless it is in a parking garage. The best form of travel is walking, bus, tour bus, or taxi for the traveler. The hotel is near most of the historic things you want to see anyway. I would return to this hotel and would recommend it highly."</p><p>Note. Potential sentence topics are highlighted in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5</head><p>Graphical Representation of the SC-LDA Model with Sticky Topics</p><formula xml:id="formula_5">2 2 = 1 3 3 = 0 4 4 = 1 5 5 = 0 z 2 z 2 z 1 z 1 s 2 s 2 s 3 s 3 s 4 s 4 s 5 s 5 s 1 s 1 z 3 z 3 z 5 z 5 z 4 z 4 Φ Φ</formula><p>topics and demonstrate statistical identification using a simulation study. We also address in the appendix, through simulation, the question of whether a standard LDA model, which assumes that topics are assigned to words and not sentences, is able to recover topic probabilities that are sentence based. We find that the standard LDA model exhibits low recovery rates of the true topic assignments when topics are characterized by larger sets of co-occurring terms and longer sentences (i.e., more words). Only when topics are associated with a few unique terms and when sentences contain a few words will using the LDA model yield results similar to that of the SC-LDA model.</p><p>An assumption common to all LDA-based models analyzed in this research, including the sentenceconstrained topic model and the model with sticky topics, is independence of the latent topics to the number of words and sentences in a review. In effect, we treat these observed quantities as independently determined and uninformative with respect to topics. Because the topic probabilities (and topic assignments) are latent in our model, this can only be ascertained by building a new model that allows for a dependency and comparing its fit relative to the fit of our proposed model. We note that while we postulate a priori that d (or z ds ) is independent of S d (or N ds ), this does not imply that they are a posteriori independent, given the data. We investigated this issue and found the average (absolute) correlation of the topic shares to the number of sentences across topics and data sets to be 0.08 (standard deviation (SD) = 0 08). The maximum (absolute) correlation of d to S d for one of the topics from any data set is 0.3. We find the same result for the correlation of d to the number of words in a review. Additionally, conditional on the topic assignment of sentences (z ds ), we find that the topics are very similar with respect to the number of words generated for each sentence across all data sets. In conclusion, we do not believe our assumption of independency to be a significant issue for our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Empirical Analysis</head><p>This section presents results from applying the LDA, SC-LDA, and sticky SC-LDA models to consumer review data. Since ratings are available in all data sets, we only use topic models that incorporate the rating as a function of . We employ three data sets for comparison purposes: reviews of Italian restaurants from the website www.we8there.com and two sets of reviews from www.expedia.com pertaining to upscale hotels in Manhattan and hotels near John F. Kennedy</p><p>Marketing Science 35(6), pp. 953-975, © 2016 INFORMS (JFK) International Airport. We find that our proposed SC-LDA-Rating model more accurately predicts consumer ratings than other models and is shown to lead to more coherent inferences about the latent topics. Characteristics of the data and preprocessing are discussed first, followed by a model comparison of insample and predictive fit.</p><p>Prior to data analysis, reviews were preprocessed using the following sequence of steps:</p><p>1. Splitting text into sentences identified through ".", ",", "!", or "?"; after the sentence split, all punctuation is removed 2. Substituting capital letters with lower-case letters 3. Removing all terms which appear in less than 1% of the reviews of a data set (i.e., "rare words") 4. Removing stop words using a standard vocabulary of stop words in the English language</p><p>The removal of rare words is motivated by the search for co-occurring terms ("topics"). Rare words make little to no contribution to the identification of such topics because of their rarity. Similarly, the removal of stop words is motivated by their lack of discriminatory power with respect to topics as all topics typically contain such words.</p><p>Stemming is absent from our preprocessing procedure. This is because words sharing the same stem may have different meaning. Consider, for example, the words "accommodating" and "accommodation," which share the stem "accommod." The word "accommodating" is mostly used to describe aspects of a service process or interaction with service personnel. The term "accommodation" is often used in the context of a hotel stay and typically refers to amenities of a hotel room and does not refer to interactions with service personnel. Thus, stemming may eliminate differences in meaning which, for identification and interpretation of latent topics, is not desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>We obtained 696 reviews of Italian restaurants comprising a corpus of 43,685 words. The vocabulary of this data set consists of W = 1,312 unique terms (after preprocessing). For the analysis of hotels, we consider hotels located in downtown New York (Manhattan) and hotels within a two-mile radius of JFK airport. We obtained 3,212 reviews of Manhattan upscale hotels and 1,255 reviews of midscale hotels near JFK airport. The corpora of Manhattan hotel reviews and JFK hotel reviews comprise 73,314 and 25,970 words, respectively. Both hotel data sets are based on a working vocabulary of W = 1,011 words. The hotel and restaurant data sets contain an overall evaluation of the service experience on a five-point rating scale, where a higher rating indicates a better experience.</p><p>Table <ref type="table" target="#tab_0">1</ref> provides numerical summary statistics of the preprocessed data based on word and sentence counts. On average, upscale hotel reviews contain 4.3 sentences with 5.3 words per sentence. The standard deviation of the number of sentences per review is 3.4, indicating significant heterogeneity among reviews with regard to the amount of information contained therein. Midscale hotel reviews contain a similar number of sentences (3.8) on average, with an average of 5.4 words per sentence. The Italian restaurant reviews contain an average of 12.2 sentences, each of which contain, on average, 5.2 words. The range of the number of sentences is 90, significantly higher than in the hotel data sets. Thus, restaurant reviews are longer and significantly more heterogeneous with respect to sentence count. It appears that restaurant reviewers feel the need to inform readers about restaurants in a more detailed fashion.</p><p>Reviews provided by both hotel and restaurant customers typically exhibit a sentence structure, although such a structure is not required; that is, reviewers voluntarily organize their reviews by using periods and capital letters to structure text. For example, Expedia accepts content in many forms, and some reviews exhibit a structure more compatible with a bag-ofwords assumption. However, such a free structure is apparently not the norm. On average, hotel reviewers use about 4 sentences, which indicates their desire to differentiate statements within a review. The standard deviation of the number of sentences is about 3 across the segments, pointing at heterogeneity of structure. The Italian restaurant reviews in our data contain an average of 12 sentences, with a standard deviation of 11.</p><p>Table <ref type="table" target="#tab_0">1</ref> reveals that the Manhattan hotels received an average rating of 4.4. The standard deviation of the rating of 0.9 indicates that many customers rated their experience at the top of the scale (share of 61.3%). Very few customers (4.5%) rated their experience toward the bottom of the scale (1 or 2). This is different for the airport hotels, which, on average, received a lower rating of 3.8 and where a larger share of customers (17.4%) rated their experience as bad (rating of 1 or 2). Italian restaurants received an average rating of 3.8. Thirty-two percent of the reviewers rated their experience as bad (1 or 2 stars). Forty-seven percent chose the best rating possible. Apparently, restaurant reviews are particularly useful to identify critical issues best avoided, and Manhattan hotel reviews are more informative about positive drivers of customers' experiences. Whereas restaurant reviews contain a lot of information (by word and sentence count), the challenge in hotel reviews is to extract managerially relevant information from less data per review.</p><p>We begin our analysis of the text by providing a simple summary of words appearing by rating for the hotel and restaurant reviews, given the preprocessed data sets. A rating of four or five on overall satisfaction indicates satisfaction with the hotel stay or restaurant visit, whereas a rating of one or two indicates dissatisfaction. Table <ref type="table" target="#tab_1">2</ref> provides a list of the top 30 words occurring in good and bad overall evaluations for the hotel and restaurant data described in Table <ref type="table" target="#tab_0">1</ref>. Both good and bad upscale hotel evaluations are associated with adjectives "great," "good," "nice," and "clean." Frequent nouns in both categories are "location," "staff" "service," and "room(s)." Bad upscale reviews are uniquely associated with the adjective "small" and the nouns "bathroom" and "bed," indicating possible reasons for a bad experience. Good upscale reviews are uniquely associated with the terms "excellent" and "everything." Neither of these terms point at possible reasons for the good experience. Frequent words in midscale hotel reviews contain terms exclusive to the review selection; that is, terms such as "airport," "JFK," and "shuttle" are unique to the location of the hotels selected here. However, similar to upscale hotel reviews, we find that the vocabulary differs little with respect to ratings. The sets of the top 10 words in good and bad reviews are identical except for the term "one" in bad reviews (rank 21 in good reviews). Frequent words in both good and bad restaurant reviews include "pizza," "good," and "food," which indicates that these terms cannot discriminate ratings. In general, a simple listing of frequently observed words in good and bad reviews does not help much to discriminate good from bad ratings.</p><p>A problem with the simple analysis of word frequencies is that it is limited to the marginal analysis of predefined groups. The analysis of word counts or frequencies by rating or other observed variables is informative only of these individual classification variables. It does not identify the combinations of classification variables that lead to unique themes or topics for analysis. The reason for employing model-based analysis of the data is that it helps to reveal the combination of classification variables for which unique themes and points of differentiation are present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Fit</head><p>Table <ref type="table" target="#tab_2">3</ref> summarizes the in-sample fit and predictive fit of the topic models applied to the three data sets.</p><p>In the empirical analysis, we only use topic models that incorporate a customer's rating information in the model estimation. Table <ref type="table" target="#tab_2">3</ref> reports the log-marginal density (LMD) of the data for different models. The fit statistics are averaged over the number of topics to save space. For each model and data set, we estimate T ∈ 2 20 and find a consistent ordering of the fit statistic for the in-sample and predictive fit. We use 90% of the available data for calibration and the remaining 10% for out-of-sample prediction based on a random split of the reviews.  Table <ref type="table" target="#tab_2">3</ref> also shows that the SC-LDA model with i.i.d. topic assignments performs consistently better than the SC-LDA model with sticky topics. This result is independent of using in-sample or out-of-sample fit as the fit measure. However, the difference in fit is relatively small for all data sets. For example, for the Italian restaurant data, the in-sample log marginal density of the data, using the SC-LDA model, is −235,586, compared to −236,972 for the SC-LDA model with sticky topics. The difference in out-of-sample fit is similarly small (LMD of −26,458 compared to −26,515) for this data set. We find that the SC-LDA model with sticky topics rarely points at topics being very "sticky." In fact, we very rarely observe values for t larger than 0.20 for any topic in all three data sets. The average t across data sets and topic numbers is less than 0.03, implying that the SC-LDA model with sticky topics becomes equivalent to the SC-LDA model in many cases. This also implies that stickiness of topics across consecutive sentences is not an important feature of the customer review data sets analyzed here.</p><p>Further analysis of the results indicates that the sentence constraint reduces the likelihood of observing frequent words and increases the likelihood of infrequently occurring words within topics. To illustrate, we consider results from the Expedia midscale hotel data. Figure <ref type="figure" target="#fig_5">6</ref> plots t for the sentence constrained and unconstrained LDA model and for each topic, ordered by their word choice probabilities. Note that for all topics and models, the area under the curve of t must integrate to one. The left panels in Figure <ref type="figure" target="#fig_5">6</ref> show t for the top 200 ranked words, and the right panels in Figure <ref type="figure" target="#fig_5">6</ref> show t for the lower ranked words (ranks 201 to 1,000) in the topics.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> reveals that the sentence constraint leads to smaller probabilities for the most likely words than the unconstrained model, and higher probabilities for words that are less likely. This result is independent of the topics. It suggests that the SC-LDA model penalizes the most likely words compared with the LDA model by assigning relatively lower probabilities to these words. In comparison, the sentence constraint favors less frequent words. The reason for the penalty on frequent terms is that the sentence constraint assigns topics on the basis of context, where context is provided by the words appearing together in a sentence. The reductions in in-sample fit reported above are influenced by the tendency of the sentence constraint to assign less extreme word-choice probabilities to the terms compared to the unconstrained topic models.</p><p>The fit of the rating-based topic models can also be evaluated on the basis of the explanatory power with respect to the satisfaction rating. Table <ref type="table" target="#tab_4">4</ref> compares the share of variance of the latent continuous evaluation explained by the three topic models for the three data sets. The fit measure presented is the share of variance of explained by the covariates. We report the posterior mean and the posterior SD of this pseudo-R 2 and the number of topics (T ) for the best-fitting model. The results in Table <ref type="table" target="#tab_4">4</ref> imply that the sentence constraint leads to improved explanatory power of the latent topics with respect to the satisfaction rating in all three data sets. The improvement ranges from 10% (restaurant data) to 36% (upscale hotel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Predicting Customer Ratings</head><p>We investigate use of the latent topics to predict and explain consumer ratings of hotels and restaurants. The goal of our analysis is to identify themes associated with positive and negative reviews, comparing results from the model-free analysis reported in Table <ref type="table" target="#tab_1">2</ref> to topics in the SC-LDA-Rating model. This information is useful for improving products and services by identifying potential drivers of customer satisfaction. For all subsequent analyses, we use the SC-LDA-Rating model with a number of topics that maximizes predictive fit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Italian Restaurants</head><p>Table <ref type="table" target="#tab_6">5</ref> displays the top 30 words associated with the best-fitting SC-LDA-Rating model for the Italian restaurant data set (T = 9). Summary descriptions of the topics are offered at the top of Table <ref type="table" target="#tab_6">5</ref>. We find Words (rank ordered by Phi) Posterior mean of Phi Note. From top to bottom, results show t for t = 1 t = 2, and t = 3. that in this data set, and in the other two data sets, the words for each topic provide a more coherent description of the product than that provided by the most frequently used words list in Table <ref type="table" target="#tab_1">2</ref>. Topic 1, for example, is a description of "real pizza," as evidenced by the use of words such as "crust," "thin," "Chicago," "style," "New," and "York." Topic 3 is a collection of words associated with customers' willingness to return to the restaurant ("will," "go," "back"). Topic 5 talks about service and staff in a positive fashion. Most adjectives in this topic have positive valence ("friendly," "attentive," "wonderful," "nice"). Topics 8 and 9 describe aspects of a negative service experience. Topic 8 is concerned with various issues with customers' orders. Topic 9 talks about issues regarding time ("minutes," "time," "wait," "never") and (frustrating) interaction with personnel ("asked," "came," "told"). Interestingly, topic 9 also contains the words "owner" and "manager," indicating that customers asked to talk to them. Ordinarily, restaurant patrons only do so as a last resort to resolve escalated conflicts with service personnel.    Table <ref type="table" target="#tab_7">6</ref> displays the results of the regression analysis of overall satisfaction on the topic probabilities. We report the R 2 of the latent continuous evaluations as a measure of fit of this model. For the Italian restaurant data, the fit is high (R 2 = 0 65), indicating that topic probabilities are meaningful devices to explain customer ratings. The coefficient estimates ( * ) are the expected increase (given contrast topic) in the latent rating that is observed in censored form on the rating scale. The cut-point estimates (c i ) for the model indicate that a 0.50 increase in the latent rating is associated with a one-point increase in the observed rating. Since the coefficient estimates are multiplied by the topic probabilities ( ), a 0.10 increase in the topic probabilities are often associated with substantive increases of the ratings. For example, if the probability that a review is associated with the topic "conflict" increases by 0.10, the expected change in the latent rating is −0 56, translating to an almost one-point decline in overall satisfaction.</p><p>The regression analysis provides information on which of the coefficients have mass away from zero and which have mass near zero. The posterior standard deviations average about 0.75 (without the contrast topic), indicating that coefficients greater than 1.5 in absolute magnitude are "significant." Thus, topics 5 (service and staff), 8 (issues with order), and 9 (conflict) are worthy of special attention in our analysis, with the presence of topic 5 in a review associated with higher ratings, and that of topics 8 and 9 associated with lower ratings.</p><p>Traditional driver analysis in customer satisfaction analysis involves regressing an overall measure of satisfaction with predefined subscales such as "food quality" or "service," where higher ratings on the subscales are associated with higher expected overall ratings. Such analysis typically only produces positive coefficient values, whereas the SC-LDA-Rating model produces both positive and negative regression coefficients. Moreover, traditional analysis is prone to haloing and other factors that express themselves as colinear regressors <ref type="bibr" target="#b5">(Büschken et al. 2013)</ref>. Such problems are not present in our topic-based analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Upscale Hotels</head><p>Table <ref type="table" target="#tab_8">7</ref> displays the top words for each topic in the upscale hotel data, and Table <ref type="table" target="#tab_10">8</ref> displays the results of the associated regression analysis. Both results are based on the best-fitting SC-LDA-Rating model (T = 10). We start by noting that the topic proportions d , obtained from the SC-LDA-Rating model, explain the rating very well (R 2 = 0 66). In the subsequent analysis of the topics, we find that most of the top 30 terms of the topics are unique to the topics. Thus, the R 2 of nearly 0.7 is not the result of topic overlap.</p><p>Similar to the topics emerging from the analysis of restaurant data, we find coherent topics in the upscale hotel data that center around a common theme.</p><p>Descriptions of these themes are offered in Table <ref type="table" target="#tab_8">7</ref>. For example, topic 1 talks exclusively about problems for customers at check-in. Among the most frequent words in this topic are "one," "two," "bed," "asked," "got," "king," "ready," "told," and "booked." These words suggest that customers booked specific room types (e.g., room with a king-size bed or two separate beds), but apparently, at check-in, the front desk staff was unable to fulfill those requests. Topic 4 centers around noise problems during the night and its sources ("elevator(s)," "floor," "street," "people") and negative issues with the room ("bathroom," "small," "shower," "didn't," "work," "problem"). Topic 5, by contrast, reports aspects of a positive experience with the room (e.g., "clean," "comfortable," "nice," "spacious"). Topics 2, 6, and 10 cover various aspects of staying at a Manhattan hotel location. It seems that this location offers customers a potential for diverse experiences and that reviewers like to talk about the various aspects of that experience. Topic 3 centers around customers' willingness to return to the hotel ("will," "definitely," "go," "back") and recommend it to others.</p><p>From the regression analysis, we find that topics 4 (noise and problems with room), 7 (amenities), and   1 (problems at check-in) are all significantly negative relative to topic 2 (nearby attractions). Most hotels in this data set charge additionally for Wi-Fi Internet access or breakfast. This is not appreciated much by customers who pay premium prices for these hotels and may expect such services to be included (or priced lower). The largest contributor to a negative rating is topic 4. A 10% increase of the proportion of this topic results in a change of the rating of nearly one rating scale point. This is determined from the regression results reported in Table <ref type="table" target="#tab_10">8</ref>. A 0.10 increase in the topic probability is multiplied by the regression coefficient for topic 4, −4 176, to yield a change in the latent overall rating by −0 42, or about a onepoint difference in the rating scale as indicated by the cut-point estimates, c i . The mention of aspects of hotel check-in (topic 1) is also associated with lower reviews. Apparently, if a customer cares enough to write about their stay and mentions early arrival or the correct room (not) being available, then they probably had a bad experience. One of the themes that emerges out of topic 1 is problems with the room configuration or beds, like a king-sized bed present when it should not be or vice versa. Similarly, the mention of an elevator (topic 4) is associated with lower satisfaction for upscale hotels, and is used in conjunction with words such as "floor," "people," and "noise." Thus, it is not the mechanical operation of the elevator that is problematic, but instead the noise it brings to the floors when it opens. From Table <ref type="table" target="#tab_8">7</ref>, we find that the topics "friendly staff" (topic 9), "everything great" (topic 8), and "location" (topic 6) are all positively, but not significantly, associated with positive ratings. This result suggests that when booking upscale hotels in Manhattan, customers expect a positive experience characterized by these topics. To find expectations fulfilled seems to be worth mentioning in reviews, but it does not improve ratings. The only topic that significantly drives ratings up is topic 3, which talks about willingness to return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Midscale Hotels</head><p>Table <ref type="table" target="#tab_11">9</ref> displays the top words for each topic in the midscale hotel data, and Table <ref type="table" target="#tab_0">10</ref> displays the associated regression coefficients. For midscale hotel data, we find results very similar to those for restaurant reviews and upscale hotel reviews; that is, we obtain coherent topics from applying the SC-LDA-Rating model that positively and negatively drive the overall rating. We report results from T = 8, which is the best-fitting SC-LDA-Rating model.</p><p>Several differences emerge from comparing the topics in the two hotel data sets. In the midscale JFK data (Table <ref type="table" target="#tab_11">9</ref>), we do not find a large variety in location-related topics compared to upscale data (Table <ref type="table" target="#tab_8">7</ref>). Topic 3 in the midscale data talks about food/dinner options in the vicinity of the hotels. Hotels in this price segment typically do not have restaurants, so patrons need other accessible food options ("deliver(y)," "nearby," "restaurant(s)," "walking"). Topic 7 is also concerned with location, but from the perspective of air travelers in need of a hotel close to JFK airport for ease of access. For these travelers, the shuttle service to and from the airport is a relevant feature of the hotel (topic 8). In the upscale hotel data, we find none of these aspects of location. By contrast to upscale hotels in Manhattan, midscale hotels offer several free amenities to guests (free Wi-Fi and breakfast, topic 5) that customers feel the need to report.</p><p>From the regression analysis (Table <ref type="table" target="#tab_0">10</ref>), two topics emerge as negative drivers of satisfaction for JFK midscale hotels-topic 1 (noise and smell) and topic 6 (front desk). Topic 1 reports significant problems with the room ("carpet") and the hotel ("floor") and talks about unpleasant odors, dirt, and noise. This topic exerts a strong significant negative effect on the rating, with a 10% increase associated with an approximate one-point decrease in rating. Interaction with front desk employees (topic 6) also effects ratings negatively. The top words in this topic suggest that issues arise from the front desk failing to organize transportation at the appropriate time ("time," "get," "check," "early" "morning," "flight," "shuttle," "late").</p><p>Service (topic 4) and room/free amenities (topic 5) emerge as positive drivers of satisfaction. The change in rating as a result of an increase of 10% of topic 4 is comparable to the effect of "noise and smell." This  suggests that front desk personnel reacting properly to complaints about noise and odors may be able to neutralize the negative effect. In the price segment studied here, free amenities (Wi-Fi, breakfast) are appreciated features and generate better ratings.</p><p>The presence of topics 7 (JFK) and 8 (shuttle) in a review are associated with more positive review ratings using words such as "overnight" and "convenient" (JFK location) and free and frequent options to get to and from the airport (shuttle). Finally, we note that the fit of the model with respect to the (latent) rating is the highest for the midscale data set (R 2 = 0 72). This is despite the fact that, for this data, the smallest number of topics is needed to maximize predictive fit compared to the other data sets. This suggests that it is not the number of topics that is important to explain ratings, but their coherence. Topic 8 from the midscale hotel data provides a good example of a set of low-probability words being gathered together by the model to provide an interpretable theme for describing variation in the satisfaction ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>The advantage of using a latent topic model is the ability to uncover collections of words that co-occur in the customer reviews. In the analysis of our reviews, we find that many words are indiscriminately used in all evaluations of hotels and restaurants and therefore do not provide diagnostic value for interpreting Marketing Science 35(6), pp. 953-975, © 2016 INFORMS their use. Words like "great" and "not" are often mentioned in reviews and are not interpretable without knowing the object to which they refer. More generally, the analysis of consumer reviews is challenged by the lack of structure in the data. The words used in a bad review of a product can be different from those used in a good review, but not necessarily in a manner that is easy to detect. There may be words that are common to both bad and good reviews, as well as words that are infrequently used but which imply exceptionally good and bad aspects of the product. Simple summaries of words in the form of frequency tables may not be diagnostic of word combinations with good discriminating ability.</p><p>We introduce a sentenced-based topic model as a means of structuring the unstructured data. The idea behind topic analysis is that topics are defined by word groups that are used with relatively high probability, being distinct from the probabilities associated with other topics. The high probability word groups supports the presence of co-occurring words that provide added context to the analysis, allowing for a richer interpretation of the data. We extend the structure in topic models by restricting the topics to be the same within a sentence. In a variant of this model, we allow topics to be "sticky" and topic assignments to be non-i.i.d. From the three data sets used here, this variant is not favored over the SC-LDA-Rating model. We believe this is at least partly due to the small number of sentences present in most consumer reviews.</p><p>A casual inspection of this and other consumer reviews in our analysis, however, supports the use of the topic sentence restriction, and we find that it improves the predictive fit of the model. The effect of the sentence constraint smoothes out the author-topic probabilities because all words in the sentence are assumed to be part of the same topic. This increases the probability of infrequent words within a topic and decreases the probability of frequent words. We find that reviewers predominantly structure text by forming sentences, many of which express a single underlying topic. Our model naturally exploits this structure and correctly clusters words which are only jointly used in sentences ("front desk," "airport shuttle," "every hour," "walking distance," "comfy bed"), instead of assigning them to different topics.</p><p>We relate the topic probabilities to customers' overall satisfaction ratings using a latent cut-point model similar to that used in customer satisfaction driver analysis. We find many significant drivers in each of the data sets examined, with some drivers associated with positive ratings and others associated with negative ratings. We often find that an increase in a topic probability of 0.10 is associated with a unit increase in the rating, and that we consistently explain about 60%-70% of the variation in the latent evaluations. The regression coefficients are useful to identify significant drivers of positive and negative reviews.</p><p>Our model allows for the order of words to be changed freely within sentences, but not between sentences. This is because of the dependency of the topic assignment among words observed to part of the same sentence. Removing a word from a sentence implies that the topic assignment of the remaining words may change. The topic assignment of a sentence, however, is independent of the order of the sentences in a document. This introduces a "bagof-sentences" property to our model in contrast to the standard bag-of-words assumption in stochastic modeling of text. We believe that the bag-of-sentence property more naturally reflects the use of speech in consumer reviews.</p><p>This paper demonstrates the usefulness of modelbased analysis for unstructured data. The key in the analysis of unstructured data is to impose some type of structure on the analysis. Our analysis employs the structure of latent topics coupled with the assumption that topics change at the period. A challenge in the development of models for unstructured data is in knowing what structure to embed in models used for analysis. We believe that additional linguistic structure of the reviews, in the form of paragraphs and lists, may provide additional opportunities to extend the models used in our analysis.</p><p>Additional research is needed on a variety of topics connected to our model. First, we do not attempt to model the factors driving a respondent to post a review. In doing this, we are assuming that the objects of inference are the topics associated with good and bad reviews, and we avoid making statements of the intended consequences of any interventions the firm might undertake or the effects of incentives to get people to post reviews. In addition, we do not attempt to model the number of words per review. We assume that latent topic probabilities are independently determined and, thus, independent of the number of sentences (S d ) and the number of words per sentence (N ds ). With the data sets analyzed in this study, this assumption does not seem to be violated. With other data sets and longer reviews in particular (e.g., movie reviews often contain several hundred words), it might be inappropriate. An area of future research would therefore be to build a new model that allows for a dependency between a review's length and latent topics and compare its fit relative to the fit of our proposed model. We leave this model extension, and other generalizations of our model, to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Supplemental material to this paper is available at https:// doi.org/10.1287/mksc.2016.0993. where dn denotes word n in document d. Solving this expression gives <ref type="bibr" target="#b4">(Blei et al. 2003</ref>)</p><formula xml:id="formula_6">p z dn = t w dn = m z −dn ∝ C W T mt −dn + m C W T m t −dn + W • C T D td −dn + t C T D t d −dn + T where C W T</formula><p>mt −dn and C T D t −dn are the count matrices with the topic assignment for the current word z dn excluded. This expression can be used to obtain samples from z dn conditional on the data (w) and the topic assignments of all other words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Estimation of the Sentence-Constrained LDA Model</head><p>The LDA model can be modified as a sentence-based model (SC-LDA) in which topics are assigned to sentences instead of words. In our implementation of this model, periods in consumer reviews provided by consumers identify "sentences," which are assumed to have a unique topic (see <ref type="bibr">Figure 4)</ref>. Thus, the set of words between periods is assumed to originate from an unobserved topic from a fixed topic set T .</p><p>According to the DAG of our model presented in Figure <ref type="figure">2</ref>, a topic z d s for sentence s in document d is drawn from the observed set T . Conditional on d , a topic t is drawn independently from a multinomial distribution for each sentence s in document d. Conditional on t=z , all words in a sentence for document d are drawn. It follows that z dsi = z dsj ∀ i j ∈ s.</p><p>By Bayes' theorem, the target distribution is given by</p><formula xml:id="formula_7">p z s w z −s D = p w z D p w s w −s z −s D = p w z D p w s D p w −s z −s D ∝ p w z D p w −s z −s D</formula><p>where s denotes sentence s.</p><p>To consider the effect of removing sentence s from the corpus, we introduce count matrix C SW T , the count of words by topics for sentence s in the corpus. The matrix C SW T has zero entries except in the topic column to which all words of sentence s are allocated. It also has zero entries in all rows referring to words from the vocabulary that do not appear in sentence s. We use C W T mt −s to denote the entries in the count matrix C W T −s obtained after removing sentence s. Note that C W T mt = C W T mt −s for all topics except the topic to which the words in sentence s were allocated and for all words that do not appear in sentence s.</p><p>We define matrix C T D s as the matrix indicating the allocation of sentence s to a certain topic t in document d. Following from (4), we can write down the likelihood of observing all words, except for sentence s</p><formula xml:id="formula_8">p w −s z −s • ∝ T t=1 W w=1 C W T mt −s + w C W T mt −s + D d=1 T t=1 C T D td −s + w C T D td −s +</formula><p>(5) To arrive at the target distribution, we divide (4) by (5). For this step, consider the first factor on the right-hand side (RHS) of both equations. This implies</p><formula xml:id="formula_9">T t=1 W w=1 C W T mt + / w C W T mt + T t=1 W w=1 C W T mt −s + / w C W T mt −s + = w∈s C W T mt + C W T mt −s + A w C W T mt −s + w C W T mt + B</formula><p>We consider part A first. By the recursive property of the gamma function</p><formula xml:id="formula_10">C W T mt + C W T mt −s + = C W T mt −s + + C SW T mt C W T mt −s + = C W T mt −s + C W T mt −s + + 1 • • C W T mt −s + + C SW T mt − 1</formula><p>where C SW T mt denotes the number of times word w appears in sentence s, allocated to topic t. If</p><formula xml:id="formula_11">C SW T mt = 1 C W T mt + C W T mt −s + = C W T mt −s + If C SW T mt = 2 C W T mt + C W T mt −s + = C W T mt −s + C W T mt −s + + 1</formula><p>and so forth. It follows that</p><formula xml:id="formula_12">A = w∈s C W T mt −s + C W T mt −s + +1 • • C W T mt −s + + C SW T mt −1</formula><p>We next consider part B and denote the number of words in sentence s allocated to topic t by n wst The second factor on the RHS of (4) yields the same formal result as in <ref type="bibr" target="#b4">Blei et al. (2003)</ref>. However, the count matrix C T D is obtained over the allocation of sentences to topics. We arrive at the following expression for the target distribution: p z s w s n ds w −s D</p><formula xml:id="formula_13">= w∈s C W T mt −s + C W T mt −s + +1 • • C W T mt −s + + C SW T mt −1 • w C W T mt −s + w C W T mt −s + +1 • • w C W T mt −s + + n ds −1 −1 • C T D td −C T D s + t C T D td −C T D s + A.</formula><p>3. Estimation and Identification of the SC-LDA-Rating Model We integrate customers' ratings into the SC-LDA model via an ordinal probit regression model. More specifically, we allow the latent, continuous rating d to be a function of a reviews' topic proportions ( d )</p><formula xml:id="formula_14">r d = k if c k−1 ≤ d ≤ c k d = 0 + d + d</formula><p>where c is a vector of K + 1 ordered cut points, 0 is a baseline, is a vector of coefficients of length T , and r d is the observed rating. Cut-points c 0 and c K+1 have fixed values. We note that this model, even with cut points c 0 , c 1 , c K , and c K+1 fixed, is not identified due to the nature of the covariates. We develop an identification strategy for this unidentified model later in this appendix.</p><p>The presence of a rating r d as a function of d implies that, after integrating out and , the rating in a document and the topic assignment of the sentences in that document are no longer independent. To account for this fact, we employ a "semicollapsed" Gibbs sampler where the are integrated out p z s w s n ds w −s D  <ref type="formula">2</ref>) and for the augmentation of the continuous ratings and the cut points c, we use standard results from the literature.</p><formula xml:id="formula_15">∝ w∈s C W T mt −s + C W T mt −s + +1 ••• C W T mt −s + + C SW T mt −1 • w C W T mt −s + w C W T mt −s + +1 ••• w C W T mt −s + + n ds −1 −1 • d</formula><p>Regressing the rating on the topics requires an identification strategy. To see this, consider the case of T = 3, i.e., a model with three topics. The regression equation is then</p><formula xml:id="formula_16">d = 0 + 1 t 1 d j t j d + 2 t 2 d j t j d + 3 t 3 d j t j d + d (<label>6</label></formula><formula xml:id="formula_17">)</formula><p>where t j d : number of times a word in document d is allocated to topic j, j t j d : number of words in document d, : latent continuous rating, : regression coefficients, : regression error.</p><p>The ratio t j d / j t j d expresses the share of topic j in document d (e.g., d from LDA).</p><p>Using <ref type="formula" target="#formula_16">6</ref>) can be expressed as</p><formula xml:id="formula_18">j t j d = t 1 d + t 2 d + t 3 d , Equation (</formula><formula xml:id="formula_19">d = 0 + 1 t 1 d j t j d + 2 t 2 d j t j d + 3 1 − t 1 d + t 2 d j t j d + d (7)</formula><p>Simplifying (2) leads to</p><formula xml:id="formula_20">d = 0 + 3 + 1 − 3 t 1 d j t j d + 2 − 3 t 2 d j t j d + d</formula><p>which we rewrite as</p><formula xml:id="formula_21">d = * 0 + * 1 t 1 d j t j d + * 2 t 2 d j t j d + d<label>(8)</label></formula><p>Equation ( <ref type="formula" target="#formula_21">8</ref>) demonstrates that the regression in Equation ( <ref type="formula" target="#formula_16">6</ref>) is not identified. The reason for nonidentification is the redundancy of any one share of topics which can be expressed as the residual of the other shares. Equation ( <ref type="formula" target="#formula_21">8</ref>), however, also shows that any slope coefficient in (6) can be omitted, and the resulting * are obtained as contrasts to this coefficient. In (8), the "new" baseline * 0 is a baseline in relation to the "omitted" 3 , as are the slope coefficients * 1 and * 2 . Table <ref type="table" target="#tab_14">A</ref>.1 outlines the relationship between the nonidentified parameters of the model and the identified parameters ( * ). From Table <ref type="table" target="#tab_14">A</ref>.1 it is clear that only the differences of the true parameters are identified. The choice of the contrast is arbitrary. Table <ref type="table" target="#tab_14">A</ref>.1 also suggests a postprocessing </p><formula xml:id="formula_22">0 = 0 + 1 * 0 = 0 + 2 * 0 = 0 + 3 - * 1 = 1 − 2 * 1 = 1 − 3 * 2 = 2 − 1 - * 2 = 2 − 3 * 3 = 3 − 1 * 3 = 3 − 2 -</formula><p>strategy for the identified parameters when an MCMC procedure is applied to the estimation of Equation ( <ref type="formula" target="#formula_16">6</ref>). We can use the MCMC procedure to sample from the nonidentified parameter space and postprocess down to the identified parameter space via results in Table <ref type="table" target="#tab_14">A</ref>.1. We demonstrate postprocessing for the following example from which we generate synthetic data:</p><formula xml:id="formula_23">d = 1 + −1 t 1 d j t j d + 1 t 2 d j t j d + 2 t 3 d j t j d + d</formula><p>and 2 = 0 1, N = 2,000, and the topic shares given T = 3 generated from a Dirichlet distribution with t = 0 5 ∀ t. For the MCMC procedure, we use standard weakly informative priors and conjugate results for the conditional posterior distributions of the unknowns ( ). 1 is −2 996, and the posterior SD is 0 09. Note that * 1 = 1 − 3 = −3. This demonstrates that we can use the samples from the MCMC, using Equation ( <ref type="formula" target="#formula_16">6</ref>), and postprocess the results using the equations in Table <ref type="table" target="#tab_14">A</ref>.1. An a priori choice of contrast to identify the model as in Equation ( <ref type="formula" target="#formula_21">8</ref>) is not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Simulation Study: Efficiency of the LDA Model</head><p>In the following, we evaluate the efficiency of the LDA model when a topic sentence constraint is present in the data. Theoretically, an LDA model can assign the same topic to the words in a sentence. Also, the LDA model and the proposed SC-LDA model both operate on the same sufficient statistic, the word counts by document. This raises the issue of efficiency of the LDA model compared to the SC-LDA model. To explore this issue we conducted a simulation study. In this simulation study, we generated data from a SC-LDA-Rating model (i.e., with a sentence constraint) and then estimated an LDA rating model (i.e., without the sentence constraint). The question we tried to answer is under what conditions the LDA model without the sentence constraint is able to pick up the true data mechanism in which the words in a sentence originate from the same topic. The setup of the simulation is as follows:</p><p>• We set T = 8 and V = 1,000.</p><p>• We simulate d from symmetric Dirichlet distributions using = 2/T and t from symmetric Dirichlet distributions using = 2,000/V or = 100/V .</p><p>• We generate D = 2,500 documents with 4-10 or 18-36 sentences per document and 2-6 or 12-18 words per sentence (words and sentences uniformly distributed over indicated range).</p><p>A smaller value of reduces the number of co-occurring terms under a topic, as the t are then concentrated among relatively few terms. Assigning topics wordwise, as with the LDA model, should be less of a problem when the number of co-occurring terms is small. By contrast, a larger value of increases the number of co-occurring terms. Topics can then only be identified correctly when all words in a sentence are considered. In summary, ignoring a sentence constraint present in the data should be less important when</p><p>• the number of words per sentence is small and • the number of terms uniquely associated with a topic is small.</p><p>We evaluate the efficiency of the estimation procedure by the hit rate of the topic assignments of all words in the corpus. Recovering the true topic assignments of the words is essential for recovery of all other parameters of the model, including the parameters of the rating model. Figure A.2 reveals that the topic hit rate of the LDA model is smaller than that of the SC-LDA model for all scenarios. For a high (left panel of Figure A.2), the difference in topic hit rates is significant, especially when the number of words in the sentences is high. The advantage of the SC-LDA model is small when topics are characterized by few frequently occurring words ( = 100/V ). In this situation, specific terms are highly indicative of a topic, and co-occurrence of such terms with less frequent terms within sentences is less likely. It is in this situation that ignoring the sentence constraint in the data introduces less bias in estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. MCMC for the SC-LDA-Rating Model with Sticky</head><p>Topics To develop an MCMC estimation procedure for the SC-LDA-Rating model with sticky topics, we start by defining the generate model of SC-LDA with first-order topic carryover. The generative model of the sticky topic model with fixed priors , , and is as follows:</p><p>1. Draw t from Beta ) ∀ t i.i.   A.5.5. Prior Distributions. We use the following (fixed) prior distributions in our analysis:</p><formula xml:id="formula_24">d ∼ Dirichlet 5/T t ∼ Dirichlet 100/V 2 ∼ Inverse Gamma 1 1 reg ∼ N 0 10 t ∼ N 0 10 log ∼ N</formula><p>All fixed prior distributions are weakly informative, conjugate prior distributions. To see this for d t , consider that fixing , is equivalent to fixing prior pseudocounts from an imaginary prior data set; that is, assuming = 100/V , given V = 1,000, is equivalent to assuming 0 1 prior pseudocounts per unique term and topic. Similarly, given T = 10, = 5/T is equivalent to 0.5 prior pseudocounts per topic and document. Larger values for have a smoothing effect on estimates of , respectively. We tested prior setups for "smoothed" estimates, using larger values for , and did not find that the results obtained from the three data sets differ significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Simulation Study: Empirical Identification of the</head><p>Sticky SC-LDA-Rating Model In the following, we demonstrate statistical identification of the SC-LDA-Rating model with sticky topics using a simulation study. The study is based on a vocabulary of V = 1,000 unique terms and four topics (T = 4). The number of sentences per document is drawn from a uniform distribution across values 15 to 20, and we draw the number of words per sentence from a uniform distribution over values 3 4 6 . We generate M = 2,000 documents and a corpus of about 150,000 words.  We generate the true word-topic probabilities ( ) and the true document-topic probabilities ( d ) from symmetric Dirichlet distributions. We allow , the prior of t , to range from 100/V to 2,000/V. We set , the prior of d , to values in 1/T 2/T 4/T . We vary and independently, resulting in nine simulation scenarios (Table <ref type="table" target="#tab_14">A</ref>.2). We set to 0 12 0 02 0 05, and 0 40 for topics 1 to 4, respectively. Note that, in the limit, homogenous t lead to marginal topic frequencies equal to d .</p><p>In the SC-LDA-Rating model, the rating for each document is assumed to be generated via an ordinal probit regression model. For the simulation of data for this model, we use a baseline and slope coefficients with values reg 0 = −0 5 reg 1 = 1 reg 2 = −2, and reg 3 = 1 8. The error variance of the model is fixed at 2 = 0 2. To obtain ordinal ratings from the latent continuous evaluations generated by this model, we use cut points c fixed at values so that all rating categories are equally populated. In parameter estimation, we use data augmentation for the latent continuous evaluation and estimate all parameters of the ordinal probit model using the identification strategy outlined above.</p><p>In Table <ref type="table" target="#tab_14">A</ref>.2, we report the correlation of the simulated and true parameters of and , 2 , and the Mean Absolute Deviation (MAD) for from the nine scenarios. Recovery of 2 = 0 2 implies recovery of all parameters of the regression model, as this parameter is invariant to switches of the topic labels. For each scenario, we simulated data 100 times. For each of the 100 runs, we computed the correlation of the posterior means of and with true values, the posterior mean of 2 , and the MAD of . We then computed the mean and SD of these quantities across the 100 simulation runs for purposes of reporting (Table <ref type="table" target="#tab_14">A</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2).</head><p>Table <ref type="table" target="#tab_14">A</ref>.2 reveals that the parameters of our model can be recovered in all scenarios with high accuracy for ≤ 1,000/V and ≤ 2/T . In general, accuracy declines as and are increased. Higher values of induce a more uniform distribution of words over the vocabulary. Higher values of induce a more ambiguous relationship between documents and topics. We note that a more ambiguous relationship between documents and topics has a detrimental effect on the recovery of . This is because identification of depends on carryover of topics that are relatively rare, given d .</p><p>A viable question to ask is whether our sampler identifies the true T , which must be fixed for an empirical application of topic models. Given a fixed simulated data set using T true = 4 and an informative setup ( = 1/T , = 100/V , V = 1,000, M calib = 1,000, M pred = 500), we ran our model using alternative values for T . Table <ref type="table" target="#tab_14">A</ref>.3 shows the in-sample fit and predictive fit of the model with T ranging from 2 to 12. Reported is the log marginal density of the data for the calibration and the holdout data. In Table <ref type="table" target="#tab_14">A</ref>.3, results from uneven topic numbers are omitted for brevity. The results indicate that the model correctly identifies T = 4 as the true data generating process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1 displays a plate diagram for the LDA model. The plates indicate replications of documents (d = 1 D), words (n = 1 N d ), and topics (t = 1T ). We note that the LDA model does not impose any structure on the data related to the plates; i.e., it assumes that the latent topics z dn can vary from word to word, sometimes referred to as a "bag-ofwords" assumption in the text analysis literature. This assumption differs from the traditional marketing assumption of heterogeneity that exploits the panel structure often found in marketing data where multiple observations are known to be associated with the same unit of analysis. There is a marketing literature on what is known as context-dependent or structural heterogeneity<ref type="bibr" target="#b16">(Kamakura et al. 1996</ref><ref type="bibr" target="#b30">, Yang and Allenby 2000</ref><ref type="bibr" target="#b31">, Yang et al. 2002</ref>) that allows the model likelihood to vary across observations. Restricted versions of the assumption made by the LDA model for observational heterogeneity include models of change points<ref type="bibr" target="#b8">(DeSarbo et al. 2004</ref>) and latent Markov models<ref type="bibr" target="#b10">(Fader et al. 2004</ref><ref type="bibr" target="#b21">, Netzer et al. 2008</ref><ref type="bibr" target="#b20">, Montoya et al. 2010</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1 (Color online) Graphical Representation of the LDA Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2 (Color online) Graphical Representation of the Sentence-Constrained LDA Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Marketing</head><label></label><figDesc>Figure 3 (Color online) Graphical Representation of the SC-LDA-Rating Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 (</head><label>4</label><figDesc>Figure 4(Color online) A Hotel Review</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 (</head><label>6</label><figDesc>Figure 6 (Color online) Word Choice Probabilities ( ) of LDA and SC-LDA Models for T = 3 (Midscale Hotel Data)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>theorem, the full conditional posterior of z dn , the nth word in document d in the corpus, is given byp z dn w z −dn D = p w z D p w dn w −dn z −dn D = p w z D p w dn D p w −dn z −dn D ∝ p w z D p w −dn z −dn D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure A.1 shows results from the MCMC for 1 . The left panel shows the direct results from the MCMC. It is obvious that the sampler does not recover the true value ( 1 = −1). The posterior mean obtained from the MCMC is −1 25, and the posterior SD is 1 59. The right panel of Figure A.1 shows the postprocessed parameter * 1 , using 3 as contrast. The posterior mean of *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure A.2 displays the posterior means of the hit rates of the topic assignments for the eight simulation scenarios. The left panel of Figure A.2 shows the topic hit rates for = 2,000/V , and the right panel of Figure A.2 shows the topic hit rates for = 100/V .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>d. 2. Draw t from Dirichlet ∀ t i.i.d. 3. Draw d from Dirichlet ∀ d i.i.d. 4. For the first sentence in document d, s 1 : (a) Draw z 1 from Multinomial d (b) Draw set of words w 1 in sentence s 1 i.i.d. from Multinomial t=z 1 (c) Draw 2 from Binomial t=z 1 5. For sentences s N , N ∈ 2 n D : (a) if n = 0: draw z n from Multinomial d ; if n = 1: set z n = z n−1 (b) Draw w n i.i.d. from Multinomial t=z n (c) Draw n+1 from Binomial t=z n 6. Repeat steps 4 and 5 for all documents d ∈ D (except for draw of N d )Based on the DAG in Figure5, we can factorize the joint distribution of the knowns and unknowns for a single document as follows:p w d z d d d ∝ p w 1 z 1 × p z 1 d × N d n=2 p w n z n z n−1 n × p z n z n−1 d n × p n z n−1 × p × p d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Summary Statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Standard</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>Median</cell><cell>deviation</cell><cell>Range</cell></row><row><cell>Number of sentences per review</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Midscale hotel</cell><cell>3 8</cell><cell>3</cell><cell>2 9</cell><cell>25</cell></row><row><cell>Upscale hotel</cell><cell>4 3</cell><cell>4</cell><cell>3 2</cell><cell>41</cell></row><row><cell>Italian restaurant</cell><cell>12 2</cell><cell>8</cell><cell>11 8</cell><cell>90</cell></row><row><cell>Number of words per sentence</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Midscale hotel</cell><cell>5 4</cell><cell>5</cell><cell>3 6</cell><cell>42</cell></row><row><cell>Upscale hotel</cell><cell>5 3</cell><cell>5</cell><cell>3 4</cell><cell>52</cell></row><row><cell>Italian restaurant</cell><cell>5 2</cell><cell>5</cell><cell>3 1</cell><cell>29</cell></row><row><cell>Rating</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Midscale hotel</cell><cell>3 5</cell><cell>4</cell><cell>1 1</cell><cell>4</cell></row><row><cell>Upscale hotel</cell><cell>4 4</cell><cell>5</cell><cell>0 9</cell><cell>4</cell></row><row><cell>Italian restaurant</cell><cell>3 8</cell><cell>4</cell><cell>1 4</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Most Frequently Used Words by Rating in Reviews</figDesc><table><row><cell></cell><cell cols="2">Upscale hotel</cell><cell cols="2">Midscale hotel</cell><cell cols="2">Italian restaurant</cell></row><row><cell></cell><cell>Rating</cell><cell>Rating</cell><cell>Rating</cell><cell>Rating</cell><cell>Rating</cell><cell>Rating</cell></row><row><cell cols="2">Rank 1 or 2</cell><cell>4 or 5</cell><cell>1 or 2</cell><cell>4 or 5</cell><cell>1 or 2</cell><cell>4 or 5</cell></row><row><cell>1</cell><cell>Room</cell><cell>Hotel</cell><cell>Hotel</cell><cell>Hotel</cell><cell>Pizza</cell><cell>Pizza</cell></row><row><cell>2</cell><cell>Hotel</cell><cell>Room</cell><cell>Room</cell><cell>Room</cell><cell>Food</cell><cell>Food</cell></row><row><cell>3</cell><cell>Location</cell><cell>Great</cell><cell>Airport</cell><cell>Airport</cell><cell>Good</cell><cell>Good</cell></row><row><cell>4</cell><cell>Rooms</cell><cell>Location</cell><cell>Stay</cell><cell cols="3">Shuttle Restaurant Great</cell></row><row><cell>5</cell><cell>Stay</cell><cell>Staff</cell><cell cols="2">Breakfast Breakfast</cell><cell>Just</cell><cell>Restaurant</cell></row><row><cell>6</cell><cell>Good</cell><cell>Square</cell><cell>Shuttle</cell><cell>Good</cell><cell>One</cell><cell>One</cell></row><row><cell>7</cell><cell>Staff</cell><cell>Stay</cell><cell>Good</cell><cell>Clean</cell><cell>Us</cell><cell>Place</cell></row><row><cell>8</cell><cell>Service</cell><cell>Times</cell><cell>JFK</cell><cell>Stay</cell><cell>Back</cell><cell>Italian</cell></row><row><cell>9</cell><cell>Great</cell><cell>Clean</cell><cell>Staff</cell><cell>JFK</cell><cell>Like</cell><cell>Just</cell></row><row><cell>10</cell><cell>Times</cell><cell>New</cell><cell>One</cell><cell>Staff</cell><cell>Place</cell><cell>Like</cell></row><row><cell>11</cell><cell>Time</cell><cell>Time</cell><cell>Night</cell><cell>Service</cell><cell>Ordered</cell><cell>Best</cell></row><row><cell>12</cell><cell>One</cell><cell>Nice</cell><cell>Small</cell><cell>Free</cell><cell>Really</cell><cell>Cheese</cell></row><row><cell>13</cell><cell>Bed</cell><cell>Rooms</cell><cell>Clean</cell><cell>Nice</cell><cell>Got</cell><cell>Service</cell></row><row><cell>14</cell><cell>Nice</cell><cell>York</cell><cell>Rooms</cell><cell>Great</cell><cell>Came</cell><cell>Sauce</cell></row><row><cell>15</cell><cell>Square</cell><cell>Friendly</cell><cell>Get</cell><cell cols="2">Comfortable Order</cell><cell>Time</cell></row><row><cell>16</cell><cell>Get</cell><cell>Good</cell><cell>Place</cell><cell>Night</cell><cell>Italian</cell><cell>Really</cell></row><row><cell>17</cell><cell>Us</cell><cell>Helpful</cell><cell>Free</cell><cell>Helpful</cell><cell>Cheese</cell><cell>Will</cell></row><row><cell cols="4">18 Breakfast Comfortable Flight</cell><cell>Flight</cell><cell>Get</cell><cell>Also</cell></row><row><cell>19</cell><cell>Floor</cell><cell>City</cell><cell>Close</cell><cell>Close</cell><cell>Menu</cell><cell>Us</cell></row><row><cell>20</cell><cell>Small</cell><cell>View</cell><cell>Service</cell><cell>Friendly</cell><cell>Minutes</cell><cell>Little</cell></row><row><cell>21</cell><cell>Desk</cell><cell>Service</cell><cell>Area</cell><cell>One</cell><cell>Time</cell><cell>Go</cell></row><row><cell>22</cell><cell>Night</cell><cell>Breakfast</cell><cell>Time</cell><cell>Rooms</cell><cell>Service</cell><cell>Get</cell></row><row><cell cols="3">23 Bathroom Excellent</cell><cell>Like</cell><cell>Time</cell><cell>Go</cell><cell>Back</cell></row><row><cell>24</cell><cell>2</cell><cell>Right</cell><cell>Bed</cell><cell>Early</cell><cell>Said</cell><cell>Menu</cell></row><row><cell>25</cell><cell>Clean</cell><cell>Close</cell><cell>Next</cell><cell>Get</cell><cell>Will</cell><cell>Can</cell></row><row><cell>26</cell><cell>Like</cell><cell>Will</cell><cell>Us</cell><cell>Small</cell><cell>Sauce</cell><cell>Crust</cell></row><row><cell>27</cell><cell cols="2">Didn't Everything</cell><cell>Desk</cell><cell>Hour</cell><cell>Two</cell><cell>Got</cell></row><row><cell>28</cell><cell>Front</cell><cell>One</cell><cell cols="2">Hour Convenient</cell><cell>Salad</cell><cell>Two</cell></row><row><cell>29</cell><cell>New</cell><cell>Stayed</cell><cell cols="2">Location Morning</cell><cell>Table</cell><cell>Order</cell></row><row><cell>30</cell><cell>Also</cell><cell>Us</cell><cell>Morning</cell><cell>Us</cell><cell>Eat</cell><cell>Made</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>reveals that, in terms of predictive fit, a topic model with a sentence constraint is generally preferred over a model that assigns topics to words. This is evidenced by the predictive fit of the LDA 962</figDesc><table><row><cell>Marketing Science 35(6), pp. 953-975, © 2016 INFORMS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Model Fit of Topic Rating Models</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell>In-sample fit</cell><cell>Predictive fit</cell></row><row><cell>Italian restaurant Upscale hotel Midscale hotel</cell><cell>LDA SC-LDA Sticky SC-LDA LDA SC-LDA Sticky SC-LDA LDA SC-LDA Sticky SC-LDA</cell><cell>−214 344 3 −235 587 5 −236 972 3 −328 963 7 −361 173 2 −363 216 8 −111 289 7 −124 440 7 −126 229 5</cell><cell>−27 001 7 −26 458 3 −26 515 3 −42 675 6 −41 236 6 −41 649 9 −17 563 9 −16 970 7 −17 147 8</cell></row><row><cell cols="4">rating model to be lower than the predictive fit of</cell></row><row><cell cols="4">both SC-LDA-based topic models. Within the sample,</cell></row><row><cell cols="4">however, the LDA fits better across all data sets, com-</cell></row><row><cell cols="4">pared to both the SC-LDA model and the SC-LDA</cell></row></table><note>model with sticky topics. This result is due to the LDA model being more flexible, but this flexibility apparently does not help in predicting new data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Pseudo-R 2 from Rating-Based Topic Models</figDesc><table><row><cell></cell><cell>Midscale</cell><cell></cell><cell>Upscale</cell><cell></cell><cell>Italian</cell><cell></cell></row><row><cell>Model</cell><cell>hotel</cell><cell>T</cell><cell>hotel</cell><cell>T</cell><cell>restaurant</cell><cell>T</cell></row><row><cell>LDA-Rating</cell><cell>0.581</cell><cell>8</cell><cell>0.488</cell><cell>10</cell><cell>0.492</cell><cell>9</cell></row><row><cell>SC-LDA-Rating</cell><cell>0.719</cell><cell>8</cell><cell>0.663</cell><cell>10</cell><cell>0.649</cell><cell>9</cell></row><row><cell>Sticky SC-LDA</cell><cell>0.646</cell><cell>8</cell><cell>0.634</cell><cell>10</cell><cell>0.625</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>We8There Italian Restaurant Data, Top 30 Words from the SC-LDA-Rating Model (T = 9)</figDesc><table><row><cell></cell><cell>Topic 1</cell><cell>Topic 2</cell><cell>Topic 3</cell><cell>Topic 4</cell><cell>Topic 5</cell><cell>Topic 6</cell><cell>Topic 7</cell><cell>Topic 8</cell><cell>Topic 9</cell></row><row><cell cols="2">Rank "Real pizza"</cell><cell>"Menu"</cell><cell>"Return"</cell><cell cols="3">"Food ordered" "Service and staff" "Recommend"</cell><cell>"Layout"</cell><cell>"Issues with order"</cell><cell>"Conflict"</cell></row><row><cell>1</cell><cell>Pizza</cell><cell>Salad</cell><cell>Will</cell><cell>Sauce</cell><cell>Food</cell><cell>Food</cell><cell>Restaurant</cell><cell>Pizza</cell><cell>Us</cell></row><row><cell>2</cell><cell>Crust</cell><cell>Good</cell><cell>Restaurant</cell><cell>Cheese</cell><cell>Service</cell><cell>Italian</cell><cell>Dining</cell><cell>Just</cell><cell>Minutes</cell></row><row><cell>3</cell><cell>Really</cell><cell>Menu</cell><cell>Go</cell><cell>Pizza</cell><cell>Great</cell><cell>Restaurant</cell><cell>Room</cell><cell>Got</cell><cell>Food</cell></row><row><cell>4</cell><cell>Like</cell><cell>Ordered</cell><cell>Back</cell><cell>Ordered</cell><cell>Good</cell><cell>Best</cell><cell>Bar</cell><cell>Two</cell><cell>Order</cell></row><row><cell>5</cell><cell>Good</cell><cell>Also</cell><cell>Place</cell><cell>Fresh</cell><cell>Friendly</cell><cell>Place</cell><cell>Tables</cell><cell>Good</cell><cell>Came</cell></row><row><cell>6</cell><cell>Chicago</cell><cell>Pasta</cell><cell>Time</cell><cell>Bread</cell><cell>Staff</cell><cell>Recommend</cell><cell>Area</cell><cell>Order</cell><cell>Table</cell></row><row><cell>7</cell><cell>Thin</cell><cell>Bread</cell><cell>Food</cell><cell>Italian</cell><cell>Atmosphere</cell><cell>Pizza</cell><cell>Located</cell><cell>Really</cell><cell>Asked</cell></row><row><cell>8</cell><cell>Style</cell><cell>Food</cell><cell>Try</cell><cell>Sandwich</cell><cell>Excellent</cell><cell>Great</cell><cell>One</cell><cell>Cheese</cell><cell>Back</cell></row><row><cell>9</cell><cell>Best</cell><cell>Pizza</cell><cell>Pizza</cell><cell>Like</cell><cell>Place</cell><cell>One</cell><cell>Small</cell><cell>Get</cell><cell>Waitress</cell></row><row><cell>10</cell><cell>One</cell><cell>Wine</cell><cell>One</cell><cell>Good</cell><cell>Restaurant</cell><cell>Ever</cell><cell>Pizza</cell><cell>Back</cell><cell>Time</cell></row><row><cell>11</cell><cell>Just</cell><cell>Italian</cell><cell>Good</cell><cell>Came</cell><cell>Prices</cell><cell>Experience</cell><cell>Parking</cell><cell>One</cell><cell>Restaurant</cell></row><row><cell>12</cell><cell>New</cell><cell>Great</cell><cell>Never</cell><cell>Salad</cell><cell>Well</cell><cell>Anyone</cell><cell>Place</cell><cell>Us</cell><cell>Took</cell></row><row><cell>13</cell><cell>Pizzas</cell><cell>Salads</cell><cell>Visit</cell><cell>Just</cell><cell>Always</cell><cell>Good</cell><cell>Lot</cell><cell>Like</cell><cell>Said</cell></row><row><cell>14</cell><cell>Great</cell><cell>Delicious</cell><cell>Return</cell><cell>Tomato</cell><cell>Experience</cell><cell>Highly</cell><cell>Street</cell><cell>Pizzas</cell><cell>Just</cell></row><row><cell>15</cell><cell>Italian</cell><cell>Sauce</cell><cell>Great</cell><cell>Pasta</cell><cell>Wonderful</cell><cell>Restaurants</cell><cell>Building</cell><cell>Took</cell><cell>Get</cell></row><row><cell>16</cell><cell>Little</cell><cell>Dinner</cell><cell>Years</cell><cell>Mozzarella</cell><cell>Nice</cell><cell>Area</cell><cell>Just</cell><cell>Slice</cell><cell>Wait</cell></row><row><cell>17</cell><cell>York</cell><cell>Meal</cell><cell>Definitely</cell><cell>Sausage</cell><cell>Italian</cell><cell>Better</cell><cell>Kitchen</cell><cell>Little</cell><cell>Waiter</cell></row><row><cell>18</cell><cell>Cheese</cell><cell>Dishes</cell><cell>Many</cell><cell>Flavor</cell><cell>Wait</cell><cell>Worst</cell><cell>Table</cell><cell>Pretty</cell><cell>One</cell></row><row><cell>19</cell><cell>Place</cell><cell>Dessert</cell><cell>Just</cell><cell>Beef</cell><cell>Attentive</cell><cell>Family</cell><cell>Nice</cell><cell>Go</cell><cell>Bar</cell></row><row><cell>20</cell><cell>Get</cell><cell>One</cell><cell>Dinner</cell><cell>Garlic</cell><cell>Wine</cell><cell>Dining</cell><cell>Little</cell><cell>Time</cell><cell>Got</cell></row><row><cell>21</cell><cell>Know</cell><cell>Us</cell><cell>Going</cell><cell>Meat</cell><cell>Menu</cell><cell>Style</cell><cell>Good</cell><cell>Slices</cell><cell>Even</cell></row><row><cell>22</cell><cell>Much</cell><cell>House</cell><cell>Italian</cell><cell>Little</cell><cell>Family</cell><cell>New</cell><cell>Back</cell><cell>Said</cell><cell>Service</cell></row><row><cell>23</cell><cell>Beef</cell><cell>Special</cell><cell>Eat</cell><cell>Served</cell><cell>Reasonable</cell><cell>Favorite</cell><cell>There's</cell><cell>Came</cell><cell>Told</cell></row><row><cell>24</cell><cell>Lot</cell><cell>Large</cell><cell>Since</cell><cell>Crust</cell><cell>Dining</cell><cell>Will</cell><cell>Can</cell><cell>Much</cell><cell>Menu</cell></row><row><cell>25</cell><cell>Sauce</cell><cell>Veal</cell><cell>First</cell><cell>Delicious</cell><cell>Pleasant</cell><cell>Just</cell><cell>Front</cell><cell>Half</cell><cell>Never</cell></row><row><cell>26</cell><cell>Chain</cell><cell>Fresh</cell><cell>Family</cell><cell>Dish</cell><cell>Delicious</cell><cell>Far</cell><cell>Pretty</cell><cell>Home</cell><cell>Seated</cell></row><row><cell>27</cell><cell>Got</cell><cell>Selection</cell><cell>Like</cell><cell>One</cell><cell>Outstanding</cell><cell>Eaten</cell><cell>Like</cell><cell>Minutes</cell><cell>Owner</cell></row><row><cell>28</cell><cell>Flavor</cell><cell>Lasagna</cell><cell>Went</cell><cell>Really</cell><cell>Everything</cell><cell>Wonderful</cell><cell>Open</cell><cell>Enough</cell><cell>Manager</cell></row><row><cell>29</cell><cell>Dish</cell><cell>Shrimp</cell><cell>Experience</cell><cell>Marinara</cell><cell>Time</cell><cell>Many</cell><cell>Italian</cell><cell>Went</cell><cell>Bill</cell></row><row><cell>30</cell><cell>Find</cell><cell>Served</cell><cell>Times</cell><cell>Side</cell><cell>Just</cell><cell>Go</cell><cell>Two</cell><cell>Meal</cell><cell>Dinner</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">We8There Italian Restaurant Data: Results from Topic</cell></row><row><cell></cell><cell cols="2">Regression (T = 9, Topic 2 as Contrast)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Posterior</cell><cell>Posterior</cell></row><row><cell>Topic</cell><cell>Parameter</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>Baseline</cell><cell>0</cell><cell>0 588</cell><cell>0 478</cell></row><row><cell>Real pizza</cell><cell>1</cell><cell>0 404</cell><cell>0 728</cell></row><row><cell>Menu</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Expedia Upscale Hotel Data, Top 30 Words from the SC-LDA-Rating Model (T = 10)</figDesc><table><row><cell></cell><cell>Topic 1</cell><cell>Topic 2</cell><cell>Topic 3</cell><cell>Topic 4</cell><cell>Topic 5</cell><cell></cell><cell></cell><cell>Topic 8</cell><cell>Topic 9</cell><cell>Topic 10</cell></row><row><cell></cell><cell>"Problems"</cell><cell>"Nearby"</cell><cell>"Recommend</cell><cell>"Noise and</cell><cell>"Room</cell><cell>Topic 6</cell><cell>Topic 7</cell><cell>"Everything</cell><cell>"Friendly</cell><cell>"New York</cell></row><row><cell cols="3">Rank at check-in" attractions"</cell><cell cols="2">and return" room negative"</cell><cell>positive"</cell><cell cols="2">"Location" "Amenities"</cell><cell>great"</cell><cell>staff"</cell><cell>experience"</cell></row><row><cell>1</cell><cell>Room</cell><cell>Hotel</cell><cell>Stay</cell><cell>Room</cell><cell>Room</cell><cell>Square</cell><cell>Breakfast</cell><cell>Location</cell><cell>Staff</cell><cell>Hotel</cell></row><row><cell>2</cell><cell>Hotel</cell><cell>Square</cell><cell>Hotel</cell><cell>Hotel</cell><cell>Clean</cell><cell>Times</cell><cell>Hotel</cell><cell>Hotel</cell><cell>Helpful</cell><cell>New</cell></row><row><cell>3</cell><cell>Us</cell><cell>Location</cell><cell>Will</cell><cell>Floor</cell><cell>Comfortable</cell><cell>Hotel</cell><cell>Room</cell><cell>Great</cell><cell>Friendly</cell><cell>York</cell></row><row><cell>4</cell><cell>Check</cell><cell>Times</cell><cell>Definitely</cell><cell>Street</cell><cell>Rooms</cell><cell>Location</cell><cell>Free</cell><cell>Staff</cell><cell>Hotel</cell><cell>Stayed</cell></row><row><cell>5</cell><cell>Desk</cell><cell>Close</cell><cell>Recommend</cell><cell>Bathroom</cell><cell>Hotel</cell><cell>Great</cell><cell>Good</cell><cell>Clean</cell><cell>Desk</cell><cell>City</cell></row><row><cell>6</cell><cell>Got</cell><cell>Subway</cell><cell>Back</cell><cell>One</cell><cell>Beds</cell><cell>View</cell><cell>Great</cell><cell>Good</cell><cell>Service</cell><cell>Stay</cell></row><row><cell>7</cell><cell>Day</cell><cell>Walking</cell><cell>Go</cell><cell>Night</cell><cell>Bed</cell><cell>Right</cell><cell>Restaurant</cell><cell>Service</cell><cell>Great</cell><cell>Times</cell></row><row><cell>8</cell><cell>Rooms</cell><cell>Distance</cell><cell>Great</cell><cell>Noise</cell><cell>Nice</cell><cell>Room</cell><cell>Food</cell><cell>Room</cell><cell>Nice</cell><cell>Time</cell></row><row><cell>9</cell><cell>Early</cell><cell>Walk</cell><cell>Time</cell><cell>Elevator</cell><cell>Large</cell><cell>Time</cell><cell>Service</cell><cell>Excellent</cell><cell>Front</cell><cell>Location</cell></row><row><cell>10</cell><cell>Time</cell><cell>Great</cell><cell>Highly</cell><cell>Elevators</cell><cell>New</cell><cell>Stay</cell><cell>Bar</cell><cell>Nice</cell><cell>Clean</cell><cell>Square</cell></row><row><cell>11</cell><cell>Front</cell><cell>Station</cell><cell>New</cell><cell>Get</cell><cell>Small</cell><cell>Heart</cell><cell>Internet</cell><cell>Rooms</cell><cell>Room</cell><cell>Great</cell></row><row><cell>12</cell><cell>Arrived</cell><cell>Central</cell><cell>Next</cell><cell>Little</cell><cell>Size</cell><cell>Middle</cell><cell>View</cell><cell>Comfortable</cell><cell>Us</cell><cell>Marriott</cell></row><row><cell>13</cell><cell>One</cell><cell>Within</cell><cell>York</cell><cell>Didn't</cell><cell>Spacious</cell><cell>Located</cell><cell>Expensive</cell><cell>Overall</cell><cell>Everyone</cell><cell>Hotels</cell></row><row><cell>14</cell><cell>Bed</cell><cell>Blocks</cell><cell>Place</cell><cell>Rooms</cell><cell>York</cell><cell>Perfect</cell><cell>Price</cell><cell>Stay</cell><cell>Concierge</cell><cell>Trip</cell></row><row><cell>15</cell><cell>Get</cell><cell>Everything</cell><cell>Enjoyed</cell><cell>Lobby</cell><cell>Great</cell><cell>Everything</cell><cell>Wi-Fi</cell><cell>Experience</cell><cell>Courteous</cell><cell>First</cell></row><row><cell>16</cell><cell>Staff</cell><cell>Broadway</cell><cell>Trip</cell><cell>Like</cell><cell>Good</cell><cell>Floor</cell><cell>Also</cell><cell>Friendly</cell><cell>Extremely</cell><cell>Best</cell></row><row><cell>17</cell><cell>Told</cell><cell>Away</cell><cell>NYC</cell><cell>Shower</cell><cell>City</cell><cell>Quiet</cell><cell>Nice</cell><cell>Price</cell><cell>Excellent</cell><cell>Hilton</cell></row><row><cell>18</cell><cell>2</cell><cell>Restaurants</cell><cell>Staying</cell><cell>Time</cell><cell>Bathroom</cell><cell>Nice</cell><cell>Included</cell><cell>Perfect</cell><cell>Pleasant</cell><cell>Marquis</cell></row><row><cell>19</cell><cell>Ready</cell><cell>Easy</cell><cell>City</cell><cell>Work</cell><cell>Well</cell><cell>Hilton</cell><cell>Worth</cell><cell>Wonderful</cell><cell>Always</cell><cell>Room</cell></row><row><cell>20</cell><cell>Even</cell><cell>Just</cell><cell>Marriott</cell><cell>Great</cell><cell>Quiet</cell><cell>Want</cell><cell>Rooms</cell><cell>Value</cell><cell>Polite</cell><cell>One</cell></row><row><cell>21</cell><cell>Called</cell><cell>Block</cell><cell>Visit</cell><cell>Day</cell><cell>Staff</cell><cell>Place</cell><cell>Get</cell><cell>Helpful</cell><cell>Professional</cell><cell>Perfect</cell></row><row><cell>22</cell><cell>Asked</cell><cell>Attractions</cell><cell>Return</cell><cell>Bit</cell><cell>Big</cell><cell>Excellent</cell><cell>Buffet</cell><cell>Fantastic</cell><cell>Location</cell><cell>Place</cell></row><row><cell>23</cell><cell>King</cell><cell>Right</cell><cell>Hilton</cell><cell>Quiet</cell><cell>Comfy</cell><cell>Building</cell><cell>Just</cell><cell>Everything</cell><cell>Check</cell><cell>Nights</cell></row><row><cell>24</cell><cell>Back</cell><cell>Located</cell><cell>Anyone</cell><cell>Small</cell><cell>View</cell><cell>Clean</cell><cell>Day</cell><cell>View</cell><cell>Good</cell><cell>Experience</cell></row><row><cell>25</cell><cell>Service</cell><cell>Shopping</cell><cell>Come</cell><cell>Problem</cell><cell>Standards</cell><cell>Good</cell><cell>Coffee</cell><cell>Loved</cell><cell>Every</cell><cell>Price</cell></row><row><cell>26</cell><cell>Also</cell><cell>Macy's</cell><cell>Can't</cell><cell>Outside</cell><cell>King</cell><cell>Close</cell><cell>Floor</cell><cell>Amazing</cell><cell>Accommodating</cell><cell>NY</cell></row><row><cell>27</cell><cell>Stay</cell><cell>Many</cell><cell>Definitely</cell><cell>Also</cell><cell>Pillows</cell><cell>Views</cell><cell>One</cell><cell>Better</cell><cell>Help</cell><cell>Year</cell></row><row><cell>28</cell><cell>Check-in</cell><cell>Convenient</cell><cell>Marquis</cell><cell>Even</cell><cell>Two</cell><cell>Fantastic</cell><cell>Little</cell><cell>Quality</cell><cell>Really</cell><cell>Night</cell></row><row><cell>29</cell><cell>Two</cell><cell>Grand</cell><cell>Overall</cell><cell>People</cell><cell>Modern</cell><cell>Staff</cell><cell>Staff</cell><cell>Quiet</cell><cell>Time</cell><cell>Weekend</cell></row><row><cell>30</cell><cell>Booked</cell><cell>Penn</cell><cell>Friends</cell><cell>Stay</cell><cell>Friendly</cell><cell>Wonderful</cell><cell>Lobby</cell><cell>Food</cell><cell>Attentive</cell><cell>Much</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc>Expedia Upscale Data: Results from Topic Regression (T = 10, Topic 2 as Contrast)</figDesc><table><row><cell></cell><cell></cell><cell>Posterior</cell><cell>Posterior</cell></row><row><cell>Topic</cell><cell>Parameter</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>Baseline</cell><cell>0</cell><cell>0 558</cell><cell>0 583</cell></row><row><cell>Problems at check-in</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc>Expedia Midscale Hotel Data, Top 30 Words from the SC-LDA-Rating Model (T = 8)</figDesc><table><row><cell>Topic 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Our regression model (see a DAG of this model in Figure3) implies that the rating makes a likelihood contribution to the draw of d . As a result, the draw of d changes. LDA model, which assures that the candidates for d are always probabilities. As a result of this candidategenerating density, all elements in the Metropolis acceptance ratio cancel out, except for the likelihood component of the regression model. For the draw of the parameters of the ordinal regression model (</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>of the</cell></row><row><cell cols="5">We apply the following Metropolis-Hastings (MH) sam-</cell></row><row><cell cols="5">pling scheme to the draw of d : 1. Generate a candidate cand d 2. Accept/reject cand d based on the Metropolis ratio from Dirichlet C T D + .</cell></row><row><cell>=</cell><cell>p y d p y d</cell><cell>cand d d</cell><cell>2 c 2 c</cell></row><row><cell cols="5">which are truncated univariate normal distributions. Note</cell></row><row><cell cols="4">that we generate the candidate cand d</cell><cell>from the posterior</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell>.1</cell><cell>Relationships T = 3</cell><cell></cell></row><row><cell>Contrast 1</cell><cell>Contrast 2</cell><cell>Contrast 3</cell></row><row><cell>*</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell>.2</cell><cell cols="2">Simulation Results: Parameter Recovery</cell><cell></cell></row><row><cell></cell><cell>= 100/V</cell><cell>= 1,000/V</cell><cell>= 2,000/V</cell></row><row><cell>= 1/T</cell><cell>0.997 (0.001)</cell><cell>0.974 (0.001)</cell><cell>0.950 (0.001)</cell></row><row><cell></cell><cell>0.940 (0.002)</cell><cell>0.921 (0.002)</cell><cell>0.892 (0.003)</cell></row><row><cell>2</cell><cell>0.209 (0.011)</cell><cell>0.205 (0.014)</cell><cell>0.199 (0.018)</cell></row><row><cell></cell><cell>0.026 (0.011)</cell><cell>0.033 (0.018)</cell><cell>0.054 (0.041)</cell></row><row><cell>= 2/T</cell><cell>0.998 (0.001)</cell><cell>0.975 (0.001)</cell><cell>0.949 (0.001)</cell></row><row><cell></cell><cell>0.886 (0.003)</cell><cell>0.860 (0.003)</cell><cell>0.803 (0.004)</cell></row><row><cell>2</cell><cell>0.201 (0.013)</cell><cell>0.193 (0.017)</cell><cell>0.187 (0.024)</cell></row><row><cell></cell><cell>0.016 (0.006)</cell><cell>0.027 (0.016)</cell><cell>0.063 (0.033)</cell></row><row><cell>= 4/T</cell><cell>0.997 (0.001)</cell><cell>0.974 (0.001)</cell><cell>0.951 (0.001)</cell></row><row><cell></cell><cell>0.792 (0.004)</cell><cell>0.747 (0.005)</cell><cell>0.690 (0.007)</cell></row><row><cell>2</cell><cell>0.199 (0.012)</cell><cell>0.213 (0.020)</cell><cell>0.195 (0.025)</cell></row><row><cell></cell><cell>0.014 (0.003)</cell><cell>0.023 (0.020)</cell><cell>0.039 (0.024)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell>.3</cell><cell>Model Fit for Simulated Data</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Marketing Science 35(6), pp.953-975, © 2016 INFORMS   </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Estimation of the LDA Model</head><p>The standard LDA model proposed by <ref type="bibr" target="#b4">Blei et al. (2003)</ref> employs a Bayesian approach to augment the unobserved topic assignments z w of the words w. To derive the expression necessary to sample the topic indicators, we start by considering the joint likelihood of observing the words (w) and topic indicators (z), integrated over the word choice probabilities given topics The likelihood of a word (or sentence), conditional on n , is</p><p>The likelihood of a topic assignment, conditional on n , is</p><p>Our model with sticky topics is a sentence-based model that constrains topic assignments to sentences in the same way as in the SC-LDA model In the following, we develop an MCMC sampling scheme for the sticky topic LDA model. The factorization of the joint posterior distribution of the parameters suggests the following sampling steps:</p><p>1. On the document level (omitting subscript d for z and w to improve readability):</p><p>( </p><p>Because of the first order carryover effect of the topics, it is useful to write down the joint probability of all quantities with respect to two subsequent sentences p w n w n+1 z n z n+1 n n+1</p><p>In the above, the expression p z n+1 z n d n+1 was omitted because it is a constant with respect to z n . Note that</p><p>where the last expression presents the case of a repeated topic carryover.</p><p>A.5.1. Draw of z n and n . Analogous to Gibbs sampling for the Hidden Markov Model (Frühwirth-Schnatter 2006), we consider a joint "single-move" Gibbs sampler of the topic and the stickiness indicator. The joint posterior of z n n is obtained by dropping all elements independent of z n and n from Equation (9) (sentence-based model, from (10)) and treating the latent variables z n−1 , n−1 , z n+1 , and n+1 as observed</p><p>Using results from the above:</p><p>In the case of n = N d p w n+1 • and p n+1 • can be dropped because these distributions do not exist. Note that, in the case of a topic carryover from word n to n + 1 and n = 0, the downstream likelihood of z n consists of two words. If, however, n = 1 the posterior does not depend on z n because the topic is already determined. Essentially, the above expressions deal with the question whether to choose the "observed" previous topic assignment z n−1 for the current word w n or to consider the case that z n originates from d . The above expressions give rise to T + 1 multinomial probabilities from which we can jointly draw z n , n . An alternative sampling scheme may consider z n d , n d for a simultaneous update of all the latent topic and stickiness indicators in a document.</p><p>A.5.2. Draw of d . In MCMC sampling for the standard LDA, the full conditional draw of d is based on using the multinomial topic assignment of all sentences in a document as likelihood information. The multinomial likelihood of the topic assignments is combined with the Dirichlet prior p for a conjugate update via a Dirichlet posterior in which the topic assignments are simple counts</p><p>For the sticky LDA model, we have to keep track of the topic assignments that are downstream of d and disregard topic assignments due to = 1</p><p>We use the count matrix C T D to collect topic assignments conditional on n = 0 and then proceed as in the standard LDA.</p><p>A.5.3. Draw of . The draw of t is not affected by the mixture prior for the topic assignments because of conditioning on z and can therefore be conducted in the usual way p t else ∝ Dirichlet C W T + (12)</p><p>A.5.4. Draw of . For the model without covariates, the update of is accomplished as follows:</p><p>d n or the number of times an assignment of topic t was "observed" to be sticky. C t is the number or "trials," i.e., the total number of assignments of topic t to the sentences in the corpus except for z d 1 , the topic assignment of the first word (or sentence) in each document. For the model with covariates (Equation ( <ref type="formula">3</ref>)) we use a binary probit regression model <ref type="bibr" target="#b25">(Rossi et al. 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CORRECTION</head><p>In this article, "Sentence-Based Text Analysis for Customer Reviews" by Joachim Büschken and Greg M. Allenby (first published in Articles in Advance, July 18, 2016, Marketing Science, DOI:10.1287/mksc.2016.0993), Appendix A.5.1 has been updated and Equation 13 has been corrected.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into topic modeling via Dirichlet forest priors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Internat. Conf. Machine Learn</title>
				<meeting>Internat. Conf. Machine Learn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deriving the pricing power of product features by mining consumer reviews</title>
		<author>
			<persName><forename type="first">N</forename><surname>Archak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1485" to="1509" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Positive effects of negative publicity: When negative reviews increase sales</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="815" to="827" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Internat. Conf. Machine Learn</title>
				<meeting>23rd Internat. Conf. Machine Learn<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The dimensionality of customer satisfaction survey responses and implications for driver analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Büschken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="533" to="553" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">State-dependence effects in surveys</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="838" to="853" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring the value of online product reviews in forecasting sales: The case of motion pictures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dellarocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Awad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Interactive Marketing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="23" to="45" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling dynamic effects in repeated-measures experiments involving preference/choice: An illustration involving stated preference analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Desarbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Hollman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Psych. Measurement</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="209" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incentive-aligned conjoint analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liechty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dynamic changepoint model for new product sales forecasting</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bgs</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="65" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Frühwirth-Schnatter</surname></persName>
		</author>
		<title level="m">Finite Mixture and Markov Switching Models</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science+Business Media</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Answering the unasked question: Response substitution in consumer surveys</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Rucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="195" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Designing ranking systems for hotels on travel search engines by mining user-generated and crowdsourced content</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="520" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using online conversations to study word-of-mouth communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Godes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mayzlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="560" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<title level="m">Hidden topic Markov models. Internat. Conf. Artificial Intelligence Statist</title>
				<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling preference and structural heterogeneity in consumer choice</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Kamakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="152" to="172" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated marketing research using online customer reviews</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="881" to="894" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">More than words: The influence of affective content and linguistic style matches in online reviews on conversion rates</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>De Ruyter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Brüggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wetzels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pfann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="103" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The authorrecipient-topic model for topic and role discovery in social networks: Experiments with Enron and academic email</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Corrada-Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic allocation of pharmaceutical detailing and sampling for long-term profitability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jedidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="909" to="924" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hidden Markov model of customer relationship dynamics</title>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lattin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="204" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mine your own business: Market-structure surveillance through text mining</title>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fresko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="543" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Characterizing microblogs with topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Liebling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Internat. AAAI Conf. Weblogs Social Media</title>
				<meeting>Fourth Internat. AAAI Conf. Weblogs Social Media</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The authortopic model for authors and documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Conf. Uncertainty Artificial Intelligence</title>
				<meeting>20th Conf. Uncertainty Artificial Intelligence<address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
		<title level="m">Bayesian Statistics and Marketing</title>
				<meeting><address><addrLine>West Sussex, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overcoming scale usage heterogeneity</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gilula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">453</biblScope>
			<biblScope unit="page" from="20" to="31" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining marketing meaning from online chatter: Strategic brand analysis of big data using latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tirunillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Tellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="463" to="479" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A joint model of text and aspect ratings for sentiment summarization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="308" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Topic modeling: Beyond bag-of-words</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Internat. Conf. Machine Learn</title>
				<meeting>23rd Internat. Conf. Machine Learn<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A model for observation, structural, and household heterogeneity in panel data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="149" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling variation in brand preference: The roles of objective environment and motivating conditions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fennel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="31" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging missing ratings to improve online recommendation systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="365" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling consumer learning from online product reviews</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="169" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
