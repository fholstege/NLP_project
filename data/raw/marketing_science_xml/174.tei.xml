<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org A Flexible Method for Protecting Marketing Data: An Application to Point-of-Sale Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-08">January 8, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Schneider</surname></persName>
							<email>matt.schneider@drexel.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LeBow College of Business</orgName>
								<orgName type="institution" key="instit2">Drexel University</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sharan</forename><surname>Jagpal</surname></persName>
							<email>jagpal@rutgers.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Rutgers Business School</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>07102</postCode>
									<settlement>Newark</settlement>
									<region>New Jersey</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sachin</forename><surname>Gupta</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">S.C. Johnson Graduate School of Management</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<postCode>14853</postCode>
									<settlement>Ithaca</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaobo</forename><surname>Li</surname></persName>
							<email>shaobo.li@ku.edu</email>
							<affiliation key="aff3">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">University of Kansas</orgName>
								<address>
									<postCode>66045</postCode>
									<settlement>Lawrence</settlement>
									<region>Kansas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Yu</surname></persName>
							<email>yuyu@ucmail.uc.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Lindner College of Business</orgName>
								<orgName type="institution">University of Cincinnati</orgName>
								<address>
									<postCode>45221</postCode>
									<settlement>Cincinnati</settlement>
									<region>Ohio</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org A Flexible Method for Protecting Marketing Data: An Application to Point-of-Sale Data</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2018-01-08">January 8, 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2017.1064</idno>
					<note type="submission">Received: December 31, 2013 Accepted: June 28, 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>data protection</term>
					<term>privacy</term>
					<term>statistical disclosure limitation</term>
					<term>marketing mix models</term>
					<term>point-of-sale data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Businesses routinely share marketing data with their employees, suppliers, customers, and regulators as well as the general public. Widely known examples include data on customer purchasing histories, media viewership, and web browsing behaviors gathered by market research companies and sold to their clients; product sales ranks released by Amazon to its vendors and to the general public; movie viewing histories of Netflix subscribers released to the general public in a contest to design a better movie recommendation engine; and the channel partnership between Walmart and Procter &amp; Gamble based on information sharing in the supply chain <ref type="bibr" target="#b11">(Grean and Shaw 2002)</ref>. In all these cases the data provider stands to benefit by sharing the data, but also seeks to actively protect certain aspects of the data from disclosure. This paper proposes a framework and statistical approach to help firms (vendors) share marketing data while limiting the risk of disclosure. In particular, we address a Marketing Science Institute (2016) research priority and show how firms can trade off privacy concerns against the commercial value of their data.</p><p>To motivate the importance of data protection and to provide context, we begin with a classic example of widely used market research data. AC Nielsen, the largest marketing research company in the world, sells point-of-sale scanner data to manufacturers and retailers of consumer packaged goods. The data are obtained from a sample of retail stores to whom AC Nielsen provides a contractual assurance that their identities will not be revealed to data users. There are at least two important reasons for AC Nielsen to protect the identities of sample stores and their sales volumes. The first reason is to prevent tampering with market research results (e.g., by artificially inflating or deflating sales in sample stores to skew volumes). <ref type="bibr">1</ref> The second reason is to prevent data users from taking strategic actions based on the identities of stores, such as locating a new competing store close to a highperforming retail store in the sample.</p><p>AC Nielsen currently protects the identities of sample stores primarily through data aggregation. In particular, most AC Nielsen clients are not provided with store-level data, but only with data aggregated to a higher level, such as market-level data. Market-level sales data are linearly aggregated (i.e., summed) sales; in addition, volume-weighted average prices and promotions are provided across stores in the market. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>154</head><p>Marketing <ref type="bibr">Science, 2018</ref><ref type="bibr">Science, , vol. 37, no. 1, pp. 153-171, © 2018</ref> INFORMS <ref type="bibr">Bucklin and Gupta (1999, p. 261)</ref> analyzed data from a survey of academics and practitioners and concluded that "While Nielsen and IRI have store-and accountlevel data, third-party consultants such as MMA usually conduct their analysis on the market-level data to which they are given access." This aggregation process has a dual effect. On one hand, it raises the cost of identifying sample stores sufficiently so that the data protection goal of AC Nielsen is accomplished; on the other hand, it significantly reduces the commercial value of the data for users.</p><p>The goal of manufacturers and retailers who buy AC Nielsen data is to optimize marketing decisions by using estimates of important metrics such as price elasticities and promotion lift factors, derived from a sales response or marketing-mix model. Estimates of price elasticities and promotion effects based on the aggregated data are subject to aggregation bias, which can be very large in magnitude <ref type="bibr" target="#b6">(Christen et al. 1997)</ref>. For instance, aggregation to the market-level typically leads to an overstatement of the effects of promotional variables such as in-store displays and retailer feature advertising <ref type="bibr" target="#b6">(Christen et al. 1997)</ref>. Approaches to ameliorate aggregation bias in the price elasticities and promotion effects have been suggested (e.g., <ref type="bibr" target="#b15">Link 1995</ref><ref type="bibr" target="#b27">, Tenn 2006</ref>) but the bias is difficult to eliminate. This trade-off between data protection and commercial value lies at the heart of the problem that we study in this paper.</p><p>We use the AC Nielsen prototypical example to illustrate key elements of business situations in which the need for protecting marketing data arises. In these situations, a "data provider" (for example, AC Nielsen) obtains data from "data subjects" (retail stores) and provides these data to "data users" (consumer packaged goods manufacturers and retailers), but does not disclose certain aspects that we term "confidential data" (store identities). The goal of data users is to benefit from the data (for example, by estimating price elasticities and promotional effects for business decisions). Typically, these benefits are derived from the use of the data in a "data user's model" (a sales response or marketing-mix model). As noted earlier, the data user may derive additional benefit from learning the confidential data; we term such use "invalid use" (learning store identities and linking them to sales).</p><p>Often the attempt to make invalid use of the data is performed by "data intruders" who may be third parties who have access to the data. In this paper we do not distinguish between invalid use by data users or by data intruders. The task facing the data provider is to use "data protection" methods that will permit valid use but deter or make difficult invalid use of the data by users or intruders. A primary goal of our paper is to propose a data protection method that allows the data provider to choose a preferred data protection strategy after explicitly evaluating the trade-off between commercial value and data protection.</p><p>In Figure <ref type="figure" target="#fig_1">1</ref>, we use the AC Nielsen example to conceptualize a Marketing Data Privacy Ecosystem that identifies relationships among key players, their business goals, and the data protection imperatives that follow. An important aspect to emphasize is that data providers may be motivated to protect data not simply because of legal or contractual obligations to data subjects, but also because preserving privacy may be a key pillar of the data provider's brand positioning. When this is the case, the cost of invalid use may be very high because it damages trust in the data provider's brand.</p><p>Data protection situations that fit this ecosystem are common in marketing research; consequently, the choice of data protection method can have a major effect on decision-making by the data user. For instance, AC Nielsen and IRI collect data from household panels and provide them to their clients. IMS Health collects data on prescriber behavior from physician panels and provides it to pharmaceutical firms. It also collects prescription sales information from retail pharmacies to sell to clients. Another broad context in which data protection needs arise is when firms supply information to buyers of their products or services to help them evaluate the product or service. For instance, Google provides data to advertisers on the click-through behavior of search-engine users in response to sponsored search advertising. Google chooses to not provide impression-level data to its clients, but instead aggregates the data to the daily level to increase privacy. As in our AC Nielsen example, this leads to potential aggregation bias in the estimated effects of advertising, making it more difficult for advertisers to optimize their advertising spending <ref type="bibr" target="#b0">(Abhishek et al. 2015)</ref>.</p><p>Firms currently choose from a wide spectrum of data protection methods. At one extreme, the firm can elect to accurately reveal highly disaggregated customer data (e.g., Netflix). At the other extreme, the firm may destroy customer data for reasons of privacy, by choice or to comply with regulatory or contractual obligations, implicitly foregoing any potential gains from data sharing, as well as the opportunity to benefit in the future from analysis of a complete historical data set. In the middle of the spectrum, aggregation is commonly used to mask the data, as is the case in the AC Nielsen and Google examples. In all these cases, the firm is implicitly making a trade-off between commercial value and data protection.</p><p>In this paper, we seek to make several contributions to the marketing literature. First, we conceptualize the need for data protection in the context of a business ecosystem that is widely prevalent in marketing (Figure <ref type="figure" target="#fig_1">1</ref>). A key distinction in this framework relative to  the privacy literature in statistics and computer science is that we explicitly recognize the business goals of the data user as reflected in the data user's model, and incorporate these into the data provider's model. By contrast, to our knowledge, almost all of the extant literature on data protection, which is outside marketing, does not explicitly specify the goals of the data user (we discuss this point in detail in Section 1.1). This is in part because the literature on statistical disclosure has largely taken the perspective of governmental agencies such as the U.S. Census Bureau, who release data for a diffuse set of users, typically the general public.</p><p>Second, we contribute to the statistical disclosure literature by proposing a new approach to incorporate the data provider's data protection preferences into a Bayesian model through a prior distribution controlled by a single parameter (in this paper, we characterize the "prior" as a privacy-preserving prior distribution found in <ref type="bibr" target="#b25">Schneider and Abowd 2015)</ref>. In particular, we include a parameter kappa (κ) in the prior distribution that can be changed by the data provider to manage the trade-off between information loss to the data user and loss of protection from invalid use. The prior distribution is then used to generate synthetic but representative data from a protected posterior predictive distribution. We propose a rigorous methodology for data protection within a single formal probability model which is discussed in detail (see Figure <ref type="figure" target="#fig_2">2</ref>). This modeling strategy provides a key managerial benefit: The model allows the data provider to explicitly manage the trade-off between data protection and commercial value given the data provider's risk-return preference. This is in contrast with standard approaches such as top-coding, swapping, rounding, and aggregation, which may be considered ad hoc in this regard (these methods are discussed later in Table <ref type="table" target="#tab_3">1</ref>).</p><p>Finally, and perhaps most important, we propose new measures of identification risk inherent in a data set-Average Loss of Protection (ALP) and Maximum Loss of Protection (MLP)-and explore the theoretical and empirical relationships of these to standard measures, i.e., the Gini Coefficient and Entropy. MLP measures the highest probability of store identification across stores, and hence can be interpreted as the minimum level of privacy across stores. It is associated with the probability of just one store being identified, which may result in large losses due to, for instance, a lawsuit or a decrease in trust for the data provider. Note that MLP is a viable risk management measure for comparing minimum privacy levels across different data protection approaches applied to a data set.</p><p>We illustrate the proposed methodology using AC Nielsen point-of-sale data for brands of a consumer packaged good. We find that the parameter κ assists</p><p>Marketing <ref type="bibr">Science, 2018</ref><ref type="bibr">, vol. 37, no. 1, pp. 153-171, © 2018</ref> the data provider in choosing an appropriate prior distribution. We also find that our method performs well compared to a set of seven benchmark data protection methods, including no protection and the aggregation approach used by AC Nielsen. The main limitation of the proposed identification disclosure risk model in this empirical application is that the estimated probabilities of an observation belonging to stores in a given time period do not sum to 100%. We discuss the implications of not having this constraint in Section 2.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Privacy Literature</head><p>The academic literature in marketing has explored a few themes in data privacy. An important theme is the relationship between privacy and targetability of marketing actions. <ref type="bibr" target="#b10">Goldfarb and Tucker (2011)</ref>, for instance, explores the impact on advertising effectiveness of privacy regulations in Europe that restrict the collection and use of customer data. Similarly, <ref type="bibr" target="#b7">Conitzer et al. (2011)</ref> considers the impacts of a customer's choice of maintaining anonymity on firms' ability to price discriminate and on consumer welfare. The use of aggregation to mask sensitive consumer information has been recognized by, for instance, <ref type="bibr">Steenburgh et al. (2003, p. 40)</ref> who propose an approach to use "massively categorical" variables such as zip codes in choice models. As the number of categories increases, the number of consumers in each category decreases, thereby increasing the risk of disclosure of individual data. <ref type="bibr" target="#b8">de Jong et al. (2010)</ref> use randomized response designs in survey data collection to protect respondents' identities while allowing for unbiased aggregate inferences. Our approach is fundamentally different from this stream of research because we focus on data protection ex post, not ex ante.</p><p>Because much of the work on data protection is outside the marketing literature, we focus on the relevant literature in statistics. Standard data protection methods in use at a variety of agencies include aggregating, swapping, rounding, and top-coding (we define these methods in Section 3 and Table <ref type="table" target="#tab_3">1</ref>). The goal of data protection is usually to limit disclosure risk at an observational level (e.g., individual) while preserving as much of the information as possible. Some examples of disclosure risk measures in use include the number of unique populations in a data set or the probability of identification of a single observation. <ref type="bibr" target="#b21">Reiter (2005)</ref> used probabilities of identification as the disclosure risk measure and applied standard data protection methods to unprotected data. A later paper <ref type="bibr" target="#b22">(Reiter 2009)</ref> found that aggregation was more effective than swapping. However, standard data protection methods are so extreme that for many analyses, protected data have limited utility. <ref type="bibr">Little (1993, p. 422</ref>) recognized the disadvantages of simply providing the sufficient statistics needed for particular analyses (i.e., aggregation).</p><p>These include "lack of flexibility in the choice of variables to be analyzed, and the relative inability to do exploratory analysis and model-checking."</p><p>In response to the limitations and ad hoc nature of standard data protection methods, the data privacy community shifted to the use of synthetic data, which are simulated data generated from a probability distribution. Synthetic data provide an important advantage: They can allow theoretical guarantees of privacy. The first theoretical data protection model using synthetic data was a Dirichlet-Multinomial model that was applied to count data from the U.S. Census Bureau <ref type="bibr" target="#b17">(Machanavajjhala et al. 2008</ref>). However, due to the strong theoretical requirements for privacy, the protection "rendered the synthetic data useless" <ref type="bibr">(Machanavajjhala et al. 2008, p. 277)</ref>. Although this and subsequent papers (e.g., Charest 2011) have advanced the theoretical knowledge of synthetic data protection methods, from a practical point of view, their synthetic data were of little use or were too highly aggregated (e.g., into a single count).</p><p>Part of the problem is that these applications do not use covariates in the data protection model. Covariates allow the synthetic dependent variable to vary across observations, which improves utility for the data user. Recent literature has sought to advance data protection methods by extending them to analyze richer data with covariates. <ref type="bibr" target="#b1">Abowd et al. (2013)</ref> used covariates in a regression model for U.S. Census Bureau data, but found that the strict theoretical guarantees of privacy were still too strong to be met in a multiple regression model, and only succeeded in a simple regression model with one covariate. Those authors suggested the use of more relaxed measures of privacy to increase data utility.</p><p>Recent data protection models have relaxed theoretical guarantees of privacy to generate synthetic data for more general real-world regression problems that include several covariates. For instance, <ref type="bibr" target="#b13">Hu et al. (2014)</ref> generated synthetic data with a Dirichlet-Multinomial regression model with 14 categorical covariates. More recently, <ref type="bibr" target="#b25">Schneider and Abowd (2015)</ref> developed a privacy-preserving prior distribution from the data provider's perspective for use with a zero-inflated regression model. Their goal was to provide an alternative approach to the protection method used by the U.S. Census Bureau that was based on suppression of zeros. They found that synthetic data released from their models had a similar fit to simpler models; however, importantly, their models allowed the provider to achieve a greater level of privacy. Our paper differs from <ref type="bibr" target="#b25">Schneider and Abowd (2015)</ref> most notably in having a different goal, i.e., developing a data provider's model that is consistent with the Data Privacy Marketing Ecosystem in Figure <ref type="figure" target="#fig_1">1</ref>. In other words, our method generates protected data that are useful for specified data users. Our model is also different in terms of protecting the estimated parameters of continuous variables (such as price) by adjusting the multivariate Normal prior and parsimoniously controlling the entire protection mechanism by using a single parameter κ.</p><p>In sum, although recent work has advanced the use of synthetic data, nearly all of the work has been done from the perspective of a governmental agency that is required to release and protect data for a diffuse group (the public). These data protection methods do not allow the decision maker to balance potentially conflicting goals in a decision-theoretic framework. For example, the firm that sells data needs to balance the incremental profits from more accurate data disclosure and the potential costs of a data breach (including hidden costs such as those resulting from a loss in consumer trust in the firm).</p><p>The literature review indicates that there is a strong unmet need for a synthetic data model that incorporates three parties with different goals, i.e., the data provider as a commercial supplier who protects data with a data protection method, the data user as a customer, and the potential data intruder. As discussed, such a framework is especially needed in marketing applications. Our paper proposes one such framework. Philosophically, we agree with <ref type="bibr">Reiter (2009, p. 225</ref>) who notes that "synthetic data reflect only those relationships included in the data generation models." Thus, we gear our synthetic data and data protection method toward the business goal of enabling valid use by the data user.</p><p>One notable aspect of our paper is that the Marketing Data Privacy Ecosystem focuses on the data user's need to make important marketing decisions using the data. These needs then drive the development of the data protection method by the data provider. Prior research <ref type="bibr" target="#b21">(Reiter 2005</ref><ref type="bibr" target="#b17">, Machanavajjhala et al. 2008</ref><ref type="bibr" target="#b5">, Charest 2011</ref><ref type="bibr" target="#b1">, Abowd et al. 2013</ref><ref type="bibr" target="#b13">, Hu et al. 2014</ref><ref type="bibr" target="#b25">, Schneider and Abowd 2015</ref> used data from the U.S. Census Bureau, the Bureau of Justice Statistics or simulation. These choices obviated the need to incorporate a customer of the synthetic data, i.e., the data user, into the data protection strategy. By contrast, in our paper, we explicitly model all three players in the Marketing Ecosystem, i.e., the data provider, the data user, and the potential intruder.</p><p>The rest of the paper is organized as follows. In Section 2 we discuss the data user's model and a model to quantify the risk of disclosure, and propose an algorithm for generating synthetic protected data. In Section 3, we provide an empirical application of the algorithm to a specific data user model and discuss results, including a comparison with benchmark models. Section 4 discusses conclusions and proposes directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Models Used by the Data User and Data Provider</head><p>We believe it is useful to illustrate the proposed methodology in a specific model-based application context. In Section 2.1, we return to the example of the data provider, AC Nielsen, sharing point-of-sale data with data users and present a well known marketresponse model that is used by its data users to estimate brand price elasticities and promotion effects. In Section 2.2, we introduce a model to predict the risk of disclosure of the identities of stores who provided the data to AC Nielsen. In Section 2.3, we propose a data protection method for use by data providers such as AC Nielsen. In Section 2.4, we propose several new criteria to measure the performance of any data protection method. We also illustrate (in Section 2.4.1) the application of the identification disclosure model by the data provider, and discuss how an intruder may use additional data to predict store identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data User's Model</head><p>We illustrate our method using SCAN*PRO <ref type="bibr" target="#b14">(Leeflang et al. 2013)</ref>, a market-response model that is widely used by consumer goods manufacturers and by AC Nielsen. This model quantifies the short-term effects on a brand's unit sales of such retailers' activities as in-store prices, special displays, and feature advertising. <ref type="bibr" target="#b28">Van Heerde et al. (2002)</ref> reported that as of the date of their article, SCAN*PRO and its variants had already been used in over 3,000 different commercial applications.</p><p>The fundamental model specification in SCAN*PRO involves a multiplicative or log-log relationship between a brand's unit sales volume, and own and competitive brand prices and promotions. The model is specified at the store-level and is estimated using weekly data. To maintain sharp focus on our data protection method, we use a version of the full SCAN*PRO model. The model is estimated separately by brand, and includes store fixed effects, an own-price effect, and three own-promotion effects. The three own-promotion effects are own-display only, own-feature only, and both own-display and own-feature. <ref type="bibr">2</ref> Hence the market response model is</p><formula xml:id="formula_0">S i jt α i j P β j i jt L l 1 γ D li jt l j e i jt , i 1, . . . , n; t 1, . . . , T, (1)</formula><p>where S represents sales volume, P is price, and the Ds represent indicator variables for three kinds of promotions indexed by l, i.e., Display only, Feature only, and both Display and Feature. In the model, i indexes stores, j indexes brands, and t indexes weeks. As is well known, in this multiplicative model the own-price effects β j represent own-price elasticities, the γ l j represent own-promotion effects, and the i jt represent the <ref type="bibr">Marketing Science, 2018</ref><ref type="bibr">, vol. 37, no. 1, pp. 153-171, © 2018</ref> error terms. The promotion effects are interpretable as promotion multipliers or the factors by which baseline sales increase under promotion. We assume that the primary goal of the data user is to obtain accurate estimates of own-price elasticities and own-promotion effects; these are critical quantities for characterizing product markets as well as for determining optimal mark-ups or conducting what-if simulations.</p><p>Although AC Nielsen collects weekly store-level data from a random sample of stores, it is reluctant to release store-level data to data users. As discussed previously, this is in large part because of the concern that data users may be able to predict or guess the identities of sample stores, information which AC Nielsen is contractually bound to protect from data users. In addition, the identity of a sample store is more likely to be discovered and more damaging when the exact storelevel sales quantities are known. To fulfill its contractual obligations, AC Nielsen has typically aggregated the store-level data to market levels before release to users, thus protecting the store identities and the storelevel sales quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model for Identification Disclosure Risk</head><p>We assume that the key risk that the data provider wishes to guard against is the risk of disclosing the confidential information, i.e. , the true store identities (e.g., "this weekly point-of-sale observation is from the Kroger on Thompson Road in Indianapolis") to a data user or potential data intruder. To quantify the predictability of the identification disclosure risk for various released (protected) data sets relative to the original true data, we specify the following multinomial logit model, where the response variable is the store ID and the predictor variables are ln(sales), ln(price), and promotion indicators, for each store i, week t, and brand j.</p><p>The multinomial logit model is</p><formula xml:id="formula_1">ln P(Ŷ it ID i | S it , P it , D it ) P(Ŷ it ID 1 | S it , P it , D it ) J j 1 a i j ln S i jt + J j 1 b i j ln P i jt + J j 1 L l 1 c li j D li jt , i, 1, . . . , n; i 2, . . . , n; t 1, . . . , T, (2)</formula><p>where Y it is a random variable that represents the store ID taking values {ID 1 . . . , ID n }; ID 1 is the store ID of Store 1, which serves as a reference or base alternative in the multinomial logit model; and P(Ŷ it ID i | S it , P it , D it ) is the fitted probability in week t that Store i has ID equal to ID i , i 2, . . . , n, given sales, prices, and promotions of all brands. <ref type="bibr">3</ref> Note that the data provider has all of the information required to estimate this model, including the store identities, true and protected sales data, and prices and promotions. Evaluating the relative identification disclosure risk of the true data versus any kind of protected data (i.e., the probability that the store is the Kroger on Thompson Road in Indianapolis, given the prices, promotions, and true sales of Tide 147 ounces, versus the probability that the store is the Kroger on Thompson Road in Indianapolis, given the prices, promotions, and synthetic sales of Tide 147 ounces) is equivalent to measuring the predictive abilities of the multinomial logit models built on true data versus the protected data. To measure predictive ability we use leave-one (week)-out cross validation, where the risk of store identification is measured using the predicted probability of store identification in hold-out observations. For example, the potential data intruder might say "based on my available data, I estimate a 25% probability that this observation is from the Kroger on Thompson Road in Indianapolis." We present further details in Section 2.4.1 including the kinds of data to which potential intruders may have access in real life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Proposed Data Protection Model</head><p>We propose a Bayesian random effects model for protecting data through the use of a flexible prior distribution that reflects the data provider's risk-return preferences. To begin, we discuss some pertinent questions about the data provider's process of developing the protected data. First, the data provider's goal is to release useful yet privacy-protected data to data users. As discussed previously, in our analysis the data provider assesses the identity disclosure risks by measuring the predictability of store identities based on various forms of protected data compared to the true data.</p><p>Second, which variables in the data gathered from stores should not be released, and hence protected by transformation into synthetic data? We use the decision criterion that variables with the most power to predict store IDs in the training data should be protected. As discussed later in Section 3, we choose to protect sales quantities but not price or promotion data. We chose these variables based on analysis that is reported in detail in Appendix C, and is conceptually described here. In our available sample of AC Nielsen data, we use the multinomial logit model specified in Section 2.2 to compute the ability of variables such as prices, promotions, and sales volumes to predict store IDs. Our analysis shows that using prices alone leads to an average loss of protection of 0.062, while using sales volumes alone leads to a much higher average loss of protection of 0.511. (See Equation ( <ref type="formula" target="#formula_9">7</ref>) and the related discussion for the definition of Loss of Protection in Section 2.4.1.) Consequently, we chose to protect sales quantities in our data protection method. Why do we not protect the prices and promotions as well? There are two reasons over and above their limited ability to  True sales quantities predict store IDs. One, prices and promotions provide valuable information to data users, such as the distribution of retail prices of own and competing products: Protection would distort this information. Two, unlike brand sales volumes, prices and promotions are publicly available information that can be observed in the store. Therefore, a determined intruder could obtain such data with sufficient effort and hence these data are less necessary to protect. While price and promotion information can also be protected, this would add greater complexity to the models. We discuss this idea as a future research opportunity in Section 4. Third, in developing the protected data, we propose the use of a random effects model instead of a model-free noise approach (e.g., simply adding a random number to sales). We implement the model-free noise approach as a benchmark method for comparison. In a random effects model, the distribution of the dependent variable, i.e., sales quantities, can be altered with little difficulty to incorporate non-normally distributed data, thus allowing modeling flexibility across types of data. Additionally, and perhaps most important, it is common for estimates of random effects (e.g., store effects) to rely on only a few observations each. The privacy-preserving prior distribution naturally protects the estimates of the random effects from discovery by an intruder by scaling the estimates of the random effects toward zero, or no information. <ref type="bibr">4</ref> Figure <ref type="figure" target="#fig_2">2</ref> summarizes the process by which the data provider generates protected data to release to the data user. The protection mechanism we propose shrinks the values of the estimated random effects and fixed effects toward zero (i.e., the limiting case of no information) through the use of a privacy-preserving prior distribution on the variances of the random effects and fixed effects. This is managerially important because the data provider prevents the data intruder from knowing or approximating the true arithmetic mean of q observations in a small group. Instead, the protection mechanism scales the estimated values of the q observations toward their greater group means (e.g., overall intercept of all observations). Our proposed method is nonstandard because it first protects the random and fixed effects and then adds noise centered at the protected deviations. After controlling for all variables and shrinking the estimates of the random and fixed effects toward zero, we generate the synthetic sales quantities.</p><p>We describe the base modeling set-up and the likelihood in Section 2.3.1. A description of our flexible protective prior distribution is given in Section 2.3.2. Computational details for generating synthetic data are provided in Section 2.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Base Model.</head><p>We observe a response variable, sales S i jt for store i 1, . . . , n, brand j 1, . . . , J, and time t 1, . . . , T. Additionally, price, P i jt , and promotion indicators D li jt are covariates that affect the response. Based on Equation (1), for each brand j, we model ln S i jt using a random effects model</p><formula xml:id="formula_2">ln S i jt µ j + u i j + β j ln P i jt + L l 1 (ln γ l j )D li jt + i jt , (3)</formula><p>where µ j is the overall intercept of the brand-specific model for brand j, u i j is the random (store) effect that is assumed to be normally distributed with zero mean, and constant variance σ 2 u , β j , and ln(γ l j ) are the fixed effects of price and promotions, respectively, and i jt Marketing <ref type="bibr">Science, 2018</ref><ref type="bibr">, vol. 37, no. 1, pp. 153-171, © 2018</ref> is the observation-specific error term that is normally distributed with constant variance, τ 2 j . Note that model ( <ref type="formula">3</ref>) is brand-specific, meaning that the model is fitted separately for each brand j. For simplicity, we omit the subscript j in the rest of Section 2.3 unless otherwise indicated. A natural way to estimate the random effects model is through Bayesian modeling with conjugate priors. The Bayesian approach to generate protected (synthetic) data through a posterior predictive distribution can be traced back to <ref type="bibr" target="#b24">Rubin (1993)</ref>.</p><p>For the prior distribution of all model parameters in (3), the overall intercept term µ is assumed to follow a normal distribution with zero mean and a large constant variance K 2 so that the prior is diffuse. The variance of the random effect, σ 2 u , is assumed to be distributed according to an inverse-Gamma distribution. The fixed effects vector (β, ln γ) is assumed to be jointly distributed as multivariate normal with a mean vector of zeros and diagonal covariance matrix Σ b . In effect, we assume that each of the fixed effects, (β, ln γ), has the same prior distribution, i.e., independent normal with zero mean and variance σ 2 b . <ref type="bibr">5</ref> The variance of model error τ 2 is assumed to follow an inverse-Gamma distribution with fixed shape and scale parameters.</p><p>Formally, we have µ∼</p><formula xml:id="formula_3">N(0, K 2 ); τ 2 ∼ IG(a 0 , b 0 ); σ 2 u ∼ IG(ν 0 /2, V 0 /2); (β, ln γ) ∼ MVN(0, σ 2 b I). Among the hy- perparameters (K 2 , a 0 , b 0 , V 0 , ν 0 , σ 2 b )</formula><p>, K is set to be a large positive number; a 0 and b 0 are fixed positive numbers; and V 0 and ν 0 are functions of a single new protection parameter; we elaborate further on this in Section 2.3.2. To implement the random effects model (3), we use freely available software, an R package MCMCglmm <ref type="bibr" target="#b12">(Hadfield 2010)</ref>. Details of the specification of hyperparameters are discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Flexible Prior Distribution.</head><p>The random effects model can be interpreted as a mean model <ref type="bibr" target="#b19">(McCulloch and Searle 2001)</ref>. Thus, posterior samples of a function of the unprotected parameters, u i + β ln P it + L l 1 (ln γ l )D lit , represent unprotected "deviations" from the intercept of all observations, µ. These deviations are linear combinations of the data provider's continuous and categorical variables and the estimated coefficients (which are conditional on the original unprotected data). Since posterior samples of the linear predictor u i + β ln P it + L l 1 (ln γ l )D lit can be predictive of the identity of store i, they require protection.</p><p>To achieve data protection, the flexible prior distribution takes information away from the unprotected deviations by tuning the hyperparameters of the prior on the variance components. It scales the unprotected deviations toward no information, as a mechanism for data protection. The priors on the variablespecific fixed effects and random effects shrink their posterior estimates toward zero through an adjustable protection parameter. This is motivated by the fact that the Bayesian estimator with an informative prior is a shrinkage estimator.</p><p>To see how the protection parameter controls the protection level, we start from our prior distributions of fixed effects and the variance of the random effect. Specifically, we introduce a single protection tuning parameter κ that is defined as the inverse of the prior variance of the fixed effect (β, ln γ). That is, κ : 1/σ 2 b , where the fixed effect vector has prior distribution (β, ln γ)∼ MVN(0, σ 2 b I). This is a conjugate prior; hence we can derive the conditional posterior mean and variance of (β, ln γ) as follows:</p><formula xml:id="formula_4">A b (X T X + κτ 2 I) −1 X T (ln S − µ1 nT − Zu); B b τ 2 (X T X + κτ 2 I) −1 ,<label>(4)</label></formula><p>where, for each brand, using matrix notation, X [ln P, D 1 , . . . , D L ], ln S is an nT-dimensional response vector, X is an nT × (1 + L)-dimensional covariates matrix for brand j, u is an n-dimensional random effect vector, and Z is an nT × n-dimensional indicator matrix for store i such that Zu [u 1 , . . . , u 1 , . . . , u i , . . . , u i , . . . , u n , . . . , u n ] is a nT-dimensional vector.</p><p>We illustrate the role of κ in generating synthetic data through the posterior form (4). Note that by using (4) we can shrink the fixed-effect estimates of (β, ln γ) toward 0 by increasing the parameter κ. At the other extreme, when κ tends to zero or equivalently the prior variance σ 2 b goes to infinity, we obtain a diffuse prior, in which case (4) becomes equivalent to the ordinary least squares (OLS) estimator.</p><p>Hence the single tuning parameter κ can capture the preference of the data provider with regard to trading off data protection (privacy) versus information loss. A smaller value of κ (equivalently, a larger value of hyperparameter σ 2 b ) results in a weaker protection. A larger value of κ (equivalently, a smaller value of the hyperparameter σ 2 b ) results in a stronger protection. Hence, κ, the data privacy protection parameter, and each value of κ corresponds to a particular implicit trade-off between information loss and privacy.</p><p>The conjugate prior distribution of the variance of the random effect is an inverse-Gamma distribution</p><formula xml:id="formula_5">σ 2 u ∼ IG(ν 0 /2, V 0 /2) with mean V 0 /(ν 0 − 2) and variance 2V 2 0 /((ν 0 − 2) 2 (ν 0 − 4)). The conditional posterior of σ 2 u isσ 2 u | u∼ IG((n + ν 0 )/2, (u u + V 0 )/2).</formula><p>To incorporate the privacy protection parameter κ, we set V 0 1/(100κ) and ν 0 1,000κ, which makes the mean arbitrarily close to zero as κ increases. With this specification of hyperparameters, a larger value of κ is equivalent to stronger informative priors for (β, ln γ) and σ 2 u . Since the means of (β, ln γ) and σ 2 u are 0 and V 0 /(ν 0 − 2), respectively, a stronger informative prior shrinks the posteriors toward their respective means.</p><p>Note that one can specify different forms of V 0 and ν 0 to incorporate κ. Generally, a stronger protection corresponds to a smaller value of V 0 and a larger value of ν 0 such that the mean and variance ofσ 2 u tend to 0, and equivalently, the posterior samples of the random effect, u i , scale toward zero. The full conditionals for the other model parameters can be easily derived analytically. We present details in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Protected Data for</head><p>Release to Data User. The proposed data protection method generates protected synthetic values of ln S it for valid use by data users. These synthetic values, lnS it , are generated by sampling from the protected posterior predictive distribution, which contains the protected model parameters, (β, lnγ) andũ. To do this, we first run the Markov Chain Monte Carlo (MCMC) with a set number of iterations as a burn-in. Then, for the remaining iterations, m 1, . . . , M, all posterior samples of the protected model parameters are saved. After verifying convergence of the posterior samples of all parameters, for each iteration m and each observation it, the protected deviation,ũ i +β ln P it + L l 1 (lnγ l )D lit , is calculated. Then, a disturbance term it is sampled from a normal distribution with mean zero and varianceτ 2 , the posterior sample of residual variance. The sum of the protected deviation and the disturbance results in a single protected synthetic value. Together, for each brand j, the protected deviation, disturbance, and associated intercepts and covariates determine the protected posterior predictive distribution for each observation it</p><formula xml:id="formula_6">F p it (κ) p(lnS it | S, P, D, κ, a 0 , b 0 , K) ∫ Θ p(lnS it | Θ;lnP it , D it )× p(Θ | Θ H , S, P, D) dΘ,<label>(5)</label></formula><p>where Θ (µ, β, ln γ, u, σ 2 u , τ 2 ) is the vector of model parameters, and Θ H is the vector of hyperparameters for priors.</p><p>This process can be repeated for a desired number of protected synthetic vectors for all brands, lnS, of length n × J × T. We suggest that the data provider release only one vector of synthetic data to the data user so that it can reduce the chance of protected model parameters being subject to invalid use. For a detailed discussion, see <ref type="bibr" target="#b23">Reiter et al. (2014)</ref>, who found that multiple releases of synthetic data are more informative of the confidential data. In this regard, note that multiple releases of synthetic data for the same time period are similar to releasing all of the parameters of a model and disclosing the entire posterior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Criteria to Measure Performance of Data</head><p>Protection Method As noted, all data protection methods imply a trade-off between two criteria, i.e., identity disclosure risk and information loss. This trade-off can be analyzed using a Risk-Utility curve <ref type="bibr" target="#b9">(Duncan et al. 2004)</ref>, which represents the natural trade-off between data protection and the utility of valid use. We discuss these two criteria in detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Measures of Identification Disclosure Risk.</head><p>To evaluate the identification disclosure risk of the protected data versus the true data, we adopt a leave-one (week)-out cross validation approach. Figure <ref type="figure" target="#fig_3">3</ref> is a flow chart that describes the steps to compute measures of identification disclosure risk for various released data sets as well as true data.</p><p>Specifically, for each week k, k 1, . . ., T, a multinomial logit model ( <ref type="formula">2</ref>) is estimated using T − 1 weeks of available data A {Y it ,S it , P it , D it }, i 1, . . . , n stores, and t 1, . . . , (−k), . . . , T weeks. Y it is the true store ID of Store i in week t,S it represents the J-vector of protected sales (using the proposed method or any benchmark method), P it is the J-vector of prices, and D it is the L × J vector of promotions. Here (−k) indicates that information for week k is not used for estimating the multinomial logit model (2). The probabilities P(Ŷ ik ID i ) of the left-out kth-week store ID are then calculated for the fitted model (2) using the explanatory variables {S ik , P ik , D ik }, i 1, . . . , n. For the special case in which the identity disclosure risk of true data is evaluated, the true sales S it are used.</p><p>Note that in our particular empirical application (discussed in Section 3), for each held-out week we obtain an n × n predicted conditional probability matrix, which is calculated by plugging in values of covariates (sales, prices, and promotions) into the estimated multinomial logit model (2). In this case, the predictive model has the limitation that it does not incorporate the information that the hold-out sample contains exactly n distinct masked entities, and there are n distinct entities in the training sample. In other words, the column sum of the probability matrix, i.e., n i P(Ŷ ik ID i ), is not guaranteed to be 1. In practice, however, in the data multiple records for a brand in a given period could be from the same store depending on, for instance, how stock keeping units are aggregated. Not imposing the constraint implies that there are measurement errors associated with sales so that multiple samples of the same store for the same period may result in different sales measures. Note that frequently, in practice, the number of stores whose identity is to be predicted is very likely to be smaller than the number of stores used in the training sample. Therefore, the constraint should not be imposed in general. Despite this limitation of the predictive model in our application, it appears to be a natural first attempt in identifying store identities.</p><p>For each Store i, the predicted probability that its store ID is i is computed as the mean of the predicted  </p><formula xml:id="formula_7">, i = 1, …, n</formula><p>Obtain summary measures from distribution of LP i : MLP and ALP probability vector across held-out weeks to obtain the n-vector {P(Ŷ i ID 1 ), . . . , P(Ŷ i ID n )}</p><formula xml:id="formula_8">P(Ŷ i ID i ) 1 T T k 1 P(Ŷ ik ID i ),<label>(6)</label></formula><p>where P(Ŷ ik ID i ) is the predicted probability that Store i is Store i , i 2, . . . , n, in the held-out week k.</p><p>The proposed method uses a (pseudo) out-of-sample fit criterion to avoid overfitting and to mimic the prediction problem for the potential intruder: Synthetic sales and covariates are known, and the objective is to predict store identities. One way to do this is to use data for T − 1 weeks and predict the data for the omitted week. To avoid capitalizing on the idiosyncrasies of just one week, the method repeatedly leaves out one week at a time (k 1, . . . , T) and uses T − 1 observations to predict store IDs for week k. An alternative way is to split the data into an estimation sample (weeks 1, 2, . . . , T ) and a validation sample (weeks T + 1, . . . , T). Both hold-out methods are used in the robustness check in Section 3.5.</p><p>We define the following measure, called Loss of Protection (LP i ), for Store i:</p><formula xml:id="formula_9">LP i n n i 1 [P(Ŷ i ID i )] 2 − 1.<label>(7)</label></formula><p>In summary, LP i measures the intruder's confidence in the ability of the available data to identify Store i. LP i also has a natural lower bound of 0 for randomly guessing the identity of store i where P(Ŷ i ID 1 )</p><formula xml:id="formula_10">P(Ŷ i ID 2 ) • • • P(Ŷ i ID n ) 1/n.</formula><p>It has an upper bound of √ n − 1 if one store identification probability is 100% and each of the other probabilities is 0%. In general, a smaller value of LP i implies that the individual store is better protected. LP i thus captures the variability of store identification probabilities (or intruder confidence).</p><p>Note that for market-level data, LP i cannot be computed because there is no store information at all in the data. Therefore, we define the LP i of market-level data as 0. Our proposed LP i measure is closely related to, but distinct from, popular measures in the literature on information theory, such as Gini impurity, which is commonly used in classification trees <ref type="bibr" target="#b3">(Breiman et al. 1984)</ref>, and Entropy. <ref type="bibr">6</ref> The use of Gini impurity and Entropy in classification trees, however, is very different from use of the proposed LP i measure, although there is a strong similarity in the formulae. Gini impurity and Entropy are mainly used to measure the impurity of a node in decision trees; however, the proposed LP i statistic is a measure of loss of protection based on estimated probabilities of store identification.</p><p>As a measure of the protection level for the full set of stores, we propose using MLP, which is calculated as</p><formula xml:id="formula_11">MLP max{LP 1 , . . . , LP n }. (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>MLP is useful in measuring the minimum level of privacy across all stores; this measure is especially useful to a data provider concerned with the problems arising from the identification of any store. In addition to MLP, one can use other statistics such as average, median, and minimum LP i . For example, ALP can be used as an overall measure of the protection level for the full set of stores. The leave-one (week)-out cross validation approach we use helps the data provider to evaluate the out-ofsample predictability of store IDs based on the released protected data versus the true data. An alternative view of this process is that the data user or intruder has access to training data with protected or true sales, and the true store IDs. The data user or intruder can then use these data as a training sample to build a predictive multinomial logit model of store IDs. In the AC Nielsen context, potential sources of such training data are individual retailers, and retail chains or wholesalers who directly sell or share their own data, and/or allow store identities to be observed. <ref type="bibr">7</ref> This model can then be used to predict store identities in newly released data in which store IDs have been disguised.</p><p>To make this idea more precise, say the data user or intruder has access to historical released data (with protected or true sales) with true store identities (e.g., "these are the prices and the (synthetic) sales of Tide 147 at the Kroger on Thompson Road in Indianapolis"): A {Y it ,S it (or S it ), P it , D it }, t 1, 2, . . . , T . The data user or intruder builds a predictive multinomial logit model on A and uses the estimates to predict the store identities,Ŷ i(t T +1, ..., T) in newly released data R {S it , P it , D it }, t T + 1, . . . , T. Note that the subscript i indicates that the data user receives a hashed version of store IDs in the newly released data so that it does not know the store identities, but knows which weekly observations belong to the same store. We provide empirical results based on this type of analysis in Section 3.5. Importantly, the results from using this method are qualitatively consistent with those from the leave-one (week)-out cross validation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Measures of Information Loss Due to Data Protection.</head><p>In our discussion of information loss from data protection, our empirical analysis focuses mainly on the estimated own-price elasticities; similar ideas apply to the estimated promotion effects. Because price elasticities are a key metric in determining optimal mark-ups and profitability, and for conducting "what if" analyses, we assume that an important goal of data users is to correctly estimate these own price elasticities. The estimates from the "unprotected" (true) store-level data are taken to be the true elasticities β j . Information loss under any data protection method is measured as the Mean Absolute Percentage Deviation (MAPD) of the estimated price elasticities based on the protected data,β j , from the true</p><formula xml:id="formula_13">β j MAPD 1 J J j 1 β j − β j β j ×100%.<label>(9)</label></formula><p>Additionally, MSE is defined as the Mean Squared Error of parameter estimates from using protected data compared to the corresponding parameter estimates from using the original data</p><formula xml:id="formula_14">MSE 1 J J j 1 (β j − β j ) 2 .</formula><p>In our paper, we disregard estimation uncertainty; consequently, we assume that the original, unprotected store-level data has a MAPD and MSE of 0%. Because an important managerial use of estimated elasticities is determining optimal prices (e.g., <ref type="bibr" target="#b20">Reibstein and Gatignon 1984)</ref>, we also compute for each brand the optimal mark-up over marginal cost (MC), defined as</p><formula xml:id="formula_15">Optimal MU j % Price j − MC j MC j × 100% 1 |β j | − 1 × 100%.</formula><p>(10) Additionally, we compute the deviations from optimal profits (i.e., maximum profit using the true data) as another measure of the loss of information. For the SCAN*PRO model, which is a constant elasticity sales response model, the assumption of constant marginal cost for any brand yields the following expression for the ratio of optimal profits relative to the no protection case (the j subscript has been suppressed throughout in the expression for simplification; the derivation of this formula is shown in Appendix D):</p><formula xml:id="formula_16">Π Π Π(P) Π(P) P − C P − C P P β β + 1 β + 1 β + 1 β + 1β β β ,<label>(11)</label></formula><p>whereΠ andP are the optimal profit and optimal price, respectively, based on the estimated price elasticities from protected data, whereas Π and P are the optimal profit and optimal price, respectively, based on the price elasticities estimated using unprotected data. Note that estimates of elasticities that are of absolute magnitude smaller than 1 result in meaningless estimates of the optimal markup and the deviation from optimal profits. We point out these cases in our discussion of empirical results as indications of the lack of face validity of the estimated elasticities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Empirical Application</head><p>We apply the model (1) to AC Nielsen point-of-sale scanner data for five brand-sizes of powdered detergents from the three largest brands in the market, i.e., 72 and 147-ounce packs of Tide and Oxydol and the 72-ounce pack of Cheer. The data are weekly store-level sales, prices, and promotions in 34 stores in Sioux Falls, SD, and Springfield, MO, collected over 102 weeks. These data have also been used in <ref type="bibr" target="#b6">Christen et al. (1997)</ref>.</p><p>To compute measures of loss of protection, we conduct analysis similar to leave-one (week)-out cross validation as discussed in Section 2.4.1. We use all-but-one</p><p>Marketing <ref type="bibr">Science, 2018</ref><ref type="bibr">, vol. 37, no. 1, pp. 153-171, © 2018</ref>  week of observations of data (A {Y it ,S it , P it , D it }, i 1, . . . , n stores and t 1, . . . , (−k), . . ., T weeks) to predict the store ID for the leave-one (week)-out observation. We repeat this process for all weeks and compute all reported measures of loss of protection.</p><p>We compare the performance of the proposed method with the performances of seven benchmark data protection methods. Benchmark Method 1 is the unprotected, store-level data, where we have no information loss by definition, and the largest loss of protection. Benchmark Methods 2-6, respectively, are as follows: adding random noise, rounding, top coding, 20% swapping, and 50% swapping. Finally, Benchmark Method 7 is based on using (aggregated) market-level data, which reflects the type of data AC Nielsen offers its clients. See the definitions in Table <ref type="table" target="#tab_3">1</ref>.</p><p>For adding random noise, due to the large variance of original sales, we first bin observations into deciles based on sales, and then separately add random noise for each bin using its empirical variance. For rounding, the unprotected sales are simply rounded to the nearest hundred. For top coding, any observation in which sales is greater than the 95th percentile is truncated so that extreme values can be protected. For swapping, we chose a specified percentage of observations (20% and 50% in our analysis) at random and divided these observations into two groups at random. Then the values of sales were exchanged between these two groups. The remaining variables, i.e., store ID, prices, displays, and feature were unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Trade-Off Between Information Loss and</head><p>Loss of Protection We focus first on the loss of information with respect to estimates of the own-price elasticities of the five brandsizes as measured by MAPD, and loss of protection as measured by our proposed measure, MLP. The reason to focus on MLP (instead of ALP) is that this measure corresponds to a worst case scenario and reflects the largest potential cost to the data provider from disclosure of even one store's ID. Figure <ref type="figure" target="#fig_4">4</ref> shows the results of our proposed method as we vary κ from 0.1 to 15, as well as those for the seven benchmark methods. <ref type="bibr">8</ref> As discussed, in the proposed method, κ is a managerially determined parameter that reflects the tradeoff between the level of protection and information loss. As expected, increasing κ leads to greater information loss, and reduces the ability of the data user to accurately estimate price elasticities. In addition, it protects the data by lowering the risk of identification of store IDs. The choice of κ reflects the criterion selected by the data provider to choose the preferred trade-off between the level of protection across all stores and the implicit degree of precision in estimating elasticities that the data provider chooses to offer its clients.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> shows that there are considerable differences in the performances of the different methods using the two criteria, i.e., information loss and loss of protection. Importantly, Figure <ref type="figure" target="#fig_4">4</ref> makes it clear that the choice of a data protection strategy requires the firm to make a trade-off between these criteria. We note that while AC Nielsen's extant approach of aggregating data to the market-level is the most effective in terms of protection, it leads to a substantial loss of information with an MAPD of 43.7%. This result is consistent with the literature on aggregation bias <ref type="bibr" target="#b6">(Christen et al. 1997)</ref> which reports large biases in parameter estimates due to aggregation.</p><p>Note that none of the benchmark methods dominates (i.e., lies to the southwest of) the proposed method at any level of κ. By using the proposed method, the data provider has the choice of giving up protection to provide more information. For instance, a data provider who faces strong competition from rival data providers who promise clients higher data quality may decide to pursue that option by choosing smaller values of κ.</p><p>We see from Figure <ref type="figure" target="#fig_4">4</ref> that random noise, top coding, and rounding offer the same levels of protection as the original store-level data, but lead to a greater loss of information. Thus, given our data, it would not be prudent for the data provider to use these methods. Although 50% swapping and 20% swapping provide greater protection than store-level data, they imply a considerable loss of information. Nevertheless, both methods dominate providing market-level data and hence are reasonable options for the data provider to consider. Our proposed method allows the decision maker the flexibility through the choice of κ to choose a data protection strategy that dominates 20% swapping and 50% swapping. For illustrative purposes, the results shown henceforth for the proposed method assume κ 1. As an illustration, we show in Figure <ref type="figure" target="#fig_5">5</ref> the average predicted probabilities from the estimated multinomial logit model where the observed prices, promotions, and sales come from each of the 34 stores. The probabilities are shown for the true sales data and synthetic sales data (generated using the proposed method with κ 1) and are based on Equation ( <ref type="formula" target="#formula_8">6</ref>). Note that the data, in fact, come from Store 12. The figure shows that the true data give the intruder relatively high confidence (average predicted probability is about 25% and the largest among the 34 probabilities) that the released data are from Store 12. By contrast, the synthetic data give the intruder much lower confidence (average predicted probability is about 5%) that the released data are from Store 12. Note that 5% is close to the outcome from random guessing, which has a corresponding identification probability of 1/34 (2.9%). From a managerial perspective, this drastic change in intruder confidence about the discoverability of store ID (25% to 5%) could imply the difference between the intruder taking an undesirable action (from the data provider's perspective) or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Price Elasticities and Implied Optimal</head><p>Markups and Profits Table <ref type="table" target="#tab_4">2</ref> reports the profit-maximizing percentage markups over marginal cost based on the estimated price elasticities from the proposed and benchmark methods, for each of the five brand-sizes. If the estimated price elasticity is smaller than one in absolute value, the optimal mark-up is "not meaningful," which we indicate as NM in the table. Taking the optimal mark-ups in the "Unprotected (True)" row to be the true mark-ups, we find that the extent of deviation from the true mark-ups for the other methods roughly corresponds with the loss of information indicated by the MAPD in Figure <ref type="figure" target="#fig_4">4</ref>. However, we see some systematic deviations.</p><p>Rounding and random noise lead to small deviations as expected based on their close-to-zero MAPDs. We find that Top Coding, 20% Swapping, 50% Swapping, and Market-level data each have at least one instance of "not meaningful" mark-ups, with 50% Swapping leading to NM results for all five brand-sizes. Such results would lead data users to question the validity of the protection method. Furthermore, Top Coding and 20% Swapping lead to larger-than-true optimal mark-ups in all cases when the results are meaningful. By contrast, market-level data lead to smaller-than-true optimal mark-ups for the four brand-sizes for which results are meaningful. This is consistent with past literature (e.g., <ref type="bibr" target="#b6">Christen et al. 1997</ref>) which shows that marketlevel data often overestimate the magnitude of the own price elasticity.</p><p>The mark-up results for the proposed method are reasonable ranging from the worst case of Tide 147 where the estimated mark-up is 65% of the true value in the first row of Table <ref type="table" target="#tab_4">2</ref>, to the best case of Oxydol 147 with a mark-up of 108% of the true value. For all brands, the estimated mark-ups are closer to the true markups than those implied by market-level data.</p><p>Table <ref type="table" target="#tab_5">3</ref> shows the ratios of optimal profits computed under each data protection method relative to optimal profits under the unprotected scenario. Consistent with the results on optimal mark-ups, we see that rounding and random noise lead to close to optimal profits for all five brand-sizes. In cases where the optimal mark-up shown in Table <ref type="table" target="#tab_4">2</ref> is not meaningful (NM), the ratios of optimal profits cannot be computed and are shown as not available (NA) in Table <ref type="table" target="#tab_5">3</ref>. Disregarding those cases, the ratios under top coding are close to 100% with the exception of one brand (Tide 147) where the ratio is about 51%. Under 20% swapping we find poor results for four of five brands, and under marketlevel data we find poor results for three of five brands.</p><p>Marketing <ref type="bibr">Science, 2018</ref><ref type="bibr">Science, , vol. 37, no. 1, pp. 153-171, © 2018</ref>   Under the proposed method we find good results for four of five brands; the worst case brand is Cheer 147 with a ratio of about 96%. Note that in the current empirical application (a constant elasticity demand function with constant marginal costs), the profit function for many of the brands appears to be quite flat near the maximum, suggesting that the cost to the user of imprecision in elasticities is relatively small. This finding may not hold in more complex models.</p><note type="other">INFORMS</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with Market-Level Data</head><p>In Table <ref type="table" target="#tab_6">4</ref> we compare the estimated price and promotion effects for AC Nielsen's extant method of data protection, i.e., market-level data, with the corresponding estimates for the proposed data protection method. We find that the absolute averages of the relative differences for each of price, display only, feature only, and display and feature effects are substantially, and in some cases dramatically, smaller for the proposed method than the corresponding effects computed using market-level data. For the promotion effects in particular, some of the deviations of marketlevel estimates are unreasonably large, similar to the results of <ref type="bibr" target="#b6">Christen et al. (1997)</ref>. See, for instance, the estimates of the effects of display only and feature only for the two sizes of Oxydol, and the estimate of feature and display for Cheer 147. These results suggest that our proposed method can relatively easily dominate the extant approach of aggregating data to the market level in terms of information if the data provider b Absolute average is defined as the average of absolute value of relative difference.</p><p>c Absolute averages for Feature and Display do not include brandsize Cheer 147 because of the unreasonably large estimated effect for market-level data.</p><p>is willing to tolerate a somewhat higher level of risk of disclosure of store identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Impact of Kappa</head><p>In Figure <ref type="figure" target="#fig_6">6</ref> we show the values of model parameters as the data protection parameter κ changes. We find that all parameters tend toward zero as κ increases, further demonstrating the trade-off between data protection and information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Robustness of Findings</head><p>We conducted several additional analyses to assess the robustness of our findings and report the results in Table <ref type="table" target="#tab_7">5</ref>. First, we report the results for ALP as an alternative to MLP. ALP is an overall average measure of the store identification risk in a given data set, whereas MLP is a worst case scenario across all stores in a data set. Second, we use an alternative measure of information loss in addition to Mean Absolute Percentage Deviation: MSE. We compute these measures for price elasticities (betas), promotion effects (gammas), and for both. In all cases we find that the performance of the proposed method relative to any of the benchmark methods remains substantially unchanged from that shown in Figure <ref type="figure" target="#fig_4">4</ref> and Table <ref type="table" target="#tab_6">4</ref>. Thus, the proposed method continues to dominate the standard method of providing market-level data.</p><p>As a robustness check we also considered a situation in which the data user or intruder has access to some historical true sales data with true store identities, as discussed in Section 2.4.1 where the available training data A {Y it , S it , P it , D it }, t 1, 2, . . . , T is used to predict the store identities,Ŷ i(t T +1, ..., T) using newly released data R {S it , P it , D it }, t T + 1, . . . , T. Table <ref type="table" target="#tab_8">6</ref> gives the ALP and MLP estimates based on the proposed method and benchmark methods when half of true sales data from week 1 till week T 51 are used to estimate a multinomial logit model for store-ID prediction, and predictions of store IDs are made in the remaining weeks 52 to T 102. In addition, we conducted analyses when different proportions of data or protected data A {Y it ,S it (or S it ), P it , D it } are used to build the multinomial logit model. Overall, the results are qualitatively consistent with those from the leaveone (week)-out cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Future Research Directions</head><p>This paper proposes a synthetic data methodology that captures the roles of three parties, i.e., the data provider as a commercial supplier who protects data with a data protection method, the data user as a customer, and the potential data intruder. A key distinguishing feature of our framework relative to the privacy literature in statistics and computer science is that we explicitly recognize the business goals of the data user as reflected in the data user's model, and incorporate these into the data provider's model for protecting data. We propose a flexible Bayesian methodology in which the decision maker uses a tuning parameter (κ) to analyze the trade-off between the conflicting goals of profitability and risk of data disclosure (confidentiality). We measure information loss using the MAPD criterion. In addition, we propose two new metrics to measure the risk of data disclosure, i.e., ALP and MLP. We test the proposed methodology using retail point-of-sale data marketed by a vendor to its commercial customers. The vendor sells data but seeks <ref type="bibr">Marketing Science, 2018</ref><ref type="bibr">, vol. 37, no. 1, pp. 153-171, © 2018</ref>   a For unprotected, the metrics for information loss are 0 by definition. b For market-level data, we assume that the predicted probabilities for each store ID are equal; that is, 1/n for n 34 stores. Therefore, by the definition of the loss of protection metrics, we have MLP ALP 0. to protect the identities of sample stores from potential intruders (confidentiality). By contrast, commercial customers use the data to estimate brand-level price elasticities to determine optimal mark-ups, and the sales effects of promotions. We show that by enabling the data provider to choose the degree of protection to infuse into the synthetic data, our method performs well relative to seven benchmark data protection methods, including the extant approach of aggregating data across stores (e.g., AC Nielsen).</p><note type="other">INFORMS</note><p>An important limitation of the proposed identification disclosure risk model in the empirical application reported in this paper is that the estimated probabilities of an observation belonging to stores in a given time period do not sum to 100%. Development of estimation approaches that can incorporate this constraint when needed is an important area for future research. Furthermore, it is important to recognize that our framework and approach are most relevant when it is possible for a data provider to identify a primary valid use model of data users. In our application, we used the SCAN*PRO model that is widely used by users of AC Nielsen retail data. In such situations our proposed method allows the data provider to protect the data taking account of its data users' business goals. In other situations where the data user's primary purpose is to use the data to conduct exploratory analysis, implying that data users' models are not well structured, or that data users have very different models, our framework is not as directly applicable. An example of exploratory analysis is examining the distribution of sales volumes across stores for the purpose of creating retail segments. Although our framework was not specifically geared toward such analysis, we found (results are not shown in this paper) that using synthetic store-level data produced similar results to using the true store-level data. Note that such retail segmentation analysis cannot be performed using the market-level data currently released by vendors such as AC Nielsen. Additionally, a data user may be interested in knowing the precision of the synthetic data relative to the true data. When the measures of precision are for market-level statistics such as brand sales or brand market shares, we do not expect such additional information to change the level of protection. However, if data users desire measures of precision that may reveal additional store-level information, such as information about precision of ranks of stores based on sales volumes, the level of protection will be reduced, regardless of method. We conjecture that the relative rankings of different data protection methods will be unchanged. We leave a detailed investigation of this issue to future research.</p><p>We believe several extensions and generalizations of the models presented in this paper should be of interest to academics and practitioners alike. We discuss some of these possibilities next. In this paper, we considered a single random effect in the data provider's model. Generalization of the data provider's model to more than one group of random effects, or variable-specific effects, should be of interest when the data provider would like to choose different levels of data protection for different market segments (subgroups of data). For example, in the context we have modeled, one group of store IDs (e.g., large stores) may be highly confidential and require high levels of protection, whereas another group of store IDs (e.g., small stores) may require lower levels of protection. Our data protection methodology readily extends to more general cases wherein random effects are hierarchical or it is necessary to distinguish M groups, each with its own variance. By using the hierarchical framework of the Bayesian random effects model, our methodology will allow the data provider to choose which groups of effects or segments require more protection.</p><p>Additionally, a data user may be interested in other marketing mix models that include competition or more general interactions among marketing mix elements. One weakness of the current framework is that the synthetic data only contain information about the variables that are included in the data generating process in the data provider's model. An adjustment of the data provider's model is certainly possible to accommodate other variables. In this regard note that <ref type="bibr" target="#b25">Schneider and Abowd (2015)</ref> found that a much stronger prior was needed to achieve the same privacy levels in a model with three-way interactions, although the fit of the resulting model on the protected data was similar to that of a model with no interactions. Our findings in this paper are similar in that there is an inherent trade-off between data protection and commercial value, but we leave the investigation of more complex marketing mix models to future research.</p><p>In the current paper we assumed that only the sales data needed to be protected, whereas data on the covariates, prices and promotions, could be released without protection since they were much less informative about the confidential data. Our recommendation is that this approach is most suitable for stores that belong to one chain with a uniform pricing and promotion strategy. If in fact the covariates are informative and not publicly available, one would want to generate "triply synthetic" data for multiple variables, such as sales, prices, and promotions. This would result in multiple conditional models, with the added challenge that the collection of conditional distributions may not result in a proper joint distribution <ref type="bibr" target="#b22">(Reiter 2009)</ref>. We leave the investigation of this problem to future research. <ref type="bibr">9</ref> In our application we used continuous sales data, hence a log-linear model with additive Gaussian errors was appropriate. Marketing data, however, are often categorical in nature. A prototypical example is consumer brand choice data gathered from household panels. The appropriate statistical models for such data are multinomial logit and probit models. When the data user's model is a generalized linear model, the data provider's base model (3) can be extended to a generalized linear mixed effects model (GLMM) g(E(ln Y i jt )) µ j +u i j +β j X i jt , where g( • ) is a link function, such as the logistic link or probit link and E( • ) denotes the conditional expectation. In terms of estimation, the MCMCglmm R package used in this paper can also be used for categorical dependent variables <ref type="bibr" target="#b12">(Hadfield 2010)</ref>.</p><p>Even though the analytical results such as full conditionals we have presented in Appendix A are no longer available for non-Gaussian GLMM, the proposed Bayesian MCMC framework remains valid; however, such cases will require more intensive computation. We can use a similar algorithm as in Section 2 and draw protected (synthetic) data from the appropriate nonnormal conditional distributions. For example, for the logistic link g(E(ln y i jt )) ln((E(ln y i jt ))/(1−E(ln y i jt ))) with binary choice response, the protected (synthetic) response can be drawn from Bernoulli trials with mean probability g −1 (µ j +u i j +β j X i jt ). The empirical performance of such data protection methods should be of great interest to marketing practitioners and academics.</p><p>DenoteP as the optimal price based on the estimated price elasticityβ, which is obtained from protected data. Substituting (D.3) into (D.1), by simple algebra, we see that the ratio Π(P)/Π(P) has the following form: Π Π Π(P) Π(P)</p><formula xml:id="formula_17">P − C P − C P P β β + 1 β + 1 β + 1 β + 1β β β .</formula><p>Endnotes 1 This concern is similar to the one faced by the New York Times Best Sellers List of books, which is based on a survey of a closely guarded set of retail booksellers. Despite the secrecy, several cases have been reported in the media of authors or their agents making "strategic purchases" of books at retail stores to artificially boost their own rankings. <ref type="bibr">2</ref> We omit competitive price and promotion effects to maintain parsimony of specification for this application. Inclusion of competitive effects would require four additional parameters per competing product in the model for each brand. As we discuss in Section 3 (see Table <ref type="table" target="#tab_6">4</ref>), model fit does not suffer much due to this omission since the average (across brands) adjusted R 2 of fitted models exceeds 0.95.</p><p>3 Note that the data used in this multinomial logit model are a different configuration of the same data that are used in the data user's model ( <ref type="formula">1</ref>), plus store identities. The data set has nT observations. The response variable Y it is the ID of Store i in week t, and the predictors are ln(prices), promotions, and ln(sales) of all brands in store i in week t. Thus, we have 5 × J predictors in this model. <ref type="bibr">4</ref> Previous research <ref type="bibr" target="#b2">(Bleninger et al. 2011</ref>) has shown that a data intruder can strategically uncover sensitive data (e.g., sales quantities of specific observations) when the data protection method is to simply add noise. 5 When the covariance matrix Σ b takes a general form, it is not immediately obvious how to incorporate the protection parameter even though the full conditionals can still be analytically derived. We leave this extension as a future research opportunity. <ref type="bibr">6</ref> In our particular case, Gini impurity for store i can be written as Gini i 1 − n i P(Ŷ i ID i ) 2 . It is easy to see the link between LP and Gini impurity: LP i n n i 1 P(Ŷ i ID i ) 2 − 1 n(1−Gini i ) − 1. Entropy for Store i is defined as Entropy i − n i 1 [P(Ŷ i ID i ) • log 2 P(Ŷ i ID i )].</p><p>7 Some examples of retailers' data sharing programs include Retail Link (Walmart), Partners Online (Target), Workbench (Sears), and Vendor Dart (Lowe's). The primary goals of such programs are to facilitate better management of shipments, inventory, out-of-stocks, and forecasts, often at the store level. Note that these data are typically not a substitute for retail data provided by syndicated data providers such as AC Nielsen, which are based on careful samplings of stores and hence provide the benefits of projecting sales volumes, market shares, prices, and promotional activities to regional and national markets. <ref type="bibr">8</ref> We thank an anonymous reviewer for useful suggestions on the presentation of Figure <ref type="figure" target="#fig_4">4</ref> and its interpretation. 9 Across all protection methods, results are qualitatively similar to those presented in the paper when we used protected sales and added a normally distributed random noise to protect the price data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Schneider et al.: A Flexible Method for Protecting Marketing Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (Color online) Marketing Data Privacy Ecosystem for Point-of-Sale Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (Color online) Data Provider's Process for Generating Synthetic Data for Release to the Data User</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (Color online) Leave-One (Week)-Out Cross Validation Process for True Data and Various Released (Protected) Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (Color online) Performance of Alternative Data Protection Methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (Color online) Average Predicted Probabilities That Observed Point-of-Sale Data from Store 12 Came from Each of the 34 Stores</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (Color online) Shrinkage Plots of Fixed and Random Effects as Protection Increases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MarketingScience, 2018, vol. 37, no. 1, pp. 153-171, © 2018 </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Definition of Benchmark Protection Methods</figDesc><table><row><cell>Benchmark method</cell><cell>Description</cell></row><row><cell cols="2">1 "True" or Unprotected Original store-level sales data</cell></row><row><cell>store-level data</cell><cell>without any protection</cell></row><row><cell>2 Random noise</cell><cell>Observations are binned into deciles</cell></row><row><cell></cell><cell>based on sales, and random noise</cell></row><row><cell></cell><cell>is added to the sales in each decile</cell></row><row><cell>3 Rounding</cell><cell>Sales are rounded to the nearest</cell></row><row><cell></cell><cell>hundred</cell></row><row><cell>4 Top coding</cell><cell>Sales greater than the 95th percentile</cell></row><row><cell></cell><cell>are truncated</cell></row><row><cell>5 20% swapping</cell><cell>20% of observations are divided into</cell></row><row><cell></cell><cell>two groups and their sales data are</cell></row><row><cell></cell><cell>exchanged</cell></row><row><cell>6 50% swapping</cell><cell>50% of observations are divided into</cell></row><row><cell></cell><cell>two groups and their sales data are</cell></row><row><cell></cell><cell>exchanged</cell></row><row><cell>7 Market-level</cell><cell>For each week, sales are summed</cell></row><row><cell></cell><cell>and prices and promotions are</cell></row><row><cell></cell><cell>averaged across stores to the</cell></row><row><cell></cell><cell>market level</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Optimal Mark-Up Percentages Implied by Estimated Price Elasticities</figDesc><table><row><cell></cell><cell>Tide</cell><cell>Tide</cell><cell>Cheer</cell><cell>Oxydol</cell><cell>Oxydol</cell></row><row><cell></cell><cell>72</cell><cell>147</cell><cell>147</cell><cell>72</cell><cell>147</cell></row><row><cell>Unprotected (true)</cell><cell>144.0</cell><cell>267.9</cell><cell>168.9</cell><cell>186.7</cell><cell>214.8</cell></row><row><cell>Random noise</cell><cell>128.3</cell><cell>121.3</cell><cell>176.7</cell><cell>153.1</cell><cell>225.5</cell></row><row><cell>Rounding</cell><cell>137.5</cell><cell>237.9</cell><cell>133.9</cell><cell>186.5</cell><cell>172.6</cell></row><row><cell>Top coding</cell><cell>183.9</cell><cell>NM</cell><cell>193.8</cell><cell>213.0</cell><cell>234.5</cell></row><row><cell>20% swapping</cell><cell>405.3</cell><cell>NM</cell><cell>272.0</cell><cell>478.4</cell><cell>491.4</cell></row><row><cell>50% swapping</cell><cell>NM</cell><cell>NM</cell><cell>NM</cell><cell>NM</cell><cell>NM</cell></row><row><cell>Market-level</cell><cell>115.1</cell><cell>77.9</cell><cell>74.8</cell><cell>NM</cell><cell>153.8</cell></row><row><cell>Proposed method</cell><cell>120.3</cell><cell>175.5</cell><cell>113.9</cell><cell>117.6</cell><cell>232.0</cell></row><row><cell>(κ 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Note. NM, Not meaningful.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Ratio of Optimal Profits Relative to Unprotected Case</figDesc><table><row><cell></cell><cell>Tide</cell><cell>Tide</cell><cell cols="3">Cheer Oxydol Oxydol</cell></row><row><cell></cell><cell cols="5">72 (%) 147 (%) 147 (%) 72 (%) 147 (%)</cell></row><row><cell cols="3">Unprotected (true) 100.00 100.00</cell><cell>100.00</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>Random noise</cell><cell>99.94</cell><cell>99.13</cell><cell>99.19</cell><cell>99.44</cell><cell>99.98</cell></row><row><cell>Rounding</cell><cell>99.96</cell><cell>99.80</cell><cell>98.99</cell><cell>100.00</cell><cell>99.23</cell></row><row><cell>Top coding</cell><cell>98.80</cell><cell>50.87</cell><cell>99.65</cell><cell>99.70</cell><cell>99.88</cell></row><row><cell>20% swapping</cell><cell>81.98</cell><cell>NA</cell><cell>96.08</cell><cell>87.19</cell><cell>90.78</cell></row><row><cell>50% swapping</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>Market-level</cell><cell>98.97</cell><cell>78.87</cell><cell>87.91</cell><cell>NA</cell><cell>98.17</cell></row><row><cell>Proposed method</cell><cell>99.59</cell><cell>98.90</cell><cell>95.88</cell><cell>99.87</cell><cell>99.51</cell></row><row><cell>(κ 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Note. NA, Not available.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Estimates of Price and Promotion Effects: Comparison of Results from Market-Level Data and Proposed Method</figDesc><table><row><cell></cell><cell cols="3">Coefficient estimates</cell><cell cols="2">Relative difference a</cell></row><row><cell></cell><cell cols="5">Store-Market-Proposed Market-Proposed</cell></row><row><cell></cell><cell>level</cell><cell>level</cell><cell>(κ 1)</cell><cell cols="2">level (%) (κ 1) (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Price</cell><cell></cell><cell></cell></row><row><cell>Tide 72</cell><cell>−1.69</cell><cell>−1.87</cell><cell>−1.80</cell><cell>10.3</cell><cell>6.5</cell></row><row><cell>Tide 147</cell><cell>−1.37</cell><cell>−2.28</cell><cell>−1.42</cell><cell>66.3</cell><cell>3.9</cell></row><row><cell>Cheer 147</cell><cell>−1.59</cell><cell>−2.34</cell><cell>−1.95</cell><cell>46.8</cell><cell>22.4</cell></row><row><cell>Oxydol 72</cell><cell>−1.54</cell><cell>−0.27</cell><cell>−1.59</cell><cell>−82.4</cell><cell>3.5</cell></row><row><cell cols="2">Oxydol 147 −1.47</cell><cell>−1.65</cell><cell>−1.55</cell><cell>12.6</cell><cell>5.7</cell></row><row><cell>Absolute</cell><cell></cell><cell></cell><cell></cell><cell>43.7</cell><cell>8.4</cell></row><row><cell>average b</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Feature only</cell><cell></cell></row><row><cell>Tide 72</cell><cell>2.56</cell><cell>5.68</cell><cell>2.63</cell><cell>121.9</cell><cell>2.8</cell></row><row><cell>Tide 147</cell><cell>2.41</cell><cell>3.52</cell><cell>2.11</cell><cell>46.0</cell><cell>−12.4</cell></row><row><cell>Cheer 147</cell><cell>10.75</cell><cell>9.40</cell><cell>9.69</cell><cell>−12.6</cell><cell>−9.9</cell></row><row><cell>Oxydol 72</cell><cell>4.91</cell><cell>34.78</cell><cell>5.49</cell><cell>609.1</cell><cell>11.7</cell></row><row><cell>Oxydol 147</cell><cell>4.47</cell><cell>36.33</cell><cell>4.04</cell><cell>712.7</cell><cell>−9.7</cell></row><row><cell>Absolute</cell><cell></cell><cell></cell><cell></cell><cell>300.5</cell><cell>9.3</cell></row><row><cell>average b</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Display only</cell><cell></cell></row><row><cell>Tide 72</cell><cell>2.61</cell><cell>20.59</cell><cell>2.88</cell><cell>688.9</cell><cell>10.5</cell></row><row><cell>Tide 147</cell><cell>2.44</cell><cell>13.09</cell><cell>2.21</cell><cell>436.5</cell><cell>−9.2</cell></row><row><cell>Cheer 147</cell><cell>5.83</cell><cell>14.34</cell><cell>5.68</cell><cell>146.0</cell><cell>−2.6</cell></row><row><cell>Oxydol 72</cell><cell>3.48</cell><cell>23.08</cell><cell>3.38</cell><cell>562.9</cell><cell>−2.8</cell></row><row><cell>Oxydol 147</cell><cell>5.00</cell><cell>121.25</cell><cell>5.65</cell><cell>2,325.4</cell><cell>13.1</cell></row><row><cell>Absolute</cell><cell></cell><cell></cell><cell></cell><cell>831.9</cell><cell>7.6</cell></row><row><cell>average b</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Feature and display</cell><cell></cell></row><row><cell>Tide 72</cell><cell>4.51</cell><cell>0.97</cell><cell>3.93</cell><cell>−78.4</cell><cell>−12.9</cell></row><row><cell>Tide 147</cell><cell>5.74</cell><cell>3.22</cell><cell>6.58</cell><cell>−44.0</cell><cell>14.7</cell></row><row><cell>Cheer 147</cell><cell cols="2">14.94 3.29E + 13</cell><cell>9.57</cell><cell></cell><cell>−36.0</cell></row><row><cell>Oxydol 72</cell><cell>6.10</cell><cell>0.18</cell><cell>5.29</cell><cell>−97.0</cell><cell>−13.3</cell></row><row><cell>Oxydol 147</cell><cell>6.16</cell><cell>0.00</cell><cell>7.63</cell><cell>−99.9</cell><cell>23.9</cell></row><row><cell>Absolute</cell><cell></cell><cell></cell><cell></cell><cell>79.9 c</cell><cell>16.2 c</cell></row><row><cell>average b</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adjusted R 2</cell><cell>0.958</cell><cell>0.522</cell><cell>0.957</cell><cell></cell><cell></cell></row><row><cell>(avg.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">a Relative difference (Estimate − Store-level estimate)/Store-level</cell></row><row><cell>estimate.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Robustness Check Using Different Measures</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Information loss</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Loss of protection</cell><cell>MAPD</cell><cell>MSE</cell><cell>MAPD</cell><cell>MSE</cell><cell>MAPD</cell><cell>MSE</cell></row><row><cell></cell><cell>MLP</cell><cell>ALP</cell><cell>beta</cell><cell>beta</cell><cell>gamma</cell><cell>gamma</cell><cell>both</cell><cell>both</cell></row><row><cell>Unprotected (true) a</cell><cell>2.250</cell><cell>0.796</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Random noise</cell><cell>2.269</cell><cell>0.797</cell><cell>0.053</cell><cell>0.008</cell><cell>0.026</cell><cell>0.003</cell><cell>0.032</cell><cell>0.005</cell></row><row><cell>Rounding</cell><cell>2.260</cell><cell>0.795</cell><cell>0.046</cell><cell>0.008</cell><cell>0.008</cell><cell>0.000</cell><cell>0.017</cell><cell>0.002</cell></row><row><cell>Top coding</cell><cell>2.277</cell><cell>0.787</cell><cell>0.093</cell><cell>0.032</cell><cell>0.303</cell><cell>0.484</cell><cell>0.250</cell><cell>0.371</cell></row><row><cell>20% swapping</cell><cell>1.471</cell><cell>0.425</cell><cell>0.243</cell><cell>0.141</cell><cell>0.266</cell><cell>0.261</cell><cell>0.260</cell><cell>0.231</cell></row><row><cell>50% swapping</cell><cell>1.025</cell><cell>0.180</cell><cell>0.445</cell><cell>0.498</cell><cell>0.437</cell><cell>0.487</cell><cell>0.439</cell><cell>0.490</cell></row><row><cell>Market-level b</cell><cell>0</cell><cell>0</cell><cell>0.437</cell><cell>0.610</cell><cell>1.995</cell><cell>60.630</cell><cell>1.606</cell><cell>45.625</cell></row><row><cell>Proposed method</cell><cell>1.566</cell><cell>0.478</cell><cell>0.084</cell><cell>0.026</cell><cell>0.115</cell><cell>0.130</cell><cell>0.108</cell><cell>0.104</cell></row><row><cell>(κ 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Robustness Analysis for the Scenario When the Intruder Has True Historical Sales Data and True Store IDs</figDesc><table><row><cell>Protection method</cell><cell>ALP</cell><cell>MLP</cell></row><row><cell>Unprotected (true)</cell><cell>0.773</cell><cell>1.649</cell></row><row><cell>Random noise</cell><cell>0.754</cell><cell>1.641</cell></row><row><cell>Rounding</cell><cell>0.776</cell><cell>1.628</cell></row><row><cell>Top coding</cell><cell>0.766</cell><cell>1.649</cell></row><row><cell>20% swapping</cell><cell>0.556</cell><cell>1.058</cell></row><row><cell>50% swapping</cell><cell>0.412</cell><cell>0.994</cell></row><row><cell>Proposed</cell><cell>0.419</cell><cell>1.143</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to gratefully acknowledge the input of the senior editor, former senior editor Fred Feinberg, an anonymous associate editor, and three anonymous reviewers. This work was completed while Shaobo Li was a Ph.D. student at the University of Cincinnati.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Marketing <ref type="bibr">Science, 2018</ref><ref type="bibr">, vol. 37, no. 1, pp. 153-171, © 2018</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Key Theoretical Results and Algorithm for Data Protection Method Appendix A. Full Conditionals of Other Model Parameters</head><p>The full conditionals for other model parameters can be analytically derived as shown below</p><p>where</p><p>Using matrix notation, ln(S) is an nT dimensional response vector, X [ln PD 1 , . . . , D L ] is an nT × (L + 1) matrix, u is an n-dimensional random effect vector, Z is an nT × n dimensional indicator matrix such that Zu [u 1 , . . . , u 1 , . . . , u i , . . . , u i , . . . , u n , . . . , u n ] is an nT dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Algorithm for Proposed Data Protection Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Estimation Procedure (Based on the MCMCglmm</head><p>Package in R). Given the conjugate prior of overall intercept µ, fixed effect β and random effect u, and the variance of error term τ 2 and variance of random effect σ 2 u , we can derive the full conditional distribution for each model parameter.</p><p>1. MCMC procedure by Gibbs sampling: Based on the full conditional distributions, the model parameters can be sampled for thousands of iterations. In particular 1.1. Start from a set of initial values <ref type="bibr">1)</ref> , τ 2 (1) . 1.2. Given kth draw of parameters:</p><p>u , make the (k + 1)th draw based on the full conditional distributions.</p><p>2. Burn-in a certain number of samples from the beginning, and use the remaining samples for Bayesian estimation and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Generating Procedure.</head><p>1. Take a draw of all parameters from the MCMC samples. Then draw the response based on its conditional distribution</p><p>Step 1 generates a column of synthetic responses, which is called protected data. To generate another column of synthetic response, we take another draw of parameters, and use the same procedure.</p><p>Note that in general Bayesian estimation and inference we need to average the MCMC draws of parameters. The mean values are treated as estimated parameters. However, in a data protection framework, we only take one draw of parameters as estimates instead of averaging all MCMC draws. The reason is that averaged values contain much more information than one draw; the result is that the generated values are close to the true values. Consequently, averaging may result in worse protection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Analysis of Key Variables to Protect</head><p>We analyze different variables and their combinations to identify key variables to protect. A natural way for intruders to predict the store ID is via a multinomial logistic regression modeling approach using a training data set at hand with variables such as sales, price, and promotion, and their combinations.</p><p>Table <ref type="table">C</ref>.1 shows the overall average, median, and maximum loss of protection (LP). The ALP with Sales-only is 0.511 compared with 0.062 with Price-only, 0.015 with Promo-only, and 0.104 with Price + Promo combinations. A similar qualitative finding holds for median and maximum LP measures. This shows that sales has the strongest predictive power of store ID; hence sales may be the most important variable to protect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Derivation of Formula for Deviation from Optimal Profit</head><p>Let Π be the profit, C be the marginal cost, P be the price, and S be the sales. Then we have</p><p>By substituting SCAN*PRO model (1) for each brand, we have Π(P) (P − C)αP β γ F . (D.1)</p><p>Here we drop the subscripts for simplicity. It is easy to see that Π(P) is a concave function of P. By taking the first-order derivative of (D.1) with respect to P, and setting to 0, we have</p><p>We solve Equation (D.2) for P, and find the optimal price P as</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aggregation bias in sponsored search data: The curse and the cure</title>
		<author>
			<persName><forename type="first">V</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hosanagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="77" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Differential privacy applications to Bayesian and linear mixed model estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Privacy Confidentiality</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="105" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Remote data access and the risk of disclosure from linear regression: An empirical study</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bleninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Drechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ronning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Oper. Res. Trans. (Special Issue: PSD</title>
		<imprint>
			<biblScope unit="page" from="7" to="24" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Chapman and Hall/CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Commercial use of UPC scanner data: Industry and academic perspectives</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bucklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="273" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How can we analyze differentially-private synthetic data sets?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Charest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Privacy Confidentiality</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="33" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using market-level data to understand promotion effects in a nonlinear model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Staelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="334" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hide and seek: Costly consumer privacy in a market with repeat purchases</title>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wagman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="271" to="292" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing social desirability bias through item randomized response: An application to measure underreported desires</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-P</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="14" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Disclosure risk vs. data utility: The RU confidentiality map</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Keller-Mcnulty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Stokes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chance</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="20" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Privacy regulation and online advertising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supply-chain partnership between P&amp;G and</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integrated Series Inform. Systems</title>
		<editor>Wal-Mart. Shaw MJ, ed. E-Business Management</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="155" to="171" />
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MCMC methods for multi-response generalised linear mixed models: The MCMCglmm R package</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hadfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Statist. Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Disclosure risk evaluation for fully synthetic categorical data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Databases</title>
		<title level="s">Lecture Notes Comput. Sci.</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Domingo-Ferrer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8744</biblScope>
			<biblScope unit="page" from="185" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Building Models for Marketing Decisions</title>
		<author>
			<persName><forename type="first">Psh</forename><surname>Leeflang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Naert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are aggregate scanner data models biased?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Link</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Advertising Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8" to="12" />
			<date type="published" when="1995-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical analysis of masked data</title>
		<author>
			<persName><forename type="first">Rja</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Official Statist</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="407" to="426" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilhuber</surname></persName>
		</author>
		<title level="m">Privacy: Theory meets practice on the map. ICDE 2008. IEEE 24th Internat. Conf. Data Engrg</title>
				<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="http://www.msi.org/uploads/articles/MSI_RP16-18.pdf" />
		<title level="m">Marketing Science Institute (2016) 2016-2018 research priorities</title>
				<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized, Linear, and Mixed Models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Searle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Series Probab. Statist</title>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal product line pricing: The influence of elasticities and cross-elasticities</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Reibstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gatignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="267" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating risks of identification disclosure in microdata</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="1103" to="1112" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple imputation for disclosure limitation: Future research challenges</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Privacy Confidentiality</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="233" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian estimation of disclosure risks for multiply imputed, synthetic data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Privacy Confidentiality</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="33" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discussion: Statistical disclosure limitation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Official Statist</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="462" to="468" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new method for protecting interrelated time series with Bayesian prior distributions and synthetic data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Abowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. A Statist. Soc</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="963" to="975" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Massively categorical variables: Revealing the information in zip codes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Steenburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ainslie</forename><forename type="middle">A</forename><surname>Engebretson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="57" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Avoiding aggregation bias in demand estimation: A multivariate promotional disaggregation approach. Quant. Marketing Econom</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tenn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="383" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How promotions work: SCAN*PRO-based evolutionary model building</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Heerde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Psh</forename><surname>Leeflang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Schmalenbach Bus. Rev</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="198" to="220" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
