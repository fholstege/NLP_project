<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Text Analysis Using Sentence Conjunctions and Punctuation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-07">July 7, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joachim</forename><surname>Büschken</surname></persName>
							<email>joachim.bueschken@ku.de</email>
							<affiliation key="aff0">
								<orgName type="department">School of Management</orgName>
								<orgName type="institution">Catholic University of Eichstätt-Ingolstadt</orgName>
								<address>
									<postCode>85049</postCode>
									<settlement>Ingolstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
							<email>allenby.1@osu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Fisher College of Business</orgName>
								<orgName type="institution" key="instit2">Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>Ohio</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Text Analysis Using Sentence Conjunctions and Punctuation</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2020-07-07">July 7, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2019.1214</idno>
					<note type="submission">Received: January 31, 2017 Revised: April 4, 2018; January 30, 2019; July 16, 2019 Accepted: August 22, 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>user-generated content</term>
					<term>latent Dirichlet allocation (LDA)</term>
					<term>topic dependency</term>
					<term>syntactic covariates</term>
					<term>Bayesian analysis</term>
					<term>customer satisfaction analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>User-generated content in the form of customer reviews, blogs, and tweets is an emerging and rich source of data for marketers. Topic models have been successfully applied to such data, demonstrating that empirical text analysis benefits greatly from a latent variable approach that summarizes high-level interactions among words. We propose a new topic model that allows for serial dependency of topics in text. That is, topics may carry over from word to word in a document, violating the bag-of-words assumption in traditional topic models. In the proposed model, topic carryover is informed by sentence conjunctions and punctuation. Typically, such observed information is eliminated prior to analyzing text data (i.e., preprocessing) because words such as "and" and "but" do not differentiate topics. We find that these elements of grammar contain information relevant to topic changes. We examine the performance of our models using multiple data sets and establish boundary conditions for when our model leads to improved inference about customer evaluations. Implications and opportunities for future research are discussed.</p><p>History: Yuxin Chen served as the senior editor and Peter Fader served as associate editor for this article.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text data in the form of customer reviews, blogs, and tweets is a fast-growing and rich source of data for marketing researchers. Websites such as Tripadvisor.com and Yelp.com offer a growing range of products and services for which customers can post reviews. An important and fruitful area of model-based empirical text analysis is the application of latent topic models <ref type="bibr" target="#b1">(Blei et al. 2003</ref><ref type="bibr" target="#b22">, Tirunillai and Tellis 2014</ref><ref type="bibr" target="#b10">, Humphreys and Wang 2017</ref> to such data. Topic models identify sets of words that frequently co-occur, giving rise to the ability to account for high-level interaction among words. Essentially, topic models are devices to detect latent clusters of co-occurring multinomial variables such as words from a given vocabulary. The clusters emerging from these models can be used to analyze the relationship between topics and variables of interest such as purchase intention and customer satisfaction <ref type="bibr" target="#b2">(Büschken and Allenby 2016)</ref>, providing insights into consumer preferences and behavior. Topic models compete with approaches in text analysis that are based on observed features of text such as dictionarybased approaches <ref type="bibr" target="#b10">(Humphreys and Wang 2017)</ref>. The advantage of topic models lies in the detection of latent variables that provide high-level summaries of text.</p><p>A challenge in applying topic models to customer reviews is the limited amount of data contained in any one review. The number of words in a review is typically fewer than 100, making it difficult to assess topic and word probabilities without imposing additional structure. An assumption typically present in topic models is that topics exhibit zero autocorrelation in that the probability of the topic assignment to word t + 1 is independent of the topic assignment to word t. This assumption gives rise to the bag-of-words property and to word counts being sufficient statistics for the standard model. Recently, <ref type="bibr" target="#b2">Büschken and Allenby (2016)</ref> proposed a model in which topics are constrained to not change within a sentence. They show that this restriction leads to better fit to the data and more interpretable topic word probabilities in customer review data. A similar approach was proposed by <ref type="bibr" target="#b17">Nallapati and Allan (2002)</ref>, who used sentence boundaries as structural information to a unigramtype probabilistic language model. The common element of both these models is the introduction of a sentence-based constraint to the model, imposing common topics within observed boundaries of text.</p><p>In this paper, we propose a new model that allows for topic assignments to carry over to the next word.</p><p>The model is an autocorrelated topic model in which the probability of a word-to-word topic carryover is parameterized as a binary logit model with covariates, and we find that punctuation (e.g., periods, exclamation marks, and commas) and conjunctions (e.g., "and," "but," and "because") are predictive of topic carryover. These syntactic elements are frequently discarded as part of data cleaning in text analysis and have not been previously analyzed for their value in predicting topic changes within sentences and inferences about the topics themselves. An alternative approach to modeling correlation among topic assignments to observed sequences of words involves the changepoint models that, in marketing, have been applied to time-series data to discover changes in the underlying data-generating process <ref type="bibr" target="#b4">(Chib 1998</ref><ref type="bibr" target="#b5">, DeSarbo et al. 2004</ref><ref type="bibr" target="#b6">, Fader et al. 2004</ref><ref type="bibr" target="#b18">, Netzer et al. 2008</ref><ref type="bibr" target="#b8">, Gopalakrishnan et al. 2016</ref>.</p><p>A key element in our model is to relate the unobserved topic shares at the document level to the observed overall rating provided by reviewers in the way of a supervised latent Dirichlet allocation (LDA) model <ref type="bibr" target="#b14">(Mcauliffe and Blei 2007)</ref>. With this approach, the rating provides additional likelihood information to the latent topic shares, assuming a common mapping of topics to ratings across documents through a regression model. In our empirical analysis, we find this mapping to be highly discriminatory regarding the importance and valence of the topics.</p><p>We apply the proposed autocorrelated topic model to multiple data sets, and we find the autocorrelated topic model to fit the data better, so we establish boundary conditions for when it is preferred to a standard LDA model. For this analysis, we evaluate the performance of all models in terms of predictive fit, given holdout reviews, and the explained variance of customers' overall satisfaction rating. The autocorrelated topic model works best for reviews containing more words and longer sentences and in which we observe more incidents of conjunctions and punctuation, suggesting that a model of local topic dependency works better for linguistically more complex data. The LDA model, in comparison, works better when text is less complex. Interestingly, this includes the case where many relatively short reviews contain a richer vocabulary, suggesting that the number of unique terms in a corpus plays a minor role. We demonstrate managerial relevance of results from our topic model by identifying topics that disproportionately influence a customer's overall evaluation of a product or service experience.</p><p>The remainder of this paper is organized as follows. In Section 2, we develop the autocorrelated topic model. In Section 3, we present a summary of the data that we use in our empirical analysis. In Section 4, we present results from applying our model to the data and compare it with various alternative models. Section 5 presents results from using topics as predictors of customer ratings. And finally, in Section 6, we summarize our findings and offer concluding comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model Development</head><p>The LDA model assumes that each word in a text document is generated from latent topics characterized by topic-specific word probabilities across a fixed vocabulary. Each document d is described by a vector of topic probabilities θ d of (typically a priori fixed) dimension T, and each topic is characterized by vector φ t , which specifies the word probabilities associated with that topic. Words in a document are generated by first drawing a topic indicator variable z from a discrete (multinomial) distribution with probability vector θ d and then drawing a word from the vocabulary list with probability vector φ t z . <ref type="bibr" target="#b2">Büschken and Allenby (2016)</ref> propose a constrained version of this model by restricting all words within a sentence to be generated from the same topic. This is accomplished by drawing the latent variable z once for all words in a sentence and then "rolling out" this topic assignment across all words in a sentence.</p><p>The LDA and sentence-constrained LDA (SC-LDA) models represent two extremes in topic generation. Topics generated from the LDA model are assumed to be independent and identically distributed (i.i.d.) across all words, whereas the SC-LDA is a model of deterministic dependency within an observed locale and with only a sentence's period (e.g., full stop, exclamation point) allowing for topic variation within a document. In this section, we develop an approach to the LDA model that does away with the i.i.d. assumption to topic assignments. The model we propose is different in the way (local) dependence of topic assignments emerges. Our model proposes probabilistic topic carryover on the word level. This implies that topic assignments are locally correlated.</p><p>The issue of correlated topics has been examined previously in the literature. <ref type="bibr" target="#b9">Griffiths et al. (2004)</ref> propose a model in which words differ in syntactic (i.e., related to placement or arrangement) and semantic (i.e., related to meaning) content. A hidden Markov model (HMM) is used to generate sentence syntax, and an LDA (topic) model is used to generate its content. The HMM introduces serial correlation between syntactic vocabularies and topics, but not among the topics themselves. <ref type="bibr" target="#b24">Wallach (2006)</ref> proposes a model in which topics generate words conditional on the previous word, introducing first-order autocorrelation in word generation but not topic generation. That is, the topic indicator variable is still assumed to be i.i.d. across words. <ref type="bibr" target="#b0">Blei and Lafferty (2007)</ref> employ a logistic Normal distribution instead of a Dirichlet distribution to allow for correlations in the prior to topics. Their model affects the topic probabilities θ d but does not induce autocorrelation of topics within the document indicated by the latent indicator variables z. <ref type="bibr" target="#b23">Trusov et al. (2016)</ref> propose a model with correlated topics for website visitation data to account for latent interests (or, as they are called by the authors, "roles"; p. 406), simultaneously driving the number of times different websites are visited up (or down). The common element of these approaches is that topics may exhibit a priori dependence (e.g., if reviewers talk extensively about service problems in restaurant reviews, they might also talk more about inflated prices). Finally, in a version of their SC-LDA model, <ref type="bibr" target="#b2">Büschken and Allenby (2016)</ref> allow for a probabilistic carryover of topics from sentence to sentence but do not find empirical support for this model.</p><p>Topic dependency across sequences of words has important implications for inference regarding θ d , the topic shares of individual documents. If topics exhibit word-to-word carryover, estimates of θ d obtained via the LDA model are potentially biased. This is because, in the LDA model, every topic assignment on the word level is treated as a draw from θ d . For example, if a document consists of two topic runs (consecutive topic carryover) of three words and six words, respectively, the LDA model treats these as three i.i.d. draws of z t 1 from p(z|θ d ) and six i.i.d. draws of z t 2 from p(z|θ d ). The expected value of θ d suggested by the LDA model (ignoring the role of the prior) is then ( 3 9 , 6 9 ). A model with autocorrelated topics, however, suggests that E(θ d ) (0.5, 0.5) because topic repeats as a result of carryover do not originate from p(z|θ d ).</p><p>It is clear that when documents contain sequences of words exhibiting topic carryover, estimates from the LDA model are biased toward topics with high word-to-word carryover probability. As result, LDA model-based estimates may differ greatly from those using models that account for the serial structure of topic assignments. In our empirical analysis, we show that topics in customer reviews exhibit strong local dependency and that models accounting for this dependency result in better estimates of θ d because they carry higher predictive power with respect to customers' satisfaction ratings. Next, we turn to the development of our model with autocorrelated topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Autocorrelated Topic Model</head><p>Our autocorrelated topic model assumes topic carryover at the word level, with the degree of carryover affected by observed structural information in the text. An important structural feature of written Romanic and Germanic languages is the use of conjunctions such as "and" and "but" that play a syntactic role by joining parts of a sentence. As such, they do not represent topics or semantic content. A typical example of this role is present in this hotel review, "Comfy beds but not your usual large American beds," in which the conjunction "but" links two different perspectives of evaluation, one personal and the other more general ("not American"). In comparison, punctuation typically does not join, but separates, parts of speech. A full stop, for example, indicates the end of a sentence and introduces a pause to the flow of thoughts. Such a pause is a natural candidate for a topic change. Other examples of punctuation are exclamation points or question marks, both of which introduce structure to text in a similar way but also add weight or an interrogative notion to a statement. Structural punctuation for the purpose of this analysis consists of marks that act on parts of documents typically not larger than a sentence and not smaller than a word (e.g., hyphens; <ref type="bibr">Meyer 1987, Say and</ref><ref type="bibr" target="#b21">Akman 1996)</ref>. The central idea of our model is to use the observed structural information in text presented by punctuation and conjunctions for inference regarding the dynamics of topics in text.</p><p>It is interesting to note that in empirical applications of topics models, conjunctions as well as incidents of punctuation are typically removed from the data prior to analysis (i.e., preprocessing). Conjunctions are removed because they are stopwords. Stopwords typically carry very little power to discriminate topics. This is evident in the nearly uniform probabilities of stopwords, if included in the data, to appear under any topic. However, we propose that conjunctions and punctuation, as carriers of structural information, present information to topic change and introduce this information to our model. The challenge is in retaining the structural information without compromising inference about topics. In Figure <ref type="figure" target="#fig_0">1</ref>, we present a stylized way of using these data in our model.</p><p>In the review at the top of Figure <ref type="figure" target="#fig_0">1</ref>, we highlight conjunctions, punctuation, and stopwords other than conjunctions. In two versions of this review, numbered 1 and 2, we present different ways of exploiting structural information. Version 1 results from removal of all stopwords, including conjunctions, and all punctuation. This preprocessing of data is consistent with the bag-of-words assumption and uses no structural information other than the remaining words. For version 2, which is applied here, we view conjunctions (green) and punctuation (red) as prior information to a topic carryover between consecutive words. For example, the conjunction "but" is used as a covariate to the probability of a topic carryover from the word (street slightly) "dingy" to the word "general" (hotel offered good value). In a similar fashion, the full stop preceding the word "hotel" is Büschken and Allenby: Improving Text Analysis Using Conjunctions and Punctuation <ref type="bibr">Marketing Science, 2020</ref><ref type="bibr">, vol. 39, no. 4, pp. 727-742, © 2020</ref> covariate to the probability of a carryover from (staff) "helpful" to "hotel" (frontage). The difference between the two approaches lies in the use of otherwise ignored data as observed covariates to topic change. In the empirical application of our model, we find topic carryover to be heavily driven by structural elements of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Joint Distribution of Model Parameters and</head><p>Data. The model with autocorrelated topics is based on the LDA topic model <ref type="bibr" target="#b1">(Blei et al. 2003)</ref>, which proposes the following joint distribution of knowns and unknowns:</p><formula xml:id="formula_0">p w n , z n , θ d , φ, α, β ( ) p w n |φ z n ( ) × p z n |θ d ( ) × p θ d |α ( )× p φ|β ( ) × p α ( ) × p β ( ) ,<label>(1)</label></formula><p>where w n is the nth word of document d, z n is the topic assignment of word w n , θ d is a vector of prior topic probabilities for document d, φ t is a vector of word probabilities given topic t, and α, β are fixed priors of θ d and φ t , respectively. We extend this model so that the topic z n−1 assigned to word w n−1 may carry over to word w n independent of θ d so that topics are autocorrelated. We define carryover of a topic as z n z n−1 and introduce the latent binary variable ζ n to indicate whether the topic assignment to word w n is the result of carryover:</p><formula xml:id="formula_1">ζ n 1 : z n z n−1 , ζ n 0 : z n ∼ Multinomial θ d ( ).<label>(2)</label></formula><p>In the LDA model, ζ n 0 ∀n, implying that this model is a special case of the autocorrelated-topics LDA (AT-LDA) model. The same holds for the SC-LDA, which imposes ζ n,s 1 ∀n s &gt; 1, where s is a sentence. We assume ζ n to be a distributed binomial with probability ψ n :</p><formula xml:id="formula_2">ζ n ∼ Binomial ψ n |z n−1 ( ) ψ n |z n−1 exp δ 0,z n−1 +x n δ [ ] 1 + exp δ 0,z n−1 +x n δ [ ]<label>(3)</label></formula><p>wherex n is a vector of dummy variables that indicates conjunctions and punctuation to the current word (Figure <ref type="figure" target="#fig_0">1</ref>), and δ are estimated coefficients that affect the probability of topic change. Negative values of δ increase the likelihood of an i.i.d. topic draw, whereas positive values indicate that a topic carryover is more likely. We allow for intercepts δ 0,z n−1 that depend on the previous word's topic z n−1 . Equation (3) specifies common coefficients δ for the conjunctions and punctuation inx n−1 . In our empirical analysis, we also consider interaction effects of the latent topics and covariates giving rise to topic-specific effects of conjunctions and punctuation.</p><p>2.1.2. Data-Generating Process. The generative model of the AT-LDA with covariates to topic change, the regression of the customer rating on topic shares of documents, and fixed priors α, β, μ δ , Σ δ , μ β , Σ β , a, b, λ is as follows:</p><p>1. Draw independently a. δ from MV Normal(μ δ , Σ δ ). b. φ t from Dirichlet(β</p><formula xml:id="formula_3">) ∀t i.i.d. c. θ d from Dirichlet(α) ∀d i.i.d. d. β (reg) from MV Normal(μ β , Σ β ). e. σ 2 from IG(a, b). f. c from U(λ). 2. Draw (latent) rating τ d from N(θ T d β (reg) , σ 2 ); com- pute r d given τ d via (5). 3. For the first word in document d, w 1 a. Draw z 1 from Multinomial(θ d ). b. Draw w 1 from Multinomial(φ t z 1 ). c. Compute p(ψ 2 |x 2 , δ, z 1 ) using (3); draw ζ 2 given ψ 2 .</formula><p>4. For words w n n ∈ 2 :</p><formula xml:id="formula_4">N d a. If ζ n 0, draw z n from Multinomial(θ d ); if ζ n 1, set z n z n−1 . b. Draw w n from Multinomial(φ t z n ). c. Compute p(ψ n+1 |x n+1 , δ, z n ), draw ζ n+1 given ψ n+1 .</formula><p>5. Repeat steps 2-4 for all documents d ∈ D (except for draw of ζ N d ).</p><p>Note that the draw of the words and the rating are independent given θ d . The joint distribution of the Büschken and Allenby: Improving Text Analysis Using Conjunctions and Punctuation knowns and unknowns of the AT-LDA model with covariates, given document d, factorizes as follows:</p><formula xml:id="formula_5">p r d , τ d , w { } d , z { } d , θ d , φ, ζ { } d , β reg ( ) , σ 2 , δ, α, β, ( x, a, b, c ) ∝ p r d |τ d , c ( )× p τ d |θ T d β reg ( ) , σ 2 ( ) × p w 1 |φ, z 1 ( ) × p z 1 |θ d ( ) × ∏ N d n 2 p w n |φ, z n , z n−1 , ζ n ( ) × p z n |z n−1 , θ d , ζ n ( ) [ × p ζ n |, x n , δ, z n−1 ( ) ] × p φ|β ( ) × p θ d |α ( )× p σ|a, b ( ) × p c|λ ( )× p β ( ) × p α ( ) × p δ ( ) × p a ( ) × p b ( ) × p λ ( ),<label>(4)</label></formula><p>where we, as usual, assume independent prior distributions. The likelihood of a word, conditional on</p><formula xml:id="formula_6">ζ n , is p w n |φ, z n , z n−1 , ζ n 0 ( ) p w n |φ, z n ( ) , p w n |φ, z n , z n−1 , ζ n 1 ( ) p w n |φ, z n−1 ( ) .</formula><p>The likelihood of a topic assignment, conditional on</p><formula xml:id="formula_7">ζ n , is p z n |z n−1 , θ d , ζ n 0 ( ) p z n |θ d ( ), p z n |z n−1 , θ d , ζ n 1 ( ) p z n z n−1 ( ) 1.</formula><p>In Online Appendix A.2, we outline our Markov chain Monte Carlo (MCMC) approach to estimating the AT-LDA model, and in Online Appendix A.2.7, we use simulation to show that the model is empirically identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Incorporating Rating Information</head><p>We relate topic probabilities θ d to the overall rating in the form of a supervised topic model <ref type="bibr" target="#b14">(Mcauliffe and Blei 2007)</ref>. The "supervised" aspect of this feature of our model results from relating the observed rating provided by each reviewer to the latent topic shares of each review. More specifically, we assume the rating for a review r d to be related to the latent topic probabilities using a standard cut-point model:</p><formula xml:id="formula_8">r d e, if c e−1 ≤ τ d ≤ c e ,<label>(5)</label></formula><p>and</p><formula xml:id="formula_9">τ d ∼ N θ d β reg ( ) , σ 2 ( ) .<label>(6)</label></formula><p>The cut-point model implies E cut points for E − 1 categories, where cut points c 0 and c E are −∞ and ∞, respectively, and c 1 and c E−1 are fixed values for identification of location and scale of the latent response variable τ, which we augment the usual way. The cut-point model applied here is a standard model from the literature <ref type="bibr" target="#b12">(Johnson and Albert 2006</ref>; see Online Appendix A.2). Note that Equations ( <ref type="formula" target="#formula_8">5</ref>) and ( <ref type="formula" target="#formula_9">6</ref>) result in additional likelihood information to θ d because the topic shares must also adhere to the cross-sectional mapping of shares to the rating through β <ref type="bibr">(reg)</ref> . In a standard LDA model, topic shares are informed by the topic assignments of the words in d, z d , only. Also note that the fit of the regression model in ( <ref type="formula" target="#formula_9">6</ref>) provides a way of assessing the predictive plausibility of competing topic models. In Section 5, we explore the mapping of topic shares to the rating in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head><p>We examine four customer review data sets that differ with respect to size and lexical complexity. All data sets were obtained from publicly available sources and were selected based on length and richness. Prior to running the models, we preprocessed the data in the following way: 1. Changing capital letters to lowercase letters 2. Removing rare words and signs (appearing fewer than 10 times in the corpus)</p><p>3. Removing stopwords other than conjunctions used for structural analysis (e.g., "there" and "rather").</p><p>Note that we do not remove very frequently occurring terms (e.g., "restaurant" or "tent"), nor do we remove punctuation and conjunctions. The latter are used as covariates to word-to-word topic carryover. Stemming is absent from our preprocessing because terms carrying different meanings may share the same stem. Overall, our preprocessing strategy is targeted at using (nearly) all of the observed text data as information either to topics or to topic carryover. From our experience, more aggressive preprocessing (performing stemming, removing all words with low frequency, removing common terms other than stopwords, etc.) leads to shorter documents and smaller corpora, which, in turn, lead to models with hard constraints (e.g., the SC-LDA model) fitting the data very well. In other words, text data can be preprocessed in a way that favors particular models. It is also important to note that aggressive pruning results in destruction of the local context of conjunctions or punctuation and subsequent word(s). This is another reason why we preprocess raw data as little as possible prior to a model-based analysis.</p><p>Table <ref type="table" target="#tab_1">1</ref> presents descriptive statistics of the data sets used in our study. Descriptive statistics were obtained after applying the same preprocessing procedure. The restaurant data set, obtained from We8there.com, contains 2,351 textual reviews of restaurants. This corpus contains a total of 171,385 words, 1,531 of which are unique terms. On average, each restaurant review consists of 72.9 words. The standard deviation of the number of words per review of 84 words indicates the long right tail of the words per review distribution. The 72.9 words are spread, on average, over 13.4 sentences with 5.6 words per sentence. Sentence breaks present natural breaks in the narrative and opportunities for topic change. The camping tents data are made up of 7,973 reviews of threeseason, multiple-person camping tents in the price range of $100-$200, obtained from Amazon.com. Its vocabulary consists of 3,664 unique terms. This implies that despite applying the same preprocessing rules, the vocabulary used by tent reviewers is much more diverse, suggesting higher lexical complexity of the data. On average, the camping tent reviews contain 6.2 sentences with 7.6 words per sentence, the highest number of words per sentence across all data sets. The third data set in our analysis is a set of reviews of luxury hotels (five stars) located in downtown New York (Manhattan), obtained from Expedia.com. This data set consists of 3,481 reviews with a vocabulary of 1,060 terms. The average number of words per review is 24.7, and the average number of sentences is 4.9 (five words/sentence). The dog food data, also obtained from Amazon.com, are comprised of 6,018 reviews with a total of 94,165 words. On average, dog food reviews contain 15.7 words over three sentences (5.3 words/sentence). Thus, these reviews are lexically less complex than the other data sets. All data sets exhibit significant heterogeneity in terms of review length, as indicated by the standard deviation and range of the number of words in each review. The coefficient of variation of the number of words exceeds one in all data sets, and the distributions of the number of words specifically in reviews of restaurants and camping tents exhibit very long tails, as indicated by the maximum number of words, suggesting that many customers feel the need to report about their experience in great detail. For all data sets, we find that customer ratings are skewed toward the right, as indicated by a mean rating close to the upper end of the scale.</p><p>Table <ref type="table" target="#tab_2">2</ref> reports counts of the use of conjunctions and various forms of punctuation in our data sets.</p><p>From Table <ref type="table" target="#tab_2">2</ref>, it is clear that all data sets in our analysis are rich in syntactic content. The conjunction "and" appears, on average, 5.8 times (13,683/2,351) in the restaurant review and 2.7 times in each tent review. Similarly, on average, full stops mark boundaries between sentences 6.5 times in restaurant reviews and 2.6 times in hotel reviews. A special case of punctuation is presented by the use of (round) parentheses. We record the use of parentheses more than 1,000 times in the corpus of restaurant reviews and about 2,400 times in the camping tent reviews. Apparently, reviewers feel the need to structure some part of their narrative by placing words within parentheses. Typically, parentheses are used to clarify preceding text or, when combined with a full stop, as a side remark. Parentheses provide an observable signal of words belonging together, suggesting some form of topical dependency. On average, a review in the restaurant data set contains 38 structural elements in the form of conjunctions or punctuation, and a review of a camping tent contains 17. The frequency at which structural elements appear in our data raises the question of why this information can be ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Analysis</head><p>We evaluate the performance of the proposed model by examining model fit, the prediction performance for customer ratings, topic carryover, and the direction of effects of the various conjunctions and punctuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Fit</head><p>Performance of the proposed autocorrelated topic model is compared with the following models:</p><p>1. Latent Dirichlet allocation (LDA). 2. Models of (local) topic chunking: a. SC-LDA, b. A conjunction-and punctuation-constrained LDA (CPC-LDA) model with sections in the reviews defined by observed conjunctions and punctuation, and c. Variants of both, allowing for topic carryover across observed word sequences ("sticky" SC/ CPC-LDA).</p><p>3. Topic models with autocorrelated topics across words (AT-LDA) a. Without covariates and b. Using conjunctions and punctuation as covariates to word-to-word topic change.</p><p>The LDA model is a common approach to analyze text data and a natural benchmark model for us to use. Our implementation of the LDA model (and all other models) includes the cut-point regression model in Equations ( <ref type="formula" target="#formula_8">5</ref>) and ( <ref type="formula" target="#formula_9">6</ref>) but retains the assumption that topics are generated i.i.d. from the prior topic distribution, indexed by θ d for each word in a document.</p><p>The SC-LDA restricts all words within a sentence to originate from a single topic <ref type="bibr" target="#b2">(Büschken and Allenby 2016)</ref>. In our case, sentences are given by observed use of full stops, exclamation points, and question marks in the reviews. The CPC-LDA uses observed conjunctions and punctuation to define similar but smaller locales in reviews in which topics are constrained to be identical. For example, the CPC-LDA model assumes that between the word "and" and the word "but" (or a question mark), all words in a review carry the same topic. Thus, the SC-LDA model is a special case of the CPC-LDA model that only considers specific forms of punctuation but not conjunctions as local boundaries for topics. For both the SC-LDA and CPC-LDA models, we consider the option that topics carry over (sticky topics) across local boundaries, giving rise to two additional models. In the AT-LDA model with covariates, the amount of autocorrelation is affected by covariates in (3), reflecting syntactic content. We fit the AT-LDA model with covariates allowing for the effect of covariates to be topic specific. This allows, for example, for the effect of a comma on topic change of the subsequent word to be different across topics. In the AT-LDA model without covariates, we estimate a topic-specific baseline probability for topic carryover only and thus ignore all conjunction/punctuation (C/P) information in terms of topic change.</p><p>Our set of models allows us to analyze the role of structural information in text in a detailed way. The standard LDA model makes no use of conjunctions or punctuation in text because both are treated as noise with respect to topic discovery and, consequently, are discarded from the data prior to running the models. Thus, the LDA model can be viewed as a null model that assumes that structural information is irrelevant to topic discovery. The SC-LDA model uses punctuation to a priori define sections in which topics are identical so that, compared with the LDA model, the benefit of assigning homogeneous topics to all words in sentences can be evaluated. The CPC-LDA model, compared with the SC-LDA model, allows us to observe the benefit of breaking (typically longer) sentences into shorter sections with homogeneous topics, defined by conjunctions. The AT-LDA model with covariates uses conjunctions and punctuation as covariates to a probabilistic topic carryover across words. Compared with the LDA model with "hard" constraints (SC/CPC-LDA models), homogeneous topic assignments to strings of words are possible but not imposed. Instead, the extent to which topics carry over across words within local sections of text is learned from the data (Table <ref type="table" target="#tab_2">2</ref>). In Section 4.3, we show how results from these two assumptions differ.</p><p>For model comparison, we report the (log) marginal likelihood of the text data (Table <ref type="table" target="#tab_3">3</ref>), allowing for direct comparison of the models using Bayes' factors. For this measure, we use all reviews. Because the number of topics is not a parameter of our models, we estimate each model for alternative T and choose the best-fitting model in terms of predictive fit for all further analysis. Note that the likelihood reported is based on the corpus of words. It does not account for the observed rating, for which we report a separate fit statistic in Table <ref type="table" target="#tab_8">8</ref>. Across all data sets, we find that imposing hard constraints on topic assignments on locales defined by punctuation or conjunctions is not supported by our data (Table <ref type="table" target="#tab_3">3</ref>). Compared with results from the standard (supervised) LDA model, both the SC-LDA and CPC-LDA models do not improve in-sample fit to the text data. More flexible ways of modeling (LDA, AT-LDA) are preferred.</p><p>An interesting result is obtained from the dog food data: With respect to in-sample fit, the standard LDA model outperforms all other models, including the proposed AT-LDA model. For the other three data sets, we obtain the opposite result. It appears that the LDA model is a useful tool for topic detection when reviews are short (Table <ref type="table" target="#tab_1">1</ref>). In summary, applying the set of models to our data sets suggests that, in longer reviews, (1) topics exhibit serial dependency, (2) this autocorrelation of topics is not well modeled by imposing prior constraints via observed structural elements of text, and (3) using this structural information as covariates to word-to-word topic change improves the fit of the model. The latter is evidenced by a significant improvement in fit of the AT-LDA model when covariates are used (Table <ref type="table" target="#tab_3">3</ref>).</p><p>Table <ref type="table" target="#tab_4">4</ref> reports out-of-sample fit results. For holdout data, we use a randomly selected sample of 20% of reviews from each data set. We report the log of the average likelihood obtained by first computing the posterior mean probability of observing each word in holdout reviews and then aggregating the (log) probabilities across all words in holdout reviews. We find that the proposed AT-LDA model with covariates outperforms all other models with respect to predictive fit across all four data sets, providing evidence for not eliminating structural elements of text in preprocessing and linking this information to the topic flow in customer reviews. This result also implies that models of local topic chunking are not preferred for our data. The AT-LDA model provides direct estimates of (local) topic dependency via ψ t , the topic-specific probability of a wordto-word topic carryover. This parameter indicates how topics extend across words independent of the prior topic distribution p(z|θ d ). In Table <ref type="table" target="#tab_5">5</ref>, we report results with respect to local topic dependency from our data sets. Results from our model indicate that topics exhibit dependency across words. This is evidenced by topic carryover probabilities of approximately 0.5, indicating that, for example, a topic run of length three has probability 0.5 2 0.25. For some topics, we find ψ to be close to 0.7 (e.g., restaurants, camping tents), suggesting that extended sequences of words generated by carryover of a single topic have a nonmarginal probability. For example, in the restaurant data, we find sequences of repeated topic carryover of up to 34 words (camping tents: 28 words). In summary, our results suggest that topics in our data sets exhibit significant serial dependency, implying that the i.i.d. assumption of the LDA model is untenable. This also suggests that inference regarding θ d may be biased when the standard LDA model is applied, In our analysis of models with respect to predicting customer ratings (Section 5), we return to this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Influence of Syntactic Covariates on Local Topic</head><p>Carryover. An important element of our model with autocorrelated topics is the relationship of syntactic elements of text and local topic dependency. The model allows for conjunctions and punctuation to change the probability of topic carryover. Table <ref type="table" target="#tab_6">6</ref> shows how ψ changes as a result of the presence of selected covariates, marginalized over topics and the presence of other covariates (note that, e.g., a sentence may begin with the word "and," which leads to the presence of multiple covariates). For our illustration, we use results from the camping tent data. Table <ref type="table" target="#tab_6">6</ref> reveals that full stops, commas, and question marks have a significant influence on the probability of a topic carryover. The presence of a full stop in front of a word reduces the probability of a topic carryover from the previous word from 51% to 4%, a 92% reduction in probability. A comma reduces this probability from 48% to 21%. A question mark cuts ψ by more than half (from 46% to 21%); an exclamation point or closed parenthesis drives it to nearly zero, indicating a possible topic change point. We obtain similar results from conjunctions (Table <ref type="table" target="#tab_6">6</ref>): The presence of "because" or "but" significantly reduces ψ, whereas "and" has relatively little effect. These results suggest that structural elements in text are indicators of possible topic change. In Online Appendix A.1, we present more detailed results of the hierarchical regression of topic carryover on the structural covariates (Equation ( <ref type="formula" target="#formula_2">3</ref>)). These results provide further evidence that structural covariates exhibit significant influence on word-to-word topic carryover. In our analysis, we find that the influence of covariates on topic carryover is different across topics and that some covariates, given topic, increase the probability of a carryover. Our results suggest that a sentence constraint that implicitly assumes (with respect to topics) homogeneous effects of full stops, etc. on topic change does not reflect the flow of topics well. Notes. Reported is the log average likelihood of holdout data. The number of topics is the same as in Table <ref type="table" target="#tab_3">3</ref>. The best-fitting model is highlighted. Covariates refer to use of conjunctions and punctuation as prior information to topic carryover. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Topic Analysis</head><p>The intent of our analysis is to discover topics that are relevant to consumer ratings (Section 5). Topics, however, emerge ex post from the analysis and are not necessarily unique or coherent <ref type="bibr" target="#b16">(Morstatter and Liu 2016)</ref>. We begin our analysis of topics obtained from our data by considering the uniqueness and coherence of topics identified by the different models and then examine the effect of sentence conjunctions and punctuation on topic carryover.</p><p>4.3.1. Topic Uniqueness. The successful analysis of customer reviews should identify distinct and interpretable topics for improving customer experience. Uniqueness is important because our analysis of customer satisfaction aims to identify specific issues requiring action and further investigation. We investigate topic uniqueness by analyzing the degree to which topics are characterized by unique words.</p><p>A perfectly unique topic does not share any of its most frequent words with another topic, and vice versa.</p><p>For each model and data set, we compute the number of times the top 50 words from a topic appear among the top 50 words of the other topics for a given model. This gives rise to T(T − 1)/2 overlap scores ranging, after normalization, from 0 (0 of 50 in common) to 1 (50 of 50 in common). Figure <ref type="figure">2</ref> plots the frequency of shared words of the topics emerging from the LDA, AT-LDA, SC-LDA, and CPC-LDA models for our three data sets, revealing that the AT-LDA model results in topics exhibiting less overlap of the top 50 words for the restaurant, luxury hotel, and camping tent data. For the dog food data, we find the LDA model to generate the most unique topics in terms of most frequent words. Again, this points to the LDA model as being a useful approach to modeling topics for less complex data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Topic Coherence.</head><p>Coherence refers to the semantic relationship among the most frequent words of a single topic. In a coherent topic, the most frequent words jointly describe a single theme. Compared with uniqueness, which is an across-topic measure, coherence is a within-topic measure of topic quality with no reference to other topics. Coherence is necessary for topics to be interpretable and for finding meaningful topic labels. Our investigation of topic coherence starts by presenting topics resulting from the proposed model with autocorrelated topics and covariates. Table <ref type="table" target="#tab_7">7</ref> shows the most frequent words for the camping tent data. Topic 12 talks about breaking poles ("pole," "broke"), ripped ground or plastic sheets, and resulting problems when staking out the tent. Topic 9 talks about a particular occasion or camping trip when the tent was used. Words such as "camping," "first," "trip," "weekend," "summer," "went," and "camp" all point at when or how the tent was used by a customer for the first time. In a similar fashion, one could proceed and identify the underlying theme for all topic and data sets in the way of a human expert. It is clear, however, that this is not an objective way of labeling a particular topic.</p><p>A formal linguistic analysis of coherence is based on semantic similarity of words making up a particular topic <ref type="bibr" target="#b11">(Jiang and Conrath 1997)</ref>. This approach uses structural lexical taxonomies of words (e.g., WordNet) that define cognitive synonyms of words and hierarchical relationships among words. On the basis of such strong a priori knowledge, the similarity of any pair of words can be computed. If the average pairwise similarity of all words in a topic is high, a topic is found to be coherent. Different approaches have been suggested for this purpose: Lowest common subsumer <ref type="bibr" target="#b11">(Jiang and Conrath 1997)</ref> in the hierarchy of synsets (e.g., "breakfast" and "lunch" share the immediate superordinate "meal"), the number of nodes on the shortest path in the hierarchy from one term to the other, or the distance between two terms in the taxonomy, given the length of edges on that path. By the logic of distance of words in lexical taxonomies, topics consisting exclusively of words that are synonyms or exhibit an immediate super/ subordinate relationship are highly coherent. From our analysis of text-based customer reviews, we find that this is rarely the case. Consider, for example, a topic corresponding to items purchased in restaurant reviews. Word pairs such as "chicken" and "barbecue" and "sauce" and "onion" exhibit relatively low textual similarity. The same is true for pairs such as "bought" and "tent" and "problem" and "rainfly" obtained from the camping tent data (Table <ref type="table" target="#tab_7">7</ref>). This suggests that methods based on semantic similarity do not apply well to an analysis of coherence of topics in customer reviews.</p><p>In the following, we employ two different approaches to evaluate topic coherence. The first approach is empirical and model based and will be explained in more detail later. A topic can be viewed as coherent if its distribution over words is different from suitable benchmark distributions of words <ref type="bibr" target="#b7">(Fang et al. 2016)</ref>. The second approach is human evaluation of topics, which is often viewed as a gold standard in measuring topic coherence <ref type="bibr" target="#b3">(Chang et al. 2009</ref><ref type="bibr" target="#b19">, Newman et al. 2010</ref><ref type="bibr" target="#b13">, Lau et al. 2014</ref><ref type="bibr" target="#b16">, Morstatter and Liu 2016</ref>. We present results from human evaluation to measure topic coherence in Online Appendix A.3. As the model-based approach, we find human evaluation of topics from the AT-LDA model to be more coherent than those from the standard LDA model. A statistical model-based approach to measuring topic coherence is given by the Kullback-Leibler (KL) divergence of φ t with respect to a benchmark distribution Q:</p><formula xml:id="formula_10">D KL φ t Q ( ) ∑ V v 1 φ v,t log φ v,t q v ,<label>(7)</label></formula><p>where Q (q 1 , . . . , q V ). Different choices for Q are available. An "uninformative" prior guess for Q is q v 1 V , a uniform distribution. Equation ( <ref type="formula" target="#formula_10">7</ref>) then measures the entropy of φ t relative to all terms being equally likely. A data-driven choice for Q is based on q v c v C , the relative frequency of terms in the corpus. Essentially, this is φ, the word probabilities marginalized with respect to topics. Marginal probabilities present the least coherent "topic" available from the data as, in this observed "topic," all latent topics coappear. Figure <ref type="figure" target="#fig_1">3</ref> presents the distribution of statistical coherence scores based on KL divergence for our models and our data sets. Note that for each topic from a model applied to one data set, we obtain a KL divergence score given a particular choice of Q. So, for example, applying the AT-LDA model to the camping tent data results in 16 KL divergence scores relative to the uniform distribution. Figure <ref type="figure" target="#fig_1">3</ref> shows the distribution of these scores for given models and our four data sets. In general, a uniform distribution as choice of Q results in higher KL divergence scores than using the observed word frequency. Figure <ref type="figure" target="#fig_1">3</ref> reveals that the proposed model with autocorrelated topics generates topics, relative to marginal φ, that are statistically more coherent than any other model for the restaurant data, the camping tent data, and the hotel data. For the dog food data, the LDA model generates the most coherent topics. We find the same ranking of models relative to a uniform prior distribution of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis of Customer Ratings</head><p>Central to our analysis is to discover how latent topics in reviews relate to customers' overall evaluations. We consider two analyses. The first is an analysis of the distribution of topics across documents for different ratings. We find that the distribution of topic probabilities changes markedly in moving from a one-star to a five-star rating, and we also show that the topic probabilities θ d can be used to identify customer reviews of interest for further non-modelbased analysis. The second analysis results from the cut-point regression model (Equation ( <ref type="formula" target="#formula_8">5</ref>)), which provides a simultaneous mapping of all topic probabilities to the rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Distribution of Topic Probabilities Given</head><p>Customers' Ratings Figure <ref type="figure" target="#fig_2">4</ref> displays the distribution of topic probabilities for the restaurant data and the camping tent broken down by customer rating. For this analysis, we computed the posterior means of the topic shares at the document level and then computed acrossdocument means of the topic shares given the different rating categories (one to five stars). The left panel of Figure <ref type="figure" target="#fig_2">4</ref> displays how (on average) topic shares change in documents when moving from a one-star rating to a five-star rating for the restaurant data, and the right panel shows the same for the camping tent data. From Figure <ref type="figure" target="#fig_2">4</ref>, we see that the shares of many topics change given changes in ratings, indicating an empirical relationship of topic shares and ratings. We explore this relationship in more detail when discussing results from the cutpoint regression (Section 5.2). From the restaurant data, we can see that low ratings are associated with a prevalence of topics 3 and 7 and the high ratings have a higher likelihood of topic 6 being present. Threeand four-star ratings of restaurants have a higher probability of being associated with topic 1. For all other topics, we observe a more uniform distribution of topic shares across ratings. For the camping tent data (Figure <ref type="figure" target="#fig_2">4</ref>, right), we can see that higher shares of topics 1, 4, and 12 are associated with a higher likelihood of a bad rating, whereas higher shares of topics 2 and 13 are associated with good ratings. For both data sets, we find most topic shares to change monotonically with a change in rating. In part, this is caused by the assumption of a linear model (Equation ( <ref type="formula" target="#formula_9">6</ref>)). In conclusion, from both data sets, we find that topic probabilities θ d are indicative of the rating, which implies that they can be used to identify example reviews for each topic. As an example for topicbased document retrieval, we show in Figure <ref type="figure" target="#fig_3">5</ref> the two restaurant reviews richest in topic 3 (i.e., highest share of topic "bad service + talked to manager") and the two camping tent reviews richest in topic 1 ("water leaks"). By examining the original reviews, an analyst can obtain information about the strengths and weaknesses that are associated with the product under review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cut-Point Regression Results</head><p>The relative importance of topics can be assessed by using an ordinal probit model in which the latent customer rating τ d is regressed on the topic probabilities θ d (Equation ( <ref type="formula" target="#formula_8">5</ref>)). Table <ref type="table" target="#tab_8">8</ref> reports results from this regression. Results from sticky constrained models and the AT-LDA model without covariates are omitted for brevity. As a fit measure, we use an R 2 -like measure of fit computed in the standard way but using the latent continuous rating τ as dependent variable. In our model, τ is augmented data, partially informed by the (also) latent θ. Thus, our fit measure for this regression is not based on exogenously given observations of the covariates. However, this holds for any model in our analysis, implying that no model is a priori at an advantage. Of course, models may differ a posteriori in how well they result in estimates of θ when the topic shares are assumed to map to the rating in a homogeneous fashion across reviews (i.e., common β <ref type="bibr">(reg)</ref> ). In this sense, R 2 is additional information of the extent to which a model results in unbiased estimates of θ (see Section 4.2).</p><p>From Table <ref type="table" target="#tab_8">8</ref>, we find models of local topic dependency to outperform simple LDA models across all data sets. This suggests that models of topic dependency result in estimates of θ d that map more closely to the customer rating. For the hotel and dog food data sets, we find the SC-LDA model to perform best among all models. The proposed AT-LDA model </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Büschken and Allenby: Improving Text Analysis Using Conjunctions and Punctuation</head><p>Marketing <ref type="bibr">Science, 2020</ref><ref type="bibr">, vol. 39, no. 4, pp. 727-742, © 2020</ref> with structural covariates is best for the camping tent and restaurant data sets, both of which contain more words per review, more and longer sentences (Table <ref type="table" target="#tab_1">1</ref>), and also more incidents of covariates per document (Table <ref type="table" target="#tab_2">2</ref>) than the dog food data set. Interestingly, the number of unique words in a corpus does not seem to play a large role. Recall that we find the SC-LDA model to perform well for the dog food data set, which contains the largest vocabulary (Table <ref type="table" target="#tab_1">1</ref>). This suggests that the advantage of the AT-LDA model with respect to predicting the customer rating is not related to the lexical complexity of the text but rather to the number and length of phrases in the reviews.</p><p>Tables <ref type="table" target="#tab_1">9 and 10</ref> summarize results from regressing customers' overall ratings on the latent topics. Note that the regression model is not a priori identified because the topic shares from an LDA-type model, by definition, sum to one. We postprocess the coefficients by setting the coefficient of one conveniently chosen topic to zero. As labels for the top three topics, we picked to the most frequent label supplied by human evaluators (Section 4.3.2). For the remaining topics, we obtained labels manually by generating phrases from the most frequent words.</p><p>Tables <ref type="table" target="#tab_1">9 and 10</ref> reveal that regression coefficients of many topics are empirically associated with the rating, as evidenced by more than 95% of their mass away from zero. It is also interesting to note that several topics map negatively on the rating. For example, in the tent data set, an increase in the topic "interaction with manager or owner" of 10% is associated with a decline of the (latent continuous) rating of -1.7. This decline is significantly higher than first differences among cut points, implying that this increase is equivalent to a decline in the rating of one or more rating scale points. Note that regression coefficients are defined as the change in y as x changes by one unit. In our case, given the use of topic shares as covariates, this would be the difference between 0% and 100% share of a topic. Hence, to compute the impact of a 1% change in the share of a topic, the coefficients in Tables <ref type="table" target="#tab_1">9 and 10</ref> have to be divided by 100. From the camping tent data set, we find that the topic "poles and stakes broke" has a similarly high negative influence on the rating. A 10% increase in this topic changes the rating by -1.9, a two-point decrease in the ordinal rating on average. As a consequence, a review in which this topic has a 50% share has the lowest rating possible. From a managerial perspective, results from the cut-point regression suggest many possible avenues to improve customer experience. For restaurant managers, avoiding frustration with waiters and escalation of conflict ("people wanted to talk to manager or owner") are of foremost importance. In comparison, disappointment with food plays a much smaller role. Of course, a positive experience with service and food has a positive influence on the rating, but it cannot compensate for a frustrating service experience. From the camping tent  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notes.</head><p>Reported is an R 2 -like measure of the (latent, continuous) customer rating (Equation ( <ref type="formula" target="#formula_9">6</ref>)). The number of topics is the same as in Table <ref type="table" target="#tab_3">3</ref>. The best model is highlighted.</p><p>Büschken and Allenby: Improving Text Analysis Using Conjunctions and Punctuation data set, it seems highly critical to supply poles and stakes that do not brake when used and to make a tent rainproof in heavy weather. Both are major sources of disappointment for users. Positive attributes are room offered (topic: "tent has plenty of room for people"; topic: "number of people") and easy setup (topic: "tent can be set up easily"), both of which make a tent a much more useful tool. In conclusion, we find that topics obtained from customer reviews identify a large and diverse set of concrete attributes of products or services that, by linking them to the rating, are ranked by customer importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>In this paper, we examine the use of an autocorrelated topic model for analyzing text data. Topics are autocorrelated when they can carry over from word to word in speech. In our empirical analysis, we find that the proposed model with autocorrelated topics outperforms standard topic models and models of topic chunking across different data sets. The reason for this result is that topic carryover is a regular feature of customer review text data for which standard topic models cannot account. Although the i.i.d. assumption of topic assignment in LDA models has been criticized in the literature as unrealistic, we provide model-based evidence for violation of this assumption and a way to solve this problem.</p><p>In our application of the model to different data sets, we examine the role played by conjunctions and punctuation in signaling topic change. The difference between these two categories of covariates is that conjunctions are joiners of speech, and incidents of punctuation present natural separators of speech. Because we incorporate this information as covariates in the model, we can use it without compromising inference with respect to the topics themselves. In our empirical analysis, we find these syntactic covariates to be highly predictive of topic carryover. Typically, conjunctions and punctuation are removed prior to the model-based analysis of text data. The primary motive for this preprocessing is that such data are not diagnostic with respect to topics. Although this is true, our results suggest that syntactic covariates are highly diagnostic of topic changes and, through this  mechanism, are useful in analyzing the latent structure of text. In short, our results present a strong case not to discard these data. From a practical perspective, we find that a model with autocorrelated topics results in topics that map better to ratings of customer satisfaction if reviews are more complex (more words, larger vocabulary). This result can guide managers in moving away from simpler models when these do not suffice. Compared with results obtained via the standard approach to topic analysis (LDA model), our model with autocorrelated topics generally results in a larger and more diverse set of topics when applied to data sets comprised of larger sentences and more words. In our analysis of the customer review data, we find that the AT-LDA model identifies topics more focused on specific themes (e.g., rainfly issue with tent, breaking poles, frustrating interaction with waiter or waitress) and that the LDA model does not identify with similar clarity. This is because the LDA model has a tendency to allocate ubiquitous words (food, menu items) more uniformly across topics. This tendency also reduces its ability to explain customer satisfaction ratings. However, the increased power of a topic model to explain customer ratings when topics are serially dependent and its ability to predict new text data are only two of several pieces of convergent evidence that we present in favor of the proposed model. We find topics from the AT-LDA model (1) to exhibit less overlap with respect to the most frequent terms, (2) to be statistically more distinct in comparison with benchmark word probabilities, and (3) to be, according to human evaluators, easier to interpret and, hence, more coherent.</p><p>To conclude, this research suggests that the analysis of serial topic dependency presents a fruitful area of future research to advance the use of topic models for the rapidly increasing amount of text data in marketing. The models proposed here are relatively simple in that they considered first-order topic dependency across an observed sequence of words only. Yet our model outperforms a model with i.i.d. topic draws and models of topic chunking across all four data sets in terms of out-of-sample fit (Table <ref type="table" target="#tab_4">4</ref>). Results from our model suggest that topic dependency often extends across longer sequences of words (topic chunking), suggesting the need for a more complex model of serial topic dependency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Using Syntactic Covariates in Topic Analysis for Actual Example of a Hotel Review</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Topic Coherence Scores Based on Kullback-Leibler Divergence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Distribution of Topics by Customer Rating for Restaurant Data (Left) and Camping Tent Data (Right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Example of (Original) Customer Reviews of Restaurants Rich in Topic 3 (Left) and Camping Tent Reviews Rich in Topic 1 (Right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Büschken and Allenby: Improving Text Analysis Using Conjunctions and PunctuationMarketingScience, 2020, vol. 39, no. 4, pp. 727-742, © 2020 </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Descriptive Statistics of Data Sets (After Preprocessing) Büschken and Allenby: Improving Text Analysis Using Conjunctions and Punctuation</figDesc><table><row><cell>Statistic</cell><cell>Restaurants</cell><cell>Camping tents</cell><cell>Luxury hotels</cell><cell>Dog food</cell></row><row><cell>Number of reviews</cell><cell>2,351</cell><cell>7,973</cell><cell>3,481</cell><cell>6,018</cell></row><row><cell>Corpus size</cell><cell>171,385</cell><cell>364,761</cell><cell>79,377</cell><cell>94,165</cell></row><row><cell>Number of unique terms</cell><cell>1,531</cell><cell>3,664</cell><cell>1,060</cell><cell>1,980</cell></row><row><cell>Number of words per review</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>72.9</cell><cell>45.7</cell><cell>24.7</cell><cell>15.7</cell></row><row><cell>Standard deviation</cell><cell>83.5</cell><cell>58.1</cell><cell>19.8</cell><cell>22.7</cell></row><row><cell>Maximum</cell><cell>606</cell><cell>792</cell><cell>205</cell><cell>536</cell></row><row><cell>Number of sentences per review</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>13.4</cell><cell>6.2</cell><cell>4.9</cell><cell>3.0</cell></row><row><cell>Standard deviation</cell><cell>14.1</cell><cell>6.8</cell><cell>3.0</cell><cell>3.1</cell></row><row><cell>Consumer rating (five-point scale)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>3.75</cell><cell>4.19</cell><cell>4.42</cell><cell>4.38</cell></row><row><cell>Standard deviation</cell><cell>1.41</cell><cell>1.23</cell><cell>0.88</cell><cell>1.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Incidents of Conjunctions and Punctuation in the Data Sets</figDesc><table><row><cell>Conjunctions</cell><cell>Restaurants</cell><cell>Camping tents</cell><cell>Luxury hotels</cell><cell>Dog food</cell></row><row><cell>for</cell><cell>4,439</cell><cell>9,274</cell><cell>1,796</cell><cell>2,929</cell></row><row><cell>and</cell><cell>13,683</cell><cell>21,683</cell><cell>5,912</cell><cell>6,929</cell></row><row><cell>but</cell><cell>3,258</cell><cell>5,374</cell><cell>1,083</cell><cell>1,629</cell></row><row><cell>or</cell><cell>933</cell><cell>1,868</cell><cell>278</cell><cell>529</cell></row><row><cell>so</cell><cell>1,878</cell><cell>3,819</cell><cell>550</cell><cell>1,321</cell></row><row><cell>after</cell><cell>737</cell><cell>1,090</cell><cell>165</cell><cell>445</cell></row><row><cell>as</cell><cell>1,725</cell><cell>3,292</cell><cell>550</cell><cell>1,114</cell></row><row><cell>because</cell><cell>512</cell><cell>1,138</cell><cell>148</cell><cell>482</cell></row><row><cell>before</cell><cell>472</cell><cell>737</cell><cell>102</cell><cell>208</cell></row><row><cell>even</cell><cell>620</cell><cell>1,155</cell><cell>199</cell><cell>299</cell></row><row><cell>if</cell><cell>1,260</cell><cell>2,509</cell><cell>430</cell><cell>484</cell></row><row><cell>now</cell><cell>264</cell><cell>696</cell><cell>23</cell><cell>425</cell></row><row><cell>once</cell><cell>226</cell><cell>652</cell><cell>41</cell><cell>101</cell></row><row><cell>since</cell><cell>341</cell><cell>441</cell><cell>60</cell><cell>385</cell></row><row><cell>than</cell><cell>615</cell><cell>1,244</cell><cell>212</cell><cell>453</cell></row><row><cell>that</cell><cell>3,867</cell><cell>6,042</cell><cell>904</cell><cell>1,772</cell></row><row><cell>though</cell><cell>217</cell><cell>483</cell><cell>71</cell><cell>93</cell></row><row><cell>when</cell><cell>1,303</cell><cell>1,912</cell><cell>370</cell><cell>515</cell></row><row><cell>where</cell><cell>326</cell><cell>541</cell><cell>114</cell><cell>68</cell></row><row><cell>which</cell><cell>1,091</cell><cell>983</cell><cell>310</cell><cell>331</cell></row><row><cell>while</cell><cell>407</cell><cell>586</cell><cell>83</cell><cell>134</cell></row><row><cell>who</cell><cell>384</cell><cell>308</cell><cell>83</cell><cell>223</cell></row><row><cell>what</cell><cell>909</cell><cell>732</cell><cell>155</cell><cell>372</cell></row><row><cell>Punctuation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>,</cell><cell>15,316</cell><cell>20,660</cell><cell>5,762</cell><cell>6,130</cell></row><row><cell>.</cell><cell>27,674</cell><cell>38,463</cell><cell>12,710</cell><cell>10.794</cell></row><row><cell>;</cell><cell>855</cell><cell>1,175</cell><cell>318</cell><cell>249</cell></row><row><cell>!</cell><cell>1,774</cell><cell>3,201</cell><cell>0</cell><cell>1,123</cell></row><row><cell>?</cell><cell>309</cell><cell>317</cell><cell>48</cell><cell>100</cell></row><row><cell>&amp;</cell><cell>330</cell><cell>335</cell><cell>2</cell><cell>210</cell></row><row><cell>(</cell><cell>1,168</cell><cell>2,391</cell><cell>532</cell><cell>716</cell></row><row><cell>)</cell><cell>1,151</cell><cell>2,445</cell><cell>538</cell><cell>718</cell></row><row><cell>Total occurrences</cell><cell>88,651</cell><cell>136,756</cell><cell>33,853</cell><cell>41,606</cell></row><row><cell>Number of documents</cell><cell>2,351</cell><cell>7,927</cell><cell>3,215</cell><cell>6,018</cell></row><row><cell>Covariates per document</cell><cell>37.7</cell><cell>17.3</cell><cell>10.5</cell><cell>6.9</cell></row><row><cell>Covariates per word</cell><cell>0.52</cell><cell>0.37</cell><cell>0.43</cell><cell>0.44</cell></row></table><note>Notes. Conjunctions appearing fewer than 200 times (e.g., "provided," "until") are omitted to reduce clutter. Total occurrences include omitted covariates.Büschken and Allenby: Improving Text Analysis Using Conjunctions and PunctuationMarketingScience, 2020, vol. 39, no. 4, pp. 727-742, © 2020 </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Fit Results</figDesc><table><row><cell>Model category</cell><cell>Model</cell><cell cols="4">Restaurant reviews Camping tent reviews Luxury hotel reviews Dog food reviews</cell></row><row><cell>Bag of words</cell><cell>LDA</cell><cell>−963,344 (10)</cell><cell>−1,878,546 (15)</cell><cell>−421,002 (11)</cell><cell>-361,274 (8)</cell></row><row><cell cols="2">Topic chunking SC LDA</cell><cell>−1,064,867 (6)</cell><cell>−2,164,162 (14)</cell><cell>−427,964 (10)</cell><cell>−449,155 (8)</cell></row><row><cell></cell><cell>Sticky SC LDA</cell><cell>−1,064,805 (6)</cell><cell>−2,151,230 (14)</cell><cell>−425,981 (10)</cell><cell>−456,477 (8)</cell></row><row><cell></cell><cell>CPC LDA</cell><cell>−1,012,657 (8)</cell><cell>−2,006,406 (14)</cell><cell>−410,286 (10)</cell><cell>−425,301 (8)</cell></row><row><cell></cell><cell>Sticky CPC LDA</cell><cell>−1,052,217 (8)</cell><cell>−2,092,006 (14)</cell><cell>−409,896 (10)</cell><cell>−443,896 (8)</cell></row><row><cell cols="2">Topic carryover AT-LDA without covariates</cell><cell>−920,380 (10)</cell><cell>−1,743,012 (16)</cell><cell>-347,457 (10)</cell><cell>−387,500 (8)</cell></row><row><cell></cell><cell>AT-LDA with covariates</cell><cell>-916,784 (10)</cell><cell>-1,731,181 (16)</cell><cell>−347,824 (10)</cell><cell>−383,425 (8)</cell></row></table><note>Notes. Reported are log marginal densities of the data, given model, and number of topics (number of topics in parentheses). The best-fitting model is highlighted. Covariates refer to use of conjunctions and punctuation as prior information to topic carryover.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Predictive Fit Results</figDesc><table><row><cell>Model category</cell><cell>Model</cell><cell>Restaurant reviews</cell><cell>Camping tent reviews</cell><cell>Luxury hotel reviews</cell><cell>Dog food reviews</cell></row><row><cell>Bag of words</cell><cell>LDA</cell><cell>−221,900</cell><cell>−481,697</cell><cell>−93,614</cell><cell>−129,378</cell></row><row><cell>Topic chunking</cell><cell>SC LDA</cell><cell>−221,709</cell><cell>−479,977</cell><cell>−92,957</cell><cell>−129,550</cell></row><row><cell></cell><cell>Sticky SC LDA</cell><cell>−221,640</cell><cell>−478,655</cell><cell>−92,986</cell><cell>−129,365</cell></row><row><cell></cell><cell>CPC LDA</cell><cell>−221,713</cell><cell>−479,509</cell><cell>−92,952</cell><cell>−129,423</cell></row><row><cell></cell><cell>Sticky CPC LDA</cell><cell>−221,594</cell><cell>−478,683</cell><cell>−92,962</cell><cell>−129,289</cell></row><row><cell>Topic carryover</cell><cell>AT-LDA w/o covariates</cell><cell>−221,600</cell><cell>−478,753</cell><cell>−92,976</cell><cell>−129,307</cell></row><row><cell></cell><cell>AT-LDA w covariates</cell><cell>-220,661</cell><cell>-475,974</cell><cell>-92,785</cell><cell>-128,802</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Estimates of Local Topic Dependency from AT-LDA Büschken and Allenby: Improving Text Analysis Using Conjunctions and Punctuation</figDesc><table><row><cell cols="3">Marketing Science, 2020, vol. 39, no. 4, pp. 727-742, © 2020 INFORMS</cell><cell></cell><cell></cell></row><row><cell>Parameter</cell><cell>Restaurants</cell><cell>Camping tents</cell><cell>Luxury hotels</cell><cell>Dog food</cell></row><row><cell>ψ</cell><cell>0.42 (0.19; 0.66)</cell><cell>0.47 (0.29; 0.65)</cell><cell>0.48 (0.36; 0.59)</cell><cell>0.40 (0.11; 0.56)</cell></row></table><note>Notes. Reported are posterior means of ψ, averaged over topics. Numbers in parentheses indicate minimum and maximum of posterior means across topics, respectively. Estimates are based on the model with covariates and marginalized over the presence of covariates.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Marginal Topic Change Probabilities Given Incidents of Punctuation and Conjunctions in the Camping Tent Data Uniqueness of Topics from the Three Data Sets Using Word Overlap Frequency Büschken and Allenby: Improving Text Analysis Using Conjunctions and Punctuation</figDesc><table><row><cell>Structural covariate</cell><cell>.</cell><cell>,</cell><cell>?</cell><cell>!</cell><cell>)</cell><cell cols="2">"Because" "But" "And" "Once"</cell></row><row><cell>Absent</cell><cell cols="5">0.509 0.475 0.460 0.463 0.462</cell><cell>0.461</cell><cell>0.465</cell><cell>0.464</cell><cell>0.460</cell></row><row><cell>Present</cell><cell cols="5">0.043 0.208 0.211 0.052 0.063</cell><cell>0.144</cell><cell>0.009</cell><cell>0.391</cell><cell>0.140</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Camping Tent Data</figDesc><table><row><cell>Rank</cell><cell>Topic 1</cell><cell>Topic 2</cell><cell>Topic 4</cell><cell>Topic 6</cell><cell>Topic 7</cell><cell cols="5">Topic 9 Topic 10 Topic 12 Topic 13 Topic 15</cell><cell>Topic 16</cell></row><row><cell>1</cell><cell>no</cell><cell>room</cell><cell>tent</cell><cell>tent</cell><cell>very</cell><cell cols="2">camping rain</cell><cell>pole</cell><cell>tent</cell><cell>two</cell><cell>i</cell></row><row><cell>2</cell><cell>not</cell><cell>mattress</cell><cell>coleman</cell><cell>area</cell><cell>tent</cell><cell>time</cell><cell>wind</cell><cell>tent</cell><cell>great</cell><cell>2</cell><cell>love</cell></row><row><cell>3</cell><cell>tent</cell><cell>air</cell><cell>i</cell><cell>porch</cell><cell>well</cell><cell>first</cell><cell cols="2">weather broke</cell><cell>good</cell><cell>times</cell><cell>used</cell></row><row><cell>4</cell><cell>water</cell><cell>queen</cell><cell>amazon</cell><cell>front</cell><cell>nice</cell><cell>trip</cell><cell>night</cell><cell>down</cell><cell>price</cell><cell>3</cell><cell>bought</cell></row><row><cell>5</cell><cell>problem</cell><cell>plenty</cell><cell>reviews</cell><cell cols="2">screened easy</cell><cell>tent</cell><cell>heavy</cell><cell>stakes</cell><cell>size</cell><cell>family</cell><cell>tent</cell></row><row><cell>6</cell><cell>rainfly</cell><cell>two</cell><cell>i've</cell><cell>screen</cell><cell>roomy</cell><cell cols="2">weekend winds</cell><cell>rainfly</cell><cell>love</cell><cell>years</cell><cell>husband</cell></row><row><cell>7</cell><cell>seams</cell><cell>fit</cell><cell>used</cell><cell>tarp</cell><cell>good</cell><cell>night</cell><cell>storm</cell><cell>not</cell><cell>huge</cell><cell>4</cell><cell>i'm</cell></row><row><cell>8</cell><cell>rain</cell><cell>size</cell><cell>bought</cell><cell>room</cell><cell cols="2">spacious next</cell><cell>during</cell><cell>stake</cell><cell cols="2">awesome 5</cell><cell>wanted</cell></row><row><cell>9</cell><cell>leaks</cell><cell>2</cell><cell>new</cell><cell>floor</cell><cell>setup</cell><cell>last</cell><cell>high</cell><cell>out</cell><cell>perfect</cell><cell>few</cell><cell>really</cell></row><row><cell>10</cell><cell>inside</cell><cell>space</cell><cell>instant</cell><cell>over</cell><cell>really</cell><cell>use</cell><cell>windy</cell><cell>ground</cell><cell>family</cell><cell>year</cell><cell>thought</cell></row><row><cell>11</cell><cell>through</cell><cell>enough</cell><cell>purchased</cell><cell>rainfly</cell><cell>happy</cell><cell>day</cell><cell>day</cell><cell>side</cell><cell>big</cell><cell>days</cell><cell>wife</cell></row><row><cell>12</cell><cell>leak</cell><cell>gear</cell><cell>buy</cell><cell>door</cell><cell>pretty</cell><cell>used</cell><cell cols="2">through top</cell><cell>best</cell><cell>old</cell><cell>took</cell></row><row><cell>13</cell><cell cols="2">waterproof inside</cell><cell>not</cell><cell>great</cell><cell>big</cell><cell>second</cell><cell>not</cell><cell>plastic</cell><cell>quality</cell><cell>took</cell><cell>wish</cell></row><row><cell>14</cell><cell>issues</cell><cell>bag</cell><cell>another</cell><cell>top</cell><cell>large</cell><cell>go</cell><cell>cold</cell><cell>ripped</cell><cell>deal</cell><cell>6</cell><cell>can</cell></row><row><cell>15</cell><cell>without</cell><cell>around</cell><cell>product</cell><cell>nice</cell><cell>made</cell><cell cols="2">summer tent</cell><cell>over</cell><cell>money</cell><cell>people</cell><cell>still</cell></row><row><cell>16</cell><cell>leaked</cell><cell>lots</cell><cell cols="3">replacement sleeping quality</cell><cell>went</cell><cell>bad</cell><cell>two</cell><cell>far</cell><cell>kids</cell><cell>expected</cell></row><row><cell>17</cell><cell>seam</cell><cell>still</cell><cell>many</cell><cell>under</cell><cell>enough</cell><cell>camp</cell><cell>light</cell><cell>door</cell><cell>buy</cell><cell>couple</cell><cell>far</cell></row><row><cell>18</cell><cell>issue</cell><cell>tent</cell><cell cols="2">eight-person keep</cell><cell>seems</cell><cell>long</cell><cell>hard</cell><cell>tie</cell><cell>thing</cell><cell>ten</cell><cell>purchased</cell></row><row><cell>19</cell><cell>drop</cell><cell>side</cell><cell>again</cell><cell>part</cell><cell>super</cell><cell>week</cell><cell>strong</cell><cell>fly</cell><cell cols="2">excellent tent</cell><cell>liked</cell></row><row><cell>20</cell><cell>floor</cell><cell cols="2">comfortably made</cell><cell>main</cell><cell>sturdy</cell><cell>rained</cell><cell>hot</cell><cell>zipper</cell><cell cols="2">amazing three</cell><cell>son</cell></row><row><cell cols="12">Notes. Most frequent words (top 20) from selected topics using the AT-LDA model with C/P covariates (T = 16). Topics 3, 5, 8, 11, and 14 are</cell></row><row><cell cols="3">omitted for readability.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Explained Variance of Customer Rating</figDesc><table><row><cell>Model category</cell><cell>Model</cell><cell>Restaurants</cell><cell>Camping tents</cell><cell>Luxury hotels</cell><cell>Dog food</cell></row><row><cell>Bag of words</cell><cell>LDA</cell><cell>0.631</cell><cell>0.603</cell><cell>0.441</cell><cell>0.626</cell></row><row><cell>Topic chunking</cell><cell>SC-LDA</cell><cell>0.652</cell><cell>0.683</cell><cell>0.656</cell><cell>0.782</cell></row><row><cell></cell><cell>CPC-LDA</cell><cell>0.628</cell><cell>0.694</cell><cell>0.559</cell><cell>0.750</cell></row><row><cell>Carryover</cell><cell>AT-LDA</cell><cell>0.794</cell><cell>0.714</cell><cell>0.658</cell><cell>0.736</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Camping Tent Data: Results from Topic Regression, Using Best-Fitting Model (AT-LDA with C/P Covariates, T 16) Notes. Reported are posterior means of coefficients and credibility level. Credibility level is posterior mass away from zero. Regression coefficients credibly different from zero on 95% level are in boldface.</figDesc><table><row><cell></cell><cell></cell><cell>Posterior</cell><cell>Credibility</cell></row><row><cell>Parameter</cell><cell>Topic</cell><cell>mean</cell><cell>level</cell></row><row><cell>Covariates</cell><cell></cell><cell></cell><cell></cell></row><row><cell>β 0</cell><cell>Intercept</cell><cell>0.865</cell><cell>0.990</cell></row><row><cell>β 1</cell><cell>Problems with water leaks and</cell><cell>-5.443</cell><cell>1.000</cell></row><row><cell></cell><cell>rainfly</cell><cell></cell><cell></cell></row><row><cell>β 2</cell><cell>Tent has plenty of room for</cell><cell>4.503</cell><cell>1.000</cell></row><row><cell></cell><cell>people</cell><cell></cell><cell></cell></row><row><cell>β 3</cell><cell>I can't recommend this tent</cell><cell>−1.774</cell><cell>0.996</cell></row><row><cell>β 4</cell><cell>Returned tent to Amazon</cell><cell>-7.335</cell><cell>1.000</cell></row><row><cell>β 5</cell><cell>Needs better instructions</cell><cell>-2.136</cell><cell>1.000</cell></row><row><cell>β 6</cell><cell>Issues with porch and screen</cell><cell>−0.563</cell><cell>0.795</cell></row><row><cell>β 7</cell><cell>Very nice tent</cell><cell>0.021</cell><cell>0.520</cell></row><row><cell>β 8</cell><cell>Issues with door, zipper or</cell><cell>-2.928</cell><cell>1.000</cell></row><row><cell></cell><cell>window</cell><cell></cell><cell></cell></row><row><cell>β 9</cell><cell>Occasion tent was used</cell><cell>-3.000</cell><cell>1.000</cell></row><row><cell>β 10</cell><cell>Heavy weather with winds and</cell><cell>-1.474</cell><cell>0.980</cell></row><row><cell></cell><cell>storm at night</cell><cell></cell><cell></cell></row><row><cell>β 11</cell><cell>Tent can be set up easily</cell><cell>3.098</cell><cell>1.000</cell></row><row><cell>β 12</cell><cell>Poles and stakes broke</cell><cell>-8.542</cell><cell>1.000</cell></row><row><cell>β 13</cell><cell>Great tent, good price</cell><cell>8.159</cell><cell>1.000</cell></row><row><cell>β 14</cell><cell cols="2">Tent kept dry inside during rain −0.110</cell><cell>0.607</cell></row><row><cell>β 15 β 16</cell><cell>Number of people "I love it"</cell><cell>1.596 0 a</cell><cell>0.991 -</cell></row><row><cell>Cut-points</cell><cell></cell><cell></cell><cell></cell></row><row><cell>c 1</cell><cell></cell><cell>−1.773 a</cell><cell>-</cell></row><row><cell>c 2</cell><cell></cell><cell>-1.306</cell><cell>1.000</cell></row><row><cell>c 3 c 4</cell><cell></cell><cell>-0.922 −0.286 a</cell><cell>1.000 -</cell></row><row><cell>R 2</cell><cell></cell><cell>0.714</cell><cell></cell></row></table><note>a Parameter fixed for identification.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Restaurant Data: Results from Topic Regression, Using Best-Fitting Model (AT-LDA with C/P Covariates, T 10) Notes. Reported are posterior means of coefficients and credibility level. Credibility level is posterior mass away from zero. Regression coefficients credibly different from zero on 95% level are in boldface.Büschken and Allenby: Improving Text Analysis Using Conjunctions and PunctuationMarketingScience, 2020, vol. 39, no. 4, pp. 727-742, © 2020 </figDesc><table><row><cell></cell><cell></cell><cell>Posterior</cell><cell>Credibility</cell></row><row><cell>Parameter</cell><cell>Topic</cell><cell>mean</cell><cell>level</cell></row><row><cell>Covariates</cell><cell></cell><cell></cell><cell></cell></row><row><cell>β 0</cell><cell>Intercept</cell><cell>0.249</cell><cell>0.662</cell></row><row><cell>β 1</cell><cell>Really good sandwich</cell><cell>−1.133</cell><cell>0.924</cell></row><row><cell>β 2</cell><cell>This is the best pizza place</cell><cell>0.922</cell><cell>0.894</cell></row><row><cell>β 3</cell><cell>People wanted to talk to</cell><cell>-7.911</cell><cell>1.000</cell></row><row><cell></cell><cell>manager or owner</cell><cell></cell><cell></cell></row><row><cell>β 4</cell><cell>Things ordered</cell><cell>1.013</cell><cell>0.875</cell></row><row><cell>β 5</cell><cell>Various items on menu</cell><cell>0.424</cell><cell>0.707</cell></row><row><cell>β 6</cell><cell>Food and service very good</cell><cell>5.600</cell><cell>1.000</cell></row><row><cell>β 7</cell><cell>Frustration with waitress</cell><cell>-3.425</cell><cell>1.000</cell></row><row><cell>β 8</cell><cell>Layout of restaurant</cell><cell>0 a</cell><cell>-</cell></row><row><cell>β 9</cell><cell>Will not go back</cell><cell>-1.570</cell><cell>0.966</cell></row><row><cell>β 10</cell><cell>First dinner at this restaurant</cell><cell>−0.612</cell><cell>0.753</cell></row><row><cell>Cut-points</cell><cell></cell><cell></cell><cell></cell></row><row><cell>c 1</cell><cell></cell><cell>−1.643 a</cell><cell>-</cell></row><row><cell>c 2</cell><cell></cell><cell>-1.163</cell><cell>1.000</cell></row><row><cell>c 3 c 4</cell><cell></cell><cell>-0.578 0.128 a</cell><cell>0.998 -</cell></row><row><cell>R 2</cell><cell></cell><cell>0.794</cell><cell></cell></row></table><note>a Parameter fixed for identification.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">MarketingScience, 2020, vol. 39, no. 4, pp. 727-742, © 2020 </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A correlated topic model of science</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentence-based text analysis for customer reviews</title>
		<author>
			<persName><forename type="first">J</forename><surname>Büschken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="953" to="975" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cki</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimation and comparison of multiple change-point models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econometrics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="241" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling dynamic effects in repeated-measures experiments involving preference/ choice: An illustration involving stated preference analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Desarbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Hollman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Psych. Measurement</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="209" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A dynamic changepoint model for new product sales forecasting</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bgs</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="65" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topics in tweets: A user study of topic coherence metrics for twitter data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Habel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Inform. Retrieval</title>
		<imprint>
			<biblScope unit="page" from="492" to="504" />
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A cross-cohort changepoint model for customer-base analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="213" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated text analysis for consumer research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rj-H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1274" to="1306" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantic similarity based on corpus statistics and lexical taxonomy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Conrath</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/cmp-lg/9709008" />
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>submitted September 7</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ordinal Data Modeling</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Albert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science and Business Media</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Conf. Eur. Chapter Assoc. Comput. Linguistics</title>
				<meeting>14th Conf. Eur. Chapter Assoc. Comput. Linguistics<address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Linguistic Study of American Punctuation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Meyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Peter Lang</publisher>
			<pubPlace>Bern, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel measure for coherence in statistical topic models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting Assoc. Comput. Linguistics</title>
				<meeting><address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Capturing term dependencies using a language model based on sentence trees</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Internat. Conf. Inform. Knowledge Management</title>
				<meeting>11th Internat. Conf. Inform. Knowledge Management<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hidden Markov model of customer relationship dynamics</title>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lattin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="204" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Language Tech</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m">Annual Conf. North Amer. Chapter Assoc. Comput. Linguistics</title>
				<meeting><address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Current approaches to punctuation in computational linguistics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Say</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Akman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Humanities</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="457" to="469" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mining marketing meaning from online chatter: Strategic brand analysis of big data using latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tirunillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Tellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="463" to="479" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crumbs of the cookie: User profiling in customer-base analysis and behavioral targeting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trusov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jamal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="426" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topic modeling: Beyond bag-of-words</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Internat. Conf. Machine Learning</title>
				<meeting>23rd Internat. Conf. Machine Learning<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
