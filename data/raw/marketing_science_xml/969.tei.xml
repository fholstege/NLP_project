<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Massively Categorical Variables: Revealing the Information in Zip Codes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Steenburgh</surname></persName>
							<email>thomas.steenburgh@yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Andrew</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ainslie</forename><forename type="middle">•</forename><surname>Peder</surname></persName>
							<email>andrew.ainslie@anderson.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Hans</forename><surname>Engebretson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06520</postCode>
									<settlement>New Haven, Connecticut</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095 ClearInfo</postCode>
									<settlement>Los Angeles, Los Angeles, Denver</settlement>
									<region>California, Colorado</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Massively Categorical Variables: Revealing the Information in Zip Codes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.22.1.40.12847</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Marketing</term>
					<term>Categorical Variables</term>
					<term>Hierarchical Bayes Analysis</term>
					<term>Variance Components</term>
					<term>Decision Theory</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>W e introduce the idea of a massively categorical variable, a variable such as zip code that takes on too many values to treat in the standard manner. We show how to use a massively categorical variable directly as an explanatory variable.</p><p>As an application of this concept, we explore several of the issues that analysts confront when trying to develop a direct marketing campaign. We begin by pointing out that the data contained in many of the common sources are masked through aggregation in order to protect consumer privacy. This creates some difficulty when trying to construct models of individual level behavior.</p><p>We show how to take full advantage of such data through a hierarchical Bayesian variance components (HBVC) model. The flexibility of our approach allows us to combine several sources of information, some of which may not be aggregated, in a coherent manner. We show that the conventional modeling practice understates the uncertainty with regard to its parameter values.</p><p>We explore an array of financial considerations, including ones in which the marginal benefit is non-linear, to make robust model comparisons. To implement the decision rules that determine the optimal number of prospects to contact, we develop an algorithm based on the Monte Carlo Markov chain output from parameter estimation. We conclude the analysis by demonstrating how to determine an organization's willingness to pay for additional data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The main prediction problem faced by direct marketing organizations is to determine as accurately as possible the likelihood that each individual prospect will accept a given offer. This is generally accomplished either by conducting a trial campaign on a subset of the prospects or by examining how a previous group of people responded to an offer in order to find a new set of prospects that is likely to respond positively. Our study is an example of the latter method, which is sometimes referred to as clone marketing. Based on the responses of a previous cohort, we show how to identify the set of prospects from the current group that should be targeted. Direct marketers have long been aware that geographic location can be a key variable in predicting consumer behavior. Geographic location can act as a proxy for demographic variables on which only limited information has been obtained. For example, people living in a poor, rural area of Arkansas exhibit very different buying patterns than those living in an expensive area of New York City. This is due to differences in the consumers' income, education, and family size; their distance from competing retailers; their inventory-carrying capacity; and other characteristics by which residents in these areas systematically differ. The addresses themselves convey useful information about the consumers. Consequently, it is not surprising that two popular sources of data used by direct marketers are reported on the basis of geographic location: the credit history and demographic data collected by consolidators such as Experian and Claritas, and the census data collected by the U.S. government. In the former case, the Federal Trade Commission (FTC) requires these consolidators to protect the privacy of individual consumers by aggregating the individual-level data of all residents of a particular zip code or "zip +4" (the 5-digit or 9-digit code given to each address in the United States). This practice masks sensitive consumer information, and hereafter we refer to these as "masked data." Census data are similarly masked, because they are made available in the form of aggregates for each census tract. In both cases, the masked data continue to be useful even after aggregation for the reasons given in the preceding paragraph.</p><p>Direct marketers generally ignore the masked nature of these data when developing their targeted marketing campaigns. They match the prospect's zip code against the demographic variables available in the masked dataset and then treat the masked variables as if these are actually individual-level data. Using a hierarchical Bayes variance components (HBVC) model, our approach improves on this in two ways: (1) We directly incorporate the zip code, thereby allowing us to account for unobserved heterogeneity across zip codes; and (2) we use a hierarchical model to appropriately use the information at the level at which it is collected, i.e., at the level of the zip code rather than of the individual. We also show that the conventional modeling practice, in ignoring the data masking, understates the uncertainty with regard to its parameter values.</p><p>Our methodology extends beyond zip codes, which are just one example of a larger class of variables. We introduce the idea of a massively categorical variable, a variable such as zip code that takes on a very large number of categorical values, and demonstrate how the HBVC model uses massively categorical variables directly as explanatory variables. This type of variable is in no way limited to being used in choice models, and the approach taken here easily extends to other applications. Other examples of massively categorical variables include using actor, director, or distributor as independent variables when predicting the performance of a movie; and using SEC codes as predictors when modeling brand equity as a component of market capitalization.</p><p>Our research tackles one further, important problem. In the previous literature, the issue of a broadly applicable framework for determining the financial gain of using superior methodologies or datasets has not been addressed. This makes determining the value of the gain from using different models difficult. We develop a decision theoretic framework to measure the competing models on a monetary basis in addition to a statistical one, as we want to judge whether a significant practical difference exists between the modeling techniques. Our approach can be used to explore an array of financial considerations (including ones in which the benefit is nonlinear in the number of positive responses) to make the comparisons as robust as possible. In the nonlinear case, we develop an algorithm based on the Monte Carlo Markov chain output to implement the decision rules that determine the optimal number of prospects to contact. We conclude the analysis by demonstrating how to determine an organization's willingness to pay for additional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Literature</head><p>Using geography as a basis for understanding marketing issues has recently received attention from several authors. Two examples include <ref type="bibr" target="#b3">Bronnenberg and Sismeiro (2002)</ref> and <ref type="bibr" target="#b14">Yang and Allenby (2002)</ref>. The methodology used in these papers makes use of the behavior in surrounding cells to assist in predicting the behavior in a cell of interest. The former does this strictly on the basis of geography, whereas the latter uses several different social and geographic bases. These methods are similar to autoregressive models, but use geography rather than time as a basis for forming correlations between cells.</p><p>Our approach is different in that we do not use an autoregressive type of scheme. Instead, it is closer in spirit to the method of <ref type="bibr" target="#b8">Hoch et al. (1995)</ref>, who used a hierarchical model to demonstrate the importance of using geographic location and demographic data in determining price elasticities. The advantages</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STEENBURGH, AINSLIE, AND ENGEBRETSON</head><p>Massively Categorical Variables of our approach to the problem are: (1) It is broadly applicable to a far wider class of categorical regressors than geographic ones; (2) in situations where zip codes are isolated because all or many of the surrounding zip codes are not represented in the data, the spatial models are less effective; and (3) We are able to include hierarchical regressors, whereas it is unclear how one would include these directly in spatial models. Hierarchical variables have two important uses: First, they improve the predictive power of the model; second, the parameter values allow marketers to better understand their target market, assisting with segmentation decisions. We leave it to future research to determine the relative effectiveness of the two approaches or to incorporate them into a single model. Very little academic research has been devoted to the target selection problem in direct marketing. In a notable exception, <ref type="bibr">Bult and Wansbeek (1995)</ref> develop a profit maximization approach to select prospects for a mailing campaign. They demonstrate how to determine appropriate cutoff values assuming a constant marginal benefit from positive responses. They also make assumptions about the distribution of independent variables and various R 2 values in developing these rules. <ref type="bibr" target="#b13">Wilcox and Hsu (2000)</ref> demonstrate the importance of accounting for uncertainty in parameter values when predicting outcomes in Logit models. Our work extends that done in both of these papers by making less restrictive assumptions than <ref type="bibr">Bult and Wansbeek (1995)</ref>, allowing for a broad range of profit functions, and accounting fully for the distribution of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Prediction Model</head><p>In our application, we address the task of determining the likelihood of enrollment of prospective students at a large university in Texas so that the institution can better target its recruiting efforts. Information is gathered on the previous year's students in order to predict the likelihood of enrollment of the current prospects. <ref type="bibr">1</ref> The data available for this analysis include the prospect's zip code (and, separately, demographic information for each zip code), the intended major of the student, and information collected by the university on the frequency and nature of contacts between the university and the prospect. This is discussed in detail in §5.</p><p>Organizations that want to conduct targeted marketing campaigns currently have numerous sources of information available to facilitate their predictions. These sources can be broadly split into two types. The first type of data is collected by the firm on its interactions with individual patrons. Marketing groups take great care to coordinate the data collection efforts among departments and make substantial expenditures to retain accurate records. In general, these data are often powerful because they are specific to individual patrons and are unique to that firm, allowing them to differentiate themselves from competitors. Unfortunately, many times they are not available. For example, historical data do not exist when a firm wants to extend its reach to new prospects. In our application, the university collects information on the frequency and type of interactions between students and the university, which we call "visitation data."</p><p>The second type of data is purchased from an outside firm. Meeting the need for comprehensive information, data consolidators and others make supplementary databases available for a fee. Although these sources tend to improve predictions about how people will respond to an offer, they present methodological challenges as well. The variables contained in these databases are commonly masked through aggregation to protect the privacy of individual consumers, making statistical models of individual behavior more difficult to construct. (These are the previously described masked data.) We propose that the conventional modeling technique not only fails to extract all of the information contained in supplementary databases, but also that some of the information can be revealed without purchasing any data at all, establishing a higher baseline from which to judge the value of acquiring additional data.</p><p>The masked data are constructed to be as useful as possible while still protecting the individuals' privacy. An implicit assumption for this to be true is that a reasonably high level of homogeneity exists among</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STEENBURGH, AINSLIE, AND ENGEBRETSON</head><p>Massively Categorical Variables the individuals within the unit of aggregation. Zip codes, for example, make a good basis of aggregation because, as commonplace observation suggests, people tend to live near others similar to themselves. Taking this idea further, we propose that the prospects' zip codes themselves may be useful because they convey a meaningful group membership. Similarities not observed in the supplementary data exist among the prospects within a zip code, and we should take advantage of these associations whether we have the supplementary data or not.</p><p>A final modeling challenge arises when analysts need to combine information from multiple sources. The difficulty occurs either when some of the information is masked while some is not, or when multiple sources are masked but at different levels of aggregation. Given the previous discussion, for example, an analyst may want to augment the organization's own data on individual prospects with a masked source purchased from a consolidator. <ref type="bibr">2</ref> The standard technique used to construct a complete database is simply to append the consolidator's data to the organization's own source on the basis of each individual's zip code. The entire record is then treated as individual-level data in the subsequent analysis, ignoring that some of the data are masked.</p><p>The conventional modeling technique can be expressed in the following manner. Assume that the relationship between the variable of interest y i and the explanatory variables for each individual is given by</p><formula xml:id="formula_0">y i = X i W z i T + i i ∼ N 0 1 for i = 1 N (1)</formula><p>where, in our case, y i represents a student's latent utility, and a student enrolls in the university if his or her latent utility exceeds zero. <ref type="bibr">3</ref> The index i corresponds to an individual, and the index z i denotes the zip code of the ith individual. The vector X T i represents the explanatory variables for individual i that are found in the organization's own database, and these are unique to each individual. The vector W z i represents the masked variables found in the consolidator's data. Every individual residing in a zip code shares the same record of information, and the masked data are repeatedly used in the regression when multiple people reside in a zip code. The vectors and are the parameters of interest in estimation.</p><p>The conventional technique simply concatenates the individually measured variables X with the masked variables W and thereafter treats all variables as if they were measured appropriately. We refer to this as the null model and note that the Bayesian estimation of it, using sufficiently diffuse prior distributions, yields parameter estimates of and that are equivalent to those found through maximum likelihood estimation. For a description of how to estimate a Bayesian Probit model, see <ref type="bibr" target="#b1">Albert and Chib (1993)</ref>; for a general introduction to the MCMC methods used in this paper, see <ref type="bibr" target="#b5">Gelman et al. (1995)</ref>.</p><p>The null model can be criticized in a few respects. First, we do not truly have N unique, individualspecific observations of the explanatory variables W , but rather only Z zip code-specific observations (Z &lt; N . 4 Subsequently, greater uncertainty exists in the posterior distributions of the parameter estimates than is found through the null model. Next, the null model assumes that the observed component W z i explains all of the variation in student responses that can possibly be explained by the masked data, overlooking the possibility of an unobserved component of variation. As systematic differences exist among people across zip codes, the zip codes themselves may reveal something useful about the prospects not captured in the data. We should take advantage of this indicator of group membership if it is meaningful, but we would expect the null model to compete reasonably well with the alternative if it is not.</p><p>In contrast to the standard practice, we develop a hierarchical Bayesian variance components model 5 to solve the problems created by using masked variables. We are the first to describe the problem of masked data, and our analysis extends the use of established hierarchical Bayesian modeling to this new area of research. The variance components approach allows us to treat each source of data, whether it is masked or not, at the appropriate level of aggregation. Furthermore, the variance decomposition helps us to understand the value of each source of information. The adaptive shrinkage inherent in our Bayesian specification ensures that we get useful parameter estimates even when a zip code contains a few individuals.</p><p>The HBVC approach to the estimation problem is given by</p><formula xml:id="formula_1">y i = X T i + z i + i i ∼ N 0 1 for i = 1 N z = W T z + z z ∼ N 0 V z for z = 1 Z (2)</formula><p>The indices i and z i and the vector of data X T i represent the same constructs that they do in the null model. We need some new notation to reflect our treatment of the masked variables and introduce the subscript z to correspond with the zip codes. The subscripts z and z i can be thought of as labels that both refer to zip codes, with z being used to refer to the zip code in general and z i being used to refer to the zip code in which the ith prospect resides. The vector W T z represents the masked demographic variables that are taken from the consolidator's data for zip code z, and the scalar parameter z represents the estimated zip code effect. The observed variation of z across zip codes is described by the term W T z , where the parameter vector is found through the hierarchical regression of zip code effects onto the <ref type="bibr">5</ref> Ainslie and Rossi (1998) is an HBVC framework in a choice model that is similar in nature but different in application. Whereas they break down the heterogeneity of consumer preferences into components that are common across and unique to supermarket categories, we use the HBVC approach to break the variance of enrollment probabilities into components associated with different sources of data. demographic data. The masked variables are treated as only Z unique observations, which differ from their treatment as N observations in the null model. The unobserved variation is captured through the parameter v z .</p><p>It is straightforward to compare the distributions of the effects associated with the masked variables in the two models. The conditional variance of in the null model is</p><formula xml:id="formula_2">N i=1 V −1 0 + W T z i W z i −1 , whereas the conditional variance of in the HBVC model is Z i=1 V −1 0 + W T z W z −1 . The N − Z repeated records ensure that W T z i W z i &gt; W T</formula><p>z W z and that the distribution of the null model is tighter than that of the HBVC model. Since we might find the marginally significant parameters in the null model to be in fact statistically insignificant if the repeated records are not treated as unique observations, the null specification can lead to misguided inference.</p><p>We now turn our attention to getting as much as we can out of the firm's data when no additional information has been bought. The underlying objective is to establish an adequate baseline from which we can assess the value of purchasing supplementary data. In the preceding exposition, we suggest that knowing where a prospect resides is useful because zip codes convey a meaningful association. This knowledge should be useful even when no further data have been bought. The university always possesses its prospects' zip codes, and the open question is how to take advantage of the prospects' group membership. This membership can be thought of as a categorical variable with a very large number of categories, too many to treat them as standard dummy variables.</p><p>We can amend the HBVC model to use the massively categorical variables directly as explanatory variables, allowing us to estimate the zip code effects without purchasing supplementary data. This model should explain less variation than one with the benefit of supplementary data, but the massively categorical variables should replicate some of the information provided in the supplement. The model would be revised as</p><formula xml:id="formula_3">y i = X T i + z i + i i ∼ N 0 1 for i = 1 N z ∼ N 0 V z for z = 1 Z (3)</formula><p>STEENBURGH, AINSLIE, AND ENGEBRETSON Massively Categorical Variables when no data are purchased from the consolidator, but we use the massively categorical variable zip code directly as an explanatory variable. The heterogeneous zip code effects z are restricted to be distributed about zero for identification. Reducing X T i to a vector of ones results in a model that includes only the massively categorical variables as explanatory variables.</p><p>In our particular dataset, in addition to knowing the prospects' zip codes, the university also knows their intended major. Adding another component to the model for this massively categorical variable can be accomplished even though zip codes and majors are not generally nested. <ref type="bibr">6</ref> A fully comprehensive model uses all of the data and is expressed as</p><formula xml:id="formula_4">y i = X T i + z i + m i + i i ∼ N 0 1 for i = 1 N z = W T z + z z ∼ N 0 V z for z = 1 Z m ∼ N 0 V m for m = 1 M (4)</formula><p>The subscripts m and m i , respectively, refer to the intended major and the intended major of the ith prospect. The heterogeneous major effects m are directly analogous to the zip code effects in the previous model. While intended majors are particular to this application, their inclusion in the model demonstrates how multiple massively categorical, masked, and unmasked variables can be simultaneously incorporated in the same model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Direct Marketer's Decisions</head><p>We now move on to determining how to evaluate different modeling or data choices under different scenarios. In general, the direct marketer wants to contact the set of prospects that maximizes its expected gain, balancing the possible money gained from greater enrollment against the certain money lost from contacting additional prospects. Without 6 Suppose we are using both the prospects' zip codes and their intended majors as massively categorical variables. Now consider an English major applicant in zip code 13021. Neither are all other residents of zip code 13021 necessarily English majors, nor do all other English majors necessarily reside in zip code 13021.</p><p>loss of generality in terms of the applicability of these techniques to the broad range of problems encountered by direct marketers, for the remainder of this section we refer to the university's particular problem of targeting prospective enrollees.</p><p>Let the function h s for s ∈ 0 1 2 , which we refer to as the benefit function, represent the benefit to the university when s individuals choose to enroll. Define s ≡ h s − h s − 1 for s ∈ 1 2 3 as the marginal benefit of the sth enrollee. In order to develop a general argument, we merely assume that the marginal benefit of enrollment is non-increasing:</p><p>s + 1 ≤ s for s ∈ 1 2 3 . Let c represent the marginal cost of contacting prospects and assume that it is constant. Financial considerations determine the values of both h s and c before any statistical analysis is undertaken.</p><p>Suppose the university is trying to determine which prospects to contact from a group of M individuals. Let R i be a Bernoulli random variable that represents the response of the ith prospect if he or she is contacted, where i is the chance of a positive response. The sum S m = m i=1 R i represents the number of positive responses when prospects 1 to m are contacted. If the probabilities of enrollment were the same for all individuals, S m would simply follow a binomial distribution.</p><p>Given the heterogeneous probabilities of enrollment, we have to use a recursive formula to obtain the distribution of S m . We find the probability mass function of S m , when contacting m ∈ 1 M prospects, through the recursive step</p><formula xml:id="formula_5">p S m s = m p S m−1 s − 1 + 1 − m p S m−1 s for s ∈ 0 m (5)</formula><p>and the end condition We begin by looking at the decision problem as if the prospects' probability of enrollment were known with certainty. The expected benefit from contacting the first m prospects, conditional on = 1 M , is</p><formula xml:id="formula_6">E h S m = m s=0 h s p S m s = E m h S m−1 + 1 + 1 − m h S m−1 for m ∈ 0 M (6)</formula><p>Given this relationship, the expected marginal benefit from contacting to the mth prospect is</p><formula xml:id="formula_7">m ≡ E h S m − h S m−1 = m−1 s=0 m s + 1 p S m−1 s for m ∈ 1 M (7)</formula><p>(Derivation one in the appendix.) Expressed in words, the expected marginal benefit of the mth enrollee is the expected benefit of an additional enrollee given that the first m − 1 contacts resulted in s enrollments, m s + 1 , weighted by the chance that the first m − 1 contacts resulted in s enrollments.</p><p>Using the expected marginal benefit, we can develop a decision rule that sequentially sorts through the prospects to maximize the university's expected profit. The university should contact the mth prospect if the expected marginal profit from doing so is positive, specifically if m − c &gt; 0. Expressing this inequality in terms of the prospects' probabilities of enrollment, the university should contact the mth prospect if</p><formula xml:id="formula_8">m − m ≥ 0 where m ≡ c m−1 s=0 s + 1 p S m−1 s for prospects m ∈ 1 M (8)</formula><p>Following <ref type="bibr">Bult and Wansbeek (1995)</ref>, we refer to m as the cutoff function because it contains the cutoff values against which the prospects are assessed.</p><p>The cutoff function does not depend on the probability of enrollment of the prospect under consideration, but it does generally depend on the probabilities of enrollment of the m−1 prospects previously considered. (As will be shown in the examples, the previously considered prospects do not play a role when the marginal benefit of enrollment is constant.) Since m does not depend on m , ordering the prospects from the highest probability of enrollment to the lowest ensures that m exceeds m by the greatest possible amount for every m. The ordering also ensures that if the mth prospect exceeds the cutoff value, all of the previously considered prospects will exceed it too and we not need reconsider them. Furthermore, since m is a weakly increasing function of the number of prospects contacted (proof in the appendix), once the mth prospect falls below its cutoff value, every subsequent prospect will fall below theirs too and we can stop considering additional prospects at this point. We now turn to developing some specific cutoff functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example One: A Constant Marginal Benefit from Enrollment</head><p>We begin by considering the standard benefit function applied in direct marketing problems, a form that corresponds with the one used in <ref type="bibr">Bult and Wansbeek's (1995)</ref> analysis. Suppose the marginal benefit of enrollment is constant regardless of the number of enrollees. Specifically, h s = bs and s = b for s ∈ 0 1 2 , where the constant b represents the marginal benefit of an additional enrollee and the variable s represents the number of people who actually enroll. Since s = b for s ∈ 1 2 3 , we can directly see that the cutoff function is</p><formula xml:id="formula_9">m = c/ b for m ∈ 1 2 M .</formula><p>This results in the familiar decision rule of contacting the mth prospect if his or her probability of enrollment is greater than the ratio of the marginal cost to the marginal benefit. </p><formula xml:id="formula_10">= b s j=1 1 − d j−1 for for s ∈ 1 2 3</formula><p>, where b &gt; 0 and 0 &lt; d &lt; 1. The marginal benefit of the first enrollee is b and the rate at which the marginal benefit diminishes over additional enrollees is d. This problem simplifies to the constant marginal benefit case when d equals zero. An example of such a cutoff function might be a case in which willingness to pay or contribute is proportional to the probability of responding. In a catalog marketing application, it is often the case that high probability customers tend to purchase more. The benefit function implies that s + 1 = 1 − d s for s ∈ 1 2 3 . A recursive relationship exists in the cutoff function such that</p><formula xml:id="formula_11">m + 1 = 1/ 1 − d m m for m ∈ 1</formula><p>M (derivation three in appendix) and 1 = c/b. As a result, the cutoff function for the mth prospect is</p><formula xml:id="formula_12">m = c/b for m = 1 c/ b m−1 i=1 1 − i d for m ∈ 2 M .</formula><p>Our knowledge regarding the probabilities of enrollment is not perfect. Following standard decision theoretic arguments, 7 we account for the parameter uncertainty by examining the posterior expected profit of contacting a prospect. The posterior expected difference between the mth prospect's probability of enrollment and the cutoff value is E data m − m =˜ m −˜ m , where˜ m and˜ m are the posterior means of m and m , respectively. Because m does not depend on m , ordering the prospects on the basis of their posterior means ensures that˜ m exceeds˜ m by the greatest possible amount for every m. The ordering also ensures that if the mth prospect makes the cutoff, all the previously considered prospects will make it, too, and we not need reconsider them. Furthermore, since˜ m is a weakly increasing function of the number of prospects contacted, once the mth prospect falls below its cutoff value, every subsequent prospect will fall below theirs too and we can stop considering additional prospects at this point.</p><p>We return to the examples to clarify how the decision rules are implemented. The decision rule in the first example is straightforward to implement because it is a comparison of the posterior mean˜ m against a fixed and known cutoff value. The decision rules in the second and third examples are more difficult to implement because m is a nonlinear function of . Finding˜ m requires taking the expectation over the joint posterior distribution of 1 m−1 . We use the following algorithm based on the MCMC draws to accomplish:</p><p>1. After obtaining T draws of the vector t from the sampler, compute the posterior mean˜ and reorder the prospects from the highest probability of enrollment to the lowest. Let m represent the position of the prospects in this reordered list.</p><p>2. Using the replicates t 1 t m−1 of the now reordered prospects, compute the M cutoff values m t for each of the T draws.</p><p>3. Compute the M posterior means of˜ m and implement the decision rule to find the optimal number of prospects to be contacted.</p><p>For the second example, step 2 of the algorithm is executed using the recursive relationship of the probabilities described in Equation (5). For the third example, step 2 of the algorithm is executed using the recursive relationship of m . The university's willingness to pay for the supplementary data (WTP) is the difference between the posterior expected gain from the decisions made in light of all available data, including the consolidator's supplement, and the posterior expected gain from the decisions made in light of only the university's own source. The supplementary data improve our knowledge by allowing us to observe a component of variation (W T z in the zip code effects, thereby reducing our uncertainty about them. The data's worth is formally expressed as</p><formula xml:id="formula_13">W TP = E i X W E h S m * − m * c − E i X E h S m * * − m * * c (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where m * and m * * denote the optimal decisions made by the university, respectively, in light of the augmented and unaugmented databases. As the equation suggests, a valuable supplementary database necessarily results in helping the university make better decisions.</p><p>A few additional points should be made in concluding this section. First, we have assumed that the direct marketer knows the parameter values associated with the decision problem. For example, the marginal cost is assumed to be the constant value c in these problems. This usually is a good approximation; for example, the cost of producing and mailing a brochure, within reasonable bounds, remains constant and is well understood by the marketer. If the parameters are not perfectly known, a sensitivity analysis is relatively easy to perform over a range of values to aid in the decision making. Second, we have assumed specific functional forms for the marginal benefit function s in the examples. If an alternative functional form is preferred, the direct marketer either can find an analytical solution of the cutoff function or can numerically calculate it using Equation (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The Empirical Results</head><p>We base our study on data that were collected by a private, southern U.S. university that plans to assess its prospects at the inquiry stage of the admissions process. The students have requested information about the university at this point, but they have not yet decided whether they will apply for admission. We split the dataset randomly into halves, using 37,551 prospects for model building and 34,179 prospects for out-of-sample model comparisons. These students reside in 7,279 zip codes, and each has declared an interest in one of 128 majors.</p><p>Both the zip codes and the intended majors are used as massively categorical variables, and the students' responses of "undecided" and "no response" for the intended major are treated separately.</p><p>Table <ref type="table" target="#tab_0">1</ref> describes the explanatory variables that we use in the various prediction models. The university has the opportunity to purchase a supplementary data source that contains over 200 variables that are masked by zip code. From the many variables that are available in this source, we select the four that produce the best prediction results in the null model. While the issue of variable selection is outside of the scope of this paper, we note that adding more variables than these generally diminishes the out-of-sample performance. The university also collects some information about the individual prospects on its own, and we examine its decision problem under two scenarios-one in which it collects campus visitation data and one in which it does notto more generally reflect the conditions that confront analysts. This approach to the problem results in four information sets based on (1) whether the university is able to collect campus visitation data, and (2) whether the university purchases supplementary data from a consolidator. We compare the HBVC against the null model in each of the four possible cases to demonstrate the incremental benefit of using the HBVC model. All models were run initially for 10,000 iterations as a run-in period, then a further 10,000 iterations for obtaining posterior distributions. We tested convergence in several ways, including comparing results between earlier and later portions of each run and visual checks, and are confident that we were very conservative in the lengths of burn-ins and runs selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The Statistical Comparison of the HBVC</head><p>Model Against the Null We begin the empirical analysis by applying the standard Bayesian hypothesis testing procedure to the problem of model selection. This procedure does not require nested models, produces results that are easy to interpret, and is coherent with foundational principles. A Bayes factor, which globally compares the models' performance, summarizes the evidence provided by the data for the alternative model against the null. We use a numerical method proposed by <ref type="bibr" target="#b10">Newton and Raftery (1994)</ref> to evaluate the Bayes factor, and the computations are based on the MCMC output produced through model estimation.</p><p>We report both the Bayes factors and log-marginal likelihoods in Table <ref type="table" target="#tab_1">2</ref>. The Bayes factors are expressed in terms of 2 log B to put them on a scale that is common with familiar measures such as the deviance or the likelihood ratio test statistics. A value of zero suggests equal evidence for both models, and a value greater than ten suggests "very strong evidence" in favor of the HBVC model. Raftery presents a full calibration of values between zero and ten in <ref type="bibr">Gilks et al. (1996, p. 165)</ref>. We find that the data provide very strong evidence for the HBVC model in every case when the same amount of information is used to estimate both models. The smallest value, 586, well exceeds a standard of 10. Moreover, the data provide very strong evidence that the HBVC models without supplementary information are superior to the corresponding null models with it. This suggests that using a superior modeling technique can be more important than purchasing more information. Finally, we see that having supplementary information makes a statistically significant difference in performance, as the HBVC models with the benefit of the data are superior to those without it. This first statistic was calculated in-sample. For the remainder of this section, all statistics are calculated out-of-sample, as this ensures that improved performance is not simply a result of over-fitting. In addition to assessing the overall out-of-sample fit of the model, we also are interested judging how well it orders the individual prospects. Being able to attain the best possible ordering from the information at hand is crucial in direct marketing because the prospects lowest on the list will be dropped from consideration. For example, even though the data suggest it doesn't fit as well, we still might prefer the null model if it provides a better ordering but systematically overstates the likelihood of enrollment. With this objective in mind, we use the holdout data to construct receiver operating curves, or ROC charts, to compare the various models. An ROC chart is constructed by repeatedly dividing the prospects into two groups on the basis of their probabilities of enrollment. <ref type="bibr">8</ref> Prospects with probabilities below the cutoff are placed in one group, and prospects with probabilities above the cutoff are placed in the other. After this has been done for all cutoff points between zero and one, we then graph the number of enrollees not selected against the number of non-enrollees selected Visitation/ Supplementary Data for each division. (We note that the cutoff points used in the ROC chart are not determined by the cutoff functions previously described. They are merely a series of fixed points used to divide the prospects.) Not only is the ROC chart a robust comparison because it is constructed over all possible cutoff points, but it also is very easy to interpret. The better the model, the more the curve will move toward the bottom-left-hand corner of the chart; or, put another way, the more the area under the curve will tend toward zero. For more information on ROC charts and their interpretation, see <ref type="bibr">Swets (1995)</ref>. In Figure <ref type="figure">1</ref>, we construct ROC charts to compare the HBVC model (solid line) against the null model (dotted line) under the four information conditions. In Table <ref type="table" target="#tab_2">3</ref>, we report the area under each curve, which can be thought of as a summary measure describing the model's ability to order prospects.</p><p>Summarizing the results, we find that the HBVC model always provides a better ordering of the prospects when the same amount of information is used to estimate both models. This claim is based both on a visual inspection of the ROC charts and on a direct comparison of the summary measures. For example, in the case where neither the visitation nor supplementary data are available, the area under the curve for the HBVC model is only 0.292, whereas for the null model it is 0.402. We also find a greater difference exists between the models when the amount of information used to estimate them is lower. This  implies using a superior modeling technique becomes more important when the university has a limited amount of information with which to work. In several panels, the difference between lines appears small. However, even small differences in the ROC charts can lead to large differences in profitability for a direct marketer, as will be demonstrated in the next section. Finally, based upon the ROC summary measures in Table <ref type="table" target="#tab_2">3</ref>, we claim that the HBVC models without supplementary information provide a better overall ordering of the prospects than do the null models with it. In all respects, the ROC charts confirm what we found in the model selection hypothesis testing.</p><p>The null model leads the researcher to be overly confident of where the parameter values lie. The null model repeatedly uses the masked data as if they represent the individuals' actual characteristics, and it understates the posterior variance as a result. To demonstrate this, we summarize the posterior distributions of both models, which are estimated using all of the available information, in Table <ref type="table" target="#tab_3">4</ref>. Three statistics are presented for all variables: the mean of the posterior distribution, its standard deviation, and the size of the 95% highest posterior distribution (HPD) width. The last of these statistics is a good measure of parameter dispersion for non-symmetric distributions.</p><p>The posterior means are similar across the null and HBVC models; the biggest exceptions to this are the housing variables BLT_50 and BLT_80. These variables have a high negative correlation (−0 67 in the null model, −0 55 in the HBVC) that may account for the disparity in the parameter estimates. The posterior standard deviations and HPD widths for the visitation variables, which are not masked, also are very similar. This is to be expected because the models are essentially identical for these variables. The posterior standard deviations and HPD widths of masked variables, on the other hand, are much tighter in the null model than they are in the HBVC model. The excessive confidence found in the null model can lead to misguided inference, because statistically insignificant parameters may erroneously be found significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">The Empirical Decision Analysis</head><p>Having shown that the HBVC model is statistically superior to the null, we now examine how choosing a model affects the university's decisions, and we confirm that a significant practical difference in the generated profit exists between models. Once more, all these calculations are based on the model's ability to predict out-of-sample. We begin the discussion by illustrating how the university identifies which prospects to contact. In Figure <ref type="figure">2</ref>, we graph the posterior means of the probabilities of enrollment and the cutoff values against the ordered prospects. To construct this diagram, we have assumed the diminishing marginal benefit function from the third example 9 and use the estimation results from the "No Visitation/With Supplementary Data" case. We graph the cutoff function for several values of d to show how the discount rate affects the university's decision.</p><p>The arrows represent the appropriate cutoff score for d = 0 04% (the leftmost arrow), 0.02% and 0 (the rightmost arrow) A simple geometric interpretation of the diagram explains the university's decision process. The point at which the probabilities of enrollment and the cutoff curves intersect determines the optimal number of prospects to be contacted, as the university should contact the first m * prospects such that the expected marginal benefit of enrollment exceeds the marginal cost contacting the prospect. Reducing the rate at which additional enrollees are discounted flattens the slope of the cutoff function, and the university contacts more prospects. At d = 0 (the constant marginal benefit case) the cutoff function becomes a line of slope zero. With regard to the other financial conditions, reducing the ratio of the marginal cost to the marginal benefit shifts the cutoff function downward, and again the university contacts more prospects. At c b = 0, every prospect with a positive chance of enrollment is contacted.</p><p>The HBVC and null models' influence on the university's decisions is clear. The HBVC model provides greater discrimination among the prospects because greater variation exists in the probabilities of enrollment. Subsequently, both the probabilities of enrollment and the cutoff curves have a steeper slope, and fewer prospects are contacted when using the HBVC model rather than the null, no matter the discount rate. By itself, the diagram does not suggest that using the HBVC model is more profitable because it does not judge whether the model accurately determines which prospects are best. Nonetheless, when coupled with the ROC chart analysis that shows the HBVC</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STEENBURGH, AINSLIE, AND ENGEBRETSON</head><p>Massively Categorical Variables model provides a better ordering of the prospects, the diagram suggests that the HBVC model cuts out an unprofitable segment of prospects and should be more profitable. We use predictive distributions to assess this notion directly.</p><p>The posterior predictive distributions in our analysis are based on the prospects in the holdout sample. We predict the responses of individual prospects, the number of prospects that enroll when the university acts optimally, and the profit arising from each model under various financial constraints. In doing so, we are assessing the models' performance at the point at which decisions are being made. LetS m * represent the predicted number of enrollees when the university acts optimally by mailing to m * of M prospects. The inverted cap on Srepresents that the predicted number of enrollees, being a response from the holdout prospects, is yet unobserved. In the universally diminishing marginal benefit case, the posterior predictive distribution of profit can be expressed as</p><formula xml:id="formula_15">Profit S m * = b  S m * j=1 1 − d j−1 − m * c/b  </formula><p>We express the profit in this manner so that we can reduce the number of financial parameters from three to two in our diagrams.</p><p>The procedure to obtain a sample from the posterior predictive distribution,S m * , for the prospects in the holdout sample entails several steps. First, using both the characteristics of the M prospects in the holdout sample and the T draws of the parameters in the probit model, calculate the T draws of the vector of enrollment probabilities˘ t . Second, determine the optimal number of prospects to contact m * given the probabilities of enrollment. Finally, use a Bernoulli random variable to draw the predicted responsesȒ t of the contacted prospects given the probabilities of enrollment˘ t . The predictive valueS t m * is the sum of the m * responses on the tth iteration of the sampler; the collection of predictive values is a sample from the posterior predictive distribution.</p><p>In Figure <ref type="figure">3</ref>, we graph the expected percent of profit lost from using the null model. (We report the per-cent of profit lost from using the null model instead of the percent of profit gained from using the HBVC model because the expected total profit from using the null model is zero sometimes.) We assume an array of financial conditions to construct the diagram. The marginal cost to marginal benefit ratio c b ranges from 0 to 0.1 in increments of 0.005. Near zero, the model choice should have very little influence on the total profit gained because all of the prospects with a positive chance of enrollment are contacted. The discount rate of additional enrollees d ranges from 0 to 0.04% in increments of 0.002%. At zero, the graph describes the degenerate constant marginal benefit case. Our assumptions result in having to compute 1,000 posterior predictive distributions ofS m * in each of the four information conditions.</p><p>The four diagrams show that the HBVC model makes a significant practical difference in the university's profit. Averaged over the four information sets and the array of financial conditions, the expected percent of profit lost is 43.6%. The expected loss from using the null model is greatest when the university has the least amount of information on which to base its predictions, such as when no campus visitation data are available.</p><p>We notice an anomaly in the expected profit calculations when c b = 0, particularly near d = 0, when both the campus visitation and the supplementary data are available. The expected loss is negative, but clearly it should be zero, as both models recommend that all prospects should be contacted. This suggests that at least one of the models does not accurately predict the amount of profit to be gained. We test the predictions of the model against the actual data by calculating the proportion of profit values simulated from the posterior predictive distribution that are greater than the realized profit (calculated using the actual number of holdout prospects that enroll rather than the predicted number of enrollees). In the area of the diagram where the expected loss is negative, we find that the realized data plausibly could have come from the HBVC model, as 35.8% of the predicted profit values are greater than the actual profit Visitation / Supplementary Data realized from the holdout prospects (a posterior predictive p-value of 0.358). The realized data, on the other hand, do not seem to come from the null model because 100% of the predicted values are greater than the realized value. The null model over predicts the amount of profit generated in this region, which we attribute to the mull model's excessive certainty about the parameter values, reflected in the overly tight HPDs in Table <ref type="table" target="#tab_3">4</ref>.</p><p>This raises an interesting concern with using the predictive distributions to compare the models. While we would like to use the difference between the expected posterior predictive profits to compare the models because they are not specific to the responses found in the dataset, the comparison is useful only if both models provide accurate predictions. We note that even though the null model does not tend to over-predict the profit on the whole, a concern still remains about the usefulness of the comparisons.</p><p>To alleviate any concern about the predictive accuracy of the models, we graph the realized percent of profit lost from using the null model in Figure <ref type="figure">4</ref>. The realized profit is a noisier measure than the average predictive profit because its value is based on the actual enrollment decisions of the prospects in the holdout sample rather than their expected responses. On the other hand, it represents the actual difference in profit that would be realized if the university takes the actions prescribed by the models for the holdout sample. The probabilities influence the decision to target a prospect but do not determine the prospect's value in this measure. The realized profit is useful when comparing different modeling techniques because a prospect's value is not inflated by an overly optimistic prediction about whether he or she will enroll, and it is particularly helpful when the underlying assumptions of any of the models are brought into question. In this case, the realized profit diagrams confirm what we found in the expected </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Research</head><p>In this paper, we describe the problems that arise from using the masked data sold by consolidators. We develop a flexible statistical model to take full advantage of the information contained in masked data and show that the conventional modeling practice, which ignores the masking, is overly certain about where the parameter values lie. We also introduce the idea of a massively categorical variable and show how it can be used to replicate some of the information contained in a consolidator's database. We examine the competing models over an array of financial conditions in order to make robust comparisons. In the case of a nonlinear benefit function, we demonstrate how to use the MCMC output from parameter estimation to implement the decision rule that determines how many prospects are contacted. This research might proceed in several directions. The first promising area is to develop a hierarchical Bayes technique to select variables from the abundant databases that are sold by consolidators. Variable selection continues to be a major concern for direct marketers, and it is clear that conventional methods cannot be used when some of the variables are masked. The second area is to consider incorporating semiparametric or nonparametric techniques (such as those in Bult 1993) within the Bayesian model's hierarchy. Direct marketers often suspect that many variables have complex, nonlinear relationships with the dependent variable, which has led to the widespread use of neural nets and similar methodologies. Semiparametric approaches might allow for such complexity without the disadvantages of neural nets. Finally, we foresee the use of massively categorical variables in developing models of consumer behavior on the Internet. Just as zip codes provide useful information in our application, Web-page addresses might do the same in others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derivation One</head><p>We want to derive Equation ( <ref type="formula">7</ref>). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>p S 0 s = 1 for s = 0 0 otherwise where p S m s ≡ Pr S m = s . The terms of Equation (5) result from the two possible ways that s positive responses can arise from contacting m prospects: Either (1) s − 1 positive responses are produced from the first m − 1 contacts and the mth prospect responds positively, or (2) s positive responses are produced from the first m−1 contacts and the mth prospect does not respond positively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Example Two: A Capacity Constraint Next we examine a nonlinear benefit function. Suppose a physical constraint, perhaps the amount of classroom space, exists such that the benefit of additional enrollees is less to the university once theSTEENBURGH, AINSLIE, AND ENGEBRETSON Massively Categorical Variables number of enrollees surpasses a given level. Assume the marginal benefit of an enrollee is s = b for s &lt; q b − a for s ≥ q where b &gt; a &gt; 0. In this case, the cutoff function is m = c/ b − a Pr S m−1 &gt; q for m ∈ 1 2 M (derivation two in the appendix), which depends on probabilities of enrollment of the m − 1 previously considered prospects through the term Pr S m−1 &gt; q .Example Three: A Diminishing Marginal Benefit from Enrollment Suppose the university faces a universally diminishing marginal benefit. Assume the benefit function is h 0 = 0 and h s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure1ROC Charts Comparing the HBVC Versus Null Models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2Probabilities of Enrollment and Cutoff Values</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 3Expected Percent of Profit Lost from Using the Null Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 4Realized Percent of Profit Lost from Using the Null Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>STEENBURGH=</head><label></label><figDesc>p S m−1 s + m−1 s=q+1 b − a p S m−1 s = c b − a m−1 s=0 p S m−1 s = c b − a Pr S m−1 &gt; q Since s + 2 = 1 − d s + 1 in this example, m s=0 s + 1 p S m s = m 1 − d + 1 − m showthat the cutoff function weakly increases as more prospects are contacted. The cutoff function given in Equation (8) can be rewritten as m = c m / m . p S m−2 s − 1 + 1 − m−1 p S m−2 s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Description of the Data</cell></row><row><cell></cell><cell>The Massively Categorical Variables</cell></row><row><cell>Z</cell><cell>The zip codes in which the prospective students reside</cell></row><row><cell>M</cell><cell>The intended majors of the students</cell></row><row><cell cols="2">The University's Data (Available for Each Individual-Referred to as X )</cell></row><row><cell cols="2">Available in All Scenarios</cell></row><row><cell>RES</cell><cell>A dichotomous indicator of whether the student is an</cell></row><row><cell></cell><cell>in-state resident (1 = Yes)</cell></row><row><cell cols="2">Available in the Campus Visitation Data Scenarios</cell></row><row><cell>VISIT</cell><cell>An indicator of whether the student visited campus</cell></row><row><cell></cell><cell>(1 = Yes)</cell></row><row><cell cols="2">OPENHOUSE An indicator of whether the student attended a campus</cell></row><row><cell></cell><cell>open house (1 = Yes)</cell></row><row><cell>CONTACTS</cell><cell>The number of times that the student contacted</cell></row><row><cell></cell><cell>the university</cell></row><row><cell cols="2">The Consolidator's Data (Available for each Zip code-Referred to as W )</cell></row><row><cell>COL_ED</cell><cell>The proportion of college educated households in</cell></row><row><cell></cell><cell>the zip code</cell></row><row><cell cols="2">FEM_MARRY The proportion of married females in</cell></row><row><cell></cell><cell>the zip code</cell></row><row><cell>BLT_50</cell><cell>The proportion of structures built in the 1950s in</cell></row><row><cell></cell><cell>the zip code</cell></row><row><cell>BLT_80</cell><cell>The proportion of structures built from 1980 to 1985</cell></row><row><cell></cell><cell>in the zip code</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Model Comparisons by Bayes Factors</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Log-Marginal</cell><cell>Bayes</cell></row><row><cell cols="2">Information Available</cell><cell cols="2">Likelihood</cell><cell>Factor</cell></row><row><cell>University's</cell><cell>Supplementary</cell><cell></cell><cell></cell><cell>HBVC vs.</cell></row><row><cell>Source</cell><cell>Source</cell><cell>Null</cell><cell>HBVC</cell><cell>Null</cell></row><row><cell cols="4">Without visitation Does not purchase −5429 −4921</cell><cell>1016</cell></row><row><cell>data</cell><cell>Does purchase</cell><cell cols="2">−5271 −4839</cell><cell>864</cell></row><row><cell>With visitation</cell><cell cols="3">Does not purchase −4253 −3960</cell><cell>586</cell></row><row><cell>data</cell><cell>Does purchase</cell><cell cols="2">−4196 −3889</cell><cell>614</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Model Comparisons by ROC Area Summaries</cell><cell></cell></row><row><cell cols="2">Information Available</cell><cell cols="2">ROC Area</cell></row><row><cell>University's Source</cell><cell>Supplementary Source</cell><cell>Null</cell><cell>HBVC</cell></row><row><cell>Without visitation data</cell><cell>Does not purchase</cell><cell>0.402</cell><cell>0.292</cell></row><row><cell></cell><cell>Does purchase</cell><cell>0.310</cell><cell>0.275</cell></row><row><cell>With visitation data</cell><cell>Does not purchase</cell><cell>0.169</cell><cell>0.148</cell></row><row><cell></cell><cell>Does purchase</cell><cell>0.155</cell><cell>0.144</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Posterior Distributions of the Full Models</figDesc><table><row><cell>Null Model</cell><cell>HBVC Model</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Marketing Science/Vol. 22, No. 1, Winter 2003</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The university data used in this paper are representative of the general class of clone models used in direct marketing. For an indepth analysis of students' choice of college, the reader is referred toManski and Wise (1983).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The related problem of two masked sources arises, for example, when an analyst wants to combine a consolidator's data source that is masked through zip code-level aggregation with census bureau data that are masked through census-block aggregation.3 Although our application involves a binomial choice problem modeled using Probit, the problem may occur in any regression structure and can be solved in the manner outlined without loss of generality.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We do observe N individual-specific explanatory variables (X T i and responses (y i in the university's data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7"> See Berger (1985,  §4.4) for a more thorough discussion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We use the posterior means˜ to divide the prospects because of their role in the decision rule. Marketing Science/Vol. 22, No. 1, Winter 2003</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We use the benefit function from third example throughout this section because it contains the first example as a limiting case and, as will be shown, the profit can be graphed on a three-dimensional chart.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This paper was receivedAugust 23, 1999, and  was with the authors 15 months for 3 revisions; processed by Greg M. Allenby. Marketing Science/Vol. 22, No. 1, Winter 2003</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the anonymous reviewers and AE for input that greatly added to the contribution of the paper; Peter Boatwright, Ed Kaplan, Subrata Sen, K. Sudhir, P. B. Seetharaman, Xavier Drèze, and Dick Wittink for comments; participants in seminars held at UCLA, USC, Washington University, and the BAMMCONF for helpful suggestions; and Yale, UCLA, Cornell, and the BAMM-CONF for financial assistance.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: The Hierarchical Bayes Variance Components Model</head><p>The most general model we consider includes both individual-level effects and zip-level effects. The zip-level effects are modeled in a hierarchical structure.</p><p>is a 1 by k vector consisting of an intercept and (k − 1) individual-level explanatory variables, is a k by 1 vector of coefficients describing preferences, m i is the major of the ith individual, and z i is the zip code of the ith individual. The effects for the student's intended major are modeled as</p><p>Finally, the zip-level effects are modeled as</p><p>is a 1 by d vector of demographic explanatory variables for the zth zip code, and is a d by 1 vector of zip code level effects. For indentification purposes, there is no intercept (i.e., no vector of 1s) in W t . When demographics are excluded, this step is treated the same as m .</p><p>The Prior Distributions</p><p>where mo = 4 and 2</p><p>where z0 = 4 and 2 z0 = 1</p><p>The Gibbs Sampler</p><p>The draw is truncated to be above 0 if I i = 1 (e.g. if the person enrolls), and to be below otherwise. See <ref type="bibr" target="#b1">Albert and Chib (1993)</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Set y indvl</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modification: No Hierarchical Regressors</head><p>Note that steps 9-11 above do not apply if there are no hierarchical regressors, i.e., when there are no consolidator variables. In that case, to retain identification, the distribution of z is distributed about zero, and steps 9-11 are replaced by steps identical to those used for major effects, i.e., steps 6 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Derivations</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brand choice across multiple categories: A hierarchical error components model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian analysis of binary and polychotomous response data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="669" to="679" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the heterogeneity of demand</title>
		<author>
			<persName><forename type="first">G</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="384" to="389" />
			<date type="published" when="1998-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using multimarket data to predict brand performance in markets for which no or poor data exist</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><surname>Berlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><forename type="middle">J</forename><surname>Bronnenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catarina</forename><surname>Sismeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1985" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
	<note>Statistical Decision Theory</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semiparametric versus parametric classification models: An application to direct marketing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="378" to="394" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Marketing Sci.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diagnostic checks for discrete data regression models using posterior predictive simulations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goegebeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Van Mechelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. C (Appl. Statist.)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="268" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Markov Chain Monte Carlo in Practice</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determinants of store-level price elasticity</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Hoch</surname></persName>
		</author>
		<author>
			<persName><surname>Byung-Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">College Choice in America</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Manski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wise</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Harvard University Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximate Bayesian inference with the weighted likelihood bootstrap</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="48" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Purchase history data in target marketing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="321" to="340" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Signal Detection Theory and ROC Analysis in Psychology and Diagnostics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Swets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Lawrence Erlbaum Associates</publisher>
			<pubPlace>Mahwah, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic prediction in multinomial logit models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1137" to="1144" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Modeling socially dependent preferences</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Columbus, OH</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Working Paper</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
