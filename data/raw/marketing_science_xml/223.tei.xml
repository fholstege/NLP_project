<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Modeling Multimodal Continuous Heterogeneity in Conjoint Analysis-A Sparse Learning Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-07-19">July 19, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yupeng</forename><surname>Chen</surname></persName>
							<email>yupengc@wharton.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Marketing Department</orgName>
								<orgName type="department" key="dep2">The Wharton School</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raghuram</forename><surname>Iyengar</surname></persName>
							<email>riyengar@wharton.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Marketing Department</orgName>
								<orgName type="department" key="dep2">The Wharton School</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Garud</forename><surname>Iyengar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Industrial Engineering and Operations Research</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Modeling Multimodal Continuous Heterogeneity in Conjoint Analysis-A Sparse Learning Approach</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2016-07-19">July 19, 2016</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2016.0992</idno>
					<note type="submission">Received: July 12, 2013 Accepted: December 22, 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>sparse machine learning</term>
					<term>multimodal continuous heterogeneity</term>
					<term>conjoint analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Marketing researchers and practitioners frequently use conjoint analysis to recover consumers' heterogeneous preferences <ref type="bibr">Srinivasan 1990, Wittink and</ref><ref type="bibr" target="#b40">Cattin 1989)</ref>, which serve as a critical input for many important marketing decisions, such as market segmentation <ref type="bibr" target="#b38">(Vriens et al. 1996)</ref> and differentiated product offerings and pricing <ref type="bibr" target="#b0">(Allenby and Rossi 1998)</ref>. In practice, consumer preferences can often be modeled using a multimodal continuous heterogeneity (MCH) distribution, where the consumer population is interpreted as consisting of a few distinct segments, each of which contains a heterogeneous subpopulation. Since in most conjoint applications researchers use short questionnaires because of concerns over response rates and response quality, the amount of information elicited from each respondent is limited; therefore, adequate modeling of MCH becomes critical.</p><p>Modeling MCH raises two major challenges. First, both across-segment and within-segment heterogeneity must be accommodated to fully capture preference variations among consumers. Second, when pooling data across respondents it is important to impose an adequate amount of shrinkage to recover the individual-level partworths. The widely used finite mixture (FM) model approximates MCH using discrete mass points, each representing a segment of homogeneous consumers <ref type="bibr" target="#b21">(Kamakura and</ref><ref type="bibr">Russell 1989, Chintagunta et al. 1991)</ref>. While such a discrete representation of the heterogeneity distribution accommodates across-segment heterogeneity, it does not allow for within-segment heterogeneity. Hierarchical Bayes (HB) models with flexible parametric specifications for the heterogeneity distribution have also been proposed to model MCH. For instance,  developed a Bayesian normal component mixture (NCM) model in which a mixture of multivariate normal distributions is utilized to represent consumers' heterogeneous preferences. While the NCM model is capable of modeling a variety of heterogeneity distributions, it may not be able to impose an adequate amount of shrinkage to accurately recover the individual-level partworths <ref type="bibr" target="#b14">(Evgeniou et al. 2007</ref>). Additionally, it faces inferential challenges when conducting a segment-level analysis <ref type="bibr" target="#b28">(Rossi et al. 2005)</ref>, including the label switching problem <ref type="bibr" target="#b9">(Celeux et al. 2000</ref><ref type="bibr" target="#b32">, Stephens 2000</ref> and the overlapping mixtures problem <ref type="bibr" target="#b22">(Kim et al. 2004</ref>). <ref type="bibr">1</ref> Chen, Iyengar, and Iyengar: Modeling MCH in Conjoint Analysis Marketing Science 36(1), pp. <ref type="bibr">140-156, Â© 2017 INFORMS 141</ref> In this paper, we propose an innovative sparse learning (SL) approach to address both challenges in modeling MCH and apply it in the context of metric and choice-based conjoint (CBC) analysis. Our SL approach models MCH using a two-stage divide-and-conquer framework. In the first stage, we build on recent advances in sparse learning <ref type="bibr" target="#b33">(Tibshirani 1996</ref><ref type="bibr" target="#b41">, Yuan and Lin 2005</ref><ref type="bibr" target="#b6">, Argyriou et al. 2008</ref> to "divide" the MCH distribution and recover a set of candidate segmentations of the consumer population. We make a simple observation that any two respondents from the same segment have identical segment-level partworths. Suppose the population is comprised of a few distinct segments. Then, a substantial proportion of pairwise differences of respondents' segment-level partworths will be zero vectors; in other words, the pairwise differences of respondents' segment-level partworths will be sparse. Our model leverages this observation and learns the sparsity pattern from the conjoint data to recover informative segmentations of the consumer population. In the second stage, we use each candidate segmentation to develop a set of individual-level representations of MCH by separately "conquering" the within-segment heterogeneity distribution of each segment. In particular, for each segment, we model its within-segment heterogeneity assuming a unimodal continuous heterogeneity (UCH) distribution, which is considerably easier to model compared to MCH. We select the optimal individual-level representation of MCH using cross-validation <ref type="bibr" target="#b39">(Wahba 1990</ref><ref type="bibr" target="#b30">, Shao 1993</ref><ref type="bibr" target="#b37">, Vapnik 1998</ref><ref type="bibr" target="#b16">, Hastie et al. 2001</ref>. Using the two-stage framework, our SL model accounts for both acrosssegment and within-segment heterogeneity, and is able to endogenously select an adequate amount of shrinkage for recovering the individual-level partworths. Moreover, since our SL model automatically generates a segmentation of the consumer population, a segment-level analysis can be readily conducted.</p><p>We add to the growing literature of machine learning-based methods for conjoint estimation <ref type="bibr" target="#b36">(Toubia et al. 2003</ref><ref type="bibr" target="#b35">(Toubia et al. , 2004</ref><ref type="bibr" target="#b13">Evgeniou et al. 2005;</ref><ref type="bibr" target="#b12">Cui and Curry 2005;</ref><ref type="bibr" target="#b14">Evgeniou et al. 2007</ref>). This stream of research has largely ignored consumer heterogeneity, with the exception of <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref>, who proposed a convex optimization (CO) model for capturing UCH. Our work contributes by developing the first machine learning-based approach to modeling the more general MCH.</p><p>We compare our SL model to the FM model, the NCM model, and the CO model using extensive simulation experiments and three field data sets. In simulations, the SL model shows a consistently strong performance in terms of both parameter recovery and predictive accuracy across a wide range of experimental conditions. The results from the simulations shed light on when and why the SL model outperforms other benchmarks. For instance, the performance of the NCM model relative to the SL model is weak when the within-segment variance is small or when the amount of respondent-level data is limited. The latter highlights the usefulness of our approach in contexts where researchers prefer to elicit consumer preferences using short conjoint questionnaires due to concerns over response rates and response quality <ref type="bibr" target="#b24">(Lenk et al. 1996)</ref>. This pattern of results happens largely because the amount of shrinkage imposed by the NCM model is influenced by exogenously chosen parameters for the second-stage priors and can be inadequate depending on the characteristics of a conjoint data set. In field data, the SL model also shows strong performance in terms of predictive accuracy, and its estimates of individual-level partworths display shapes consistent with MCH. Moreover, in an optimal pricing exercise, the SL model generates a more plausible revenue-maximizing price compared to that from other benchmarks, showing the managerial relevance of using our approach to model MCH in conjoint analysis.</p><p>The remainder of this paper is organized as follows. In Section 2 we present our SL model for modeling MCH in conjoint analysis. We compare the SL model and the benchmark methods using simulation experiments in Section 3 and three field conjoint data sets in Section 4. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>In this section, we present our SL approach to model MCH in conjoint analysis. Specifically, we give a detailed description of our approach in the context of metric conjoint analysis. We discuss the modifications needed for choice-based conjoint analysis in the Web appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Metric Conjoint Setup</head><p>We assume a total of I consumers (or respondents), each rating J profiles with p attributes. Let the 1 Ã p row vector x i j represent the jth profile rated by the ith respondent, for i 1, 2, . . . , I and j 1, 2, . . . , J, and denote by X i [ x i1 , x i2 , . . . , x i J ] the J Ã p design matrix for the ith respondent. For respondent i, the p Ã 1 column vector Î² i is used to denote her partworths, and her ratings are contained in the J Ã 1 column vector Y i (y i1 , y i2 , . . . , y i J ) . We assume additive utility functions, i.e., Y i X i Î² i + i , for i 1, 2, . . . , I, where i denotes the random error. The additive specification of the utility functions is a standard assumption in the conjoint analysis literature <ref type="bibr" target="#b15">(Green and Srinivasan 1990)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Overview</head><p>Under a MCH distribution, the consumer population is interpreted as consisting of a few distinct Marketing Science 36(1), pp. 140-156, Â© 2017 INFORMS segments of heterogeneous consumers. To fully capture such a heterogeneity structure, a model needs to be sufficiently flexible to accommodate both acrosssegment and within-segment heterogeneity. It is also critical that the model has the capacity to impose an adequate amount of shrinkage when recovering the individual-level partworths. These considerations motivate a divide-and-conquer strategy for modeling MCH, where the MCH distribution is "divided" into a collection of within-segment UCH distributions, and each UCH distribution is separately "conquered" using established estimation methodologies. We implement this modeling strategy using the following two-stage framework.</p><p>In the first stage, we develop a novel sparse learning model to divide the MCH distribution and recover a set of candidate segmentations of the consumer population. Our model is built on the simple observation that any two respondents from the same segment may have different individual-level partworths but must share identical segment-level partworths; i.e., the difference between their respective segment-level partworths is the zero vector. Since the consumer population consists of a few distinct segments, a substantial proportion of pairwise differences of respondents' segment-level partworths are zero vectors; in other words, the pairwise differences of respondents' segment-level partworths are sparse. Leveraging this observation, we use the sparse learning model to learn such sparsity patterns from conjoint data and recover informative candidate segmentations of the consumer population. Each candidate segmentation provides a decomposition of the MCH distribution into a collection of withinsegment heterogeneity distributions, which we utilize in the second stage.</p><p>In the second stage, we use each candidate segmentation to develop a set of individual-level representations of MCH. Given a candidate segmentation, we separately model the within-segment heterogeneity distribution of each segment assuming a UCH distribution. UCH provides a reasonable characterization of the within-segment heterogeneity distributions and is considerably easier to model than MCH. We choose the CO model of <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref> to model the within-segment distributions, which allows for an effective approach to control the amount of shrinkage imposed when modeling UCH. We select the optimal individual-level representation of MCH using cross-validation <ref type="bibr" target="#b39">(Wahba 1990</ref><ref type="bibr" target="#b30">, Shao 1993</ref><ref type="bibr" target="#b37">, Vapnik 1998</ref><ref type="bibr" target="#b16">, Hastie et al. 2001</ref>. The cross-validation procedure provides a fully data-driven approach to endogenously select an adequate candidate segmentation and an adequate amount of shrinkage to recover the individuallevel partworths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">First Stage: Recovering Candidate Segmentations</head><p>The first stage of our SL model aims at learning a set of candidate segmentations of the MCH distribution. To motivate, we consider a standard characterization of the data-generating process of MCH <ref type="bibr">(Andrews et al. 2002a, b)</ref>. The data-generating process selects the number of segments L, the segment-level partworths {Î² S l } L l 1 , and the segment-membership matrix Q â IÃL , where Q il 1 if respondent i is assigned to segment l, and Q il 0 otherwise. If respondent i belongs to segment l, she receives a copy of segment-level partworths Î² S i Î² S l , and her individual-level partworths are determined by Î² i Î² S i + Î¾ i , where Î¾ i denotes the difference between respondent i's segment-level and individual-level partworths, i.e., the withinsegment heterogeneity. LetB S {Î² S l } L l 1 , B S {Î² S i } I i 1 , and B {Î² i } I i 1 . Assuming the above data-generating process, recovering candidate segmentations can be achieved by learning the set of model parameters {L,B S , Q, B S , B} from the conjoint data. A closer examination reveals that learning {B S , B} is sufficient, as other model parameters {L,B S , Q} can be uniquely determined from {B S , B}. We highlight the following three assumptions about the data-generating process that are relevant to learning {B S , B}: Assumption 1 (A1). The ratings vector Y i is generated based on Î² i , i.e., Y i X i Î² i + i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 2 (A2).</head><p>The individual-level partworths Î² i is generated based on the segment-level partworths Î² S i , i.e., Î² i Î² S i + Î¾ i . Assumption 3 (A3). Respondents i and k belong to the same segment if and only if</p><formula xml:id="formula_0">Î² S i â Î² S k 0.</formula><p>Within an optimization framework with {B S , B} as decision variables, A1 (respectively, A2) suggests to penalize the discrepancy between Y i and X i Î² i (respectively, the discrepancy between Î² i and Î² S i ). A3, together with the observation that for a substantial proportion of i â k pairs, respondents i and k belong to the same segment, implies that the pairwise discrepancies of the true B S are sparse. It thus suggests that we can impose a sparse structure on the pairwise discrepancies of B S when learning {B S , B} and use the sparsity pattern to learn the underlying segmentation.</p><p>Motivated by these considerations, we propose the following sparse learning problem to recover candidate segmentations.</p><p>(Metric-SEG)</p><formula xml:id="formula_1">min I i 1 Y i â X i Î² i 2 2 + Î³ I i 1 (Î² i â Î² S i ) D â1 (Î² i â Î² S i ) + Î» 1â¤i&lt;kâ¤I Î¸ ik Î² S i â Î² S k 2 (1) s.t. D is a positive semidefinite matrix scaled to have trace 1, Î² i , Î² S i â p , for i 1, 2, . . . , I,</formula><p>where Î³, Î», and {Î¸ ik } are the regularization parameters that control the relative strength of each penalty term in (Metric-SEG). We will discuss the specification of the regularization parameters in a few paragraphs. In (Metric-SEG), the first two penalty terms are standard quadratic functions measuring the discrepancy between Y i and X i Î² i and that between Î² i and Î² S i , respectively. We note that the matrix D is a decision variable and is related to the covariance matrix of the partworths within each segment <ref type="bibr" target="#b14">(Evgeniou et al. 2007</ref>). The third penalty term aims to impose the sparse structure suggested by A3 and is the key to the formulation of (Metric-SEG). In particular, it aims to learn whether respondents i and k belong to the same segment by penalizing the 2 -norm of</p><formula xml:id="formula_2">Î² S i â Î² S k , i.e., Î² S i â Î² S k 2</formula><p>, for all i-k pairs. We choose the 2 -norm to measure the discrepancy between Î² S i and Î² S k since, unlike most standard measures of magnitude of vectors, e.g., the sum-of-squares measure, the 2 -norm is a sparsity-inducing penalty function in that it is capable of enforcing exact zero value in optimal solutions under a suitable level of penalty. 2 Sparsity-inducing penalty functions play a fundamental role in sparse learning <ref type="bibr" target="#b33">(Tibshirani 1996</ref><ref type="bibr" target="#b41">, Yuan and Lin 2005</ref>. Our use of the 2 -norm to penalize the pairwise differences of B S can be viewed as a generalization of the overlapping 1 / 2 -norm <ref type="bibr">(Jenatton et al. 2012, Kim and</ref> and the fused lasso penalty <ref type="bibr" target="#b34">(Tibshirani et al. 2004)</ref>, and was recently introduced in the context of unsupervised learning <ref type="bibr" target="#b17">(Hocking et al. 2011)</ref>.</p><p>The rationale for assessing whether respondents i and k belong to the same segment by penalizing the 2 -norm of Î² S i â Î² S k is as follows. For the purpose of illustration, suppose we set Î¸ ik 1 for all i-k pairs in (Metric-SEG), and thus homogenize the penalty imposed on the 2 -norm of Î² S i â Î² S k . For any two respondents i and k, we consider the following components of the objective function of (Metric-SEG):</p><formula xml:id="formula_3">G i, k r i, k Y r â X r Î² r 2 2 + Î³ r i, k (Î² r â Î² S r ) D â1 (Î² r â Î² S r ) + Î» Î² S i â Î² S k 2 .</formula><p>Within an optimization framework, the three penalty terms in G i, k induce competing shrinkage over the decision variables {Î² r , Î² S r } r i, k : the first term shrinks Î² r toward the true individual-level partworths Î² r (T), and the second term shrinks Î² r and Î² S r toward each other, for r i, k, whereas the third term shrinks Î² S i and Î² S k toward each other. Whether Î² S i â Î² S k 0 holds in the optimal solution is largely determined by the tradeoff among the three competing shrinkages, which is, in turn, determined by the distance between Î² i (T) and Î² k (T) as well as the regularization parameters Î³ and Î». If respondents i and k are from the same segment, the distance between Î² i (T) and Î² k (T) is likely to be small, and a moderate penalty imposed on Î² S i â Î² S k 2 , i.e., a small Î», should be sufficient to enforce Î² S i â Î² S k 0 due to the sparsity-inducing property of the 2 -norm. If respondents i and k are from distinct segments, the distance between Î² i (T) and Î² k (T) is likely to be large, and enforcing Î² S i â Î² S k 0 can only be achieved when a strong penalty is imposed on</p><formula xml:id="formula_4">Î² S i â Î² S k 2 , i.e.</formula><p>, a large Î» is specified. This suggests that if Î³ and particularly Î» are appropriately specified, it is possible to recover the underlying segmentation of the consumer population by solving (Metric-SEG) and identifying i-k pairs with Î² S i â Î² S k 0 in the optimal solution.</p><p>Regularization Parameters. We first discuss the specification for the regularization parameters {Î¸ ik }. A heterogeneous specification for {Î¸ ik } is useful for (Metric-SEG) because it allows us to incorporate information that could potentially facilitate the recovery of the underlying segmentation. For example, suppose there is information suggesting that the pair of respondents i and k are more likely to be drawn from the same segment compared to the pair of respondents i and k . This information can be accommodated in (Metric-SEG) by setting Î¸ ik &gt; Î¸ i k such that a stronger sparsityinducing penalty is imposed to enforce Î² S i â Î² S k 0. In this paper, we specify {Î¸ ik } as follows:</p><formula xml:id="formula_5">Î¸ ik R(W(Î² i ,Î² k )),<label>(2)</label></formula><p>where {Î² i } I i 1 are some initial estimates of the individual-level partworths, W( â¢ , â¢ ) is a distance measure of two vectors, and R( â¢ ) is a positive, nonincreasing function. The rationale for this specification is that when the distance between the initial individual-level partworth estimatesÎ² i andÎ² k is small, it is likely that respondents i and k belong to the same segment, and therefore, Î¸ ik is set to a large value to induce</p><formula xml:id="formula_6">Î² S i â Î² S k 0. The admissible choices for {Î² i } I i 1 , W( â¢ ,</formula><p>â¢ ), and R( â¢ ) are quite flexible. In the empirical implementation of our SL model, we choose to estimate {Î² i } I i 1 using the CO model of <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref>. We set W(x, y) ((x â y) D â1 (x â y)) 1/2 , whereD is the scaled covariance matrix of the partworths generated by the CO model along with {Î² i } I i 1 <ref type="bibr" target="#b14">(Evgeniou et al. 2007</ref>); such a specification gives more weight to difference between two initial individual-level partworth estimates along directions in which there is less variation across respondents. We set R(x) e âÏx , a positive, nonincreasing function parameterized by a regularization parameter Ï â¥ 0. Consequently, we adopt the following specification for {Î¸ ik }: <ref type="bibr">3 , 4</ref> Î¸ ik e âÏ((</p><formula xml:id="formula_7">Î² i âÎ² k ) Dâ1 (Î² i âÎ² k )) 1/2 .</formula><p>(3) In this specification, the regularization parameter Ï controls the extent to which {Î² i } I i 1 are used to facilitate recovering candidate segmentations. When Ï 0, {Î² i } I i 1 do not enter the specification of {Î¸ ik }, and a homogeneous penalty is imposed on the pairwise discrepancies of B S ; as Ï increases, {Î¸ ik } become more heterogeneous, and pairs of respondents with closer initial estimates, i.e., those deemed as more likely to be drawn from the same segment, are penalized more heavily than those with farther initial estimates.</p><p>Given the specification of {Î¸ ik } in (3), the regularization parameters for (Metric-SEG) are now given by the vector Î (Î³, Î», Ï). Since an appropriate value for Î is not known a priori, we specify a finite grid Î â 3 and solve (Metric-SEG) for each Î â Î. <ref type="bibr">5</ref> We denote (B(Î), B S (Î), D(Î)) as the optimal solution of (Metric-SEG) given Î. For each Î, we use B S (Î) to recover a candidate segmentation Q(Î).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution Algorithm. (Metric-SEG</head><p>) is a convex optimization problem for all regularization parameters Î â Î, which implies that it is efficiently solvable to global optimum in theory <ref type="bibr" target="#b8">(Boyd and Vandenberghe 2004)</ref>. However, solving (Metric-SEG) poses an algorithmic challenge since the third penalty term,</p><formula xml:id="formula_8">Î» 1â¤i&lt;kâ¤I Î¸ ik Î² S i â Î² S k 2</formula><p>, is a nondifferentiable and nonseparable function. Nondifferentiability implies that standard convex optimization methods requiring a differentiable objective function, e.g., Newton's method, cannot be applied to solve (Metric-SEG); nonseparability also adds to the complexity <ref type="bibr" target="#b10">(Chen et al. 2012)</ref>. We solve (Metric-SEG) using a special purpose algorithm based on variable splitting and the alternating direction augmented Lagrangian method that was proposed in <ref type="bibr" target="#b27">Qin and Goldfarb (2012)</ref>. This algorithm is specifically designed for handling complex sparsityinducing penalty functions and is capable of solving for the global optimum of (Metric-SEG). We provide a detailed description of the algorithm in the Web appendix.</p><p>Dealing with Small Segments. In many instances of (Metric-SEG) encountered in our simulation experiments and field applications, we observed that the candidate segmentation Q contains a small number of substantive segments that comprise the majority of the consumer population, as well as a few segments each consisting of very few respondents, often one or two. Since these small segments bear little practical interpretation, we employ a simple procedure to combine each of the small segments with its closest substantive segment. Formally, we define a segment in Q as a valid segment if it contains at least M respondents, where M is a prespecified threshold, and as an invalid segment otherwise. Without loss of generality, we assume that the firstL segments of Q are valid. We retain all valid segments, and for each invalid segment, i.e., the lth segment with l &gt;L, we determine its closest valid segment by computing c(l</p><formula xml:id="formula_9">) {v â {1, 2, . . . ,L} | Î² S v âÎ² S l 2 &lt; Î² S v âÎ² S l 2 , for v â {1, 2, . . . ,L}, v</formula><p>v}, and combine the lth segment (an invalid segment) and the c(l)th segment (a valid segment). We defineQ, the segmentation obtained after this processing, as the candidate segmentation, but still refer to it using Q for simplicity hereafter. <ref type="bibr">6</ref> We note that it is possible that no valid segment exists in a segmentation, i.e.,L 0. In such a case, we simply claim that no candidate segmentation is identified for this instance of (Metric-SEG).</p><p>Summary. The first stage of our SL model recovers a set of candidate segmentations in the following manner. We specify a finite grid Î â 3 from which the regularization parameters Î (Î³, Î», Ï) are chosen. For each Î â Î, we solve (Metric-SEG) and obtain the candidate segmentation Q(Î); Q(Î) could be an empty matrix in cases where no candidate segmentation is identified. We also include the trivial segmentation where all respondents are in one segment as a candidate segmentation, i.e., Q(Trivial) 1 IÃ1 . We denote the set of candidate segmentations as Î¦, i.e., Î¦ {Q(Î)} ÎâÎ: Q(Î) âª {Q(Trivial)}; Î¦ is the output of the first stage of the SL model. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Second Stage: Recovering Individual-Level</head><p>Partworths The second stage of our SL model aims at leveraging the set of candidate segmentations Î¦ to accurately recover the individual-level partworths. To this end, we develop a set of individual-level representations of MCH based on each candidate segmentation and select the optimal individual-level representation of MCH using cross-validation.</p><p>Given Q â Î¦, we propose to model MCH by separately modeling the within-segment heterogeneity distribution for each segment assuming a UCH distribution; that is, Q is interpreted as a decomposition of the MCH distribution into a collection of UCH distributions that are considerably easier to model. There are many effective approaches for modeling UCH in the marketing literature, including the unimodal HB models <ref type="bibr" target="#b24">(Lenk et al. 1996</ref><ref type="bibr" target="#b29">, Rossi et al. 1996</ref> and RR-Het, the metric version of the CO model of <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref>. We choose RR-Het to model within-segment UCH distributions because it outperforms standard unimodal HB models <ref type="bibr" target="#b14">(Evgeniou et al. 2007</ref>) and allows for a direct and parsimonious way for controlling the amount of shrinkage imposed on the individual-level partworth estimates that can be readily incorporated in a cross-validation framework for endogenously selecting an adequate amount of shrinkage.</p><p>Formally, for a candidate segmentation Q with L segments, we define a set of modeling strategies {S | S (Q, Ï, COV)}, parameterized by Ï (Ï 1 , Ï 2 , . . . , Ï L ) and COV (COV 1 , COV 2 , . . . , COV L ), where Ï l &gt; 0 and COV l â {General(G), Restrictive(R)} for l 1, 2, . . . , L. The modeling strategy S models MCH and obtains the individual-level partworth estimates {Î² i } I i 1 by solving a convex optimization problem Metric-HET(Q; l; Ï l ; COV l ) for the lth segment of Q, denoted as Î¥(Q; l), for l 1, 2, . . . , L. When COV l G, the optimization problem (Metric-HET(Q; l; Ï l ; G)) is defined as follows:</p><formula xml:id="formula_10">(Metric-HET(Q; l; Ï l ; G)) min iâÎ¥(Q;l) Y i â X iÎ²i 2 2 + Ï l iâÎ¥(Q;l) (Î² i âÎ² l 0 ) (D l ) â1 (Î² i âÎ² l 0 ) s.t. D l is a positive semidefinite matrix scaled to have trace 1, Î² i â p , for i â Î¥(Q; l);Î² l 0 â p . (<label>4</label></formula><formula xml:id="formula_11">)</formula><p>When COV l R, the optimization problem Metric-HET(Q; l; Ï l ; R) is defined as follows:</p><p>(Metric-HET(Q; l; Ï l ; R))</p><formula xml:id="formula_12">min iâÎ¥(Q;l) Y i â X iÎ²i 2 2 + Ï l iâÎ¥(Q;l) (Î² i âÎ² l 0 ) (I/p) â1 (Î² i âÎ² l 0 ) s.t.Î² i â p , for i â Î¥(Q; l);Î² l 0 â p . (<label>5</label></formula><formula xml:id="formula_13">)</formula><p>We note that (5) is obtained from (4) by restricting the decision variable D l I/p. In both optimization problems, the regularization parameter Ï l provides a direct and parsimonious way to control the trade-off between fit and shrinkage. In particular, a larger Ï l imposes more shrinkage on the individual-level partworth estimates in the lth segment towardÎ² l 0 , which can be shown to be the segment mean <ref type="bibr" target="#b14">(Evgeniou et al. 2007)</ref>, and hence results in more homogenous estimates. The matrix D l in ( <ref type="formula" target="#formula_10">4</ref>) is related to the covariance matrix of the partworths within the lth segment <ref type="bibr" target="#b14">(Evgeniou et al. 2007</ref>). Explicitly modeling D l allows for a general covariance structure and gives rise to much flexibility in modeling within-segment heterogeneity. On the other hand, restricting D l I/p in (5) imposes a restrictive covariance structure that is less flexible but is also more parsimonious and robust with respect to overfitting. We assess the relative strength of the two optimization problems with different covariance structures using cross-validation.</p><p>We note that each modeling strategy S (Q, Ï, COV) gives rise to a distinct individual-level representation of MCH. In particular, the segmentation Q determines the way in which MCH is decomposed into a collection of UCHs, and Ï and COV control the amount of shrinkage imposed and the covariance structure assumed when modeling UCH for each segment of Q, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation.</head><p>To endogenously select the optimal modeling strategy (and hence the optimal individuallevel representation of MCH it implies), we evaluate the cross-validation error of each modeling strategy S. Cross-validation is a standard technique used in the statistics and machine learning literature for model selection <ref type="bibr" target="#b39">(Wahba 1990</ref><ref type="bibr" target="#b30">, Shao 1993</ref><ref type="bibr" target="#b37">, Vapnik 1998</ref><ref type="bibr" target="#b16">, Hastie et al. 2001</ref>) and has been adopted in the recent literature of machine learning and optimization-based methods for conjoint estimation <ref type="bibr" target="#b13">(Evgeniou et al. 2005</ref><ref type="bibr" target="#b14">(Evgeniou et al. , 2007</ref>. We measure the cross-validation error of a modeling strategy S, CVE(S), identically as in <ref type="bibr" target="#b13">Evgeniou et al. (2005</ref><ref type="bibr" target="#b14">Evgeniou et al. ( , 2007</ref>. The cross-validation error CVE(S) provides an effective estimate of the predictive accuracy of the modeling strategy S on out-of-sample data using only in-sample data, i.e., the data available to the researcher for model calibration. To implement cross-validation, we prespecify a finite grid Î â , and for each Q we consider modeling strategies S (Q, Ï, COV) such that Ï l â Î and COV l â {G, R}, for l 1, 2, . . . , L. <ref type="bibr">8</ref> We select S that minimizes CVE(S) as the optimal modeling strategy and its corresponding Q as the optimal candidate segmentation, which we denote by S * and Q * , respectively. Consequently, the cross-validation procedure allows us to endogenously select the modeling strategy S * that is expected to have the optimal predictive accuracy on out-of-sample data. We recover the optimal individual-level partworth estimates {Î² * i } I i 1 by applying S * to the complete data set {X i , Y i } I i 1 . Confidence Intervals. Besides point estimates for individual-level partworths, our SL approach can also be used to produce confidence intervals for individuallevel partworth estimates via bootstrapping, similar to the CO model (as detailed in the online appendix of <ref type="bibr" target="#b14">Evgeniou et al. 2007</ref>). To generate the bootstrap estimates for confidence intervals, we first estimate the optimal modeling strategy S * (Q * , Ï * , COV * ). Next, we generate a large number of (e.g., 1,000) random bootstrap samples from the original data set and apply the modeling strategy S * to each bootstrap sample; here the bootstrap samples are obtained by keeping all respondents and for each respondent randomly sampling her conjoint profiles with replacement. We then use the empirical distributions of partworth estimates generated from the bootstrap samples to construct confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Summary</head><p>We briefly summarize our SL model Metric-SL in the following. The MATLAB code for Metric-SL is available from the authors on request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First Stage.</head><p>Step 1a. Obtain the initial estimates {Î² i } I i 1 and the scaled covariance matrix of the partworthsD using RR-Het <ref type="bibr" target="#b14">(Evgeniou et al. 2007</ref>). Step 1b. For each Î â Î, set Î¸ ik e âÏ((Î² i âÎ² k ) Dâ1 (Î² i âÎ² k )) 1/2 , and solve (Metric-SEG) (see (1)). Recover the candidate segmentation Q(Î) from B S (Î).</p><p>Step 1c. Repeat Step 1b for each Î â Î, and obtain the set of candidate segmentations</p><formula xml:id="formula_14">Î¦ {Q(Î)} ÎâÎ: Q(Î) âª {Q(Trivial)}. (<label>6</label></formula><formula xml:id="formula_15">)</formula><p>Second Stage.</p><p>Step 2a. For each Q â Î¦, define a set of modeling strategies {S | S (Q, Ï, COV) s.t. Ï l â Î, COV l â {G, R}, for l 1, 2, . . . , L}. A modeling strategy S recovers the individual-level partworths by solving a set of L optimization problems {Metric-HET(Q; l; Ï l ; COV l )} L l 1 defined in ( <ref type="formula" target="#formula_10">4</ref>) and ( <ref type="formula" target="#formula_12">5</ref>).</p><p>Step 2b. Select the modeling strategy S * (Q * , Ï * , COV * ) with the minimum cross-validation error, i.e., S * argmin S CVE(S). We select Q * as the optimal segmentation.</p><p>Step 2c. Generate the optimal individual-level partworth estimates {Î² * i } I i 1 by then applying S * to {X i , Y i } I i 1 , i.e., by solving L * optimization problems {Metric-HET(Q * ; l; Ï l * ; COV l * )} L * l 1 . The outputs of the second stage are ({Î² * i } I i 1 , Q * ), which are also the final outputs of the complete Metric-SL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Extension to Choice-Based Conjoint Analysis</head><p>CBC has been the dominant conjoint approach recently <ref type="bibr" target="#b18">(Iyengar et al. 2008</ref>). Our SL model can be readily extended to the context of CBC. In particular, our SL model can be applied to CBC by simply replacing the squared-error loss functions in all optimization problems in Metric-SL with the logistic loss functions. We discuss our SL model for CBC, Choice-SL, in the Web appendix. The MATLAB code for Choice-SL is available from the authors on request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Simulation Experiments</head><p>In this section we report the results of a set of simulation experiments designed to test the performance of our SL model. Simulation experiments have been widely adopted in the marketing literature to evaluate conjoint estimation methods <ref type="bibr" target="#b38">(Vriens et al. 1996</ref><ref type="bibr" target="#b4">, Andrews et al. 2002b</ref>). We consider both metric and choice-based conjoint simulation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Metric Conjoint Simulation Experiments</head><p>We compared Metric-SL, the metric version of our SL model, to three benchmark methods: (1) the FM model <ref type="bibr" target="#b21">(Kamakura and</ref><ref type="bibr">Russell 1989, Chintagunta et al. 1991)</ref>, (2) the Bayesian NCM model , and (3) RR-Het, the metric version of the CO model of <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref>. The FM model represents MCH using discrete mass points. The NCM model specifies a mixture of multivariate normal distributions to characterize the heterogeneity distribution and is capable of representing a wide variety of heterogeneity distributions. RR-Het is not specifically designed to model MCH; however, we included it as a benchmark method to assess the improvement made by adopting the more general Metric-SL model.</p><p>The implementation of the three benchmark methods closely followed the extant literature. In particular, the FM model was calibrated using the Bayesian information criterion (BIC) <ref type="bibr" target="#b4">(Andrews et al. 2002b)</ref>, and for the NCM model the number of components was selected using the deviance information criterion (DIC) <ref type="bibr" target="#b31">(Spiegelhalter et al. 2002</ref><ref type="bibr" target="#b26">, Luo 2011</ref>. We provide the setup of the NCM model, including the specification of parameters for the second-stage priors, in the Web appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Data.</head><p>Our experimental design and datagenerating process largely followed past work that has used simulations to evaluate methods for recovering MCH within metric conjoint settings <ref type="bibr" target="#b4">(Andrews et al. 2002b</ref>). See <ref type="bibr" target="#b4">Andrews et al. (2002b)</ref> for a discussion of the experimental design and the data-generating process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental design. We experimentally manipulated four data characteristics:</head><p>Factor 1. The number of segments: 2 or 3 Factor 2. The number of profiles per respondent (for calibration): 18 or 27 Factor 3. The error variance: 0.5 or 1.5 Factor 4. The within-segment variances of distributions: 0.05, 0.10, 0.20, 0.40, 0.60, 0.80, or 1.00</p><p>Hence, we used a 2 3 Ã 7 design, resulting in a total of 56 experimental conditions. We randomly generated 5 data sets for each experimental condition and estimated all conjoint models separately on each data set.</p><p>Data-generating process. We adopted the conjoint designs used in <ref type="bibr" target="#b4">Andrews et al. (2002b)</ref> in which six product attributes were varied at three levels each. Each data set consisted of 100 synthetic respondents and their responses were generated according to the following three-step process: we (1) generated the true segment-level partworths, (2) assigned each respondent to a segment and generated her true individual-level partworths, and (3) generated her response vector. More specifically, the true segment-level partworths for any segment l, Î² l (S), were generated as a vector of random numbers sampled independently from a uniform distribution over the interval [â1.7, 1.7]. Each respondent was randomly assigned to all segments with equal probabilities, and her true individual-level partworths Î² i (T) were generated as Î² i (T) Î² l (S) + ÏÎ¾ i if respondent i was assigned to segment l, where Ï 2 is the prespecified within-segment variance (Factor 4) and Î¾ i is a vector of independent standard normal random variables. Given Î² i (T), the response vector Y i was computed as Y i X i Î² i (T) + Î´ i , where Î´ 2 is the prespecified error variance (Factor 3), and i is a vector of independent standard normal random variables. To evaluate the predictive accuracy of the conjoint estimation methods, we generated 8 holdout profiles for each respondent regardless of whether 18 or 27 profiles (Factor 2) were used for calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Results.</head><p>We compared all four conjoint estimation methods in terms of parameter recovery and predictive accuracy. Parameter recovery was assessed using the root mean squared error (RMSE) between the true individual-level partworths Î² i (T) and the estimated individual-level partworths Î² i (E), which we denote by RMSE(Î²). Predictive accuracy was measured using the RMSE between the observed ratings Y i (O) and the predicted ratings Y i (P) on the holdout sample, which we denote by RMSE(Y). Following <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref>, we computed RMSE(Î²) and RMSE(Y) for each respondent in each data set and report the average RMSE(Î²) and RMSE(Y) across respondents and data sets for each experimental condition. <ref type="bibr">9</ref> Across experimental conditions, we find that Metric-SL overall outperforms the benchmark models both in terms of parameter recovery and predictive accuracy. In particular, Metric-SL performs best or not significantly different from best on RMSE(Î²) (at p &lt; 0.05) in 51 out of 56 conditions, and is either the best performing method or indistinguishable from the best method on RMSE(Y) (at p &lt; 0.05) in 52 out of 56 conditions. The comparisons are based on paired t-tests over the same 500 respondents, i.e., (100 respondents per data set) times (5 data sets), in each experimental condition.</p><p>To illustrate, we summarize the results for a subset of experimental conditions in Table <ref type="table" target="#tab_2">1</ref>, where Num-S denotes the number of segments in the heterogeneity distribution (Factor 1), Num-P denotes the number of profiles per respondent for calibration (Factor 2), EV denotes the error variance (Factor 3), and WSV denotes the within-segment variances of distributions (Factor 4). We note that for both RMSE(Î²) and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMSE(Y), lower numbers indicate better performance.</head><p>The full results for all 56 conditions are reported in the Web appendix.</p><p>Table <ref type="table" target="#tab_2">1</ref> shows a systematic pattern of RMSE(Î²) and RMSE(Y) for the four conjoint estimation methods with respect to WSV. When WSV is small, e.g., WSV 0.05 or 0.10, the NCM model and RR-Het perform substantially worse than Metric-SL, whereas the FM model shows a good performance. As WSV increases, the relative performance of the NCM model and RR-Het gradually improves, and that of the FM model quickly deteriorates. On the other hand, Metric-SL demonstrates a consistently strong performance across the range of WSV. This performance pattern confirms the importance of explicitly modeling both acrosssegment and within-segment heterogeneity, and also endogenously selecting an adequate amount of shrinkage to recover individual-level partworths in modeling MCH. The FM model assumes a discrete heterogeneity distribution that does not allow for within-segment heterogeneity and hence is not capable of fully capturing the variations in consumer preferences when within-segment heterogeneity is substantial. RR-Het models consumer preferences using a UCH distribution, which does not accommodate across-segment heterogeneity and thus limits its performance when the underlying heterogeneity distribution is fairly discrete. The NCM model explicitly models both acrosssegment and within-segment heterogeneity, but is not capable of endogenously selecting the amount of shrinkage, since it is influenced by exogenously chosen parameters for the second-stage priors. In Table <ref type="table" target="#tab_2">1</ref>, the relatively inferior performance of the NCM model when within-segment heterogeneity is small or moderate suggests that the amount of shrinkage imposed by the NCM model is inadequate in these experimental conditions. This provides evidence that, consistent with findings in <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref>, the amount of shrinkage imposed by the NCM model can be inadequate depending on the characteristics of a conjoint data set. By contrast, our Metric-SL model addresses both modeling challenges and shows a robust performance across conditions. We conducted a regression analysis to examine the impact of the experimental factors on RMSE(Î²) and RMSE(Y) of the four conjoint estimation methods. For RMSE(Î²), we adopted the following specification:</p><formula xml:id="formula_16">RMSE(Î²) t Î± 0 + Î± 1 Ã Num-S-Dummy t + Î± 2 Ã Num-P-Dummy t + Î± 3 Ã EV-Dummy t + Î± 4 Ã WSV t + t ,<label>(7)</label></formula><p>where the index t runs over the 56 experimental conditions. The dependent variable RMSE(Î²) t is the average RMSE(Î²) of a method in condition t. For the independent variables, we dummy coded the first three experimental factors, Num-S, Num-P, and EV, and used the original value of the fourth experimental factor, WSV. <ref type="bibr">10</ref> Table <ref type="table" target="#tab_4">2</ref> shows the results of the ordinary least squares (OLS) estimation on RMSE(Î²) for each of the four conjoint estimation methods.</p><p>We make a few observations from the results in Table <ref type="table" target="#tab_4">2</ref>. The fact that the coefficients for Num-S are insignificant for all methods suggests that the number of segments has little impact on RMSE(Î²). Num-P has significant negative coefficients for all methods except the FM model, implying that more calibration profiles improve the accuracy of parameter recovery for the three methods other than the FM model. EV has significant positive coefficients for all methods, which means that a larger error variance hurts all methods; we note that the impact of error variance on the FM model is smaller compared to other methods. WSV has significant positive coefficients, indicating that a larger within-segment variance leads to a higher error in parameter recovery for all methods. Furthermore, as WSV increases, the FM model deteriorates most quickly, followed by Metric-SL, which is in turn followed by RR-Het and the NCM model. This is consistent with our previous findings about the relative performance of the four conjoint estimation methods with respect to WSV.</p><p>We also conducted a regression analysis to understand the impact of the experimental factors on the relative performance between Metric-SL and the NCM model. In particular, we adopted a specification identical to (7) except that the dependent variable was replaced with the difference of RMSE(Î²) for Metric-SL and the NCM model. The results of the OLS estimation are reported in the last column of Table <ref type="table" target="#tab_4">2</ref>. The results show that the performance of Metric-SL relative to the NCM model improves when there are fewer calibration profiles. This finding highlights the usefulness of Metric-SL especially in contexts where researchers prefer to elicit consumer preferences using short conjoint questionnaires due to concerns over response rates and response quality <ref type="bibr" target="#b24">(Lenk et al. 1996)</ref>. We also find that a larger error variance and a smaller withinsegment variance improve the relative performance of Metric-SL.</p><p>For RMSE(Y), we used a specification identical to (7) except that the dependent variable was the average RMSE(Y) of a method in a specified experimental condition. We report the results of the OLS estimation in Table <ref type="table" target="#tab_5">3</ref>.</p><p>The impact of the experimental factors on RMSE(Y) is largely similar to that on RMSE(Î²). A couple of main differences are that for RMSE(Y), Num-S has significant positive coefficients for all methods except Metric-SL, and Num-P has the largest impact on the FM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Choice-Based Conjoint Simulation</head><p>Experiments We compared Choice-SL, the choice version of our SL model, to three benchmark methods: (1) the FM model, (2) the NCM model, and (3) LOG-Het, the choice version of the CO model. All benchmark methods were the choice versions of those in Section 3.1, and the implementations were similar to their metric version counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Data.</head><p>Our experimental design and data-generating process largely followed past work that used simulations to evaluate methods for recovering MCH Factor 4. The within-segment variances of distributions: 0.05, 0.10, 0.20, 0.40, 0.60, 0.80, or 1.00</p><p>Hence, we used a 2 3 Ã 7 design, resulting in a total of 56 experimental conditions. We randomly generated 5 data sets for each experimental condition and estimated all conjoint models separately on each data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-generating process.</head><p>In all data sets, each choice set consisted of four conjoint profiles, each associated with a distinct brand. In addition to the three (i.e., 4 â 1 3) brand dummies, the attributes also included one continuous variable and two binary variables. We created four levels for the continuous variable, each being a range: "low" â¡ [â1.3, â0.65], "medium-low" â¡ [â0.65, 0], "medium-high" â¡ [0, 0.65], and "high" â¡ [0.65, 1.3]. For each choice set, we randomly selected a value from each range and assigned the four values to the profiles such that each profile had an equal chance to be assigned with the lowest value. For each of the two binary attributes, we randomly selected a profile in a choice set and set its value on the attribute to 1. We note that the design of the continuous attribute and the two binary attributes was aimed at inducing sufficient variations in the data and is different from those in <ref type="bibr" target="#b3">Andrews et al. (2002a)</ref> and <ref type="bibr" target="#b2">Andrews and Currim (2003)</ref>, which consider scanner panel applications rather than conjoint applications.</p><p>In each data set, the choices of 100 synthetic respondents were generated using a three-step process similar to that in Section 3.1. We closely followed <ref type="bibr" target="#b3">Andrews et al. (2002a)</ref> and <ref type="bibr" target="#b2">Andrews and Currim (2003)</ref> and generated three levels of segment-level coefficients (low, medium, and high) for each of the six attributes (i.e., three brand dummies, one continuous variable, and two binary variables). The rationale for this design with three levels of coefficients was to have different segments assigned with distinct levels of coefficients for each attribute and therefore create clear separations between segments. The medium-level coefficients were generated as follows: the brand-specific constants were sampled from a uniform distribution over the interval [â1, 1], the coefficient of the continuous variable was sampled from a uniform distribution over [â2.5, â2], and the coefficients of the binary variables were sampled from a uniform distribution over [2, 2.5]. The high-level (respectively, low-level) coefficients were generated by adding to (respectively, subtracting from) the corresponding medium-level coefficients a normal random variable drawn from N(1.5, 0.15 2 ), where 1.5 was the mean separation between segments <ref type="bibr" target="#b3">(Andrews et al. 2002a, Andrews and</ref><ref type="bibr" target="#b2">Currim 2003)</ref>. In experimental conditions with three segments (Factor 1), we generated the true segment-level partworths by assigning the three levels of coefficients of each attribute randomly to the three segments. In experimental conditions with two segments, we simply retained the true segment-level partworths of the first two segments generated in the three-segment conditions. We denote the true segment-level partworths for any segment l as Î² l (S).</p><p>Each respondent was randomly assigned to the available segments with equal probabilities. As in Section 3.1, respondent i's true individual-level partworths Î² i (T) were generated as Î² i (T) Î² l (S) + ÏÎ¾ i if respondent i was assigned to segment l, where Ï 2 is the prespecified within-segment variance (Factor 4) and Î¾ i is a vector of independent standard normal random variables. Given Î² i (T), respondent i's choices were stochastically generated according to the logit model where the variance of the type-I extreme value random variables was given by the prespecified error variance (Factor 3). To evaluate the predictive accuracy of the conjoint estimation methods, we generated 8 holdout choice sets for each respondent regardless of whether 16 or 24 choice sets (Factor 2) were used for calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Results.</head><p>We compared the four conjoint estimation methods in terms of parameter recovery and  predictive accuracy. Parameter recovery was assessed using RMSE(Î²). Predictive accuracy was measured using the holdout sample log-likelihood <ref type="bibr" target="#b3">(Andrews et al. 2002a</ref>), which we denote by Holdout-LL. Again, for each experimental condition, we report the average RMSE(Î²) and Holdout-LL across all respondents and data sets. 11 Similar to metric simulation experiments, we find that Choice-SL overall outperforms the benchmark models both in terms of parameter recovery and predictive accuracy. In particular, Choice-SL is the best performing model or indistinguishable from the best model on RMSE(Î²) (at p &lt; 0.05) in 31 out of 56 conditions, and performs best or not significantly different from best on Holdout-LL (at p &lt; 0.05) in 42 out of 56 conditions.</p><p>For the purpose of illustration, we summarize the results for a subset of experimental conditions in Table <ref type="table" target="#tab_7">4</ref>, where Num-S denotes the number of segments in the heterogeneity distribution (Factor 1), Num-CS denotes the number of choice sets per respondent for calibration (Factor 2), EV denotes the error variance (Factor 3), and WSV denotes the within-segment variances of distributions (Factor 4). We note that for RMSE(Î²), lower numbers indicate better performance, whereas for Holdout-LL, higher numbers indicate better performance. The full results for all 56 conditions are reported in the Web appendix.</p><p>We find that results in Table <ref type="table" target="#tab_7">4</ref> are qualitatively similar to those in Table <ref type="table" target="#tab_2">1</ref> except that the FM model becomes the best performing model when WSV is small. As in the metric simulation experiments, we conducted a regression analysis to examine the impact of the experimental factors on both performance measures, RMSE(Î²) and Holdout-LL. The regression specifications were similar to (7). Tables <ref type="table" target="#tab_8">5 and 6</ref> report the results of the OLS estimation.</p><p>Table <ref type="table" target="#tab_8">5</ref> shows that the performance of Choice-SL relative to the NCM model in terms of parameter recovery improves with more segments and a smaller within-segment variance. Table <ref type="table" target="#tab_9">6</ref> shows that the performance of Choice-SL relative to the NCM model in terms of predictive accuracy improves with fewer choice sets for calibration. This finding, consistent with what we found in the metric simulation experiments, further emphasizes the usefulness of our model in contexts in which concerns over response rates and response quality prompt researchers to use short conjoint questionnaires. We also find that more segments and a smaller within-segment variance improve the relative performance of Choice-SL. In Section 4, we leverage these findings to explain the relative performance among models on field data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Field Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metric Conjoint</head><p>We evaluate the performance of our Metric-SL model using a metric conjoint data set of personal computers that was first introduced in <ref type="bibr" target="#b24">Lenk et al. (1996)</ref>. The same data set was also used in <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref> to compare conjoint estimation methods. In the study, 180 respondents each rated 20 hypothetical personal computers on an 11-point scale (0 to 10). Each hypothetical profile was represented using 13 binary attributes and an intercept. The first 16 profiles formed an orthogonal and balanced design and were used for calibration, and the last 4 were used for holdout validation. See <ref type="bibr" target="#b24">Lenk et al. (1996)</ref> and <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref> for details of this data set. We compared the predictive accuracy of four models, Metric-SL, the FM model, the NCM model, and RR-Het using RMSE(Y) and the first choice hits in the holdout sample <ref type="bibr" target="#b4">(Andrews et al. 2002b</ref>), which we denote by 1stCH. For any respondent, 1stCH was set to 1 if the holdout profile with the highest observed rating was correctly predicted and 0 otherwise. We report the average RMSE(Y) and 1stCH across 180 respondents for each method. Table <ref type="table" target="#tab_10">7</ref> summarizes the results. We note that for RMSE(Y), lower numbers indicate better performance, whereas for 1stCH, higher numbers indicate better performance.</p><p>Using paired t-tests over the 180 respondents, we find that Metric-SL and RR-Het perform best or not significantly different from best (at p &lt; 0.10) both in terms of RMSE(Y) and 1stCH. This performance comparison validates the predictive accuracy of Metric-SL; it also suggests that the assumption of a UCH distribution made by RR-Het is not restrictive on this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Choice-Based Conjoint 4.2.1. Application 1-Hotel Choice.</head><p>A total of 188 respondents participated in this study, and each of them was shown 12 choice sets. Each choice set consisted of three hotel profiles and a no-choice option. Seven attributes, including brand, room rate, location, restaurant, gym, Internet access, and rewards points, were used to represent the profiles. The brand attribute was treated as discrete with five levels, e.g., Westin, whereas all other attributes were treated as continuous. We randomly selected 10 out of the 12 choice * Significantly different from best at the p &lt; 0.10 level; * * significantly different from best at the p &lt; 0.05 level; * * * significantly different from best at the p &lt; 0.01 level. sets for each respondent for calibration and used the remaining 2 choice sets for holdout validation.</p><p>We compared the predictive performance of four models-Choice-SL, the FM model, the NCM model, and LOG-Het-using Holdout-LL and the holdout sample hit rate, which we denote by Holdout-HIT. We report the average Holdout-LL and Holdout-HIT across all 188 respondents for each method. The results are summarized in Table <ref type="table" target="#tab_10">7</ref>. We note that for both Holdout-LL and Holdout-HIT, higher numbers indicate better performance. Using paired t-tests over 188 respondents, we find that Choice-SL performs best or not significantly different from best (at p &lt; 0.10) for both Holdout-LL and Holdout-HIT. The NCM model performs significantly worse than best on Holdout-LL, and the FM model and LOG-Het perform significantly worse than best on Holdout-HIT. Thus, the empirical </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Application 2-Cell Phone Plan Choice.</head><p>A total of 72 respondents participated in this study, and each of them was shown 18 choice sets that consisted of three profiles and a no-choice option. Six attributes were used for constructing the conjoint profiles: access fee, per-minute rate, plan minutes, service provider, Internet access, and rollover of unused minutes. The same data set was used in <ref type="bibr" target="#b18">Iyengar et al. (2008)</ref>. We used the best fitting "nonlinear-effects" specification in <ref type="bibr" target="#b18">Iyengar et al. (2008)</ref> that adds logarithmic terms in access fee, per-minute rate, and plan minutes to the standard conjoint specification. <ref type="bibr">12</ref> We randomly selected 15 out of the 18 choice sets for each respondent for calibration and used the remaining 3 choice sets for holdout validation.</p><p>We compared the predictive performance of four models-Choice-SL, the FM model, the NCM model, and LOG-Het-using Holdout-LL and Holdout-HIT. Since the sample size of this data set is relatively small, the paired t-tests among the four models were insignificant on both performance measures. To tackle this issue, we adopted the following alternative statistical test procedure. We generated 10 random replications of the data set. In each replication, we retained all 72 respondents, randomly selected 15 out of the 18 choice sets for each respondent for calibration, and used the remaining 3 choice sets for holdout validation. Each conjoint estimation method was separately applied to each of the 10 replications, and Holdout-LL and Holdout-HIT for each respondent were computed in each replication. We computed the average Holdout-LL and Holdout-HIT across 10 replications for each respondent and compared the four conjoint estimation methods using paired t-tests over the 72 respondents. The results are summarized in Table <ref type="table" target="#tab_10">7</ref>. Recall that for both Holdout-LL and Holdout-HIT, higher numbers indicate better performance. We see from Table <ref type="table" target="#tab_10">7</ref> that Choice-SL performs best (at p &lt; 0.10) on Holdout-LL and the NCM model performs best (at p &lt; 0.10) on Holdout-HIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Comparison Between the Two Choice-Based</head><p>Applications. Table <ref type="table" target="#tab_10">7</ref> shows that the predictive accuracy of Choice-SL compared to the NCM model is more favorable on the hotel data set than on the cell phone plan data set. It is instructive to interpret this comparison using our findings in Section 3.2 regarding how the predictive performance of Choice-SL relative to the NCM model varies with respect to the data characteristics. First, Choice-SL recovers 2 segments in the hotel data set as well as in most replications of the cell phone plan data set, and hence there is no clear evidence suggesting that the two data sets have different numbers of segments. Second, the number of calibration choice sets of the hotel data set (i.e., 10) is smaller than that of the cell phone plan data set (i.e., 15). Third, we use Choice-SL to infer the within-segment variances in both data sets and find that the average inferred within-segment variance for the hotel data set is smaller than that for the cell phone plan data set. Recall that in Section 3.2 we found that Choice-SL is likely to perform better relative to the NCM model in terms of predictive accuracy when the number of calibration choice sets is small and the withinsegment variance is small. Therefore, the results in Table <ref type="table" target="#tab_10">7</ref> are consistent with our findings in the simulation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Graphical Illustration of</head><p>Partworth Estimates In this section, we provide graphical illustrations of the individual-level heterogeneity representations recovered by the four methods on the three field data sets. Given a conjoint estimation method and a data set, we estimate a density for each partworth by applying a kernel smoothing density estimator to the individual-level point estimates of the partworth for all respondents.</p><p>To illustrate, we plot the density estimates for the following partworths. Figure <ref type="figure" target="#fig_1">1</ref> displays the density of intercept in the personal computer data set. The density curves estimated by Metric-SL, the NCM model, and RR-Het are qualitatively similar and exhibit largely unimodal continuous shapes, while the FM model recovers three spikes in the density curve. Figure <ref type="figure" target="#fig_2">2</ref> shows the density of the partworth corresponding to location in the hotel data set. It is evident that the density curves estimated by Choice-SL and the FM model display multimodal continuous shapes, whereas those estimated by the NCM model and LOG-Het are unimodal. In Figure <ref type="figure" target="#fig_3">3</ref>, we plot the density of the partworth corresponding to plan minutes in the cell phone plan data set. The density curves of all methods except LOG-Het show multimodal continuous shapes, with the multimodality estimated by Choice-SL and the FM model being more pronounced than that estimated by the NCM model. Density estimates for other partworths are available from the authors on request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison on Pricing Implications</head><p>We use the hotel data set as an example to compare the pricing implications of the four conjoint estimation methods. To illustrate, we consider a hotel profile with the following attributes: brand set to Westin, location and Internet access set to high levels, and restaurant, gym, and rewards points set to medium levels. We use the individual-level partworth estimates obtained from each method to derive the individuallevel willingness to pay (WTP) defined as the price at which a respondent is indifferent between choosing this particular hotel profile and the no-choice option <ref type="bibr" target="#b19">(Jedidi and Zhang 2002)</ref>. To ensure that the WTP estimates are plausible, we set the minimum (respectively, maximum) feasible WTP to $0 (respectively, $1,000). We find that the primary distinguishing characteristic between the WTPs estimated by Choice-SL and the other three methods is that the latter infer a large WTP for more respondents. Choice-SL infers that 34.6% of respondents have a WTP greater than $300, whereas the NCM model, the FM model, and LOG-Het estimate this proportion to be 50.5%, 41.0%, and 43.1%, respectively. This difference in the estimates for the fraction of respondents with large WTPs has a substantial impact on the revenue-maximizing prices implied by different methods. <ref type="bibr">13</ref> Choice-SL, the NCM model, the FM model, and LOG-Het set the revenue-maximizing prices to $216, $477, $458, and $790, respectively. Furthermore, Choice-SL, the NCM model, the FM model, and LOG-Het estimate the proportion of respondents who would prefer the hotel profile described above to the no-choice option at the revenue-maximizing prices to be 55.3%, 33.5%, 37.2%, and 17.6%, respectively. Hence, the four conjoint estimation methods imply different pricing strategies. Choice-SL recommends using a moderate price to capture a large chunk of the market, whereas the other three methods (especially LOG-Het) recommend using a high price to extract revenue from a smaller segment of respondents with high WTPs. Given that the highest price shown in all hotel profiles was $250, we find that the pricing decision of Choice-SL has higher face validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Consumer preferences can often be modeled using an MCH distribution, and adequate modeling of MCH is critical for accurate conjoint estimation. In this paper, we propose an innovative SL approach for modeling MCH. The SL approach models MCH via a two-stage divide-and-conquer framework, in which  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LOG-Het</head><p>MCH is decomposed into a small collection of withinsegment UCH distributions using sparse learning methodology, and each UCH is then modeled separately. Consequently, we explicitly account for both across-segment and within-segment heterogeneity in the SL model. In addition, the amount of shrinkage imposed to recover the individual-level partworths is endogenously selected using cross-validation.</p><p>We test the empirical performance of our SL model and compare it to the finite mixture model <ref type="bibr" target="#b21">(Kamakura and</ref><ref type="bibr">Russell 1989, Chintagunta et al. 1991)</ref>, the Bayesian normal component mixture model , and the convex optimization model of <ref type="bibr" target="#b14">Evgeniou et al. (2007)</ref> using extensive simulation experiments and three field data sets. We find that our SL model demonstrates a consistently strong performance across a wide range of experimental conditions as well as field data sets with distinct characteristics. We also show the managerial relevance of our SL model using an optimal pricing exercise in which the SL model generates a more plausible revenue-maximizing price.</p><p>There are several promising avenues for future research. First, we can consider an extension of our SL model by incorporating kernel methods <ref type="bibr" target="#b37">(Vapnik 1998)</ref>, which were introduced to marketing by <ref type="bibr" target="#b12">Cui and Curry (2005)</ref> and <ref type="bibr" target="#b13">Evgeniou et al. (2005)</ref>. Second, researchers can also consider other population-based complexity controls to improve the capability for modeling MCH. Third, our SL model, like the finite mixture model and the Bayesian normal component mixture model, can be applied to estimate consumers' heterogeneous preferences in settings other than conjoint analysis, e.g., scanner panel data sets, and it may be fruitful to compare our SL model with benchmark models in such settings. Finally, an interesting research direction is to explore the potential of machine learning methods in modeling other phenomena in marketing beyond consumer heterogeneity. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Marketing</head><label></label><figDesc>Science 36(1), pp. 140-156, Â© 2017 INFORMS performance comparison validates the predictive accuracy of Choice-SL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (Color online) Density Plots: Intercept in the Personal Computer Data Set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (Color online) Density Plots: The Partworth Corresponding to Location in the Hotel Data Set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (Color online) Density Plots: The Partworth Corresponding to Plan Minutes in the Cell Phone Plan Data Set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>RMSE(Î²)  and RMSE(Y) for a Subset of Experimental Conditions</figDesc><table><row><cell>RMSE(Î²)</cell><cell>RMSE(Y)</cell></row></table><note>Note. Bold numbers in each experimental condition for each performance measure indicate best or not significantly different from best at the p &lt; 0.05 level based on paired t-tests.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Regression Analysis of RMSE(Î²) for Metric Simulations</figDesc><table><row><cell>Variable</cell><cell>Metric-SL</cell><cell>NCM</cell><cell>FM</cell><cell>RR-Het</cell><cell>Metric-SL â NCM</cell></row><row><cell>Intercept</cell><cell>0.2033  *  *  *</cell><cell>0.2884  *  *  *</cell><cell>0.2392  *  *  *</cell><cell>0.2582  *  *  *</cell><cell>â0.0851  *  *  *</cell></row><row><cell>Num-S</cell><cell>0.0062</cell><cell>0.0068</cell><cell>0.0146</cell><cell>0.0105</cell><cell>â0.0006</cell></row><row><cell>Num-P</cell><cell>â0.0427  *  *  *</cell><cell>â0.0609  *  *  *</cell><cell>â0.0123</cell><cell>â0.0589  *  *  *</cell><cell>0.0181  *  *</cell></row><row><cell>EV</cell><cell>0.1217  *  *  *</cell><cell>0.1493  *  *  *</cell><cell>0.0253  *  *</cell><cell>0.1471  *  *  *</cell><cell>â0.0277  *  *  *</cell></row><row><cell>WSV</cell><cell>0.2254  *  *  *</cell><cell>0.1106  *  *  *</cell><cell>0.7622  *  *  *</cell><cell>0.1539  *  *  *</cell><cell>0.1147  *  *  *</cell></row><row><cell>R 2</cell><cell>0.86</cell><cell>0.97</cell><cell>0.98</cell><cell>0.93</cell><cell>0.69</cell></row></table><note>Notes. The dependent variables in the second, third, fourth, and fifth columns are RMSE(Î²)'s of Metric-SL, NCM, FM, and RR-Het, respectively; the dependent variable in the sixth column is the difference between RMSE(Î²)'s of Metric-SL and NCM.* * p &lt; 0.05; * * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Regression Analysis of RMSE(Y) for Metric Simulations The dependent variables in the second, third, fourth, and fifth columns are RMSE(Y)'s of Metric-SL, NCM, FM, and RR-Het, respectively; the dependent variable in the sixth column is the difference between RMSE(Y)'s of Metric-SL and NCM.</figDesc><table><row><cell>Variable</cell><cell>Metric-SL</cell><cell>NCM</cell><cell>FM</cell><cell>RR-Het</cell><cell>Metric-SL â NCM</cell></row><row><cell>Intercept</cell><cell>0.7662  *  *  *</cell><cell>0.8154  *  *  *</cell><cell>0.8861  *  *  *</cell><cell>0.8024  *  *  *</cell><cell>â0.0492  *  *  *</cell></row><row><cell>Num-S</cell><cell>0.0108</cell><cell>0.0119  *  *</cell><cell>0.0327  *  *</cell><cell>0.0140  *  *</cell><cell>â0.0012</cell></row><row><cell>Num-P</cell><cell>â0.0488  *  *  *</cell><cell>â0.0607  *  *  *</cell><cell>â0.1058  *  *  *</cell><cell>â0.0632  *  *  *</cell><cell>0.0120  *  *</cell></row><row><cell>EV</cell><cell>0.5497  *  *  *</cell><cell>0.5636  *  *  *</cell><cell>0.3989  *  *  *</cell><cell>0.5639  *  *  *</cell><cell>â0.0139  *  *  *</cell></row><row><cell>WSV</cell><cell>0.1395  *  *  *</cell><cell>0.0728  *  *  *</cell><cell>0.9374  *  *  *</cell><cell>0.0954  *  *  *</cell><cell>0.0667  *  *  *</cell></row><row><cell>R 2</cell><cell>0.99</cell><cell>0.99</cell><cell>0.98</cell><cell>0.99</cell><cell>0.67</cell></row><row><cell cols="2">Notes. *  p &lt; 0.05;  *  *  *  p &lt; 0.01.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">using choice data (Andrews et al. 2002a, Andrews and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Currim 2003).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Experimental design. We experimentally manipu-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">lated four data characteristics:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Factor 1. The number of segments: 2 or 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Factor 2. The number of choice sets per respondent</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(for calibration): 16 or 24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Factor 3. The error variance: standard (1.645) or</cell><cell></cell><cell></cell><cell></cell></row><row><cell>high (3.290)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*   </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>RMSE(Î²)  and Holdout-LL for a Subset of Experimental Conditions</figDesc><table><row><cell>RMSE(Î²)</cell><cell>Holdout-LL</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Regression Analysis of RMSE(Î²) for Choice-Based Simulations</figDesc><table><row><cell>Variable</cell><cell>Choice-SL</cell><cell>NCM</cell><cell>FM</cell><cell>LOG-Het</cell><cell>Choice-SL â NCM</cell></row><row><cell>Intercept</cell><cell>0.4446  *  *  *</cell><cell>0.6287  *  *  *</cell><cell>0.3143  *  *  *</cell><cell>0.6153  *  *  *</cell><cell>â0.1841  *  *  *</cell></row><row><cell>Num-S</cell><cell>0.0270  *  *</cell><cell>0.0668  *  *  *</cell><cell>0.0364  *  *</cell><cell>0.0535  *  *  *</cell><cell>â0.0398  *  *</cell></row><row><cell>Num-CS</cell><cell>â0.0919  *  *  *</cell><cell>â0.1093  *  *  *</cell><cell>â0.0450  *  *  *</cell><cell>â0.0958  *  *  *</cell><cell>0.0174</cell></row><row><cell>EV</cell><cell>0.0426  *  *  *</cell><cell>0.0658  *  *  *</cell><cell>0.0070</cell><cell>0.0415  *  *  *</cell><cell>â0.0232</cell></row><row><cell>WSV</cell><cell>0.3834  *  *  *</cell><cell>0.1405  *  *  *</cell><cell>0.7626  *  *  *</cell><cell>0.2230  *  *  *</cell><cell>0.2429  *  *  *</cell></row><row><cell>R 2</cell><cell>0.94</cell><cell>0.88</cell><cell>0.96</cell><cell>0.93</cell><cell>0.71</cell></row><row><cell cols="6">Notes. The dependent variables in the second, third, fourth, and fifth columns are RMSE(Î²)'s of</cell></row><row><cell cols="6">Choice-SL, NCM, FM, and LOG-Het, respectively; the dependent variable in the sixth column is the</cell></row><row><cell cols="4">difference between RMSE(Î²)'s of Choice-SL and NCM.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* * p &lt; 0.05; * * * p &lt; 0.01.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Regression Analysis of Holdout-LL for Choice-Based Simulations</figDesc><table><row><cell>Variable</cell><cell>Choice-SL</cell><cell>NCM</cell><cell>FM</cell><cell>LOG-Het</cell><cell>Choice-SL â NCM</cell></row><row><cell>Intercept</cell><cell>â0.7234  *  *  *</cell><cell>â0.7580  *  *  *</cell><cell>â0.7156  *  *  *</cell><cell>â0.7609  *  *  *</cell><cell>0.0346  *  *  *</cell></row><row><cell>Num-S</cell><cell>â0.0002</cell><cell>â0.0067</cell><cell>â0.0034</cell><cell>â0.0035</cell><cell>0.0064  *  *  *</cell></row><row><cell>Num-CS</cell><cell>0.0117</cell><cell>0.0175  *  *</cell><cell>0.0059</cell><cell>0.0164  *  *</cell><cell>â0.0058  *  *</cell></row><row><cell>EV</cell><cell>â0.1888  *  *  *</cell><cell>â0.1899  *  *  *</cell><cell>â0.1713  *  *  *</cell><cell>â0.1874  *  *  *</cell><cell>0.0011</cell></row><row><cell>WSV</cell><cell>0.0058</cell><cell>0.0370  *  *  *</cell><cell>â0.1037  *  *  *</cell><cell>0.0473  *  *  *</cell><cell>â0.0312  *  *  *</cell></row><row><cell>R 2</cell><cell>0.91</cell><cell>0.92</cell><cell>0.88</cell><cell>0.92</cell><cell>0.68</cell></row><row><cell cols="6">Notes. The dependent variables in the second, third, fourth, and fifth columns are Holdout-LL's of</cell></row><row><cell cols="6">Choice-SL, NCM, FM, and LOG-Het, respectively; the dependent variable in the sixth column is the</cell></row><row><cell cols="4">difference between Holdout-LL's of Choice-SL and NCM.</cell><cell></cell><cell></cell></row><row><cell cols="2">*  *  p &lt; 0.05;  *  *  *  p &lt; 0.01.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Field Conjoint Data Sets</figDesc><table><row><cell></cell><cell cols="3">The personal computer data set</cell><cell></cell></row><row><cell></cell><cell>Metric-SL</cell><cell>NCM</cell><cell>FM</cell><cell>RR-Het</cell></row><row><cell>RMSE(Y)</cell><cell>1.6099</cell><cell>1.6558  *  *  *</cell><cell>1.8639  *  *  *</cell><cell>1.6072</cell></row><row><cell>1stCH</cell><cell>0.7056</cell><cell>0.6722  *  *</cell><cell>0.5889  *  *  *</cell><cell>0.6944</cell></row><row><cell></cell><cell cols="2">The hotel data set</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Choice-SL</cell><cell>NCM</cell><cell>FM</cell><cell>LOG-Het</cell></row><row><cell>Holdout-LL</cell><cell>â0.9270</cell><cell>â1.0297  *  *</cell><cell>â0.9192</cell><cell>â0.9305</cell></row><row><cell>Holdout-HIT</cell><cell>0.6330</cell><cell>0.6410</cell><cell>0.5878  *  *</cell><cell>0.6090  *</cell></row><row><cell></cell><cell cols="3">The cell phone plan data set</cell><cell></cell></row><row><cell></cell><cell>Choice-SL</cell><cell>NCM</cell><cell>FM</cell><cell>LOG-Het</cell></row><row><cell>Holdout-LL</cell><cell>â0.9205</cell><cell>â0.9540  *</cell><cell>â0.9944  *  *  *</cell><cell>â0.9389  *</cell></row><row><cell>Holdout-HIT</cell><cell>0.6278  *</cell><cell>0.6407</cell><cell>0.5856  *  *  *</cell><cell>0.6190  *  *  *</cell></row><row><cell cols="5">Notes. For RMSE(Y), lower numbers indicate better performance;</cell></row><row><cell cols="5">for 1stCH, higher numbers indicate better performance; for</cell></row><row><cell cols="5">Holdout-LL, higher numbers indicate better performance; and for</cell></row><row><cell cols="5">Holdout-HIT, higher numbers indicate better performance. Bold</cell></row><row><cell cols="5">numbers indicate best or not significantly different from best at the</cell></row><row><cell>p &lt; 0.10 level.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Marketing Science 36(1), pp.140-156, Â© 2017 INFORMS    </figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Chen, Iyengar, and Iyengar: Modeling MCH in Conjoint Analysis Marketing Science 36(1), pp. 140-156, Â© 2017 INFORMS</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Rick Andrews for sharing the conjoint designs used in Section 3.1, Peter Lenk for sharing the personal computer data set used in Section 4.1, and Rajan Sambandam from TRC Market Research for sharing the hotel data set used in Section 4.2.1. The authors also thank Eric Bradlow, Jeff Cai, Daria Dzyabura, Arun Gopalakrishnan, and Olivier Toubia for their insightful comments. The first two authors acknowledge the generous support of the Alex Panos Research Fund of the Wharton School of the University of Pennsylvania.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Endnotes <ref type="bibr">1</ref> Applications of nonparametric Bayesian methods in marketing include the Dirichlet process mixture model <ref type="bibr">Mela 2003, Kim et al. 2004</ref>) and the centered Dirichlet process mixture model <ref type="bibr" target="#b25">(Li and Ansari 2013)</ref>. While nonparametric Bayesian methods provide more flexibility, they still suffer from the same limitations faced by the NCM model. With ongoing research in this area, we expect to see systematic comparisons between the benefits of using parametric and nonparametric Bayesian methods. In this paper, we compare our model with the FM and NCM models, which are more established modeling frameworks. <ref type="bibr">2</ref> We discuss the rationale behind sparsity-inducing penalty functions in the Web appendix. <ref type="bibr">3</ref> The specification for {Î¸ ik } in (3) uses only information contained in the conjoint data. Other information sources, e.g., consumers' demographic variables, can be readily incorporated in the specification for {Î¸ ik }, and hence our SL model via a simple extension of (3). We discuss the extension in the Web appendix. <ref type="bibr">4</ref> We note that in (Metric-SEG), the amount of penalty imposed on</p><p>In the empirical implementation of our SL model, we normalize Î¸ (Î¸ ik ) such that ||Î¸|| 2 1 and interpret the regularization parameter Î» as controlling the "total" amount of penalty imposed on Î² S i â Î² S k 2 's. <ref type="bibr">5</ref> The specification for Î used in simulation experiments and field applications is summarized in the Web appendix. <ref type="bibr">6</ref> In the empirical implementation of our SL model, we set M 10%I, such that any valid segment contains a nonnegligible portion of the population. The simulation experiments and field applications confirm the effectiveness of our choice of M. 7 Recall that we also obtain a set of individual-level partworth estimates {B(Î)} by solving (Metric-SEG). We retain only the set of candidate segmentations Î¦ and exclude {B(Î)} as the output of the first stage because the latter are biased. We provide a detailed discussion about the bias in {B(Î)} in the Web appendix. The specification for Î used in simulation experiments and field applications is summarized in the Web appendix. 9 In addition to parameter recovery and predictive accuracy, we also compared the computation times of Metric-SL and the NCM model and report the results in the Web appendix. 10 Num-S-Dummy t 1 when Num-S t 3; Num-P-Dummy t 1 when Num-P t 27; and EV-Dummy t 1 when EV t 1.5. 11 In addition to parameter recovery and predictive accuracy, we also compared the computation time of Choice-SL and the NCM model and report the results in the Web appendix. <ref type="bibr">12</ref> We differed from <ref type="bibr" target="#b18">Iyengar et al. (2008)</ref> in that we standardized all continuous attributes, i.e., each continuous attribute was demeaned and divided by its standard deviation, before model estimation. The standardization is a widely adopted technique in the statistics and machine learning literature <ref type="bibr" target="#b33">(Tibshirani 1996)</ref> that ensures that all continuous attributes have similar scales. <ref type="bibr">13</ref> If cost data were present, we could determine the profit-maximizing price.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marketing models of consumer heterogeneity</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econometrics</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="78" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the heterogeneity of demand</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="384" to="389" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of segment retention criteria for finite mixture logit models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Currim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical comparison of logit choice models with discrete versus continuous representations of heterogeneity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ainslie</forename><forename type="middle">A</forename><surname>Currim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="487" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical Bayes versus finite mixture conjoint analysis models: A comparison of fit, prediction, and partworth recovery</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Currim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">E-customization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Mela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="145" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convex multi-task feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learn</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="272" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convex optimization with sparsity-inducing norms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="19" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computational and inferential difficulties with mixture posterior distributions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hurn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">451</biblScope>
			<biblScope unit="page" from="957" to="970" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smoothing proximal gradient method for general structured sparse regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="719" to="752" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigating heterogeneity in brand preferences in logit models for panel data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chintagunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Vilcassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="428" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prediction in marketing using the support vector machine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Curry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="595" to="615" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized robust conjoint estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boussios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zacharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convex optimization approach to modeling consumer heterogeneity in conjoint estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="805" to="818" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conjoint analysis in marketing: New developments with implications for research and practice</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Elements of Statistical Learning</title>
		<title level="s">Springer Series in Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clusterpath: An algorithm for clustering using convex fusion penalties</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Hocking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Internat. Conf. Machine Learn</title>
				<meeting>28th Internat. Conf. Machine Learn<address><addrLine>Bellevue, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A conjoint approach to multipart pricing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jedidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="210" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Augmenting conjoint analysis to estimate consumer reservation price</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jedidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1350" to="1368" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiscale mining of fMRI data with hierarchical structured sparsity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="835" to="856" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A probabilistic choice model for market segmentation and elasticity structure</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Kamakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="379" to="390" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Assessing heterogeneity in discrete choice models using a Dirichlet process prior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Menzefricke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Feinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eQTL mapping</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1095" to="1117" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical Bayes conjoint analysis: Recovery of partworth heterogeneity from reduced experimental designs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Desarbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="191" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Bayesian semiparametric approach for endogeneity and heterogeneity in choice models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1161" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Product line design for consumer durables: An integrated marketing and engineering approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="139" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structured sparsity via alternating direction methods</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1435" to="1468" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<title level="m">Bayesian Statistics and Marketing</title>
				<meeting><address><addrLine>West Sussex, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The value of purchase history data in target marketing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="340" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linear model selection by cross-validation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">422</biblScope>
			<biblScope unit="page" from="486" to="494" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian measures of model complexity and fit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc.: Ser. B (Statist. Methodology)</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="639" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dealing with label switching in mixture models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc.: Ser. B (Statist. Methodology)</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="795" to="809" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc.: Ser. B (Statist. Methodology)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparsity and smoothness via the fused lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc.: Ser. B (Statist. Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="108" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Polyhedral methods for adaptive choice-based conjoint analysis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Simester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast polyhedral adaptive conjoint estimation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="303" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Metric conjoint segmentation methods: A Monte Carlo comparison</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vriens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Spline Models for Observational Data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<pubPlace>SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Commercial use of conjoint analysis: An update</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cattin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="91" to="96" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc.: Ser. B (Statist. Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
