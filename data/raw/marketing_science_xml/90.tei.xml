<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Using Conditional Restricted Boltzmann Machines to Model Complex Consumer Shopping Patterns</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-07-12">July 12, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feihong</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rhode Island College of Business Administration</orgName>
								<address>
									<postCode>02881</postCode>
									<settlement>Kingston, Rhode Island</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Rabikar</forename><surname>Chatterjee</surname></persName>
							<email>rabikar@katz.pitt.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Katz Graduate School of Business</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jerrold</forename><forename type="middle">H</forename><surname>May</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Katz Graduate School of Business</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Using Conditional Restricted Boltzmann Machines to Model Complex Consumer Shopping Patterns</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2019-07-12">July 12, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2019.1162</idno>
					<note type="submission">Received: August 19, 2015 Revised: February 27, 2017; June 22, 2018; February 5, 2019 Accepted: February 11, 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>big data</term>
					<term>sparsity</term>
					<term>regularization</term>
					<term>consumer decision patterns</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Marketers have recognized that the probability of a consumer's (or household's) purchase in a particular product category may be influenced by past purchases in the same category and also, purchases in other related categories. Past studies of crosscategory effects have focused on a limited number of product categories, and they have often ignored intertemporal effects in their analyses. Those studies have generally used multivariate logit or probit models, which are limited in their ability to analyze enormous data sets that contain consumer purchase records across a large number of categories and time periods. The availability of such enormous consumer shopping data sets and the value of analyzing the complex relationships across categories and over time (for example, for personalized promotions) suggest the need for computationally efficient modeling and estimation methods. Such models can capture associations among buying decisions across all product categories and over all time periods for which data are available, but they must also have a tractable and clear model structure that permits meaningful interpretation of the results. We explore the nature of intertemporal crossproduct patterns in an enormous consumer purchase data set using a model that adopts the structure of conditional restricted Boltzmann machines (CRBMs). Our empirical results demonstrate that our proposed approach using the efficient estimation algorithm embodied in the CRBM enables us to process very large data sets and capture the consumer decision patterns for both predictive and descriptive purposes that might not otherwise be apparent. In addition to persistent intertemporal within-category effects, we find that there are also significant intertemporal cross effects between product categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Big data and mobile marketing have transformed the landscape of marketing, with companies using consumer data to launch effective personalized coupons and highly targeted advertisements. In this brave new world, marketing managers must "get the timing right" and "rely on data to make offers relevant" <ref type="bibr" target="#b24">(Miles 2013</ref>). Machine learning, especially deep learning, has fueled much of the rapid progress in consumer product recommendation and purchase prediction. Deep networks have achieved remarkable results in image and speech recognition, bringing huge benefits to businesses and consumers alike. Google, Microsoft, Apple, Facebook, Netflix, Amazon, and others have created their own deep learning engines to generate recommendations based on analysis of consumer data. Some companies have released their open source deep learning framework (e.g., Google with TensorFlow). However, most deep networks used in business applications, such as convolutional neural networks, deep reinforcement learning, and recurrent neural networks, are nonprobabilistic, with model structures that provide no basis for theoretical interpretation of the results.</p><p>For personalized marketing, an ideal model should capture and predict shopping patterns efficiently and accurately from large consumer data sets to both assist in forecasting consumers' future purchase decisions and permit inferences with marketing decision-making implications. We propose an efficient probabilistic model that captures intertemporal multicategory purchasing patterns from such data to address these needs. Our ability to model intertemporal crossproduct effects on a massive scale has been influenced by two factors. First, we now have access to massive amounts of consumer-level (or household-level) purchase data recorded at the point of sale by individual retailers and compiled by marketing research companies, such as Nielsen. Second, efficient methods have been developed that permit the analysis of such rich and massive data sets.</p><p>Traditional modeling approaches in marketing are econometrics based, emphasizing unbiased parameter estimation rather than algorithm efficiency <ref type="bibr" target="#b23">(Mehta 2007)</ref>. However, machine learning models (e.g., neural networks) can handle large data sets, but the results are typically difficult to interpret. To model large-scale, intertemporal, crosscategory effects using the massive data sets available today, we need a parsimonious model with parameters that must be estimated efficiently but a tractable and clear model structure that permits meaningful interpretation of the results.</p><p>Previous research on multicategory consumer purchasing decisions has focused primarily on a few product categories owing to model estimation limitations. Furthermore, discussion of the intertemporal aspects of multicategory buying decision is meagre <ref type="bibr" target="#b22">(Manchanda et al. 1999</ref><ref type="bibr" target="#b8">, Edwards and Allenby 2003</ref><ref type="bibr" target="#b30">, Seetharaman et al. 2005</ref>. One major challenge for such analysis is the rapid growth in the size of the correlation matrix as the numbers of product categories and time periods increase. <ref type="bibr">1</ref> To make the analysis less time consuming, the typical approach is to preselect a few categories, which are expected be either complements or substitutes ex ante, and then, use multivariate probit (MVP) or multivariate logit methods to model the cross effects <ref type="bibr" target="#b22">(Manchanda et al. 1999</ref><ref type="bibr" target="#b26">, Russell and Petersen 2000</ref><ref type="bibr" target="#b7">, Duvvuri et al. 2007</ref><ref type="bibr" target="#b2">, Boztug and Hildebrandt 2008</ref>.</p><p>A more recent stream of literature in marketing focuses on empirical methods for handling highdimensional data. One intuitive approach is to first reduce the dimensionality to a manageable level and then, conduct the analysis on the lower-dimensional data set <ref type="bibr" target="#b20">(Lawrence 2004)</ref>. Latent linear models, such as factor analysis and principal component analysis, are the most popular dimension reduction methods in marketing research. However, the benefits of applying such methods in multicategory analysis are not apparent. <ref type="bibr" target="#b6">Duvvuri and Gruca (2010)</ref> develop a Bayesian multilevel factor model for consumer price sensitivity analysis across product categories, but this model yields less accurate predictions than the regular MVP model. The algorithm of <ref type="bibr" target="#b34">Talhouk et al. (2012)</ref> for efficient and sparse MVP modeling improves efficiency and reduces the number of parameters, but the estimation relies on Markov chain Monte Carlo (MCMC) sampling, which is inefficient when analyzing very large data sets. <ref type="bibr">2</ref> A model that utilizes a parsimonious (or technically, sparse) representation of the associations between product purchasing decisions and an efficient inference algorithm is needed for high-dimensional multicategory purchasing decision analysis. The restricted Boltzmann machine (RBM) possesses such advantages <ref type="bibr" target="#b17">(Hruschka 2014)</ref>.</p><p>We significantly extend the RBM to develop an efficient interpretable model for the analysis of consumer purchasing patterns on a very large scale that can capture (i) intertemporal in addition to contemporaneous multicategory cross effects, (ii) the impact of additional variables (e.g., marketing mix) that may influence purchasing patterns, and (iii) consumer heterogeneity. To the best of our knowledge, this is the first time that a conditional restricted Boltzmann machine (CRBM) has been adapted for application to a marketing problem. We describe in detail how the model parameters are estimated, including our extension of the CRBM in the machine learning literature to handle the sparseness that is typical of marketing data (as in our illustrative application), and then, compare its performance with that of currently popular models to demonstrate its strengths, especially in analyzing very large data sets.</p><p>The empirical illustration analyzes household-level supermarket shopping histories based on panel data on purchases made by each of 4,000 households across 1,055 product categories over 208 weeks, totaling nearly 6.7 million observations. We focus on describing the methodology and illustrating its application, and we only briefly touch on the substantive implications of the results. Thus, the contribution of this paper is primarily methodological, and a more comprehensive application (or multiple applications) of the model for improved marketing decision making (for example, in personalizing mobile couponing) is left to future research.</p><p>Next, Section 2 develops the model and motivates our methodological contribution. Section 3 describes the model estimation procedure, which is both efficient and effective. Section 4 provides an illustrative empirical application with an evaluation of the model's performance relative to the multivariate probit benchmark. Section 5 concludes by summarizing our contributions and noting some avenues for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model Development</head><p>We first describe the latent variable model in general and then, the basic RBM as it applies to modeling large crosscategory purchase data. Next, we extend the RBM using a CRBM structure to model the contemporaneous and intertemporal crosscategory effects efficiently while including other variables that might impact consumer behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Latent Variable Model</head><p>In probabilistic modeling, associations between variables can be captured in several ways. The "coincidence" of, say, milk and bread purchases can be represented Xia, Chatterjee, and May: Conditional Restricted Boltzmann Machines simply by a correlation coefficient. Alternately, one can explicitly model an unobservable common cause driving purchases of both milk and bread, which can help explain the coincidence. <ref type="bibr">3</ref> A nonlinear latent structure has several advantages over (linear) correlations in modeling associations. First, the correlation matrix assumes that all variables are Gaussian, which works well if the associations are indeed linear. For latent variable models, the association can be linear (linear factor model) or nonlinear (RBM), with the latter providing greater modeling flexibility <ref type="bibr" target="#b3">(Burnap et al. 2014)</ref>. Second, although correlations capture pairwise associations, a latent variable structure can model associations among multiple variables. Third, in consumer choice modeling, there can be various types of associations among purchasing decisions, where a single correlation matrix is insufficient to represent them all. For example, at the universal product code (UPC) level, there are associations within and between brands as well as within and between product categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Restricted Boltzmann Machines</head><p>RBMs, unlike nonprobabilistic feedforward neural networks, such as convolutional neural network, are essentially hierarchical Bayesian models with structures that can be adjusted and interpreted just as for "traditional" statistical models, albeit with latent structures. Researchers can construct the RBM based on theory or hypotheses and gain insights from the parameters and latent variables. In theory, the RBM is a probabilistic model that defines the joint distribution of binary visible and hidden (latent) variables <ref type="bibr" target="#b15">(Hinton 2002)</ref>. The network distribution is based on the energy function in physics:</p><formula xml:id="formula_0">E(X, H) − ∑ I ∑ J β ij x i h j − ∑ I β i x i − ∑ J β j h j ,<label>(1)</label></formula><p>where X is a vector of I visible variables and H is a vector of J hidden variables. High-energy states tend to be unstable, and hence, they are rare events in statistical terms. The joint probability distribution function of the network then should assign a low probability to highenergy states and a high probability to low-energy states:</p><formula xml:id="formula_1">P(X, H) exp( − E) z exp( ∑ I ∑ J β ij x i h j + ∑ I β i x i + ∑ J β j h j ) z .<label>(2)</label></formula><p>The exponential function ensures that the probability is nonnegative; z is the normalizing constant. In the basic RBM, all variables are binary. Instead of using correlation coefficients to capture the interdependencies between visible variables directly, the RBM uses hidden variables to model the multivariate distribution in the visible layer. The structure of the RBM is analogous to that of the latent factor model-all of the visible variables are connected to the hidden variables, and there are no direct connections among visible variables. However, unlike the factor model, where only the conditional distribution of visible variables based on hidden variables is defined, the RBM has a symmetric structure between hidden and visible layers so that the conditional distributions are defined in both directions (Figure <ref type="figure" target="#fig_0">1</ref>). Following Bayes' rule, the joint distribution of the visible variables conditioned on all hidden variables is</p><formula xml:id="formula_2">P(X 1|H) P(X 1, H) ∑ X P(X, H) ∏ X exp ( ∑ J β ij h j + β i ) exp ( ∑ J β ij h j + β i ) + 1 .</formula><p>(3) Equation ( <ref type="formula">3</ref>) shows an interesting property of the RBM: that all visible variables are conditionally independent or</p><formula xml:id="formula_3">P(X 1|H) ∏ X P(x i 1|H).</formula><p>Because of the symmetric structure of the RBM, the hidden variables are also independent conditioned on all visible variables. In our model setting, the visible layer consists of binary variables representing the observed purchasing decisions for a single household s. <ref type="bibr">4</ref> Let i index a product category and t denote a time period, typically a week. The variable x sit in the visible layer takes on the value of one if household s purchased at least one unit of one item from category i in period t and zero otherwise. The conditional distribution of each visible variable takes the simple logistic form</p><formula xml:id="formula_4">P(x sit 1|H s ) 1 1 + exp ( −β i − ∑ J β ji h sj ),<label>(4)</label></formula><p>where H s is a household-specific J-dimensional hidden vector, H s (h s1 , h s2 , ⋯h sJ ), and β i is a constant term for each product category. For multicategory analysis ignoring intertemporal effects, our task is to model the joint distribution P(x s1t , x s2t . . .x s3t ). In the MVP model, the symmetric covariance matrix captures the crosscategory associations explicitly. As discussed, the MVP becomes rapidly Marketing <ref type="bibr">Science, 2019</ref><ref type="bibr">, vol. 38, no. 4, pp. 711-727, © 2019</ref> as the number of categories increases. In contrast, the RBM captures relationships among the variables in time period t with far fewer parameters than are necessary for MVP by using hidden variables to capture relationships among the variables in the visible layer. In fact, the complexity of the RBM model grows at a rate less than O(n).</p><p>The inclusion of hidden variables in the RBM model is consistent with the intuition that purchasing decisions are influenced by unobservable household attributes <ref type="bibr" target="#b33">(Shocker et al. 2004)</ref>. For instance, some households may be more likely to buy frozen pizza for dinner than vegetables owing to different lifestyles, which may not be observable but can be inferred from each household's purchasing pattern. One can use unobservable household attributes to explain the relationships among purchasing decisions. Being unobservable, they are modeled as binary hidden variables, with values derived from the observed purchasing decisions. Several hidden variables may be needed to describe a household's complex purchasing behavior. The household decisions then become a mixture of patterns determined by these hidden variables.</p><p>For illustration, as part of our empirical application described later in Section 4, we analyzed household decision patterns for the soup, pasta sauce, dog food, and pasta categories, which are ordered in terms of purchasing frequency. We used two hidden units in a two-layer RBM to capture the associations. Table <ref type="table" target="#tab_0">1</ref> shows the parameter values of the connections between hidden and visible variables.</p><p>Just as in factor analysis, we can infer the associations between purchasing decisions by examining the signs and magnitudes of the parameters. For each hidden variable, the product categories are positively associated if the connections to those product categories have the same sign. The magnitudes capture the strength of the associations. Viewing the connections to the first hidden variable in Table <ref type="table" target="#tab_0">1</ref>, the strongest associations are among soup, pasta sauce, and pasta (all positive), whereas dog food is independent of other product categories. The parameters of the second hidden units tell a slightly different story: only soup and pasta sauce are positively associated. Households in general would exhibit either purchasing pattern or a mix. The parameters are akin to those in logistic regression if we think of the hidden units as the independent variables and purchase decisions as the dependent variables.</p><p>The binary hidden variables can be viewed as discrete labels that summarize household shopping patterns, performing the task of market segmentation implicitly. For example, with two hidden variables, we have 2 2 possible combinations of hidden variables, yielding 4 market segments based on different shopping patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Proposed Model: Conditional RBM</head><p>The basic RBM model only captures contemporaneous crosscategory effects. However, we know from the marketing literature that households' past purchasing decisions are predictive of their future purchases <ref type="bibr" target="#b18">(Keenan 1982</ref><ref type="bibr" target="#b9">, Fader et al. 2005</ref>. In practice, contemporaneous and intertemporal effects have been typically modeled and analyzed separately owing to computational power constraints. We propose a model adapting an extension to the RBM, the CRBM, to incorporate both contemporaneous and intertemporal effects for a complete picture of household purchasing patterns. The base model is the same RBM described in previous section:</p><formula xml:id="formula_5">P(X st , H s ) exp( −β X X st − β H H s − β XH X st H s ) z .<label>(5)</label></formula><p>All parameters and variables are in vector form in the equation. For example, X st is a binary vector representing purchasing decisions of household s over the set of product categories at time t. The literature suggests that household demographics (such as size and income) may account for some of the heterogeneity in purchasing patterns. Moreover, marketing mix variables, such as price and promotion, have causal effects on buying decisions. We incorporate these effects by modeling the parameter vector β X as a function of marketing mix C st at time t and household demographics D s . Finally, the intertemporal effects are captured similarly so that</p><formula xml:id="formula_6">β X β C C st + β D D s + ∑ m t ′ T−m β t ′X st ′,<label>( 6 )</label></formula><p>where X st′ is a vector of purchasing decisions in each of the previous m periods. Figure <ref type="figure" target="#fig_1">2</ref> provides a graphical representation of the full model constructed to capture the effects and interactions discussed above.</p><p>Although we cannot determine the direction of contemporaneous effects a priori, we know with certainty that the intertemporal effects are directional and should be modeled as such. <ref type="bibr">5</ref> In a CRBM, we add another type of connection to the model that is directional and may imply causal effects. A graphical model with directed edges is called a "directed acyclic graph" (DAG) or Bayesian Network, and it has been widely used for causal inference <ref type="bibr" target="#b25">(Pearl 2014)</ref>. Thus, intertemporal effects should be modeled using a DAG as should the effects of demographics and marketing mix variables.</p><p>The joint distribution of hidden and visible variables (represented by the RBM) is conditioned on these causal variables-hence, it is a CRBM. In essence, we have a hierarchical Bayesian model with an implicit prior over the hidden units. A random effects specification captures unobserved heterogeneity, with the hidden units sampled for each household. The joint distribution of visible and hidden variables is expressed in standard Bayesian form as</p><formula xml:id="formula_7">f (X st , H s ) f (X st |H s )f (H s ),<label>( 7 )</label></formula><p>where f denotes the probability density function.</p><p>The alternative representation indicates that we can treat household s's purchasing decision at time t as a sample from a conditional distribution determined by the hidden variables at the household level. The hidden variables, which can be interpreted as parameters representing each household, are then sampled from an implicit common prior defined by integrating out X in the CRBM. We can also make the implicit common prior explicit by adding another layer of hidden units.</p><p>Other variables, such as price and promotion, do not change the hierarchical structure, because they are constructed as independent of the hidden variables. Unlike the intertemporal effects, the contemporaneous effects are captured implicitly by the hidden variables. We can infer the cross effects from the partial derivatives ∂x it ∂x jt . By the chain rule, ∂x it ∂x jt ∂x it ∂H s ∂H s ∂x jt , and the partial derivative ∂x it ∂x jt changes with the values of x it and x jt . We replace x it and H s with their expected values <ref type="bibr" target="#b17">(Hruschka 2014</ref>) and use variational inference for efficient estimation (see Appendix B for details).</p><p>The CRBM has been successfully used in video and image processing <ref type="bibr" target="#b35">(Taylor and</ref><ref type="bibr">Hinton 2009, Salakhutdinov et al. 2013)</ref>. Unlike video and image data, household decision patterns are often very sparse, and large fractions of purchasing decisions across time and product categories are independent of each other. Hence, only a subset of all possible crosscategory intertemporal effects should be in the final model. In other words, many of the parameters in the original model should be zero to make the final model parsimonious, interpretable, and accurate. Our proposed model combines a dropout method with regularization, which extends current CRBM models, and captures such sparse data as household decision patterns efficiently as discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Estimation</head><p>The parameters of the CRBM model are estimated numerically by searching for the values that maximize the log likelihood of the training data. A first-order algorithmic approach to the maximization is to calculate the gradient of the log-likelihood function with respect to the parameters and then, use that gradient to perform a search. The bipartite CRBM structure implies that each component of the gradient has the form</p><formula xml:id="formula_8">∂logP(x sit ) ∂β ij E(x sit h sj ) data − E(x sitĥsj ) model ,<label>(8)</label></formula><p>where β ij is the parameter capturing the interaction between the purchase decision variable for category i and hidden variable h sj . The first term in Equation ( <ref type="formula" target="#formula_8">8</ref>) is the expected value of the product of x sit and h sj over households and time periods in the training data, and the second term is the expected value over the distribution of the same product assumed by the model. The second term could be estimated by brute force by conducting Gibbs sampling to generate samples from the model with random initial values forx sit andĥ sj and then, calculating the average of the product of x sit andĥ sj for each product category. However, such a naïve sampling approach would require a lot of training time, a problem for large-scale data analysis. Although the RBM is differentiable and has a simpler structure than the MVP, we need an efficient model training algorithm for parameter estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Methods to Improve Efficiency-Contrastive</head><p>Divergence, Momentum, and Minibatch Various methods have been proposed for making the inference procedure efficient. Contrastive divergence <ref type="bibr" target="#b15">(Hinton 2002)</ref> estimates the data-dependent statistics E(x it h j ) data in Equation ( <ref type="formula" target="#formula_8">8</ref>) by a one-step sampling process. The hidden variables H are drawn conditional on the original training data. For the dataindependent term, E(x itĥj ) model , we first sample hidden units from the original data and then, sample simulated data from the sample hidden units. The average value of the product of the simulated data and the hidden variable sample is then used as the data-independent Marketing <ref type="bibr">Science, 2019</ref><ref type="bibr">, vol. 38, no. 4, pp. 711-727, © 2019</ref> term in the gradient. Although one-step Gibbs sampling cannot yield an accurate estimate of E(x itĥj ) model , the gradient based on the sampling method has been shown to work well in practice <ref type="bibr" target="#b15">(Hinton 2002)</ref>.</p><p>We shortened the time to convergence in parameter estimation by using the momentum method and batch gradient descent <ref type="bibr" target="#b27">(Salakhutdinov and Larochelle 2010)</ref>. The momentum method speeds up the learning rate when the direction of the gradient is unchanged on consecutive iterations and slows it down when the gradient changes direction. This process yields the optimal parameter value in far fewer iterations than if we use a constant step size.</p><p>The updated equation is as follows <ref type="bibr" target="#b14">(Hinton 2010)</ref>:</p><formula xml:id="formula_9">Δβ ij (t) αΔβ ij (t − 1) + ε ∂log P(x it ) ∂β ij (t),<label>(9)</label></formula><p>where t and t − 1 indicate the current and previous iterations, Δβ ij (t) is the parameter increment at t − 1, α is the momentum, ε is the learning rate, and logP(x it ) is the log-likelihood function being maximized.</p><p>Dividing the training data into batches and updating the parameters after each batch can also speed up the model training process, because a group of smaller matrices can typically be processed faster than can a single very large matrix <ref type="bibr" target="#b5">(Cotter et al. 2011</ref>). Both gradient descent and stochastic gradient descent are essentially variations of batch gradient descent <ref type="bibr" target="#b1">(Bottou 2010)</ref>. Stochastic gradient descent uses each single point as a batch, whereas traditional gradient descent uses the whole training data as a batch. In our empirical application, we used each household's shopping data as a batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Methods to Improve Estimation of Sparse</head><p>Models-Elastic Net and Dropout Across many product categories and over several time periods, only small fractions of all household purchases are likely to be truly interrelated. Hence, we only need a subset of the modeled effects in the final model to improve its predictive accuracy and interpretability. Without a proper parameter selection process, the model is prone to overfitting. According to statistical learning theory, a complex model can fit any data very well but often fails to separate the idiosyncrasies of the training set from the true general patterns in the data, thus performing poorly on testing data.</p><p>Marketing researchers have normally relied on conventional methods, such as stepwise regression or bootstrapping, for model (feature) selection. These methods do not scale easily for large-dimension models and often, do not improve predictive accuracy. Another popular option is to use analytical measures, such as information criteria (Akaike information criterion <ref type="bibr">[AIC]</ref> or Bayesian information criterion <ref type="bibr">[BIC]</ref>). These criteria require the estimation of the normalizing constant for Markov random field models, such as our model, and also, the estimation of all possible models for comparison, which could be very time consuming. L1 (Lasso) and L2 (Ridge) regularization methods have been used to achieve sparsity (L1 only), prevent overfitting, and improve prediction accuracy. <ref type="bibr">6</ref> They place a constraint on the value of parameters during the model training process, encouraging movement of parameter values toward zero. From a Bayesian perspective, we place a Laplace (L1) and a Gaussian (L2) prior on the parameters, both zero centered and symmetric, to reduce the effect of the data-specific idiosyncrasies on posterior estimation.</p><p>L2 regularization shrinks the parameter values to prevent overfitting, but no parameters are dropped from the model. Therefore, L2 regularization may improve model prediction accuracy but does not help with model sparsity. In contrast, L1 regularization actually pushes some parameters to be exactly zero, leading to a sparse model with fewer (nonzero) parameters. However, when there are many parameters and some variables are highly correlated, L1 would lead to unstable model estimates. Combining L1 and L2 in model estimation-the elastic net <ref type="bibr" target="#b36">(Zou and Hastie 2005)</ref>-is an effective way to prevent overfitting and achieve a sparse model in big data analysis. Such mixed regularization methods have been successfully used in linear and logit regression, and they are often based on coordinate descent.</p><p>There have been applications of L1 regularization in the RBM and the CRBM <ref type="bibr">(Salakhutdinov et al. 2013)</ref>. Most have been implemented by using either subgradient methods that ignore the nonsmoothness of the L1 penalty term or truncation methods in which parameter values below an arbitrary threshold are dropped during model training. We have used the elastic net on models with 4 and 100 product categories to examine its effect on model performance. With only four categories, the elastic net does not show any significant effect on model fit or performance. With 100 categories, testing error reduces considerably with 100 of categories. Without the elastic net, the 100category model fits the training data much better than the 4-category model, with negligible training error. However, the testing error is 9.37% higher for 100 relative to 4 categories owing to overfitting. Model interpretability is the second goal of the elastic net. We seek to identify a small subset of predictors with the strongest effects in the model to gain meaningful insight <ref type="bibr" target="#b13">(Hastie et al. 2016</ref>). In our case, after applying the elastic net to the 100-category model, fewer than two-thirds of the parameters remain. <ref type="bibr">7</ref> In estimating our model with the elastic net, we successfully merged the proximal method with the contrastive divergence (CD) training algorithm to Xia, Chatterjee, and May: Conditional Restricted Boltzmann Machines achieve model training efficiency and sparsity (see Appendix C for details). The proximal gradient method allows for searching for the optimal path in gradient descent when part of the objective function is nondifferentiable. In this instance, the L1 penalty term is not differentiable at zero, and conventional gradient methods, such as CD training, do not work properly with the regularization term.</p><p>For high-dimension CRBM models, the large number of active hidden variables also contributes to overfitting (in addition to the parameters). These variables affect the associations among visible variables. Although we only need a limited number of activated hidden variables to capture the data distribution, the elastic net does not constrain their number. Crossvalidation can determine the optimal number of hidden variables but would be very time consuming for high-dimensional data.</p><p>The dropout method <ref type="bibr">Larochelle 2010, Makhzani and</ref><ref type="bibr" target="#b21">Frey 2015)</ref> has been shown to effectively prevent overfitting and improve a neural network's predictive performance. This method requires a change to the implementation of the training algorithm. During each batch training phase, some hidden variables are randomly dropped from the model with probability p. After the model training is complete, the parameters are multiplied by p, and all hidden variables are retained in the final model. The dropout procedure prevents hidden variables from trying to fit the idiosyncratic characteristics of the training data, and it can also be viewed as a form of model averaging.</p><p>We evaluated the effect of elastic net and dropout on our proposed model. We found that neither elastic net nor dropout provided much benefit when the model is simple (e.g., with only four products and no intertemporal effects). Regularization methods improved model fit when the model becomes complex. With a large number of hidden variables, dropout plays a greater role in improving model fit than the elastic net. Detailed results of our comparative evaluation are available in the online appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Illustrative Empirical Application</head><p>We reiterate that our empirical study is to illustrate how the model can be applied and to compare its predictive performance and robustness relative to the traditional MVP model, even in the case of relatively small applications with few product categories and/ or time periods. We describe the data first (Section 4.1) followed by the comparison with the MVP model (Section 4.2). Next, we report the results of the application of the CRBM to the large-scale model with up to 100 product categories and 12 time periods (Section 4.3) and discuss the implications of marketing mix (Section 4.4) and crossproduct and intertemporal effects (Section 4.5). Finally, we show how an augmented ("deep") CRBM can be used to estimate the model at the UPC rather than product category level (Section 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Data</head><p>The Nielsen household panel data set (provided for the sole purpose of academic research) contains households' shopping histories with their demographic information for a total of 6,695,833 observations. Each observation is a purchase transaction made by a household between 2004 and 2008. In the data set, the three most frequented stores are Kroger, Walmart, and Meijer. There are 4,000 households' purchasing histories over 208 weeks and across 1,055 product categories (as defined by Nielsen). Each household in the data set averaged 2.74 shopping trips a week. We selected 2,086 households with complete purchasing histories and demographic information for our analysis.</p><p>The 1,055 product categories vary significantly in the frequency of their purchase, with milk the most frequently purchased category. Figure <ref type="figure" target="#fig_2">3</ref> plots the category purchase frequencies (based on purchasing histories across households), with the categories sorted in decreasing order of frequency. Note the "long tail" of infrequently purchased product categories. For our illustrative application, we selected the 100 most purchased categories, including products from dairy milk to nutritional supplements.</p><p>We converted each household's purchasing history into a binary matrix. The rows represent the 208 weeks recorded in the data set, and the columns represent the 100 product categories ordered in decreasing overall purchase frequency. For example, if a household purchased at least one unit of one item from the dairy milk category in week 1, then the value in row 1, column 1 is one and zero otherwise (no purchase). There are 2,086 such matrices in all, one for each selected household. We randomly selected 1,500 matrices for the training set and used the remaining 586 matrices for testing the model.</p><p>Only demographic variables that demonstrated effects on purchasing decisions in previous research  (e.g., the presence of children, household income, and household size) were initially included <ref type="bibr" target="#b22">(Manchanda et al. 1999</ref><ref type="bibr" target="#b12">, Hansen et al. 2006</ref>). We also trained and tested models with promotion and price indices at the product category level. These indices were constructed following approaches in previous multicategory research <ref type="bibr" target="#b22">(Manchanda et al. 1999</ref><ref type="bibr" target="#b7">, Duvvuri et al. 2007</ref><ref type="bibr" target="#b6">, Duvvuri and Gruca 2010</ref>. For the price index, we used the unit price actually paid if the household made a purchase in the category that week; if not, a weighted average based on the household's purchasing history across all brands in that category was computed as the price index. The promotion index was similarly constructed: if there was a promotion in the week of purchase, the index was one; otherwise, it was the weighted average.</p><p>We did not include a household inventory variable in the model owing to multiple concerns with the inclusion of inventory variables: collinearity with the intertemporal effects, the strong assumption of a constant rate of consumption, and endogeneity <ref type="bibr" target="#b22">(Manchanda et al. 1999)</ref>. The inventory effect is implicitly captured by the intertemporal effects. Furthermore, we found that the assumption of a constant rate of purchase does not hold for this data set, even for products such as milk. We examined the data using model-free approaches first, which confirmed the irregularity of households' purchasing behavior. On average, a household buys milk about every 2.6 weeks; the conditional frequencies also show that a household is more likely to buy milk after a milk purchase in the previous week than otherwise. Our final model also captured this interesting, nonobvious intertemporal pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Comparison</head><p>Vs. Multivariate Probit 4.2.1. RBM Vs. MVP. We first compared the performance of a simple RBM with no time lags with that of the MVP. No household heterogeneity, marketing mix effects, or regularization methods were considered for either model to ensure a fair comparison. Given the MVP's ability to handle only a small number of categories owing to computational constraints, we limited the analysis to four categories-soup, pasta sauce, dog food, and pasta-which have been used in previous multicategory research (pasta and pasta sauce are complements; dog food is considered to be independent of the other three categories). The result was consistent with the finding of <ref type="bibr" target="#b17">Hruschka (2014)</ref> that the basic RBM performs at least as well as the MVP (Table <ref type="table" target="#tab_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">CRBM Vs. Hierarchical MVP.</head><p>Our proposed model is not a basic RBM but a CRBM incorporating intertemporal effects and unobserved heterogeneity. Hence, to be fair, we compared our proposed model with a hierarchical multivariate probit (HMVP). The HMVP has a very similar model structure (using a covariance matrix to model the contemporaneous effects instead of hidden variables), and the householdlevel covariance matrix is sampled from a population prior. Details on the HMVP are provided in the online appendix.</p><p>For the comparison, we examined the same four product categories but now, considered intertemporal effects over four weeks. We calculated false positive and false negative rates across all basket combinations, and once again, the results show that, although the HMVP achieves the same false negative rate, the CRBM performs better in terms of the false positive rate (Table <ref type="table" target="#tab_3">3</ref>). We have also included a confusion matrix to visualize the predictive performance of the CRBM relative to the HMVP in the online appendix.</p><p>For additional comparison, we used receiver operating characteristics (ROC) curves to measure the performance of both HMVP and CRBM at various thresholds settings (Figure <ref type="figure" target="#fig_3">4</ref>). CRBM outperformed HMVP as demonstrated by the area under the ROC, especially when the threshold is in the intermediate range; both models outperformed a random classifier (denoted by the dotted diagonal line in Figure <ref type="figure" target="#fig_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Extension to 10 Categories.</head><p>We then extended the model to the 10 most frequently purchased product categories, including price and promotion, and found that the CRBM again outperformed the MVP in terms of model fit and predictive accuracy.</p><p>The CRBM can be viewed as the nonlinear version of the MVP model with correlated latent factors. We, therefore, also compared our model with an MVP with latent linear factors and found again that the CRBM performs better. Its flexible structure can capture more complex associations between purchasing decisions with fewer hidden variables, reducing dimensionality without sacrificing performance.   <ref type="table" target="#tab_4">4</ref>). With 10 categories, it was not possible to estimate the multiperiod intertemporal HMVP model. The drastic reduction in training time for CRBM is critical when it comes to big data analysis. Although this efficiency is the most significant advantage of our model, we have shown that, even with small data sets, the CRBM performs at least as well as MVP (actually better using mean square error and false positive rate for comparison). In terms of statistical computation, we used stochastic gradient descent, which has a runtime bound. In contrast, for the MVP, where the sampling method was used, there is no guaranteed time bound <ref type="bibr" target="#b31">(Shai and Ambuj 2011)</ref>.</p><p>Recent developments in variational inference offer efficient alternatives to Bayesian model training by casting posterior inference as an optimization problem to improve estimation efficiency while sacrificing some accuracy <ref type="bibr" target="#b11">(Girolami and</ref><ref type="bibr">Rogers 2006, Blei et al. 2017)</ref>. Keeping the structure of the HMVP model, we trained the model by automatic differentiation variational inference (ADVI), which uses stochastic gradient descent to find the optimal parameter values <ref type="bibr" target="#b4">(Carpenter et al. 2017</ref>. Monte Carlo integration is still needed to estimate the gradient in most cases, which slows down the process. In our experiment, it took over 46 minutes to estimate the HMVP model with four time lags with multithread parallel computing for sampling enabled <ref type="bibr" target="#b4">(Carpenter et al. 2017)</ref>. Although ADVI improved model training efficiency relative to MCMC (see row 4 in Table <ref type="table" target="#tab_4">4</ref>), accuracy declined significantly, with a 100% higher false positive rate. In comparison, our CRBM is considerably more efficient and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Large-scale Crosscategory and</head><p>Intertemporal Effects Having demonstrated our model's dramatically greater efficiency relative to the MVP with low-dimensional data with comparable (or better) model fit and predictive performance, we next expanded our analysis to include all 100 product categories and a 12-week horizon for intertemporal effects to examine purchasing patterns more comprehensively. In essence, we need to model the associations between 1,300 (12 × 100 + 100) categories × time periods using regular computing power, which to our knowledge, has not been attempted in the marketing literature. Estimation of the more traditional multivariate models is simply not feasible given the size of the data.</p><p>We first trained the four-product category model with a four-period time lag (model 4P4T). <ref type="bibr">8</ref> Price and promotion indices and demographic variables, with possible causal effects on household purchasing decisions, were also incorporated. The joint distribution of decision outcomes is thus conditioned on these variables. We set the range for the regularization penalty term λ to be between 0.0005 and 0.05. The initial value of λ was set at 0.05 and then decreased by 0.00025 after each epoch; α was set at 0.6 to put more weight on the L1 regularization in the elastic net. A momentum term of 0.9 was applied to the training process after five epochs. We used the recommended dropout rate of 0.5 for training <ref type="bibr" target="#b32">(Srivastava et al. 2014</ref>). These values are typical for CRBM training <ref type="bibr" target="#b35">(Taylor and Hinton 2009)</ref>, and we did not find a significant difference in results for different values, consistent with <ref type="bibr" target="#b14">Hinton (2010)</ref>. Optimal values for the learning rate and penalty terms can be determined by crossvalidation; in our case, we did not find it necessary, because the improvement was very limited. 9 Each  The own effects and cross effects of promotional index are consistent with the marketing literature (Table <ref type="table">5</ref>). The signs of parameter coefficients indicate positive or negative effects, whereas their values capture the effect magnitudes. The own effects are much greater across all four product categories than the cross effects as expected. Pasta sauce shows the strongest own effects. Most cross effects are negative; for example, promotion of dog food negatively affects sales of the other three products. For pasta and pasta sauce, the cross effects are positive, which we would expect between complements.</p><p>Table <ref type="table">6</ref> shows the coefficients that represent the intertemporal effects. Most intertemporal effects are positive. Within-category effects seem to be positive for all products and across all time lags. Although dog food purchase does not seem to be affected by purchases of other products in previous weeks, its purchase suggests a positive indication that the household may buy the other products in the following week.</p><p>Next, we compared CRBM models with varying numbers of time lags and product categories to examine whether the added complexity could improve model fit and performance. If there are strong interdependencies in purchasing patterns across several categories and if some of the relationships are intertemporal, a model with more product categories and time lags should be able to perform better in capturing the decision pattern and making predictions.</p><p>For comparison, we trained a model with no time lags (4P). We then added four time lags (4P4T) to examine if the added time dimension was beneficial. We also calibrated models with 100 product categories and 12 time lags. We compared models with different time lags and discovered that the testing error dropped as the number of lags was expanded to 12 weeks but then, stayed relatively flat. This may be because of most of the products in the data set having a shopping cycle of 12 weeks or less. We used the mean squared error (the Brier score) as the error measure for both testing and training errors. The testing error should be a better metric for model comparison and model selection, because it is a better approximation of the generalization error than other metrics, such as AIC or BIC, which are based on training errors <ref type="bibr" target="#b10">(Friedman et al. 2001</ref>). Tables <ref type="table" target="#tab_5">7 and 8</ref> display the testing and training errors (in terms of both Brier scores, false positives, and false negatives) for the various CRBM-based models.</p><p>The results from our illustrative application suggest that there are indeed intertemporal crosscategory associations among a large number of products. In general, we expect that incorporating these associations would improve both fit and prediction. The numbers of both time lags and product categories matter. We also noticed variations in intertemporal crosscategory effects on different product categories. Dog food, which seems to be independent of the other three categories, saw a larger decrease in the training and testing errors than the others as the number of time lags increased. The intuition is that, for purchases in categories that tend to be independent of other categories, past purchases might be more strongly indicative of future purchases of the same product. 10 4.3.1. Robustness Check. To examine the robustness of the parameter estimates, we compared models with the top 100 and top 200 product categories. All within-product category effects, such as price and promotion, and most other parameters for the top 100 categories remain nonzero in the model with the top 200 categories. The number of active hidden variables increased as we increased the number of product categories, which however, did not seem to affect the number of parameters that remained nonzero. The categories outside the top 100 have very sparse Table <ref type="table">5</ref>. Own Effects and cross Effects of Promotional Index crosscategory associations, and the original 100 × 100 associations are essentially qualitatively unchanged. The value of the elastic net penalty term does affect the number of nonzero parameters. Nonzero parameters in the model selected by the elastic net do not mean that they are statistically significant from a frequentist perspective; they can be viewed as having nonzero posterior expected values from a Bayesian perspective. Therefore, the results should be seen as exploratory and in need of testing via confirmatory experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Examination and Discussion of Marketing</head><p>Mix Effects Note that the proposed CRBM model is designed to predict shopping behavior of a household on the basis of the marketing mix, past behavior (say, over the last 12 weeks), the hidden variables associated with the household, and key demographics. In this subsection, we discuss the effects of promotion based on the model parameter estimates. In the following plot (Figure <ref type="figure" target="#fig_5">5</ref>), each column represents the product category purchasing decision at time T, and each row represents the product promotion index. The categories are ordered in decreasing purchase frequency from left to right and from top to bottom. The value in each cell represents the effect of promotion on the product purchase decision shown as color-coded "pixels"-blue for a positive effect, red for negative effect, and white for no significant effect of one product's promotion on the purchase of the product indicated by the column. The pattern formed by all of the pixels provides an initial "big picture" across 100 categories.</p><p>Although we note that the findings are indicative and require far more exhaustive empirical and experimental research for substantive interpretation, they provide some face validity and tentatively suggest the type of less obvious implications that warrant additional research. As expected, the promotional effect is always positive within a category (the blue pixels along the diagonal). Also, in general, promotions of the more frequently purchased products have greater effect on the purchase of other products than promotions of the more rarely purchased products. Unlike withincategory promotional effects, the crosscategory effects can be either positive or negative. For example, according to the plot, the promotion of milk implies a greater likelihood of purchasing cereal but a lower likelihood of buying carbonated soft drinks, which makes intuitive sense. Promotion of rarely purchased products (e.g., dog treats) has little or no effect on other product purchases: hence, the many white dots along the rows for such categories.</p><p>Interestingly, there are also product promotions that are associated with reduced purchases of almost all other products. For example, the promotion of cigarettes coincides with lower sales of most products   <ref type="bibr">, 2019</ref><ref type="bibr">, , vol. 38, no. 4, pp. 711-727, © 2019</ref>., the corresponding row has mostly red dots). This may be because smokers are more likely to visit the supermarket just for this promotion compared with their usual shopping trips when cigarettes are not on sale. The price effects show similar patterns as those of promotion. In general, all within-category price effects are negative, and most price effects are concentrated on the most frequently purchased products.</p><p>The effect of household size on purchasing decisions is also interesting. As expected, the likelihood of purchase increases in the size of the household but only up to a point-beyond a household size of four, the effects disappear in most cases. Perhaps larger households are constrained in terms of purchasing power. Future research can examine how these effects differ across product categories (e.g., necessities versus nonessentials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Examination and Discussion of Intertemporal</head><p>and Contemporaneous Effects Although most intertemporal own category effects are positive, the results suggest that including cross effects over multiple categories and over multiple periods can capture a more complete picture of household purchasing patterns, which can be complex, asymmetric, and time varying. Figure <ref type="figure" target="#fig_4">6</ref> plots the intertemporal effects at T − 1 as an illustration. Similar to the previous plot of promotional effects, each column represents the product category purchasing decision at time T, and each row represents product category purchasing decisions at T − 1. The categories are ordered in decreasing purchase frequency from left to right and from top to bottom. Each cell represents the association between a category purchase in period T and that in T − 1 shown as colorcoded pixels-blue, red, and white for positive, negative, and no association, respectively.</p><p>Most (but not all) within-category intertemporal effects are positive, especially for the most frequently purchased items. For independent products, such as dog treats and cat food, purchases at T − 1 do not affect purchases of most other products at T. In contrast, cigarette purchases typically suggest fewer purchases across categories in the next period.</p><p>In the CRBM (as in the RBM), the parameters denoting associations between hidden and visible variables representing contemporaneous category purchases are estimated explicitly. However, there are no parameters directly denoting associations between the contemporaneous crosscategory effects, which must be estimated indirectly from the associations between visible and hidden variables (see Section 2.3). The results (Figure <ref type="figure" target="#fig_6">7</ref>) seem to confirm previous findings that products with similar purchase frequencies tend to have positive contemporaneous associations with each other <ref type="bibr" target="#b22">(Manchanda et al. 1999)</ref>. As expected, there are positive associations between complements, such milk and cookies, and negative ones between substitutes, such as cookies and donuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Modeling at the UPC Level</head><p>Associations at the UPC level are not only multidimensional but also, hierarchical, presenting a considerable challenge for traditional models, such as the MVP. There are associations both within and between brands, and there are also similar associations at the  We model the UPC-level associations by welding together two CRBMs (Figure <ref type="figure">8</ref>). UPCs are first connected to layers representing product categories and brands. A UPC is only connected to the hidden variables that represent its category and brand. A second layer then is added to model the associations among categories and brands.</p><p>We trained the proposed model using the same data set with the 200 most frequently purchased UPCs used for the analysis. There is a huge drop in terms of the purchasing frequency past around the 200th UPC (Figure <ref type="figure">9</ref>).</p><p>Apart from the twofold RBMs that capture the contemporaneous associations, the UPC-level model is similar to the proposed model at the product category level, including purchases over 12 weeks. Price and promotion for each UPC are included directly in the model without converting them into indices as in the product category-level analysis.</p><p>We trained the model with different starting values of the elastic net penalty terms and the number of hidden variables. The general pattern is stable and tells an interesting but logical story-all associations within a brand are either positive or zero, whereas associations within product categories can be either positive or negative. Purchasing a UPC suggests that the household is more likely to purchase another UPC of the same brand (suggestive of brand loyalty). Purchasing a UPC within a certain product category, however, might either increase or decrease the purchasing probability of another UPC from the same product category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our proposed CRBM-based model significantly extends the marketing literature on multicategory joint purchasing decision models by expanding the number of product categories and incorporating multiple time periods. It offers all of the benefits of traditional models but with the huge advantage of efficient scale up for big data analysis. Our approach is comprehensive in that it allows for the inclusion of intertemporal multicategory effects via a clearly specified model incorporating explanatory variables without having to impose a priori constraints to ensure model tractability. Furthermore, the clear structure of the model permits meaningful interpretation of the results.</p><p>The proposed model should find application in marketing practice, especially in online or mobile marketing. The so-called "retail apocalypse" has increasingly driven businesses online, and personalized marketing has become vital for success in retail. By using the proposed model, retailers can potentially capture and predict each individual consumer's (or household's) complex shopping patterns with greater accuracy for personalized marketing. For example, the model can suggest an automated shopping list based on past shopping record, which can then be conveniently incorporated in applications, such as Google shopping list, benefiting both retailers and consumers. Recommendation systems built on RBM have already found success in the movie context <ref type="bibr" target="#b28">(Salakhutdinov et al. 2007)</ref>. With the abundance of consumer data, the proposed model can even be more useful by analyzing consumer decision patterns across platforms. What a consumer watches on YouTube Marketing <ref type="bibr">Science, 2019</ref><ref type="bibr">Science, , vol. 38, no. 4, pp. 711-727, © 2019</ref> INFORMS might be correlated with her movie choices on Netflix and the music that she listens to on Spotify. Our model is able to pick up the associations efficiently among the choices for better personalized recommendation and targeted online advertising, improving conversion rate and consumer satisfaction.</p><p>Future research should include more extensive empirical applications of the proposed model in different settings and shopping contexts and perhaps, variants of this model that enjoy the same benefits. There are, of course, several other avenues for extensions to this paper. We modeled consumer purchasing decisions as binary outcomes and did not explore the associations of purchasing decisions in other forms, such as the amount spent in each product category or the quantity purchased. The amount spent may be modeled as a continuous variable, and an extension of the RBM may be used to model the associations <ref type="bibr" target="#b16">(Hinton and Salakhutdinov 2006)</ref>. The Poisson RBM can be used to model the quantities of products purchased by the consumer. It would be interesting to see if using spending or purchase quantity as the dependent variable (especially at the brand level) leads to new consumer insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Endnotes</head><p>1 If there are n product categories, the correlation matrix grows at the rate of O(n 2 ). If we also include the crosscategory intertemporal effects over t time periods, the number of associations that we need to model is O(n 2 t 2 ), which further increases the running time for model estimation.</p><p>2 MCMC requires a large number of samples for relatively accurate estimation, the time to convergence is not guaranteed, and it is difficult to run in parallel. Although some methods have been proposed for running MCMC in parallel, these are mostly approximations with limited accuracy.</p><p>3 Of course, the assumption of latent causes does not imply that we are able to uniquely identity them. <ref type="bibr">4</ref> Because our data are at the household level (as is typically the case), we henceforth use a "household" rather than a "consumer" as the buying unit. <ref type="bibr">5</ref> We can still use the basic RBM structure to model both intertemporal and contemporaneous effects-by connecting all visible variables (past and current purchasing decisions) to the hidden variables. The latter captures the associations among visible variables indirectly, including both intertemporal effects and contemporaneous effects. However, this is an incorrect model specification. 6 These methods were originally proposed for predictive variable selection in linear models and then extended to a wide variety of statistical models, including conventional neural networks. <ref type="bibr">7</ref> Although this is still a large number of parameters, these are also regularized by the L2 penalty. <ref type="bibr">8</ref> The usual practice is to set the number of hidden variable to slightly below the number of visible variables (100 in our case). Dropout training (which we use) essentially prevents overfitting from an initial specification of too many hidden variables. <ref type="bibr">9</ref> For example, we conducted a coarse-grid search for the optimal learning rate with a 0.0002 interval and found that the optimal learning rate would reduce the average error by 0.00002. <ref type="bibr">10</ref> We also tested the predictive performance of the CRBM against a null model that excludes crosscategory intertemporal effects. Specifically, 12 weeks' past purchases and 100 categories were included in the comparison between the CRBM and a regression model that ignores crosscategory purchases in predicting the probability of purchase in a given week. The CRBM outperforms this null model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Graphical Representation of the Restricted Boltzmann Machine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (Color online) Graphical Representation of the Proposed CRBM Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Product Category Purchase Frequencies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Model Comparison: ROC Curves for HMVP and CRBM Models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Plot of Intertemporal Effects of a One-Week Lag</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Plot of Promotional Effects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Plot of Contemporaneous Effects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>RBM Coefficients for Small-Scale Illustrative Application</figDesc><table><row><cell></cell><cell>Hidden variable 1</cell><cell>Hidden variable 2</cell></row><row><cell>Soup</cell><cell>−0.752</cell><cell>−0.898</cell></row><row><cell>Pasta sauce</cell><cell>−0.066</cell><cell>−0.268</cell></row><row><cell>Dog food</cell><cell>0.000</cell><cell>0.000</cell></row><row><cell>Pasta</cell><cell>−0.201</cell><cell>0.000</cell></row></table><note>Xia, Chatterjee, and May: Conditional Restricted Boltzmann Machines   </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b29">Xia, Chatterjee, and</ref> May: Conditional Restricted Boltzmann Machines   Marketing Science, 2019, vol. 38, no. 4, pp. 711-727, © 2019 INFORMS   </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Model Comparison: False Positive and Negative Rates for RBM and MVP Models (Four Product Categories)</figDesc><table><row><cell></cell><cell>False positive</cell><cell>False negative</cell></row><row><cell>RBM</cell><cell>0.0679</cell><cell>0.8346</cell></row><row><cell>MVP</cell><cell>0.0681</cell><cell>0.8369</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Model  </figDesc><table><row><cell></cell><cell cols="2">Comparison: False Positive and Negative</cell></row><row><cell cols="3">Rates for CRBM and HMVP Models (Four Categories over</cell></row><row><cell>Four Weeks)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>False positive</cell><cell>False negative</cell></row><row><cell>CRBM</cell><cell>0.0456</cell><cell>0.6919</cell></row><row><cell>HMVP</cell><cell>0.0580</cell><cell>0.6920</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Model Training Times for HMVP and CRBM Models</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of categories</cell><cell>4</cell><cell>4</cell><cell>10</cell></row><row><cell>Intertemporal effects?</cell><cell>No</cell><cell>Yes (four periods)</cell><cell>No</cell></row><row><cell>RBM/CRBM, seconds</cell><cell>&lt;30</cell><cell>&lt;60</cell><cell>&lt;120</cell></row><row><cell>MVP/HMVP (MCMC), hours</cell><cell>&gt;7</cell><cell>&gt;0.50</cell><cell>&gt;90</cell></row><row><cell>MVP/HMVP (VB), minutes</cell><cell>&gt;35</cell><cell>&gt;46</cell><cell>&gt;57</cell></row><row><cell>Note. VB, Variational Bayes.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Comparison of Errors Across CRBM Models</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Brier score (mean squared error)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Testing error</cell><cell cols="5">Soup Pasta sauce Dog food Pasta All products</cell><cell cols="2">False positive False negative</cell></row><row><cell>4P</cell><cell>0.0947</cell><cell>0.0509</cell><cell>0.0271</cell><cell>0.0402</cell><cell>0.0532</cell><cell>0.0461</cell><cell>0.7244</cell></row><row><cell>4P4T</cell><cell>0.0925</cell><cell>0.0506</cell><cell>0.0238</cell><cell>0.0403</cell><cell>0.0518</cell><cell>0.0456</cell><cell>0.6919</cell></row><row><cell>4P12T</cell><cell>0.0910</cell><cell>0.0501</cell><cell>0.0230</cell><cell>0.0403</cell><cell>0.0511</cell><cell>0.0464</cell><cell>0.6770</cell></row><row><cell>100P4T</cell><cell>0.0908</cell><cell>0.0500</cell><cell>0.0239</cell><cell>0.0400</cell><cell>0.0500</cell><cell>0.0443</cell><cell>0.6775</cell></row><row><cell>100P12T</cell><cell>0.0896</cell><cell>0.0495</cell><cell>0.0229</cell><cell>0.0399</cell><cell>0.0490</cell><cell>0.0476</cell><cell>0.6541</cell></row><row><cell cols="8">Notes. 4P and 100P indicate CRBM models with 4 and 100 product categories, respectively. 4T and 12T</cell></row><row><cell cols="8">represent 4 and 12 time lags in the models, respectively. The all products column shows the mean errors</cell></row><row><cell cols="4">over four product categories in the model.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Comparison of Errors Across CRBM ModelsNotes. 4P and 100P indicate CRBM models with 4 and 100 product categories, respectively. 4T and 12T represent 4 and 12 time lags in the models, respectively. The all products column shows the mean errors over four product categories in the model.</figDesc><table><row><cell cols="4">Xia, Chatterjee, and May: Conditional Restricted Boltzmann Machines</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Marketing Science</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Brier score (mean squared error)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training error</cell><cell cols="5">Soup Pasta sauce Dog food Pasta All products</cell><cell cols="2">False positive False negative</cell></row><row><cell>4P</cell><cell>0.0794</cell><cell>0.0334</cell><cell>0.0198</cell><cell>0.0242</cell><cell>0.0392</cell><cell>0.0496</cell><cell>0.5501</cell></row><row><cell>4P4T</cell><cell>0.0785</cell><cell>0.0332</cell><cell>0.0166</cell><cell>0.0241</cell><cell>0.0381</cell><cell>0.0486</cell><cell>0.5127</cell></row><row><cell>4P12T</cell><cell>0.0776</cell><cell>0.0330</cell><cell>0.0157</cell><cell>0.0238</cell><cell>0.0375</cell><cell>0.0500</cell><cell>0.5002</cell></row><row><cell>100P4T</cell><cell>0.0760</cell><cell>0.0328</cell><cell>0.0169</cell><cell>0.0241</cell><cell>0.0385</cell><cell>0.0453</cell><cell>0.519</cell></row><row><cell>100P12T</cell><cell>0.0747</cell><cell>0.0325</cell><cell>0.0158</cell><cell>0.0237</cell><cell>0.0375</cell><cell>0.0462</cell><cell>0.513</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">MarketingScience, 2019, vol. 38, no. 4, pp. 711-727, © 2019 </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the senior editor, the associate editor, and two reviewers for their insightful comments and suggestions throughout the review process.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Glossary of Notation</head><p>The symbols used to describe the model are summarized in the following table.</p><p>Figure <ref type="figure">9</ref>. Purchase Frequency at the UPC Level Symbol Definition</p><p>The normalizing constant of a probability function β i , β j Constant terms in probability functions for x i and h i β ij</p><p>Associations between product purchase decision x i and hidden variable h j β it Dynamic terms in probability functions for We can use a factorized distribution Q to approximate the posterior of H s as follows:</p><p>Then, the lower bound becomes</p><p>To maximize the lower bound, we set the derivative to zero, and then,</p><p>.</p><p>To predict X st , we set h sj µ sj and estimate the expected value E(X st ) by solving simultaneously the above equation and the conditional probability</p><p>We compared the prediction results from variational inference with those of sampling methods. Both methods have very similar performance in terms of prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Training CRBM with Elastic Net</head><p>For clarity, we use β to represent any of the parameters in the model, because they are estimated by following the same training algorithm. We use X to represent the training data, which can be either the entire training data or a small batch of them. To train the model with L1 and L2 regularization using stochastic gradient descent, we first start with the unconstrained objective function:</p><p>The objective function is the negative likelihood of the RBM or the CRBM. We update just one parameter at each iteration.</p><p>When training the model with the gradient-based method, we attempt to search iteratively for the optimal value. During each step, we look for the next optimal value around a point β ′ . The initial parameter value can be random. We first perform a Taylor series expansion around the starting point β ′ :</p><p>} .</p><p>If we replace = 2 log(P(X; β ′ )) with t, the objective function becomes</p><p>where the term (β ′ + t=log(P(X; β ′ )) performs a one-step gradient descent with step size t. We then can add both regularization terms to the objective function:</p><p>Because the L1 regularization term is differentiable, the only issue with the optimization problem here is the L1 regularization term:</p><p>The next optimal value around β ′ that minimizes the objective function is as follows, which is called the softthresholding operator:</p><p>After finding the optimal value for the parameter, we add the second penalty term, the L2 regularization, to the objective function. The final optimal value for this iteration is</p><p>We also include a momentum term for each iteration to accelerate the training process.</p><p>Tuning parameters, such as λ, t, and α, need to be determined before model training. λ determines the size of the penalty. If λ is too large, the elastic net will push all parameter values toward zero. If λ is too small, the regularization terms will have little or no effect on the parameter values. Although it is challenging to find the optimal value for λ, we can define an optimal range for λ, start the model training with a large λ, and decrease λ after each run through the data <ref type="bibr" target="#b13">(Hastie et al. 2016</ref>). The parameter estimates may not reach the optimal value if t is too big. If t is too small, the convergence rate could be slow, and the estimates might stick in local optimal. We can estimate the magnitude of the parameter value and choose a step size that is appropriate for estimates of that magnitude. For RBM training, the recommended step size t can be found in the tutorial by <ref type="bibr" target="#b14">Hinton (2010)</ref>. α determines the weight put on L1 and L2 regularizations. It takes on a value within the range [0, 1]. If α is one, then the elastic net just becomes L1 regularization. If α is zero, it is L2 regularization. More weight should be put on L1 regularization if the majority of the parameters in the model are insignificant.  <ref type="bibr">.2019.1162)</ref>, the authors note that the performance of the basic restricted Boltzmann machine versus multivariate probit is consistent with the findings of <ref type="bibr" target="#b17">Hruschka (2014)</ref>. While the cited paper references both the multivariate probit and multinomial logit models, it compares the restricted Boltzmann machine to the multinomial logit model, not the multivariate probit model as stated in this paper. The authors apologize for this error. <ref type="bibr" target="#b29">Xia, Chatterjee, and</ref><ref type="bibr">May: Conditional Restricted Boltzmann Machines Marketing Science, 2019, vol. 38, no. 4, pp. 711-727, © 2019 INFORMS</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COMPSTAT&apos;2010</title>
				<meeting>COMPSTAT&apos;2010<address><addrLine>Berlin, Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Physica-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling joint purchases with a multivariate MNL approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boztug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hildebrandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Schmalenbach Bus. Rev</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="400" to="422" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving preference prediction accuracy with feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burnap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Papalambros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASME 2014 Internat. Design Engrg. Tech. Conf. Comput. Inform. Engrg. Conf. (American Society of Mechanical Engineers</title>
				<meeting>ASME 2014 Internat. Design Engrg. Tech. Conf. Comput. Inform. Engrg. Conf. (American Society of Mechanical Engineers<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="V02A" to="T3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stan: A probabilistic programming language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riddell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Statist. Software</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better mini-batch algorithms via accelerated gradient methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1647" to="1655" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Bayesian multi-level factor analytic model of consumer price sensitivities across categories</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Duvvuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Gruca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="558" to="578" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Consumers&apos; price sensitivities across complementary categories</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Duvvuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1933" to="1945" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multivariate analysis of multiple response data</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="334" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Counting your customers&quot; the easy way: An alternative to the Pareto/NBD model</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="284" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Elements of Statistical Learning</title>
				<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variational Bayesian multinomial probit regression with Gaussian process priors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1790" to="1817" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding storebrand purchase behavior across categories</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chintagunta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="90" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibishirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<title level="m">Statistical Learning with Sparsity</title>
				<meeting><address><addrLine>Boca Raton, FL</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Momentum</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">926</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analyzing market baskets by restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hruschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OR Spectrum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="228" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A time series analysis of binary data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Keenan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">380</biblScope>
			<biblScope unit="page" from="816" to="821" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic differentiation variational inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="430" to="474" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for visualisation of high dimensional data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Processing Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="336" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Winner-take-all autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2791" to="2799" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Manchanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<title level="m">The &quot;shopping basket&quot;: A model for multicategory purchase incidence decisions. Marketing Sci</title>
				<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="95" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Investigating consumers&apos; purchase incidence and brand choice decisions across multiple product categories: A theoretical and empirical analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="217" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">7 strategies for boosting digital coupon conversions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miles</surname></persName>
		</author>
		<ptr target="http://streetfightmag.com/2013/10/03/7-strategies-for-boosting-digital-coupon-conversions/" />
		<imprint>
			<date type="published" when="2013-03-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analysis of cross category dependence in market basket selection</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Retailing</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="392" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient learning of deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Internat. Conf. Artificial Intelligence Statist</title>
				<meeting>13th Internat. Conf. Artificial Intelligence Statist<address><addrLine>Brookline, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Microtome Publishing</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="693" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Internat. Conf. Machine Learn</title>
				<meeting>24th Internat. Conf. Machine Learn<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning with hierarchical-deep models</title>
		<author>
			<persName><forename type="first">Chatterjee</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">May</forename><forename type="middle">;</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conditional Restricted Boltzmann Machines Salakhutdinov R</title>
				<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1958" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Models of multi-category choice behavior</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ainslie</forename><forename type="middle">A</forename><surname>Boatwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Strijnev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="239" to="254" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic method for L1 regularized loss minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ambuj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1865" to="1892" />
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Product complements and substitutes in the real world: The relevance of &quot;other products</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Shocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Bayus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient Bayesian inference for multivariate probit models with sparse inverse correlation matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Talhouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graphical Statist</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="739" to="757" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Factored conditional restricted Boltzmann machines for modeling motion style</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annual Internat. Conf. Machine Learn</title>
				<meeting>26th Annual Internat. Conf. Machine Learn<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1025" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B. Statist. Methodology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
