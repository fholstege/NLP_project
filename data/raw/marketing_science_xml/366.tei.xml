<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Model Selection Using Database Characteristics: Developing a Classification Tree for Longitudinal Incidence Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-01-17">January 17, 2014.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
							<email>ericmsch@umich.edu</email>
						</author>
						<author>
							<persName><roleName>Peter</roleName><forename type="first">Eric</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
							<email>ebradlow@wharton.upenn.edu</email>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
							<email>faderp@wharton.upenn.edu</email>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Ross</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">The Wharton School</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Model Selection Using Database Characteristics: Developing a Classification Tree for Longitudinal Incidence Data</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2014-01-17">January 17, 2014.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2013.0825</idno>
					<note type="submission">Received: August 2, 2011; accepted: September 4, 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>model selection</term>
					<term>machine learning</term>
					<term>data science</term>
					<term>business intelligence</term>
					<term>hidden Markov models</term>
					<term>classification tree</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The explosion in technology-enabled data collection has changed the focus of marketing modelers away from aggregated data at the store or market level toward more granular, panel-oriented data structures and associated statistical methodologies. Companies have reduced their reliance on "rolled-up" data provided by syndicated vendors (e.g., IRI, Nielsen) and now build more of their analytics around customerlevel longitudinal patterns that they can obtain from their own internal operations. But while this increased reliance on "site-centric" data <ref type="bibr" target="#b39">(Zheng et al. 2011</ref>) offers a number of meaningful benefits to the firm, it also comes with some potential costs to the researcher.</p><p>First, site-centric data provide a detailed description of each customer's stream of purchases (and other actions that the firm can measure directly), but such data often lack information about marketing variables, competitive tactics, and other potential "drivers" of the behavior(s) of interest <ref type="bibr" target="#b7">(Donkers et al. 2007</ref><ref type="bibr" target="#b34">, Schweidel et al. 2008</ref>) that are typically provided by a third-party firm and are often difficult to link to purchase data. Thus, many firms are focusing their decision-making efforts around the flow of incidence activities, i.e., the timing and nature of each transaction, which is very rich but also quite different from the inputs used in more traditional marketingmix models <ref type="bibr" target="#b15">(Hanssens et al. 2005)</ref>.</p><p>Second, this detailed stream of incidence actions can be characterized by an ever-larger swath of mathematical models. That is, increased granularity comes with the potential for increased model complexity and hence a more difficult model selection problem than faced by previous generations of researchers, who often relied on relatively standard model specifications <ref type="bibr" target="#b6">(Cooper and</ref><ref type="bibr">Nakanishi 1988, Wittink et al. 1988</ref>) that were sufficient for the relatively standard data structures made available by a small set of third-party data providers.</p><p>Schwartz, Bradlow, and Fader: Model Selection Using Database Characteristics Marketing Science 33(2), pp. <ref type="bibr">188-205, Â© 2014 INFORMS 189</ref> With this "data evolution" in mind, consider a business intelligence manager for an e-commerce firm who is examining panel data from three recent product launches (see Figure <ref type="figure" target="#fig_0">1</ref>). Her goal is to project repeat purchase patterns for each data set because her company's production, marketing, and customer relationship management activities depend on an accurate forecast. How should she choose which statistical model is most appropriate for each product's data set? She could run a number of different paneloriented incidence models and choose the one that fits each data set best, but a series of separate "model bake-offs" would be a highly inefficient process and would offer no assurances that the chosen model(s) will be best suited for forecasting purposes of similar data sets. Instead, are there clues in each data set that might help her make the right choices without having to run an array of models over and over again for each new data set? Can we look at many data sets and model performances to extract general rules about when to use which model? That is the goal of this paper: We want to help managers choose among competing longitudinal incidence models, based only on observed data set-level summary statistics, i.e., database characteristics, before they need to run any models.</p><p>We will create a "decision tree" that can guide the manager toward the most appropriate model specification for a given data set, based only on observable (and easy-to-compute) summary statistics on that data set. In other words, we will do the "up-front work" so that the decision tree is a time-saving tool for other analysts. We recognize that each data set consists of a mix of heterogeneous customers who may go through different kinds of dynamic purchasing patterns over time, and we want to identify the most suitable model specification to capture these within-and across-customer sources of variation. However, we do not use the database characteristics directly in our models to predict future purchasing (i.e., we do not treat them as X variables in a statistical model), but instead, we use them to help identify the best model (chosen from a class of different latent-state model specifications) that can be used for forecasting and other diagnostic purposes.</p><p>For instance, referring back to Figure <ref type="figure" target="#fig_0">1</ref>, data set A's steadily declining sales may indicate that latent customer attrition is prevalent but occurs at different rates for different customers, so a "buy till you die" model such as the Pareto/NBD <ref type="bibr" target="#b31">(Schmittlein et al. 1987)</ref> or the beta-geometric/beta-binomial (BG/BB; see <ref type="bibr" target="#b10">Fader et al. 2010</ref>) model might be appropriate. In contrast, the sales for data set B seem to show a substantial rise toward the end of the observation period, so a hidden Markov model (HMM), in which customers move back and forth between different states of purchasing propensities <ref type="bibr" target="#b20">(Liechty et al. 2003</ref><ref type="bibr" target="#b25">, Netzer et al. 2008</ref>, might be the best model to employ for forecasting purposes. Finally, the sales curve for data set C is harder to classify as a buy till you die or an HMM-type pattern-it seems to reflect elements of both specifications. Perhaps we need a hybrid version of these two models to capture and project it.</p><p>Although there are innumerable models that could be viable candidates for this kind of longitudinal incidence data, we choose a particular set that is tightly Marketing Science 33(2), pp. <ref type="bibr">188-205, Â© 2014 INFORMS</ref> connected to each other but still very flexible. The models we consider are the HMM and three different constrained variants of it (including the BG/BB model). Since they are part of an integrated family, they offer an opportunity to detect when each underlying model component (in this case, the presence of an absorbing state and/or the need for a "nopurchase" state) is worth including or "turning off." This provides added insight to the analyst about the nature of customer dynamics, above and beyond simplified model implementation and improved model performance.</p><p>For this context (i.e., repeat-transaction incidence data, the HMM and its constrained variants), we do all of the "legwork" for the analyst. We run an array of constrained and unconstrained HMM models on dozens of synthetic data sets, generated to broadly represent the kinds of patterns that are likely to occur in real-world settings. Although this is computationally expensive initially (a high up-front cost for us as the researcher), it yields significant savings for the downstream user-the manager simply follows our advice and selects the most appropriate model given the nature of her data set and runs it-the "winner"and not the entire class of models.</p><p>The focal managerial criterion we use to select among models is the forecast error for each cohort's purchases, so the winning model has the minimum mean absolute error in a holdout period. We use wellestablished machine-learning methods known as classification and regression trees (CART) and random forests to derive general rules to suggest which model to use under different circumstances, based entirely on observed (and managerially meaningful) patterns in the customer-base data. The database characteristics that turn out to be most important (in our setting) include the nature of the decline in cohort-level sales over time as well as purchase concentration (e.g., the "80:20 rule") across customers.</p><p>Since the development of the decision tree is our key contribution, the structure of the paper centers around it. There are three "ingredients" for the classification approach, and we devote a section of the paper to each one: the candidate models in Â§2, the database characteristics in Â§3, and the performance criterion to determine the "winning" model for each data set in Â§4. Putting these three ingredients together, we create the decision tree in Â§5 and focus on its interpretation, validation, and managerial implications.</p><p>Although we perform our analysis for a specific data/modeling context (albeit an important one in today's marketing environment), the same basic "recipe" developed here can be applied to many other settings. Thus, we formalize our approach as a more general methodology in Appendix B, using the same ingredients outlined above: a set of models, database characteristics, and a selection criterion (i.e., performance or error measure with loss function). We now begin the process of laying out these elements to build our decision tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Which Models to Consider? The HMM and Its Constrained Variants</head><p>The decision tree recommends which model to use for a given data set, but we have to provide a consideration set of models: the HMM and its constrained variants. Why do we consider this class? First, they are appropriate for this popular context of understanding and projecting repeat-purchase patterns of a cohort of customers using longitudinal incidence data <ref type="bibr" target="#b20">(Liechty et al. 2003</ref><ref type="bibr" target="#b22">, Montgomery et al. 2004</ref><ref type="bibr" target="#b23">, Montoya et al. 2010</ref><ref type="bibr" target="#b25">, Netzer et al. 2008</ref><ref type="bibr" target="#b33">, Schweidel et al. 2011</ref>. Second, these models cover a wide range of underlying "stories" of customer behavior, leading to different observable data patterns. This helps us achieve the goal of the paper: establish the link between data setlevel summaries and model performance.</p><p>Third, these models form an integrated family; that is, each model is a constrained or unconstrained version of another in the set. Some of these are established yet seemingly unrelated models, such as buy till you die and latent-class models, among others. But they are all special cases of the HMM. These connections have only been partially explored and in an ad hoc manner in the previous literature, as we discuss below <ref type="bibr" target="#b25">(Netzer et al. 2008</ref><ref type="bibr" target="#b33">, Schweidel et al. 2011</ref>). However, the extra insight that we provide is that the four variants of the HMM that we consider are described by two model components each with two levels (as seen in the 2 Ã 2 framework discussed in Table <ref type="table" target="#tab_0">1</ref>). So the decision tree not only recommends a specific model but also emphasizes the presence or absence of more general model components, thereby adding more insight and comparability across data sets.</p><p>The unconstrained HMM used here has two states, and the within-state purchase likelihoods are repeated Bernoulli trials by individuals who can begin the calibration period in state 1 or state 2. We allow for unobserved continuous heterogeneity for the withinstate purchase propensities as well as the betweenstate transition probabilities. Formally stated, we let y it = 1 for day t if the customer i purchased and y it = 0 otherwise. 1 Then, </p><formula xml:id="formula_0">y it â¼ Bernoulli(p 1i ) if in state 1 Z it = 1 Bernoulli(p 2i ) if in state 2 Z it = 2 (1)</formula><formula xml:id="formula_1">p i = p 1i p 2i and i = 1 â 12i 12i 21i 1 â 21i p i = p 1i 0 and i = 1 â 12i 12i 21i 1 â 21i</formula><p>Only forward transitions Hot then cold BG/BB</p><formula xml:id="formula_2">p i = p 1i p 2i and i = 1 â 12i 12i 0 1 p i = p 1i 0 and i = 1 â 12i 12i 0 1</formula><p>Notes. The rows and columns illustrate how constraints on two model components lead to different models. The p i and i are the individual-level within-state transaction probabilities and between-state transition probability matrix, respectively.</p><p>where the latent-state variable, Z it , indicates which state a customer occupies on each day. The individuallevel parameters of the HMM are the withinstate propensities, p i , and the transition probability matrix, i . That is,</p><formula xml:id="formula_3">p i = p 1i p 2i and i = 1 â 12i 12i 21i 1 â 21i (2)</formula><p>We let the initial state membership be a populationlevel parameter, and any individual can start in state 1 with probability 1 or state 2 with probability 1 â 1 . We assume independent beta distributions to allow for heterogeneity across individuals for the components of p i and i . 2 Specifically, the prior distributions used are</p><formula xml:id="formula_4">p 1i â¼ beta p 1 p 1 p 2i â¼ beta p 2 p 2 12i â¼ beta 12 12 21i â¼ beta 21 21<label>(3)</label></formula><p>where = a/ a + b is the mean and = 1/ a + b + 1 is the polarization index of the beta distribution with shape parameters a and b <ref type="bibr" target="#b30">(Sabavala and Morrison 1977)</ref>. In general, for S â¥ 2 states, each row r of the transition probability matrix is a vector r1i rSi â¼ Dirichlet r1 rS . <ref type="bibr">3</ref> We distinguish between states by referring to state 1 as having a within-state propensity at least as large as that of state 2 for each individual (i.e., p 1i â¥ p 2i for all i). This prevents the label-switching problem known to exist with latent-state models <ref type="bibr" target="#b36">(Stephens 2000)</ref>.</p><p>We highlight the off-diagonal entries of the transition probability matrix, since 12i denotes the probability an individual moves "forward" (state 1 to 2) and 21i represents the probability an individual <ref type="bibr">2</ref> The initial state membership probability is assumed to be a population-level parameter as a result of the definition of a cohort of customers acquired at the same time. Additionally, the results are robust to using a logit-normal for heterogeneity on all individual parameters and for allowing correlations among them. <ref type="bibr">3</ref> We use highly uninformative hyperpriors on the populationlevel parameters of the beta (or Dirichlet) distributions. For more details about the distributions used in the sampling procedure, see Appendix A.</p><p>moves "backward" (state 2 to 1). Not allowing backward transitions is equivalent to making state 2 absorbing.</p><p>Given this formulation of the unconstrained HMM, the three nested models emerge as we constrain either or both model components. To start, when we apply both constraints to all individuals, such that there is an off state (p 2i = 0) and backward transitions are prohibited ( 21i = 0), the buy till you die BG/BB model emerges. <ref type="bibr">4</ref> Then, as we think about how the BG/BB model and HMM differ along these two dimensions, we can consider each of those dimensions separately (i.e., either p 2i = 0 or 21i = 0). These constraints determine the two dimensions of Table <ref type="table" target="#tab_0">1</ref>. When applying each of the two constraints separately, different models emerge (the off-diagonal cells of Table <ref type="table" target="#tab_0">1</ref>), and each tells a distinct story of customer behavior.</p><p>When only p 2i = 0, the on and off model (OF) emerges. Consumers can make back-and-forth transitions between an "on" state of activity and an "off" state of inactivity. Like the HMM, customers can make backward transitions, yet like the BG/BB model, when in the off state, customers have no chance of activity. This kind of model has been explored in papers on Markov-modulated Poisson processes <ref type="bibr" target="#b21">(Ma and Buschken 2011)</ref>.</p><p>Alternatively, when only 21i = 0, we get the hot then cold model (HC). At any time, customers can be either in a "hot" state (higher propensity to purchase) or a "cold" state (purchasing is less likely but still possible). Like the BG/BB model, once the customer reaches the cold state, she remains there (no backward transitions), and like the HMM, in the cold state, purchasing is possible. The hot-then-cold ordering is informed by the prevalence of customer attrition or, at least, the slowing down of transactions (in aggregate) that is common in most cohort-level Marketing Science 33(2), pp. 188-205, Â© 2014 INFORMS data sets. <ref type="bibr">5</ref> Such behavior appears in queuing theory models, such as phase-type distributions <ref type="bibr" target="#b0">(Bladt and</ref><ref type="bibr">Neuts 2003, O'Cinneide 1990)</ref>, and in marketing models <ref type="bibr">(Fader et al. 2004, Schweidel and</ref><ref type="bibr" target="#b32">Fader 2009)</ref>. Past literature has noted how the latent-class model <ref type="bibr" target="#b17">(Kamakura and Russell 1989</ref>) is a special case of an HMM ( 12i = 21i = 0), and other work often utilizes a nested model with a "death" state <ref type="bibr" target="#b25">(Netzer et al. 2008</ref><ref type="bibr" target="#b33">, Schweidel et al. 2011</ref>). However, the other links among the HMM and its constrained models (e.g., BG/BB, HC, OF) that we consider have not been documented in full detail as an integrated framework with the 2 Ã 2 structure as described here.</p><p>Viewing the HMM and its constrained variants as an integrated family provides an opportunity to detect when (i.e., for which types of data sets) each model component is worth including. One may initially (but erroneously) think that the nested structure would guarantee that the more flexible HMM would perform at least as well as any of its constrained versions (with one or both model components shut off) on all model-performance criteria. But this is not guaranteed in practice. We illustrate that when forecasting repeat transactions out of sample, the more general model does not always beat its nested versions, and hence there is value in the decision tree provided in this paper.</p><p>The decision tree answers our key question: For what kinds of database characteristics does each model perform best? To perform this classification, we need a range of different data sets generated from the 2 Ã 2 framework. We generate 64 synthetic data sets, each with T = 30 weeks of data in calibration (and 30 for holdout) and N = 500 customers, with considerable variation by simulating them from unconstrained and constrained versions of the HMM (i.e., to capture each of the submodels as well as the full unconstrained HMM) with a generous range of populationlevel parameters:</p><formula xml:id="formula_5">p 1 â 0 05 0 50</formula><p>p 2 â 0 00 0 10 12 â 0 10 0 35 21 â 0 00 0 25</p><formula xml:id="formula_6">p 1 p 2 1 2 â 0 10 0 45 1 â 0 50 1 00 (4)</formula><p>We discuss these synthetic data sets and the variability across them in the next section. However, after creating these data sets, we put aside the datagenerating process and describe them entirely by easy-to-compute and managerially relevant database characteristics, which we now cover in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Selecting Database Characteristics</head><p>The decision tree is a tool that predicts which specification is likely to be the winning model by only looking at summary statistics of a particular database. Just as we need a set of reasonable models from which to choose, we also need a set of database characteristics to drive the choice process. But which database characteristics should we consider? We illustrate our process of identifying relevant database characteristics by returning to one of the opening examples, repeat purchasing for data set A. Before running any models, analysts frequently examine two typical displays of a cohort's purchasing behavior: a cross-sectional histogram of customer-level transactions and a longitudinal tracking plot of cohort-level purchases over time. These two graphs appear in Figure <ref type="figure" target="#fig_1">2</ref>. What are the key features of each graph? We want to choose summaries that are both managerially relevant and easy to compute directly from these aggregate plots. We identify four summaries that offer a fairly complete characterization of each plot. For the histogram, we propose summaries to capture the nature of the head and the tail of the distribution as well as its central tendency. For the tracking plot, we focus on the early and late trends in purchasing as the cohort ages and the trend's overall "bowed" shape, as well as the overall variability over time.</p><p>More specifically, Table <ref type="table" target="#tab_1">2</ref> contains a listing of these measures, which we will use in our subsequent empirical analysis. Although there is not an exact science to selecting these measures, we choose them here to represent central tendency (e.g., average frequency), higher moments (e.g., top percentile, purchase concentration 80:20-type rule), and trend behavior (e.g., steepness, shape, trend variability). We do not claim this list to be comprehensive, but these values vary widely and in systematic ways across the data sets generated by the HMM and its constrained versions.</p><p>The variation in these measures across databases is essential: it allows us to explicitly show the range of empirical patterns we consider here and is required to obtain a meaningful classification tree linking these summaries to the model selection process. We illustrate some of this variation in values of these summary statistics for data sets A, B, and C (see Table <ref type="table" target="#tab_2">3</ref>). It is interesting to see how the data sets are indistinguishable on some dimensions (e.g., frequency), quite distinct from each another on others (e.g., penetration), and occasionally exhibit pairwise similarities (e.g., late trend for data sets B and C).</p><p>In most empirical settings, we think about the amount of information as being related to the number of observations within a data set. But in this setting, each data set is reduced to a single observation described along multiple dimensions, i.e., the database characteristics described above. Thus, we construct a "data set of data sets," a collection of 64 simulated data sets reflecting variation along the summary statistics and representing real-world data sets <ref type="bibr" target="#b10">(Fader et al. 2010</ref><ref type="bibr" target="#b25">, Netzer et al. 2008</ref><ref type="bibr" target="#b33">, Schweidel et al. 2011</ref>. Specifically, we generate data sets from all possible combinations of the parameter values noted in Â§2, which allows us to reflect both the structural variation and the "natural randomness" that arises from simulating the purchases. Once the data sets are created, the true values of the population-level parameters are no longer taken into consideration.</p><p>Figure <ref type="figure">3</ref> shows the large variability along the values of the database characteristics across the simulated data sets. For instance, nearly half of the data sets have penetration rates between 40% and 70%. About 40% of them have a very steep declining trend (steeper than a drop in transactions equivalent to 15% of the cohort size), whereas others show some growth in purchases for the cohort over time. Thus, we believe that by selecting and creating data sets in this way, we will have avoided biasing our classification results to favor any particular model specification.</p><p>To ensure that our chosen characteristics are explaining most of the meaningful variation across the collection data sets, we ran a principal components analysis and an exploratory factor analysis on an even larger set of summary statistics beyond the ones described earlier. We do not present the detailed results but note a few highlights. The principal components analysis indicates that 99% of the measured variation across the 64 data sets can be captured by six independent components. The loadings of the  principal components analysis and the loadings of the exploratory factor analysis (with five, six, and seven factors) all point to a very similar set of summary statistics, such as central tendency, concentration, and variation over time.</p><p>We also recognize that a number of these database characteristics are naturally correlated with each other. Some measures are quite independent (e.g., late trend and penetration, r = 0 01), but other pairs have correlations that are large and significant (e.g., average frequency and penetration, r = 0 86). Although this kind of multicollinearity could be a serious problem in a typical regression-like model, it does not affect the classification tree and random forest approach since they are nonparametric methods designed specifically for (sequential) variable selection <ref type="bibr" target="#b1">(Breiman 2001a</ref><ref type="bibr" target="#b3">, Breiman et al. 1984</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Assessing Model Performance</head><p>The final ingredient that goes into the classification tree is a rule for declaring a winning model for a given data set. Here, we select a winner based on each model's ability to predict an important managerial quantity that is widely used for purchasing data because of its link to customer lifetime value and other profit measures: aggregate incremental sales over a holdout period. Specifically, we select an error measure that summarizes the time series of discrepancies between the model and the observed sales for each "Markov chain Monte Carlo (MCMC) world." We will look at the variability of the errors across "worlds" and also average the errors across the worlds to obtain a measure of the model's error for that data set that integrates over the posterior uncertainty. The error measure we use, mean absolute error (MAE), assumes a linear loss function and is frequently used for time-series data. In the more general formulation of this procedure (see Appendix B), one can select any managerial quantity (replacing outof-sample aggregate sales over time) and error measure with a different loss function (to replace MAE). Although we present results using MAE for our context, our classification tree results are robust to alternative common summary error measures (e.g., mean absolute percent error and root mean squared error). <ref type="bibr">6</ref> Formally stated, we quantify performance as the degree to which the model-based posterior predictive distribution of out-of-sample aggregate sales is outlying with respect to the quantity's observed value. We assume the posterior distribution has been obtained using standard MCMC procedures (detailed in Appendix A), yielding posterior draws g = 1 G. For data set k, y obs kt is number of the observed incremental transactions at time period t, and y * g kmt is one replicate from model m's corresponding posterior predictive distribution for that quantity (i.e., incremental transactions). Then for each posterior replicate g, we compute the mean absolute error: <ref type="bibr">6</ref> We note that our choice of error measure for model selection is in contrast to commonly used likelihood-based summary criteria, such as Bayesian information criterion (BIC) and deviation information criterion (DIC) <ref type="bibr" target="#b22">(Montgomery et al. 2004</ref><ref type="bibr" target="#b23">, Montoya et al. 2010</ref><ref type="bibr" target="#b25">, Netzer et al. 2008</ref><ref type="bibr" target="#b33">, Schweidel et al. 2011</ref>. We use an empirical quantity for model selection since many scholars caution against using purely likelihood-based measures <ref type="bibr" target="#b12">(Gelman and Rubin 1995)</ref>, especially for latent-state models, such as the HMM and its variants, because one must face issues with unstable estimators, computation of the posterior distribution, and correction factors of the log-marginal likelihood <ref type="bibr" target="#b4">(Chib 1995</ref><ref type="bibr" target="#b18">, Lenk 2009</ref><ref type="bibr" target="#b26">, Newton and Raftery 1994</ref><ref type="bibr" target="#b35">, Spiegelhalter et al. 2002</ref> </p><formula xml:id="formula_7">d g km = 1 T T t=1 y * g kmt â y obs kt (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>195</head><p>We will use the values of d g km in two ways. On the one hand, we will examine the average posterior MAE for all four models on each data set to determine a single winner per data set. On the other hand, to provide a more nuanced set of findings, we characterize the full posterior uncertainty of the MAE by computing the probability that each model has lowest value (i.e., the proportion of times each model is the winner across the G posterior replicates) because we would not want to overly penalize a model that is a "close second," for instance. We also use the latter directly in our classification tree. Now, armed with a set of models (the HMM and its constrained variants), the in-sample database characteristics for each data set (see Table <ref type="table" target="#tab_1">2</ref>), and an error measure (out-of-sample MAE), we have all of the ingredients for the decision tree, which is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">When to Use Which Model?</head><p>A Classification Tree</p><p>We classify data sets to reveal how we can select the model with the best out-of-sample error by only using in-sample database characteristics. This enables us to answer the paper's central question: Given a data set's summary statistics, which model best fits the data?</p><p>The winning model, m</p><p>Winner g k , for data set k and posterior world g is determined by the identifying the model with the minimum error d g km among all M models:</p><formula xml:id="formula_8">m Winner g k = arg min m=1 M d g km<label>(6)</label></formula><p>We use a classification tree to relate the identity of the winning model, m</p><p>Winner g k , to the vector of database characteristics, Y obs k . Given the performance of all M models across all K data sets and G posterior replicates, we explain variations in the model performance (i.e., which model wins) as a function of the observed summaries of that data set. Stated formally, we capture this relationship as follows:</p><formula xml:id="formula_9">m Winner g k = Tree Y obs k (7)</formula><p>where the function "Tree" denotes the classification tree predicting the winning modelm Winner g k for each of the data sets k = 1 K and posterior world g = 1 G. The classification tree provides cutoff values of the data set-level summary statistics to place entire data sets into "buckets." This classifies data sets in an easy-to-interpret manner. Each bucket of data sets has a similar profile of data set-level summary statistics and similar patterns of model performance. Therefore, when a new data set is encountered, it can be classified using this decision rule to identify which of the models will likely be most suited for it. This allows us to uncover relationships between observed patterns in the data and model fit that are easy to interpret while avoiding the need to make any additional assumptions about functional form or error distributions common to ordinary regression models.</p><p>Additionally, our classification tree approach goes one step further because it also reflects the natural parameter and model uncertainty. We reflect that uncertainty since our Bayesian modeling approach provides the full posterior distribution of performance for each data set-model pair. As a result, each case to be classified is unique to a particular posterior draw from a model run on a data set. This means that the data to be used to construct the classification tree contain G = 100 model-based replicates of the K = 64 observed data sets. By using G replicates of each set of observed data set summaries (independent variables), we allow for G different values of errors from each model-data set pair; hence, each data set has a distribution of different winning models (dependent variables) and therefore receives an appropriate number of "votes."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Classification Tree</head><p>The classification tree in Figure <ref type="figure">4</ref> can be easily read by starting at the top and following a series of "if then" decisions down to a terminal node at the bottom of each branch. These terminal nodes represent a group of data sets with the same observed summary statistic branch values (predictor variables). Each node has a recommended winning model but also displays the within-node winning percentages for each model (based on the number of posterior worlds in which each model had the lowest forecast error). Note that the N values in the tree sum to 6,400 cases, reflecting the use of 100 posterior replicates for each of the 64 simulated data sets.</p><p>Four database characteristics were selected by the classification tree's sequential variable selection algorithm as being diagnostic: early trend, late trend, concentration, and trend Gini (trend shape).</p><p>The early and late trend statistics reflect the change in transactions over each half of the calibration period (15 days in each half) expressed as a percentage of the total customer base (500 customers). The classification tree partitions early trend into three levels: very steep (steeper than a drop in daily transactions equivalent to 11% of the customer base), moderately steep (a drop between 1% and 11%), and relatively flat or positive (a slope that is more positive than â1%). Late trend is partitioned into two levels, which we label moderately steep (a drop steeper than 3% of the total customer Notes. The tree is estimated on 6,400 cases, using 100 posterior samples for each of the 64 data sets. For any data set, the tree should be read from top to bottom: the ovals represent the partitions, the rectangles indicate the terminal nodes, and the listed model is the recommended one for that particular combination of database characteristics. Also listed is a vector summarizing the posterior winning percentage for all four models (left to right: BG/BB, HC, OF, and HMM). The highlighted data sets A, B, and C appear where they are best classified.</p><p>base) and relatively flat or positive (a slope that is more positive than â3%). Next, the split for concentration has a remarkable resemblance to the 80:20 rule. A data set is either highly concentrated (more than 82% of purchases are made by the top 20% of customers) or not highly concentrated.</p><p>The trend Gini summary statistic reflects the shape of the curve. How much does the actual curve deviate from a line connecting the first and last days of the calibration period (i.e., how much area is there between curve and the trend line)? In other words, this measures the degree to which the curve is "bowed." The variable is split into a less bowed shape (close to linear with value less than 5%) and a more bowed shape (value greater than 5%). Negative values indicate that there is more area between the curve and the trend line that sits above the trend line than below the trend line.</p><p>We illustrate the use of the tree by returning to our three introductory data sets. Recall their database characteristics were shown in Table <ref type="table" target="#tab_2">3</ref>. We can trace how the tree classifies these data sets to illustrate exactly how a manager can use our decision tree. For instance, data set A exhibits a sales pattern that is downward sloping early on (steeper than â1%) and not strongly downward sloping later on (equal to â3%) and where more than 82% of the purchases are made by the top 20% of customers. Thus, the classification tree recommends that it would be best modeled using the BG/BB model since that model provides the best out-of-sample forecast for 40% of the posterior replicates associated with the 15 different data sets that have similar values of database characteristics. (And indeed, the BG/BB model does provide the best forecast for data set A, as we show in Tables <ref type="table" target="#tab_5">4-6</ref>.) It is interesting to note the internal consistency of the tree. In particular, the precise value of the late trend for data set A is exactly the classification tree's cutoff value (â3%). So even if the data set's late trend were just slightly less than that cutoff, the data set would still fall into a node dominated by the BG/BB model (i.e., in the leftmost terminal node of the tree, the BG/BB model is the best-performing model in 46% of posterior replicates).</p><p>For data sets with a declining early trend and a flat or increasing late trend, but without a high purchase concentration (fewer than 82% of the purchases made by the top 20% of customers), a different pattern emerges. Data sets B and C are two such examples, so they fall into two terminal nodes in this part of the tree. Data sets represented in this part of the tree show a strong need to allow for back-and-forth transition (HMM and OF). But within the back-and-forth pair, there is less certainty about which one wins.</p><p>Further splitting the data sets by trend Gini (trend shape) over the calibration period and by early trend one more time allows the analyst to better discriminate  Note. These winning percentages illustrate the uncertainty in declaring a winner.</p><p>when each model is likely to perform better. For data sets with a more bowed shape (trend Gini greater than or equal to 5%) and a very steep early trend (steeper than 9%), such as data set B, the HMM wins with 45% of votes versus the OF with 32%. However, for others with a moderately steep early trend (between a 1% and 9% drop) and a more bowed shape, such as data set C, the OF wins with 55% of votes versus the HMM with 32%. Data sets B and C are therefore best classified by the HMM and OF, respectively. The split on trend shape (trend Gini) and an additional split on early trend should be intuitive because the HMM is a more general model than the OF. As a result, the HMM can generate a wider range of patterns across data sets than the OF can because of the extra model flexibility (e.g., state 2 purchase probability is not necessarily zero). To understand this, keep in mind the patterns common to the data sets in this part of the tree: not highly concentrated purchasing and flat or increasing late trend. On the one hand, for less bow-shaped curves, the OF has difficulty capturing a nearly linear pattern since the off state induces a moderate steep early drop. On the other hand, for markedly bow-shaped curves, the OF also has difficulty capturing both the very steep early declining trends and flat or increasing later trend. Capturing such an interaction among database characteristics is an advantage that CART methods have over traditional linear regression approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Uncertainty in Model Performance</head><p>As we take a deeper dive into particular branches of the decision tree, we examine the uncertainty in model performance. That is, although the model with the lowest error is declared the winner, we describe how we one can use votes for each winning model by utilizing the full posterior from the Bayesian model output.</p><p>As an illustration, we return to data sets A, B, and C to look at the comparative performance of the HMM and its variants from the 2 Ã 2 framework. The average performance seen in the plots of Figure <ref type="figure">5</ref> are quantified in Table <ref type="table" target="#tab_5">4</ref>.</p><p>We summarize each model's performance for a data set using MAE averaged over the posterior uncertainty. For example, for data set A, the BG/BB model has the best average out-of-sample prediction (MAE = 4 37, mean across replicates), closely followed by the HC (4.91). For data set B, the HMM (8.35) clearly outperforms the other three models, and data set C is best modeled by the OF (7.09).</p><p>Whereas those are posterior means of model performance, we also convey the degree of uncertainty in these assessments using replicated data sets associated with the full posterior predictive distribution. To illustrate this uncertainty, we plot the predicted incremental sales for each posterior replicate for data set A and the observed daily incremental sales (see Figure <ref type="figure" target="#fig_4">6</ref>). By visual inspection of these tracking plots alone, it is difficult to detect whether the BG/BB model truly predicts better than the other three models.</p><p>Although we would like to declare a single winning model for each data set, the high level of uncertainty around the model predictions seems to raise a warning flag about making any strong statements about differences among the models. Therefore, we want to quantify the "shades of gray" in model performance by recognizing that when declaring a winning model, the vote need not be unanimous.</p><p>Thus, instead of only examining posterior mean of MAE, we characterize its full distribution. For the highlighted data sets A, B, and C, we show the distribution of each model's MAE across all replicates (see Figure <ref type="figure">7</ref>). Table <ref type="table" target="#tab_6">5</ref> displays the corresponding distribution summaries (e.g., median and interquartile ranges of MAE across replicates).</p><p>Not surprisingly, the densities of the performance measure of the four models are somewhat overlapping. For example, in data set A, although most of the mass of the BG/BB model density is lower (better) than that of the HMM and HC densities, there is some probability that the HMM or HC has a lower MAE than the BG/BB model. This suggests there is not a unanimous winner. By contrast, in data set C, for instance, there is much less overlap, suggesting that the OF has an even higher chance of a lower MAE than do the others.</p><p>But what is the probability that each model is the winning model for a data set? We take advantage of the Bayesian output to make this probability statement. Table <ref type="table" target="#tab_7">6</ref> shows each model's winning percentage for data sets A, B, and C. That winning percentage, or percentage of votes, is the proportion of posterior worlds in which each model has the lowest error. For instance, the OF is quite clearly the winner for data set C since it wins 81% of the time. For data set A, although the BG/BB model is the winner, the distributions of the error for three of the four models overlap. So it is not surprising that they split the votes, and the BG/BB model wins 45% of the time compared with 27%, 6%, and 22% for HC, OF, and HMM, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Assessing the Predictive Value of the</head><p>Decision Tree: In Sample How accurate are the resulting recommendations from the tree? We answer this question to assess the tree's predictive value. First, we focus on the simple measure of the hit rate of the classification tree. The hit rate is the number of times the tree recommends a model that is, in fact, the best model to use on that Notes. These are densities of error measures, so smaller values indicate better model fit. When the densities overlap a great deal (e.g., data set A), there is not a clear winning model. When one density that stands out as better than the others (e.g., data set C), there is a clear winning model. data set. Averaged across all models and iterations, the hit rate is 46%. We put this hit rate in context by noting that from a purely operational standpoint, the tree allows the analyst to run one model instead of four. In other words, by reducing the work of an analyst by 75%, the tree makes a recommendation of which model to use that is about twice as good as guessing (a 25% hit rate) randomly among the four models. This hit rate also fares well when compared with tougher comparative yardsticks, such as the proportional chance criterion and maximum chance criterion <ref type="bibr" target="#b24">(Morrison 1969)</ref>, which yield benchmark hit rates of 28% and 35%, respectively. The latter metric is often hard to beat in a discriminant analysis setting. It assesses how much better our classifications are compared with using the most common actual winner (in this case, the HMM) every time. Thus the decision tree clearly offers some improvements over that simple (but often effective) approach.</p><p>However, this measure is purely an in-sample one: it uses the same 64 data sets for calibration and classification purposes, so it may be subject to overfitting. We next describe a procedure, random forests, that will allow us to reflect the uncertain nature of the tree itself and its application to holdout data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Assessing the Predictive Value of the Tree</head><p>Using Random Forests: Out of Sample To answer the question about the value of the tree, using another lens, we turn to another machinelearning method closely related to CART known as random forests <ref type="bibr">(Breiman 2001a, Liaw and</ref><ref type="bibr" target="#b19">Wiener 2002)</ref>. Although the single classification tree we have described above takes into account the parameter and model uncertainty, it does not take into account uncertainty in the structure of the classification tree itself. The random forest captures extra variation around the classification. This requires many classification trees, so the random forest algorithm "grows" many trees (hence, the "forest").</p><p>What is special about the random forest is that it has a built-in monitoring system to make sure it produces predictions that are validated on a holdout set and that are utilizing important predictor variables. Both aspects of predictions prevent overfitting <ref type="bibr">(Breiman 2001a, Liaw and</ref><ref type="bibr" target="#b19">Wiener 2002)</ref>. Fortunately, the random forest algorithm has a builtin cross-validation procedure calculating an n-fold cross-validation, where the holdout sample size, n, is typically about 1/3 of the cases <ref type="bibr" target="#b1">(Breiman 2001a</ref>). The holdout misclassification rate, in the language of machine learning, is called the "out-of-bag" error rate, or the "generalized error rate" since it is intuitively similar to cross-validation error, which indicates the ability of the predictive model to generalize to cases outside of the given data set.</p><p>The random forest out-of-sample error rates broken down by each model are in Table <ref type="table" target="#tab_8">7</ref>, and the hit rate across all four models is 48%. This closely matches the in-sample hit rate using one classification tree. It is Notes. Rows indicate which model actually fits the data best. Columns indicate which model was recommended by the random forest's classification for that data set using a 2/3 sample for calibration (in sample) and 1/3 sample for validation (out of sample). The hit rate is the proportion of each type of data set correctly classified as out of sample (i.e., the diagonal entries divided by row sums).</p><p>encouraging to see that, even when a data set is not used for calibration, it can be classified correctly with a high level of accuracy.</p><p>Looking more carefully at the classification tree and random forest results, several distinctive patterns arise. It is clear that the BG/BB and HMM models have substantially higher hit rates than do the other two models. It also seems that each of these polar opposite models (at least in terms of parameters and complexity) can serve as effective "representatives" to characterize the entire family of HMM models covered here.</p><p>This result raises the question about which of the two constraints/dimensions associated with our 2 Ã 2 framework is more important to capture: the presence of an off state or the existence of an absorbing state. A closer inspection of Table <ref type="table" target="#tab_8">7</ref> clearly reveals the answer: classifying whether or not the data require an absorbing-state model or a back-and-forth model is much more informative than the presence of an off state. There is a high degree of confusion between the BG/BB model and HC, and likewise for HMM and OF, but relatively little confusion between the BG/BB model and OF or between HMM and HC. In Table <ref type="table" target="#tab_9">8</ref>, we aggregate the classifications across this single dimension and see incredibly high hit rates (62% and 89%) when we ignore the presence or absence of the off state.</p><p>We have explored the predictive value of the decision tree, so it is natural to ask what is driving its good predictive ability. To better understand the drivers of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Which Database Characteristics</head><p>Are Most Diagnostic? The output of the random forests uncovers which variables are most important in explaining classification success. This not only validates the decision tree obtained via CART but also quantifies variable importance. Variable importance in random forests is a measure of the average improvement in prediction accuracy of a tree when this variable is included (and its values are intact) compared with when this variable's values are meaningless (arbitrarily permuted across observations).</p><p>Figure <ref type="figure">8</ref> displays each database characteristic's variable importance. This analysis confirms what we see in the classification tree: early trend, late trend, trend Gini (trend shape), and concentration are the four most important variables, and they are clearly separated out from the others. Among the four, however, late trend is the most important. This makes intuitive sense because the models differ in their ability to generate decreasing or increasing patterns in aggregate sales over time. For instance, for a data set with a strongly increasing late trend, the BG/BB model and HC, because of their absorbing state, would not be able to capture it at all. This provides more evidence that even before running any models, an analyst could use these easy-to-compute database characteristics to refine the decision about which model is likely to perform best. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable importance</head><p>Notes. The most important variables are those that provide the largest increase in out-of-sample (out-of-bag) classification hit rate, averaged across all trees in the forest. The late trend, early trend, and concentration are clearly the three most important and confirmed by appearing in the classification tree obtained via CART methods. The next most important variable is trend Gini (trend shape), which also appears in the classification tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">How Much Value Does the Classification Tree Add?</head><p>What does the analyst gain by using our decision tree? From the above discussion, we find the decision tree nearly doubles the hit rate compared with uninformed guessing about which of the four models to run. And much of the remaining error rate is associated with the relatively unimportant distinction between the presence or absence of an off state.</p><p>But although this information helps ease the task of choosing the right model, it also tells us how well an analyst would do using the decision tree compared with running all four models for every data set. So it is reasonable to ask: How much error would the analyst suffer by using only one model for all data sets? After all, this is the starting point for many analysts. Suppose the analyst only used BG/BB models for all data sets she encountered. How poorly would she have performed? We can compare the error incurred to the average performance if she always used the truly winning model for each data set. Table <ref type="table" target="#tab_11">9</ref> summarizes this analysis.</p><p>Running the BG/BB model on all data sets yields an error 52% worse than using the true winning model (average MAE = 9 52 versus 6.28). An analyst would do better by running only the HMM, which yields an error 21% worse than the using winning model (average MAE = 7 63). By contrast, using the model recommended by decision tree for each data set is the best option because it greatly reduces error to only 12% worse than the winning model (average MAE = 7 05). That is, in terms of relative error to the best model, only using the HMM is 75% worse than using the decision tree. So using the decision tree is a winwin: it requires 75% less effort and helps the analyst to avoid a 75% increase in relative error.</p><p>The value of the decision tree is even greater if we look beyond average performance and consider the worst case scenario of model performance. When examining the variability in performance, the 95% level of error for using any single model can be quite Notes. The MAE values reflect performance of running each model for all data sets compared with following the tree's recommendation ("Tree") and always selecting the model with best out-of-sample fit for each data set ("Winner"). The percentages illustrate the loss compared with the best-fitting model (Winner).</p><p>high. However, the decision tree greatly controls that upper tail of error. In particular, the high end of possible error for the HMM is 35% worse than the winner's error, but the decision tree is only 12% (see Table <ref type="table" target="#tab_11">9</ref>).</p><p>In short, our tree shows that an analyst should not use the same model for all occasions, and it clearly quantifies the cost of doing so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">General Discussion and Future Directions</head><p>When researchers and managers regularly encounter a particular kind of data structure and regularly choose among a standard set of models, they often develop good intuition about when to use which model. Our approach rigorously quantifies and validates this kind of intuition through a well-structured decision tree.</p><p>For the case of a database of repeat purchases over time for a cohort of customers, we make specific recommendations about when to use the HMM and its constrained variants and which data set-level summaries are important for that decision. We find that for data sets exhibiting an early decreasing trend in aggregate sales, the BG/BB model provides the best forecast when the trend continues to decrease even later in the calibration period. But when it looks like the trend has leveled off, the BG/BB model frequently underpredicts, and more complexity is often warranted. An interesting exception to this rule is the case of high purchase concentration, which suggests that the buy till you die framework is still likely to provide the best forecast. This may be reflective of the customers exhibiting high heterogeneity in purchase and churn rates rather than a more complex backand-forth state-switching process over time.</p><p>In the case of the 2 Ã 2 framework, the models are classified with strong evidence along one dimension (the presence of an absorbing state versus backand-forth movement across states), but the data offer weaker evidence to help discriminate data sets and models along the other dimension (the presence or absence of an off state). This may be surprising in light of many papers that add a death state to an HMM-like model. But it may be the case that such models work well mainly because of the constraint making that state absorbing and not necessarily because the behavior is "turned off" within it. This finding could have important implications for model builders and should be investigated more carefully in settings beyond this framework.</p><p>Beyond our HMM-based example, our proposed approach for empirical identification is more broadly relevant. We explicitly test the characteristics of data sets that distinguish one model from a related one. Although this differs from a formal theoretical identification (e.g., using economic principles), it is aligned with the calls for such activities that have been arising more frequently in marketing <ref type="bibr" target="#b16">(Hartmann et al. 2008)</ref>.</p><p>The procedure that we propose is quite general: given the appropriate inputs (i.e., database characteristics), it can generate a decision tree prescribing which model should be used for any given data set and any given outcome/goal of interest. Understanding the interplay between database characteristics and the relative performance of models (and model components) is a useful contribution beyond the illustrative (yet common) context presented here. Although we illustrate it here with the HMM on forecasting incidence data (e.g., repeat purchasing of a cohort), it is agnostic to these choices. In general, the recipe for this method requires the following elements: (1) a consideration set of candidate models, (2) a set of predictor variables consisting of observed summary statistics from each data set, and (3) the outcome variable, which is a choice of how to "pick the winner"; this requires a key managerial quantity and a loss function for computing the error measure.</p><p>Classification and regression trees and random forests, although popular in machine learning and statistics, are still relatively new to the field of marketing, so we hope our work will call more attention to this powerful and versatile tool. Furthermore, our application of it to the problem of model selection (as opposed to variable selection) is relatively uncommon even in the statistics literature, but it is clearly a natural and important issue in many marketing contexts.</p><p>Unlike traditional uses of classification methods, we add an extra twist by employing them in a fully Bayesian framework, allowing us to leverage the full posterior distribution. This differs from previous mixtures of Bayesian and classification methods, e.g., Bayesian CART <ref type="bibr" target="#b5">(Chipman et al. 1998</ref>), since we construct a decision tree from information that already incorporates a joint posterior distribution. Our mix of Bayesian approaches with classification methods is a promising area of research for the interface of marketing, statistics, and machine learning. The combination of the two approaches represents an exciting blurring of methodological boundaries, and marketing problems such as the one examined here have a great deal to offer in the debate between the "two cultures" of data modeling (statistics) and algorithmic modeling (machine learning) put forward by <ref type="bibr" target="#b2">Breiman (2001b)</ref>.</p><p>As computational costs decrease and access to grid/cloud computing increases, the procedure we propose here will be even easier to do in a variety of contexts. Of course, one could argue that with greater computing power, there is less need to worry about selecting the single best model a priori-just run a bunch of models and pick the best one. But this logic is flawed for several reasons. First, our analysis focuses on performance in a holdout period, not in-sample fit. Second, and related, there is great danger in choosing models that are overly complex and excessively customized to every different data set. And third, we believe strongly in exploring and learning from the underlying patterns that are driving the observed data patterns. This kind of "data science" not only will help analysts create and choose better models but also will help managers make better tactical decisions to create and extract more value from their customer relationships.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1Three Different Commonly Observed Patterns of Aggregate Sales over Time (e.g., Arising from a Product Launch or Repeat Purchasing of Cohort of New Customers)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2The Observed Database Characteristics Arise Naturally from Plots That Managers Typically Examine When Deciding Which Model(s) to Run</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3Histograms Summarizing the Variability for Each Database Characteristic Across All 64 Data Sets Frequency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5Three Example Data Sets with Predictions Arising from Each of the Four Models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6</head><label>6</label><figDesc>Figure 6Illustrations of the Range of Variability in Model Prediction for Data Set A BG/BB HC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 7Posterior Distribution of Out-of-Sample MAE for the Three Highlighted Data Sets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Nested Model Relationships Among the HMM and Its Constrained Variants</figDesc><table><row><cell>State 2 is cold</cell><cell>State 2 is off</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Database Characteristics that Capture Features of a Longitudinal Incidence Data Set and Can Be Computed from Summary Plots (e.g., Histogram and Tracking Plot) What is the trend in the first half of the calibration period (i.e., drop from first to middle day as a percentage of the size of the customer base)? Late trendWhat is the trend in the second half of the calibration period (i.e., drop from middle to last day as a percentage of the size of the customer base)? Trend Gini How much does the actual curve deviate from a line connecting the first and last days (i.e., how much area is there below the trend line and the curve, as percentage of the levels of the line, Ã  la the Gini</figDesc><table><row><cell>Characteristic</cell><cell>Description</cell></row><row><cell>Frequency</cell><cell>How many active days of transactions are there per</cell></row><row><cell></cell><cell>customer?</cell></row><row><cell>Penetration</cell><cell>How many unique customers have made at least one</cell></row><row><cell></cell><cell>transaction?</cell></row><row><cell>Concentration</cell><cell>How is activity spread out among customers (i.e.,</cell></row><row><cell></cell><cell>what fraction of all transactions was made by the</cell></row><row><cell></cell><cell>top 20% of customers)?</cell></row><row><cell>Top 5% level</cell><cell>How much are the most active customers</cell></row><row><cell></cell><cell>purchasing (i.e., what level of transactions is the</cell></row><row><cell></cell><cell>cutoff for the top 5% of most active customers)?</cell></row><row><cell>Early trend</cell><cell></cell></row><row><cell></cell><cell>coefficient)?</cell></row><row><cell>Trend variability</cell><cell>How much day-to-day variation is present in the</cell></row><row><cell></cell><cell>calibration period (i.e., standard deviation of</cell></row><row><cell></cell><cell>incremental sales)?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Database Characteristics for the Three Highlighted Data Sets</figDesc><table><row><cell>%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). Schwartz, Bradlow, and Fader: Model Selection Using Database Characteristics</figDesc><table><row><cell>Marketing Science 33(2), pp. 188-205, Â© 2014 INFORMS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>For Each Data Set and Each Model's Posterior Draw, the Out-of-Sample Forecast Is Generated from the Posterior Predictive Distribution and MAE Is Computed</figDesc><table><row><cell></cell><cell></cell><cell>Data set</cell><cell></cell></row><row><cell>Model</cell><cell>A</cell><cell>B</cell><cell>C</cell></row><row><cell>BG/BB</cell><cell>4.37</cell><cell>9 71</cell><cell>14 13</cell></row><row><cell>HC</cell><cell>4.91</cell><cell>10 16</cell><cell>17 29</cell></row><row><cell>OF</cell><cell>6.52</cell><cell>10 20</cell><cell>7 09</cell></row><row><cell>HMM</cell><cell>5.13</cell><cell>8 35</cell><cell>9 44</cell></row><row><cell cols="4">Notes. The posterior mean of the MAE values for each model-data set pair</cell></row><row><cell cols="4">is shown here. The lowest error value (i.e., winning model) for each data set</cell></row><row><cell>is in bold.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="8">For Each Data Set and Each Model's Posterior Draw, the</cell><cell></cell></row><row><cell></cell><cell cols="8">Out-of-Sample Forecast Is Generated from the Posterior</cell><cell></cell></row><row><cell></cell><cell cols="4">Predictive Distribution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Posterior distribution of MAE (%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Data set A</cell><cell></cell><cell></cell><cell>Data set B</cell><cell></cell><cell></cell><cell>Data set C</cell><cell></cell></row><row><cell>Model</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>25</cell><cell>50</cell><cell>75</cell></row><row><cell cols="3">BG/BB 3.7 4.2</cell><cell>4.8</cell><cell>8.3</cell><cell cols="5">9 6 11 0 11 9 14 0 16 2</cell></row><row><cell>HC</cell><cell cols="2">4.0 4.6</cell><cell>5.5</cell><cell cols="6">8.9 10 1 11 4 15 4 17 3 19 1</cell></row><row><cell>OF</cell><cell cols="2">4.9 6.2</cell><cell>7.8</cell><cell cols="3">8.6 10 0 11 6</cell><cell>6 3</cell><cell>7 0</cell><cell>7 8</cell></row><row><cell>HMM</cell><cell cols="2">4.1 4.7</cell><cell>5.8</cell><cell>7.0</cell><cell>8 1</cell><cell>9 5</cell><cell>7 7</cell><cell cols="2">9 0 10 7</cell></row></table><note>Notes. MAE is computed for each replicate data set. The posterior quantiles (25%, 50%, and 75%) of the values across for each data set are shown here. The lowest value (i.e., winning model) for each data set is in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Posterior Probabilities of Each Model "Winning" Are Computed as the Proportion of Replicated Data Sets (e.g., "MCMC Worlds") in Which Each Model Has the Lowest MAE</figDesc><table><row><cell></cell><cell cols="3">Posterior probability of model winning (%)</cell><cell></cell></row><row><cell>Data set</cell><cell>BG/BB</cell><cell>HC</cell><cell>OF</cell><cell>HMM</cell></row><row><cell>A</cell><cell>45</cell><cell>27</cell><cell>6</cell><cell>22</cell></row><row><cell>B</cell><cell>20</cell><cell>11</cell><cell>16</cell><cell>53</cell></row><row><cell>C</cell><cell>1</cell><cell>0</cell><cell>81</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Out-of-Sample Classification for Each of the 6,400 Cases (Data Set-World Pairs) from the Random Forest</figDesc><table><row><cell>Model</cell><cell>BG/BB</cell><cell>HC</cell><cell>HMM</cell><cell>OF</cell><cell>Hit rate (%)</cell></row><row><cell>BG/BB</cell><cell>840</cell><cell>151</cell><cell>215</cell><cell>114</cell><cell>64</cell></row><row><cell>HC</cell><cell>372</cell><cell>247</cell><cell>145</cell><cell>154</cell><cell>27</cell></row><row><cell>HMM</cell><cell>430</cell><cell>40</cell><cell>1 056</cell><cell>677</cell><cell>48</cell></row><row><cell>OF</cell><cell>259</cell><cell>7</cell><cell>742</cell><cell>951</cell><cell>49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>That is, by ignoring the presence or absence of an off/death state, the hit rates are quite high. Like Table7, these are out-of-sample classifications, so the hit rate is the proportion of cases correctly classified.</figDesc><table><row><cell cols="4">Combined Cases of Data Sets and Classifications Into</cell></row><row><cell cols="4">Models with Absorbing States (BG/BB and HC) and</cell></row><row><cell cols="3">Back-and-Forth Transitions (OF and HMM)</cell><cell></cell></row><row><cell>State</cell><cell>Absorbing</cell><cell>Back-and-forth</cell><cell>Hit rate (%)</cell></row><row><cell>Absorbing</cell><cell>1 383</cell><cell>855</cell><cell>62</cell></row><row><cell>Back-and-forth</cell><cell>466</cell><cell>3 696</cell><cell>89</cell></row><row><cell>Notes.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc>Absolute and Relative Benefits of Using the Classification Tree over Running Each Model for All Data Sets</figDesc><table><row><cell></cell><cell>BGBB</cell><cell>HC</cell><cell>OF</cell><cell>HMM</cell><cell>Tree</cell><cell>Winner</cell></row><row><cell>Mean MAE</cell><cell>9 52</cell><cell>9 19</cell><cell>8 14</cell><cell>7 63</cell><cell>7 05</cell><cell>6 28</cell></row><row><cell>% worse than</cell><cell>52%</cell><cell>46%</cell><cell>30%</cell><cell>21%</cell><cell>12%</cell><cell>-</cell></row><row><cell>winner</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>95% MAE</cell><cell>22 34</cell><cell>18 97</cell><cell>16 70</cell><cell>15 63</cell><cell>13 00</cell><cell>11 57</cell></row><row><cell>% worse than</cell><cell>93%</cell><cell>64%</cell><cell>44%</cell><cell>35%</cell><cell>12%</cell><cell>-</cell></row><row><cell>winner</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Without loss of generality, we use "day" to refer to the unit of discrete time and "purchase" as the observed behavior of interest. It could be, instead, for example, viewing online videos or not in a given week, donating or not in a given quarter, etc.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Unlike the BG/BB as in<ref type="bibr" target="#b10">Fader et al. (2010)</ref>, which assumes that all individuals start in the "alive" state, in our BG/BB specification we allow individuals to start in either state, according to initial state probability vector . The model utilized here is more general.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For this reason, we do not consider a separate "cold then hot" model, although the general HMM and OF specifications allow individual-level purchasing to speed up over time.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by a grant from Amazon Web Services, which funded the use of the Elastic Computing Cloud. The first author thanks the Wharton Risk Management and Decision Processes Center for its support through the Russell Ackoff Doctoral Student Fellowships, as well as Eva Ascarza, Michael Braun, Oded Netzer, and Kenneth Shirley for their useful comments. The authors all thank Yao Zhang, who contributed to a related working paper, "Children of the HMM: Tracking Longitudinal Behavior at Hulu.com," which was presented at the 2010 Marketing Science Conference.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Hierarchical Bayes Sampler Details</head><p>We provide the computational details for the models that we ran. We provide the details of the sampler for the general HMM with S states. It can be constrained for the two-state HMM and each of its nested models, as described in Â§2.</p><p>The MCMC procedure generates draws from the joint posterior:</p><p>with constants (I individuals and T time periods), individual-level parameters (p i and i ), and populationlevel parameters (a p b p and ). The procedure obtains these draws by alternating between the following conditional distributions:</p><p>For each entry in the "data set of data sets" described in Â§3, we estimate all four models from the 2Ã2 framework. We use 64 data sets, 4 models per data set, 2 chains per model, resulting in 512 independent MCMC chains. We run each chain for at least 50,000 iterations depending on the convergence criterion for that given model and chain. This requires more than 1,000 days of computing time on a single core. Instead of using only one core, we distributed the computational task to take advantage of the parallel structure of the task. On Amazon's Elastic Computing Cloud, we used 64 nodes with eight cores per node for 48 hours (24,576 "core-hours"). We ran each MCMC chain on each of the 512 cores, so we finished running all the models in two days. This was funded by a grant from Amazon Web Services.</p><p>Each model is estimated using a version of the MCMC sampler for HMM with certain components shut off or not. The code is available from the authors upon request. For each chain and for each pair of chains for each model, we perform a set of within-chain diagnostics for convergence and computation of effective sample size, as well as across-chain diagnostics for post-convergence mixing-all recommended now as standard practice <ref type="bibr" target="#b11">(Gelman and Rubin 1992</ref><ref type="bibr" target="#b13">, Gelman et al. 2004</ref><ref type="bibr" target="#b14">, Geweke 1992</ref><ref type="bibr" target="#b28">, Plummer et al. 2006</ref><ref type="bibr" target="#b29">, Raftery and Lewis 1992</ref>.</p><p>The draws of model parameters, , and latent states, Z * , have been obtained using a data-augmented Gibbs sampler <ref type="bibr" target="#b37">(Tanner and Wong 1987)</ref> with an embedded Metropolis-Hastings step. Below we describe how each subset of parameters was drawn from its corresponding conditional distribution in the MCMC procedures.</p><p>Step 1. Generate Z i = Z i1 Z iT . The customer's latent-state sequence is drawn via the forward-backward algorithm. The latent states are sampled starting at t = T moving backward based on the probabilities defined recursively starting at t = 1 and moving forward using dynamic programming. For the case of S = 2, given the observed outcome at t and the probability of being in either state at t â 1, each element i t k is a sum of the two elements from t â 1 weighted by the probability of the corresponding transition probabilities. Then the probability of drawing state k is</p><p>Once the sequences from 1 T are drawn for all individuals, then conditioning on those sampled latent states as if they were data (i.e., data augmentation) simplifies the subsequent conditional distributions. Hence, we define a vector, N i , where each element counts the number of times an individual spent a day in each latent state, N ij = T t=1 1 Z i tâ1 = j . We also define a matrix, N it , where each entry j, k counts the number of transitions made between each pair of latent states,</p><p>p Si . The customer's purchase probability vector is sampled directly from a beta distribution. The use of independent beta priors for each probability yields a beta posterior distribution (since the likelihoods have no covariates). For state k, the prior and posterior are</p><p>where a pk and b pk are the shape parameters of the beta distribution and pk and pk are the mean and polarization index, as defined in Â§2. To ensure p 1i â¥ p 2i , we use rejection sampling of the whole vector. Step 3. Generate i . The customer's transition probability matrix must have its rows sum to 1, so it is a multinomial vector. Using independent Dirichlet priors on each row yields Dirichlet posteriors. For the probability of moving from j to any other state 1 S, the prior and posterior are</p><p>where the 1:S indexes a vector of parameters and where r j is the vector of Dirichlet shape parameters, which can be summarized by mean probability vector j = j / S k=1 jk and polarization index j = 1/ 1 + S k=1 jk .</p><p>Step 4. Generate . The initial latent-state membership probability vector depends on the latent states across all individuals at t = 1. Defining N 1 k = I i=1 1 Z k1 = k , the uniform hyperprior and the posterior are</p><p>Step 5. Generate a p b p . There is a highly uninformative hyperprior for each shape parameter of each beta distribution characterizing the heterogeneity of state-specific purchase propensities. For state k, the hyperprior and posterior are a pk b pk â a pk + b pk</p><p>where L beta is the beta density function, and the prior distribution proportional to a + b â5/2 is recommended by <ref type="bibr" target="#b13">Gelman et al. (2004)</ref>. That prior is uniform on the beta distribution a/ a + b and considered weakly informative on the polarization index 1 + a + b â1 . Since the posterior has no closed-form expression, we use a Metropolis-Hastings step with a log Normal proposal density. Its tuning parameter, or variance, is set to 0 05 to obtain an appropriate acceptance probability.</p><p>Step 6. Generate . The Dirichlet distribution shape parameters j = j1 jS are generated by a generalization of the procedure used to generate the shape parameters of the beta distributions. For state j (i.e., row j of the transition probability matrix), the hyperprior and posterior are</p><p>where L Dirichlet is the Dirichlet density function. Again, the prior distribution proportional to</p><p>is a generalization for the Dirichlet shape parameters of the prior used for the beta shape parameters <ref type="bibr" target="#b8">(Everson and Bradlow 2002)</ref>. Since the posterior has no closed-form expression, we use a Metropolis-Hastings step with a log Normal proposal density. Its tuning parameter, or variance, is set to 0 05 to obtain an appropriate acceptance probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. General Recipe: Developing a Decision Tree for Model Selection Using Database Characteristics</head><p>Although we perform our analysis for a specific data/ modeling context, the same basic recipe developed in this paper can be applied to many other settings. We formalize this recipe as a general method for model evaluation and selection involving the three basic ingredients: the set of candidate models, the database characteristics, and the performance criterion. This enables the analyst to answer the questions "Which model should I use for this this data set?" and "Given a data set, how well will a given model perform?"</p><p>Step 1. Selecting models. Our procedure supposes that the analyst has a consideration set of models 1 M. The models can all be run on data sets with the same structure. We also suppose that model-based simulation can be done via Monte Carlo, or Markov chain Monte Carlo, if needed. Our procedure assumes that an MCMC sampler has been run to obtain draws g = 1 G from each model's the joint posterior distribution, m Y obs .</p><p>Step 2. Choosing database characteristics. We characterize the database with a set of summary statistics. These should be (1) easy-to-compute characteristics, (2) managerially relevant, and (3) largely comprehensive and mutually exclusive. Formally stated, we denote these the data setlevel summary statistics as a covariate vector, Y obs k , for data set k. These are to be computed before running any models on the kth data set, which itself is denoted by Y obs k . These are the independent variables of interest in the eventual classification.</p><p>Step 3. Determining performance criterion. Assessing model performance for model selection is an important step that should be driven by the business goal. We use an empirical validation approach via posterior predictive distributions. We generate data, Y * g m , from the model-based predictive distribution, Y * m Y obs , where g indexes replications 1 G. For the performance measure feature s, we summarize the generated data by T s Y * g m</p><p>. We quantify performance as model errors: the degree to which the model-based posterior predictive distribution of feature s is outlying with respect to the feature's observed value, T s Y obs k . Let D denote a loss function along a single dimensionthat is, the distance between the draws from the posterior predictive distribution and the single observed value of feature of a data set. This distance, d mks , summarizes model m's performance on data set k in terms of feature s, utilizing all replicates g = 1 G:</p><p>The choice of the function D should depend on the desired feature.</p><p>Regardless of which performance metric and error measure is chosen, a single metric is obtained for each posterior replicate. For each replicate, we select the model with the lowest error and consider it the winning model, which is the nominal categorical outcome variable to be classified.</p><p>Step 4. Classifying data sets by relating model performance to observed database characteristics. Putting those three ingredients together, we create the decision tree to infer the relationship between which model is best (outcome) and database characteristics (predictors). We formalize the classification as its own predictive tool. The independent variables of interest are the data set-level summary statistics, Y obs k , computed before running any models on . Given the performance of all M models across all K data sets for feature s, we explain variations in the model performance (i.e., which model wins) as a function of the observed summaries of that data set. Stated formally, we capture this relationship as follows:</p><p>where "Tree" denotes the classification tree predicting the winning modelm Winner g k for each of the data sets k = 1 K and replicates g = 1 G. The results will show which data set-level summaries are associated with differences in performance across the models for the feature of interest. The exact same setup used for CART methods can be used for implementing random forests. The same basic relationships are uncovered, but different methods are used.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matrix-exponential distributions: Calculus and interpretations via flows</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bladt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Neuts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Models</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="124" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical modeling: The two cultures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="231" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Marginal likelihood from the Gibbs output</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">432</biblScope>
			<biblScope unit="page" from="1313" to="1321" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian CART model search</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">443</biblScope>
			<biblScope unit="page" from="935" to="948" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Market-Share Analysis: Evaluating Competitive Marketing Effectiveness</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakanishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling CLV: A test of competing models in the insurance industry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Donkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Verhoef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Jong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Marketing Econom</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian inference for the betabinomial distribution via polynomial expansions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Everson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graphic. Statist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="202" to="207" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A dynamic changepoint model for new product sales forecasting</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bgs</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="65" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Customer-base analysis in a discrete-time noncontractual setting</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bgs</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1086" to="1108" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inferences from iterative simulation using multiple sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="472" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Avoiding model selection in Bayesian social research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociol. Methodol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="165" to="173" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating the accuracy of sampling-based approaches to calculating posterior moments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geweke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Statistics</title>
		<editor>Bernardo JM, Berger JO, Dawid AP, Smith AFM</editor>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="169" to="193" />
			<date type="published" when="1992" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Market response models and marketing practice</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hanssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Psh</forename><surname>Leeflang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stochastic Models Bus. Indust</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4/5</biblScope>
			<biblScope unit="page" from="423" to="434" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling social interactions: Identification, empirical methods and policy implications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manchanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bothner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Godes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hosanagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="304" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A probabilistic choice model for market segmentation and elasticity structuring</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Kamakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="379" to="390" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simulation pseudo-bias correction to the harmonic mean estimator of integrated likelihoods</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graphic. Statist</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="941" to="960" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification and regression by random forest</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R News</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="22" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Global and local covert visual attention: Evidence from a Bayesian hidden Markov model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Liechty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="519" to="541" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Counting your customers from an &quot;always a share</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buschken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">perspective. Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="257" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling online browsing and path analysis using clickstream data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Liechty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="595" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic allocation of pharmaceutical detailing and sampling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jedidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="909" to="924" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the interpretation of discriminant analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="163" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A hidden Markov model of customer relationship dynamics</title>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lattin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="204" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximate Bayesian inference with the weighted likelihood bootstrap</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="48" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Characterization of phase-type distributions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>O'cinneide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Statist. Stochastic Models</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CODA: Convergence diagnosis and output analysis for</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MCMC. R News</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="11" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How many iterations in the Gibbs sampler?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Statistics</title>
		<editor>Bernardo JM, Berger JO, Dawid AP, Smith AFM</editor>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="763" to="773" />
			<date type="published" when="1992" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A model of TV show loyalty</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sabavala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Advertising Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="35" to="43" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Counting your customers: Who are they and what will they do next?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Schmittlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Colombo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic changepoints revisited: An evolving process model of new product sales</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schweidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Res. Marketing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="124" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Portfolio dynamics for customers of a multi-service provider</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schweidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="486" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding subscriber retention within and across cohorts using limited information</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schweidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="94" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian measures of model complexity and fit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avd</forename><surname>Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="639" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dealing with label-switching in mixture models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="795" to="809" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Part 4</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The calculation of posterior distributions by data augmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">398</biblScope>
			<biblScope unit="page" from="528" to="540" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">SCAN * PRO: The estimation, validation and use of promotional effects based on scanner data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Addona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Hawkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<pubPlace>Ithaca, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note>Internal paper</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">From business intelligence to competitive intelligence: Inferring competitive measures using augmented site-centric data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Padmanabhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Systems Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="698" to="720" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
