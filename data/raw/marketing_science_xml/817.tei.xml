<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mean-Centering Does Not Alleviate Collinearity Problems in Moderated Multiple Regression Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-09-21">September 21, 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Raj</forename><surname>Echambadi</surname></persName>
							<email>rechambadi@bus.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Marketing</orgName>
								<orgName type="department" key="dep2">College of Business Administration</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postBox>P.O. Box 161400</postBox>
									<postCode>32816-1400</postCode>
									<settlement>Orlando</settlement>
									<region>Florida</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">D</forename><surname>Hess</surname></persName>
							<email>jhess@uh.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Marketing and Entrepreneurship, C.T. Bauer College of Business</orgName>
								<orgName type="institution">University of Houston</orgName>
								<address>
									<addrLine>375H Melcher Hall</addrLine>
									<postCode>77204</postCode>
									<settlement>Houston</settlement>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mean-Centering Does Not Alleviate Collinearity Problems in Moderated Multiple Regression Models</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 e 1526-548X 07 2603 0438</idno>
						<imprint>
							<date type="published" when="2005-09-21">September 21, 2005</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.1060.0263</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>moderated regression</term>
					<term>mean-centering</term>
					<term>collinearity History: This paper was received</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>T he cross-product term in moderated regression may be collinear with its constituent parts, making it difficult to detect main, simple, and interaction effects. The literature shows that mean-centering can reduce the covariance between the linear and the interaction terms, thereby suggesting that it reduces collinearity. We analytically prove that mean-centering neither changes the computational precision of parameters, the sampling accuracy of main effects, simple effects, interaction effects, nor the R 2 . We also show that the determinants of the cross product matrix X X are identical for uncentered and mean-centered data, so the collinearity problem in the moderated regression is unchanged by mean-centering. Many empirical marketing researchers commonly mean-center their moderated regression data hoping that this will improve the precision of estimates from ill conditioned, collinear data, but unfortunately, this hope is futile. Therefore, researchers using moderated regression models should not mean-center in a specious attempt to mitigate collinearity between the linear and the interaction terms. Of course, researchers may wish to mean-center for interpretive purposes and other reasons.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multiple regression models with interactions, also known as moderated models, are widely used in marketing and have been the subject of much scholarly discussion <ref type="bibr">(Sharma et al. 1981, Irwin and</ref><ref type="bibr" target="#b8">McClelland 2001)</ref>. The interaction (or moderator) effect in a moderated regression model is estimated by including a cross-product term as an additional exogenous variable as in</p><formula xml:id="formula_0">y = 1 x 1 + 2 x 2 + x 1 3 x 2 + 0 + c x c + (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where i and x i are k i × 1 column vectors for i = 1 2 3 is a k 1 ×k 2 matrix of coefficients that determine the interaction terms, and x c plays the role of other covariates that are not part of the moderated element. The moderator term, x 1 3 x 2 is likely to covary to some degree with the variable x 1 (and with the variable x 2 . This relationship has been interpreted as a form of multicollinearity, and collinearity makes it difficult to distinguish the separate effects of the linear and interaction terms involving x 1 and x 2 . In response to this problem, various researchers including <ref type="bibr" target="#b0">Aiken and West (1991)</ref>, <ref type="bibr" target="#b5">Cronbach (1987)</ref>, and <ref type="bibr" target="#b9">Jaccard et al. (1990)</ref> recommend mean centering the variables x 1 and x 2 as an approach to alleviating collinearity related concerns. Mean centering (1) gives the following:</p><formula xml:id="formula_2">y = 1 x 1 −x 1 + 2 x 2 −x 2 + x 1 −x 1 3 x 2 −x 2 + 0 + c x c + (2)</formula><p>In comparison to Equation (1), the linear term x 1 −x 1 in Equation (2) will typically have smaller covariance with the interaction term because the multiplier of x 1 −x 1 in the interaction term, 3 (x 2 −x 2 , is zero on average. This practice of mean-centering has become routine in the social sciences. It is common to see statements from marketing researchers such as, "we meancentered all independent variables that constituted an interaction term to mitigate the potential threat of multicollinearity" (cf. <ref type="bibr" target="#b12">Kopalle and Lehmann 2006)</ref>. Can such a simple shift in the location of the origin really help us see the pattern between variables? We use a hypothetical example to suggest an answer to this question. Let the true model for this simulated data be: y = x 1 + 1/2 x 1 x 2 + where ∼N(0, 0.1). In Figure <ref type="figure">1</ref>(a), we graph the relationship between y and x 2 -x 2</p><p>x 1 -x 1 uncentered (x 1 , x 2 ). In Figure <ref type="figure">1</ref>(b), we see the relationship between y and mean-centered (x 1 , x 2 ). Obviously, the same pattern of data is seen in both the graphs, since shifting the origin of the exogenous variables x 1 and x 2 does not change the relative position of any of the data points. Intuitive geometric sense tells us that looking for statistical patterns in the centered data will not be easier or harder than in the uncentered data.</p><p>In this paper, we will demonstrate analytically that the geometric intuition is correct: mean-centering in moderated regression does not help in alleviating collinearity. Although <ref type="bibr">Belsley (1984)</ref> has shown that mean-centering does not help in additive models, to our knowledge, this is the first time anyone has analytically demonstrated that mean-centering does not alleviate collinearity problems in multiplicative models. Specifically, we demonstrate that (1) in contrast to <ref type="bibr" target="#b0">Aiken and West's (1991)</ref> suggestion, meancentering does not improve the accuracy of numerical computation of statistical parameters, (2) it does not change the sampling accuracy of main effects, simple effects, and/or interaction effects (point estimates and standard errors are identical with or without meancentering), and (3) it does not change overall measures of fit such as R 2 and adjusted-R 2 . It does not hurt, but it does not help, not one iota.</p><p>The rest of the paper is organized as follows. We prove analytically that mean centering neither improves computational accuracy nor changes the ability to detect relationships between variables in moderated regression. Next, using data from a study of brand extensions, we illustrate the equivalency of the uncentered and the mean-centered models and demonstrate how one set of coefficients and their standard errors can be recovered from the other. Finally, we discuss the reasons why so many marketing scholars mean-center their variables and the conditions under which mean-centering may be appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mean Centering Neither Helps</head><p>Nor Hurts</p><p>Collinearity can be viewed as a particular form of near linear dependencies among a set of observed variates <ref type="bibr" target="#b1">(Belsley 1991)</ref>. Increased levels of collinearity in a data set may cause (1) computational problems in the estimation, (2) sampling stability problems wherein insignificant shifts in data may produce significant relative shifts in the estimates, and (3) statistical problems that may exhibit themselves in terms of large variances of the estimates <ref type="bibr">(Cohen and Cohen 1983, p. 115)</ref>. Considering the first problem, <ref type="bibr">Aiken and West (1991, p. 182</ref>) recommend mean-centering because it may help avoid computational problems by reducing roundoff errors in inverting the product matrix. <ref type="bibr">1 However, McCullough (1999)</ref> has demonstrated that even for complex linear regression problems, roundoff errors are extremely rare in modern double-precision, singular value decomposition statistical computing. Addressing the other two problems, <ref type="bibr" target="#b0">Aiken and West (1991)</ref> also imply that mean-centering reduces the covariance between the linear and interaction terms, thereby increasing the determinant of X X. This viewpoint that collinearity can be eliminated by centering the variables, thereby reducing the correlations between the simple effects and their multiplicative interaction terms is echoed by <ref type="bibr">Irwin and McClelland (2001, p. 109</ref>). We will show that this is incorrect.</p><p>Straight-forward algebraic manipulation of Equation (1) shows that it is equivalent to  <ref type="formula">3</ref>), there is a linear relationship between the and parameter vectors; for example, 1 = 1 + 3x2 . Since 3 is a matrix, we need to vectorize it to establish the linear relationship. The expression vec A is the vectorization operator that stacks columns on top of one another to form a column vector, and the Kronecker product is denoted A ⊗ B. A fundamental identity in vectorization is vec ABC = C ⊗ A vec B . Apply this to an interaction term to get x 1 3 x 2 = vec x 1 3 x 2 = x 2 ⊗ x 1 vec 3 = vec 3 x 2 ⊗ x 1 , and apply it to 3x2 , to get vec 3x2 = vec I 3x2 = x 2 ⊗ I vec 3 .</p><formula xml:id="formula_3">y = 1 + 3x2 x 1 −x 1 + 2 + 3x1 x 2 −x 2 + x 1 −x 1 3 x 2 −x 2 + 0 + 1x1 + 2x2 +x 1 3x2 + c x c +<label>(</label></formula><p>As a result, the relationship between from the mean-centered model and from the uncentered model is</p><formula xml:id="formula_4">=           1 2 vec 3 0 c           =           I k 1 0x 2 ⊗ I k 1 0 0 0 I k 2x 1 ⊗ I k 2 0 0 0 0 I k 1 k 2 0 0 x 1x 2x 2 ⊗x 1 1 0 0 0 0 0 I                     1 2 vec 3 0 c           = W (4)</formula><p>where I k i is an identity matrix of dimensions k i × k i . We use I for identity matrices when the dimensions are implied by the context. Reversing roles, = W −1 , where</p><formula xml:id="formula_5">W −1 =           I 0 −x 2 ⊗ I 0 0 0 I −x 1 ⊗ I 0 0 0 0 I 0 0 −x 1 −x 2x 2 ⊗x 1 1 0 0 0 0 0 I           (5) Note, that because in general A ⊗ B M ⊗ N = AM ⊗ BN , it must be that −x 1 x 2 ⊗ I = − 1 ⊗x 1 x 2 ⊗ I = − 1 x 2 ⊗x 1 I = −x 2 ⊗x 1 .</formula><p>With this observation it is easy to see that W W −1 = I, so (5) is the inverse of W . Suppose a data set consists of an n × 5 matrix of explanatory variable values</p><formula xml:id="formula_6">X ≡ X 1 X 2 X 1 * X 2 1 X c ]</formula><p>, where X j is a column n-vector of observations of the jth variable, X 1 * X 2 is an n-vector whose typical component is X i1 X i2 , and 1 is a vector of ones. The empirical version of ( <ref type="formula" target="#formula_0">1</ref>) is therefore</p><formula xml:id="formula_7">Y = X + . This is equivalent to Y = XW −1 W + = XW −1 + . It is easily seen that XW −1 ≡ X 1 −x 1 1 X 2 −x 2 1 X 1 −x 1 1 * X 2 −x 2 1 1 X c , the mean-centered ver- sion of the data.</formula><p>An immediate conclusion is that ordinary least squares (OLS) estimates of ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula">2</ref>) produce identical estimated residuals e, and because the residuals are identical, the R 2 for both formulations are identical. This is consistent with <ref type="bibr" target="#b13">Kromrey and Foster-Johnson's (1998)</ref> findings based on Monte Carlo simulations that the two models are equivalent. OLS estimators a = X X −1 X Y and b = XW −1 XW −1 −1 XW −1 Y are related to each other by b = W a. Finally, the variance-covariance of the uncentered and centered OLS estimators are S a = s 2 X X −1 and S b = s 2 W −1 X XW −1 −1 = s 2 W X X −1 W , where the estimator of 2 is s 2 = e e/ n − 5 .</p><p>Is the claim, "mean-centering increases the determinant of the X X matrix, thereby reducing collinearity" true? In the uncentered data, we must invert X X and in the centered data we must invert W −1 X XW −1 . However, mean-centering not only reduces the offdiagonal elements (such as X 1 X 1 * X 2 , but it also reduces the elements on the main diagonal (such as X 1 * X 2 X 1 * X 2 , and it has no effect whatsoever on the determinant. Therefore, while <ref type="bibr" target="#b0">Aiken and West (1991)</ref> and <ref type="bibr" target="#b8">Irwin and McClelland (2001)</ref> show that meancentering normal random variables reduces the magnitude of the correlations between the simple effects and their interaction terms, the determinants are identical for both the centered and uncentered cases. <ref type="bibr">2</ref> In other words, there is no new information added to the estimation by mean-centering <ref type="bibr">(Kam and Franzese 2005, p. 58</ref>) and hence the collinearity is not reduced or eliminated in mean-centered models.</p><p>Theorem 1. The determinant of the uncentered data product matrix X'X equals the determinant of the centered data product matrix W −1 X XW −1 .</p><p>(Proofs of all theorems are relegated to the appendix.) Since the source of computational problems in inverting these matrices is a small determinant, the same computational problems exist for mean-centered data as for uncentered data.</p><p>Also, assuming that the random variable is normally distributed, the OLS a is normally distributed with a mean and variance-covariance matrix 2 X X −1 . Since b is a linear combination of these, 2 A point not often made is that the magnitude of the covariance between X 1 and X 1 X 2 can increase with mean-centering of nonsymmetric random variables. The magnitude of this covariance with uncentered data is</p><formula xml:id="formula_8">1 Cov X 1 X 2 + 2 Var X 1 + E X 1 − 1 2 • X 2 − 2 , while for mean-centered data its magnitude is E X 1 − 1 2 X 2 − 2 .</formula><p>For normal distributions, third order moments are zero, so by mean-centering the magnitude of the covariance goes from a positive value down to zero. However, for skewed distributions, it is very easy for the magnitude of the covariance to increase. For example, if E X 1 − 1 2 X 2 − 2 and Cov(X 1 X 2 are positive but 1 and 2 are negative, when the variables are mean-centered, the magnitude of the covariance of X 1 and X 1 X 2 could increase from near zero to a positive value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>441</head><p>W a, it must be normal with mean W and an estimated variance-covariance matrix WS a W . As <ref type="bibr" target="#b0">Aiken and West (1991)</ref> have shown, estimation of the interaction term is identical for uncentered and centered data; we repeat this for completeness sake.</p><p>Theorem 2. The OLS estimates of the interaction terms 3 and 3 , a 3 for (1) and b 3 for (2), have identical point estimates and standard errors.</p><p>This result generalizes to all other effects as seen in the next three theorems.</p><p>Theorem 3. The main effect of x 1 ( 1 from Equation (2) or 1 + 3x2 from Equation ( <ref type="formula">3</ref>)) as measured by the OLS estimate b 1 or by the OLS estimate a 1 + a 3x2 have identical point estimates and standard errors.</p><p>Note that the coefficient 1 in Equation ( <ref type="formula" target="#formula_0">1</ref>) is not the main effect of x 1 ; the "main effect" means the "average effect" of x 1 across all values of x 2 , namely 1 + 3x2 . Instead, the coefficient 1 is the simple effect of x 1 when x 2 = 0. 3 Algebraic rearrangement of (4) states that this simple effect can also be measured from the main effects found in the mean-centered Equation ( <ref type="formula">2</ref></p><formula xml:id="formula_9">) since a 1 = b 1 − b 3x2 .</formula><p>Theorem 4. The simple effect of x 1 when x 2 = 0 is either 1 in Equation ( <ref type="formula" target="#formula_0">1</ref>) or 1 − 3x2 from Equation ( <ref type="formula">2</ref>) and the OLS estimates of each of these (a 1 for (1) and b 1 − b 3x2 for (2)) have identical point estimates and standard errors.</p><p>Theorem 5. The simple effect of x 1 when x 2 = 1 is either 1 + 3 1 in Equation ( <ref type="formula" target="#formula_0">1</ref>) or 1 − 3 1 −x 2 from Equation (2) and the OLS estimates of each of these (a 1 + a 3 1 for (1) and b 1 − b 3 1 −x 2 for (2)) have identical point estimates and standard errors.</p><p>In summary, while some researchers may believe that mean-centering variables in moderator regression will reduce collinearity between the interaction term and linear terms and will miraculously improve their computational or statistical conclusions, this is not so. We have demonstrated that mean-centering does not improve computational accuracy or change the ability to detect relationships between variables in moderated regression. Therefore, it is evident that if collinearity plagues uncentered data, it will also affect the estimates and standard errors of the meancentered data, as well. The cure for collinearity with mean-centering is illusory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">An Illustration from the Brand Extension Literature</head><p>The decision to extend a brand, wherein a current brand name is used to enter a completely different product class, is a strategically critical decision for many firms. As a result, marketing scholars have attempted to explain consumer evaluations of brand extensions to glean insights on why some brand extensions succeed and others fail (see <ref type="bibr" target="#b2">Bottomley and Holden (2001)</ref> for a comprehensive review of the brand extension literature). It is believed that brand extension evaluations are based primarily on the interaction of the "perceived quality" of the parent brand with the degree of "perceived fit" between the parent and the extension product categories <ref type="bibr" target="#b6">(Echambadi et al. 2006</ref>).</p><p>Although the literature has considered three separate measures of perceived fit, we use one such fit measure: perceived substitutability, defined as the extent to which consumers view two product classes as substitutes, for expositional simplicity. Specifically, we test the contingent role of substitutability on the relationship between parent brand quality and consumer evaluations of brand extensions. Based on the findings from the prior literature, we expect that the linear effects of both parent brand quality and substitutability would increase brand extension evaluations. We further expect that, at higher levels of substitutability, the positive relationship between quality and extension evaluations would be further strengthened.</p><p>We use this example from <ref type="bibr" target="#b2">Bottomley and Holden's (2001)</ref> study to illustrate the equivalency of the uncentered and mean-centered regressions. <ref type="bibr">4</ref> The estimated equation is</p><formula xml:id="formula_10">Evaluations = 1 Substitute+ 2 Quality + 3 Substitute×Quality+ c X c + (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>where Evaluations is operationalized by a composite two-item measure of perceived overall quality of the brand extension and the likelihood of trying the extension; Substitute refers to the perceived substitutability; Quality refers to the perceived quality of the parent brand; finally, X c is a vector of control variables. All variables are measured on a 7-point scale.</p><p>Similar to <ref type="bibr" target="#b2">Bottomley and Holden (2001)</ref>, we use OLS to estimate the model. Table <ref type="table" target="#tab_1">1</ref> shows the results of the uncentered and mean-centered regression models from a sample of n = 10 203 observations. The table has been graphically annotated to demonstrate how one could compute the mean-centered (main-effect) estimates and standard errors for Substitute using only the statistics from the uncentered (simple effect) regression. As </p><formula xml:id="formula_12">VAR-COV a VAR-COV b a 0 000656 - - b 1 0 000063 - a 2 0 0003 0 00023 - b 2 −0 00001 0 000085 - a 3 −0 00011 −0 00005 0 000021 b 3 −0 000003 0 000003 0 00002</formula><p>Variable mean x i 2 898 5 2077</p><formula xml:id="formula_13">R 2 0 30 R 2 0 30 VAR a 1 +x 2 a 3 = VAR a 1 + 2x 2 COV a 1 a 3 +x 2 2 VAR a 3 ≡ 0 000656 + 2 × 5.208 × ( − 0 00011) + 5 2077 2 × 0 000021 = 0 000062 = 0 008 2 - Notes. Brand Evaluation = 1 Substitute = 2 Quality + 3 Substitute × Quality + Controls.</formula><p>has been proved above, this is completely general. Conversely, estimates and standard errors for the uncentered model could be computed using only the statistics from the mean-centered regression. Both models are equally precise.</p><p>As seen from Table <ref type="table" target="#tab_1">1</ref>, both the mean-centered and the uncentered models provided an identical fit to the data, and yielded the same model R 2 . As noted by <ref type="bibr" target="#b0">Aiken and West (1991)</ref> and shown above, the coefficient (0.034) and the standard error (0.005) of the interaction (highest order) term, and hence the t statistics of this term, are identical with or without mean-centering.</p><p>An examination of the linear terms of meancentered and uncentered models from Table <ref type="table" target="#tab_1">1</ref> reveals an apparent conflicting story. Results from the uncentered model show that the coefficient of Substitute is significantly negative (a 1 = −0 047), implying that higher levels of perceived substitutability of a brand extension leads to lowered brand extension evaluations. This runs counter to the a priori expectation of a positive relationship between substitutability and brand extension evaluations. The researcher might note the large variance inflation factors VIF in Table <ref type="table" target="#tab_1">1</ref> and the high correlation (0.91) between Substitute and the Substitute × Quality interaction term in the left portion of Table <ref type="table" target="#tab_2">2</ref> and conclude that multicollinearity is the cause of this peculiar finding. If the variables were mean-centered, the correlation between Substitute and the interaction term is much lower (0.06), suggesting reduced collinearity.</p><p>When the mean-centered variables are used, the estimates confirm the prior expectation that substitutability increases brand extension evaluations (b 1 = 0 132). The researcher might believe that this improvement is due to alleviating the ill effects of collinearity, since the correlation between the Substitute and the Substitute × Quality entry is reduced from 0.91 to 0.06 by mean-centering. This explanation, although intuitively appealing, is simply false.</p><p>The effects for substitutability measured in these two models are vastly different (simple effects from the uncentered models vis-à-vis main effects from the mean-centered models) and hence, direct comparisons of the corresponding coefficients are inappropriate. The infamous "comparison of apples and oranges" metaphor is appropriate. In the uncentered regression model, the coefficients represent simple effects of the exogenous variables; i.e., the effects of each variable when the other variables are at zero. The coefficient for Substitute in this model should therefore be interpreted as the change in brand extension evaluations due to substitutability in the complete absence of parent brand quality. This makes a lot of sense. A highly substitutable extension brand with zero parent brand quality is bound to elicit negative evaluations.</p><p>However, when data are mean-centered, the coefficients represent main effects of these variables; i.e., the effects of each variable when the other variables are at their mean values. The coefficient for Substitute in the mean-centered model should be interpreted as the change in brand extension evaluations due to substitutability for parent brands with average quality. The change in estimates and reduction in standard errors is based entirely on the fact that the coefficients have different substantive meanings. <ref type="bibr">5</ref> As proved in Theorems 2-5 and demonstrated in Table <ref type="table" target="#tab_1">1</ref>, the same main effect estimates and standard errors could be computed from estimates based upon the uncentered data. In summary, the uncentered and mean-centered models are statistically equivalent. Using Equation ( <ref type="formula" target="#formula_0">1</ref>) and b = W a, we can recover an equally accurate measure of the main effect in the mean-centered model from the uncentered data. Similarly, using Equation ( <ref type="formula">2</ref>) and a = W −1 b, we can recover an equally accurate measure of the simple effect in the uncentered model from the centered data. As demonstrated in Table <ref type="table" target="#tab_1">1</ref>, the standard errors of coefficients in an uncentered model can also be recovered from the mean-centered data and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comments</head><p>Why do so many researchers mean-center their moderated variables? Clearly, there is a fear that by including a term x 1 x 2 in the regressors, they will create collinearity with the main regressor, such as x 1 , so that it will become difficult to distinguish the separate effects of x 1 and x 1 x 2 on y. If we make x 1 's multiplier in the interaction term, x 2 , closer to zero on average, then we reduce the covariance and correlation, and one simple way to do this is to replace the multiplier x 2 by x 2 −x 2 . By subtracting the mean, the typical value of the multiplier is zero and hence the covariance between the regressor and the interaction terms is typically smaller. This appears to reduce the "potential threat of multicollinearity" and hopefully improves the ability to distinguish the effect of changes in x 1 from changes in x 1 x 2 . This logic seems plausible, but it is incorrect. The complete analysis of mean-centering indicates that mean-centering often reduces covariances between the linear and the interaction terms; however, it does not add any new information to the estimation. As shown in our analytical results, compared with uncentered models, mean-centering does not change the computational precision of parameters, the sampling accuracy of the main effects, simple effects, interaction effects, or the overall model R 2 . Therefore, it is clear that mean-centering does not alleviate collinearity problems in moderated regression models.</p><p>In the light of these results, it is evident that the decision to mean-center the moderated variable data should be made independently from the specious rationale of trying to alleviate multicollinearity. Could we mean-center to obtain a better interpretation? One might argue that the main effects are more meaningful because they characterize the overall relationships better, so the data should be mean-centered. However, one might also argue that the simple effects are preferable because they provide a more fine-grained understanding of the patterns, so the data should be uncentered. Because both models are mathematically equivalent and the results for the uncentered case can be obtained from the mean-centered model and vice versa, mean-centering does not necessarily provide a better interpretation of the data. It just provides a different interpretation. Of course, recovery of the proper standard errors does require computing the diagonal elements of matrices such as WS a W in the uncentered context, or W −1 S b W −1 in the meancentered case, and this may be accomplished easier by reversing the data-centering decision with a few mouse clicks.</p><p>Mean-centering does not hurt, so there is no need to re-evaluate the conclusions of the many published papers that have used mean-centering so long as the researchers are clear about the proper interpretation of the linear terms. The correct interpretation of mean-centered coefficients in moderated regression models has often been overlooked in many marketing papers. For example, a content analysis of papers that employed mean-centering approaches shows that only four of the 70 published papers published in nine major marketing journals from 1994 to 2004 explicitly mentioned the different interpretation for the meancentered coefficients.</p><p>How do we diagnose collinearity? <ref type="bibr" target="#b7">Grewal et al. (2004)</ref> caution researchers to be careful about relying on available diagnostics for what constitutes different levels of collinearity. Our content analysis of select marketing journals reveals that bivariate correlations and VIFs are currently the most commonly used tools to diagnose multicollinearity. One drawback of the commonly used collinearity diagnostics is that they evaluate regressors one by one and hence may not be appropriate diagnostics of collinear situations by themselves <ref type="bibr" target="#b3">(Coenders and Saez 2000)</ref>. High VIFs are sufficient but not necessary to collinearity <ref type="bibr" target="#b1">(Belsley 1991)</ref>. It is, therefore, imperative that researchers use multiple diagnostic tools to diagnose potential collinearity problems in moderated regression. This reliance on multiple tools may reduce false alarms raised by empirical researchers about the presence of collinearity. If collinearity problems are indeed suspected, <ref type="bibr" target="#b6">Echambadi et al. (2006)</ref> suggest randomly estimating subsets of the data to test the plausibility and Marketing Science 26(3), pp. 438-445, © 2007 INFORMS stability of coefficients. Unstable coefficients across these random subsets of data may confirm the presence of collinearity problems.</p><p>A casual check of our journals also reveals that researchers use collinearity diagnostics on meancentered data to confirm the absence of collinearity problems. However, because mean-centering typically masks the role of the constant term in any underlying dependencies, these diagnostic measures of collinearity, such as the VIF and the correlation matrix, produce meaningless diagnostic information <ref type="bibr">(Belsley 1984, p. 73)</ref>. Partial and semi-partial correlations advocated by <ref type="bibr" target="#b4">Cohen and Cohen (1983)</ref> also suffer from the same failing when applied on meancentered data. Therefore, as <ref type="bibr">Belsley (1984)</ref> suggests, researchers must always use uncentered data for assessing collinearity.</p><p>Since mean-centering does not mitigate collinearity in moderated regression, one might ask, "What else can be done in the face of collinearity?" One alternative is to use the residual-centering method proposed by <ref type="bibr" target="#b14">Lance (1988)</ref>, but this is a distinctly bad idea. <ref type="bibr" target="#b6">Echambadi et al. (2006)</ref> show that residualcentering makes the linear effects biased and inconsistent, which is undesirable. <ref type="bibr">Morris et al.'s (1986)</ref> principal component regression procedure has also been deemed unacceptable <ref type="bibr" target="#b5">(Cronbach 1987)</ref>.</p><p>Because collinearity problems cannot be remedied after the data has been collected in most cases, we endorse Grewal et al.'s suggestion that researchers carefully design their studies prior to collecting their data. If feasible, one can address it by using a data collection scheme that isolates the interaction effect (for example, a factorial design). Likewise, if feasible, one can address the loss of power associated with multicollinearity by increasing the sample size; in this regard, <ref type="bibr" target="#b18">Woolridge (2001)</ref> notes that the effects of multicollinearity are indistinguishable from the effects of micronumerosity, or small sample sizes. More and better data always helps in reducing collinearity <ref type="bibr">(Judge et al. 1988, p. 874</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Whether we estimate the uncentered moderated regression Equation (1) or the mean-centered Equation (2), all the point estimates, standard errors and t statistics of the main effects, all simple effects, and interaction effects are identical and will be computed with the same accuracy by modern double-precision statistical packages. This is also true of the overall measures of accuracy such as R 2 and adjusted-R 2 .</p><formula xml:id="formula_14">W =           I k 1 0x 2 ⊗ I k 1 0 0 0 I k 2x 1 ⊗ I k 2 0 0 0 0 I k 1 k 2 0 0 x 1x 2x 2 ⊗x 1 1 0 0 0 0 0 I          </formula><p>then the fourth set of columns, then the third set of rows, it is clear that the determinant W = 1. Consequently, det(W −1 X XW −1 = det W −1 det X X det W −1 = det W −1 det X X det W −1 = det X X . Q.E.D. Note: the interaction terms are made into a k 1 k 2 -vector x 2 ⊗ x 1 . Thus in the variance covariance matrix S, entries like S 31 will have k 1 k 2 rows.</p><p>Proof of Theorem 2. From the third row of (4), vec b 3 = vec a 3 , so b 3 = a 3 . In this appendix, we will denote S a by S. Using matrix multiplication of (4), the third set of columns of SW is</p><formula xml:id="formula_15">          S 31 S 32 S 33 S 30 S 3c          </formula><p>The third set of rows of W is 0 0 I 0 0 , so the 3rd set of row × 3rd set of columns of WSW is S 33 . That is, Var vec b 3 = Var vec a 3 . Q.E.D.</p><p>Proof of Theorem 3. From the first row of (4), the point estimates are equal. The first column of SW is Proof of Theorem 5. A variant of the above.</p><formula xml:id="formula_16">          S 11 + S</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1 Graphical Representation of Uncentered and Mean-Centered Data in 3D Variable Space (a) Uncentered data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>S</head><label></label><figDesc>13 x 2 ⊗ I S 21 + S 23 x 2 ⊗ I S 31 + S 33 x 2 ⊗ I S 01 + S 03 x 2 ⊗ IS c1 + S c3 x 2 ⊗ I Notice that x 2 ⊗ I is k 1 k 2 × k 1 . The first row of W is I k 1 0x 2 ⊗ I k 1 0 0 , so the variance of b 1 (the 1st row × 1st column of WSW ) equals S 11 + 2S 13 x 2 ⊗ I + x 2 ⊗ I S 33 x 2 ⊗ I . The variance of a 1 + a 3x2 = a 1 + x 2 ⊗ I • vec a 3 is Var a 1 + 2 Cov a 1 vec a 3 x 2 ⊗ I + x 2 ⊗ I • Var vec a 3 x 2 ⊗ I = S 11 + 2S 13 x 2 ⊗ I + x 2 ⊗ I S 33 x 2 ⊗ I . That is, Var b 1 = Var a 1 + a 3x2 . Q.E.D.Proof of Theorem 4. The variance of b1 − b 3x2 = b 1 − x 2 ⊗ I vec b 3 equals Var b 1 − 2 Cov b 1 vec b 3 x 2 ⊗ I + x 2 ⊗ I Var vec b 3 x 2 ⊗ I).From the proof of Theorems 2 and 3 we know that var vec b 3 = S 33 and var b 1 = S 11 + 2S 13 x 2 ⊗ I + x 2 ⊗ I S 33 x 2 ⊗ I . The first column of SW is  11 + S 13 x 2 ⊗ I S 21 + S 23 x 2 ⊗ I S 31 + S 33 x 2 ⊗ I S 01 + S 03 x 2 ⊗ I S c1 + S c3 x 2 ⊗ I row of W is 0 0 I 0 0 , so the covariance of b 1 and vec(b 3 (the 3rd row × 1st column of WSW ) is S 31 + S 33 x 2 ⊗ I . Hence the variance of b 1 − b 3x2 equals S 11 + 2S 13 x 2 ⊗ I + x 2 ⊗ I S 33 x 2 ⊗ I − 2 S 31 + S 33 x 2 ⊗ I x 2 ⊗ I + x 2 ⊗ I S 33 x 2 ⊗ I = S 11 That is, Var a 1 = Var b 1 − b 3x2 . Q.E.D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>How to Compute Statistics for Main Effects from an Uncentered Moderated Regression: An Annotated Example of Brand Extension Evaluations a 1 +x 2 a 3 ≡ −0 047+ 5 208 * 0 034 = 0 132 Main effect of substitute</figDesc><table><row><cell></cell><cell cols="2">Uncentered estimate</cell><cell></cell><cell></cell><cell cols="2">Mean-centered estimate</cell><cell></cell></row><row><cell></cell><cell>Substitute a 1</cell><cell>Quality a 2</cell><cell>Substitute × Quality a 3</cell><cell></cell><cell>Substitute b 1</cell><cell>Quality b 2</cell><cell>Substitute × Quality b 3</cell></row><row><cell>Estimate</cell><cell>−0 04655</cell><cell>0 250</cell><cell>0 03426</cell><cell>Estimate</cell><cell>0 132</cell><cell>0 349</cell><cell>0 034</cell></row><row><cell>SE i</cell><cell>0 026</cell><cell>0 015</cell><cell>0 005</cell><cell>SE i</cell><cell>(0 008)</cell><cell>0 009</cell><cell>0 005</cell></row><row><cell>VIF</cell><cell>13 69</cell><cell>2 90</cell><cell>16 05</cell><cell>VIF</cell><cell>1 31</cell><cell>1 03</cell><cell>1 02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Correlations Between Variables</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Uncentered</cell><cell></cell><cell></cell><cell>Mean-centered</cell><cell></cell></row><row><cell></cell><cell></cell><cell>variables</cell><cell></cell><cell></cell><cell>variables</cell><cell></cell></row><row><cell>Substitute</cell><cell>1 00</cell><cell></cell><cell></cell><cell>1 00</cell><cell></cell><cell></cell></row><row><cell>Quality</cell><cell>0 11</cell><cell>1 00</cell><cell></cell><cell>0 11</cell><cell>1 00</cell><cell></cell></row><row><cell>Substitute × Quality</cell><cell>0 91</cell><cell>0 43</cell><cell>1 00</cell><cell>0 06</cell><cell>−0 07</cell><cell>1 00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When the determinant of X X is near zero as it might be with collinear data, the computation of X X −1 will eventually lead to division by almost zero (recall A −1 = adj A / A for a square matrix A), which produces rounding errors that might make estimates computationally unstable. Each computation done at doubleprecision on a modern computer will be accurate to at least 15 digits of accuracy, but repeated computations can cause the errors to accumulate.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Bold 0 is a vector of all zeroes and bold 1 is the unit-vector of all ones.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We thank Stephen Holden for providing us with seven of the eight data sets used in the analysis by<ref type="bibr" target="#b2">Bottomley and Holden (2001)</ref>. The data are available at http://mktsci.pubs.informs.org. For details of the data sets used, please see<ref type="bibr" target="#b6">Echambadi et al. (2006)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For an exposition of the distinction between simple effects and main effects, please refer to<ref type="bibr" target="#b8">Irwin and McClelland (2001)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The names of the authors are listed alphabetically. This is a fully collaborative work. We thank Inigo Arroniz, Edward A. Blair, Pavan Chennamaneni, and Tran Van Thanh for their many helpful suggestions in crafting this article. Vishal Bindroo's technical assistance is gratefully acknowledged.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Proof of Theorem 1. By doing a Laplace expansion down the last column of W,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple Regression: Testing and Interpreting Interactions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Statistician</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="77" />
			<date type="published" when="1991" />
			<publisher>Sage Publications</publisher>
		</imprint>
	</monogr>
	<note>Demeaning conditioning diagnostics through centering</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Conditioning Diagnostics: Collinearity and Weak Data in Regression</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Belsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do we really know how consumers evaluate brand extensions? Empirical generalizations based on secondary analysis of eight studies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bottomley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="494" to="500" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collinearity, heteroscedasticity and outlier diagnostics in regression. Do they always offer what they claim?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Coenders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Approaches in Applied Statistics</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Ferligoj</surname></persName>
			<persName><forename type="first">A</forename><surname>Mrvar</surname></persName>
		</editor>
		<meeting><address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="79" to="94" />
		</imprint>
	</monogr>
	<note>Metodološki Zvezki.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Applied Multiple Regression: Correlation Analysis for Behavioral Sciences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Lawrence Erlbaum Associates</publisher>
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical tests for moderator variables: flaws in analyses recently proposed</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Cronbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="414" to="417" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empirical generalizations from brand extension research: how sure are we?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Echambadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arroniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Reinartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Res. Marketing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="261" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multicollinearity and measurement error in structural equation models: implications for theory testing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="519" to="529" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Misleading heuristics and moderated multiple regression models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="109" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Interaction Effects in Multiple Regression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Jaccard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turrisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Sage Publications</publisher>
			<pubPlace>Newbury Park, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to the Theory and Practice of Econometrics</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lutkepohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modeling and Interpreting Interactive Hypotheses in Regression Analysis: A Refresher and Some Practical Advice</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Kam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Franzese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>University of Michigan Press</publisher>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Setting quality expectations when entering a market: what should the promise be?</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Kopalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="24" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mean centering in moderated multiple regression: much ado about nothing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kromrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Foster-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Measurement</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="67" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Residual centering, exploratory and confirmatory moderator analysis, and decomposition of effects in path models containing interactions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Lance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Psych. Measurement</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="163" to="175" />
			<date type="published" when="1988-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Assessing the reliability of statistical software: Part II</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mccullough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Statistician</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="159" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Failures to detect moderating effects with ordinary least squares-moderated multiple regression: some reasons and a remedy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Mansfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="282" to="288" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identification and analysis of moderator variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gur-Arie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="300" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Econometric Analysis of Cross Section and Panel Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Woolridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
