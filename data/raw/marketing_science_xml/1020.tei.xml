<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">THE LITTLE ENGINES THAT COULD: MODELING THE PERFORMANCE OF THE WORLD WIDE WEB SEARCH ENGINES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">THE LITTLE ENGINES THAT COULD: MODELING THE PERFORMANCE OF THE WORLD WIDE WEB SEARCH ENGINES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.19.1.43.15180</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The World Wide Web (WWW) is important to managers in three rather different respects. First, managers use it to engage in electronic commercial transactions as sellers or as buyers <ref type="bibr" target="#b0">(Alba et al. 1997</ref><ref type="bibr" target="#b14">, Hoffman et al. 1996</ref>. Second, they use it to disseminate information to customers or gather information as (business) customers, including both Web advertising (acquiring new customers) and after-sales support to retain customers <ref type="bibr" target="#b2">(Bakos 1997</ref><ref type="bibr" target="#b6">, Burke 1996</ref><ref type="bibr" target="#b14">, Hoffman and Novak 1996</ref>. Third, the Web is emerging as a rich source of managerial information that assists in decisionmaking, e.g., competitive intelligence, demographic information, market forecasts, general economic information, sources of external expertise or training, innovative managerial tools, tactics and strategies, and regulatory and other governmental information. Providers of such information include news organizations, governments, educational institutions, corporations, and nonprofit organizations, etc. Web search engines are commonly used to help locate this kind of information, and it is this performance of such engines that interests us here.</p><p>Search engine performance has begun to attract attention by both researchers and managers. <ref type="bibr" target="#b23">Selberg and Etzioni (1996)</ref> studied search queries and their results using various popular search engines for the period July through September 1995. In a more recent and comprehensive study published in Science, <ref type="bibr" target="#b16">Lawrence and Giles (1998)</ref> examined the URLs returned for a large number of queries during December 1997. A follow-up to that study, using more comprehensive search methods, a greater number of engines, and a larger number of phrase queries, has recently appeared in Nature for queries collected in February 1999. They were particularly interested in the relative number of URLs returned by different search engines and in estimating the number of URLs not found by any (or all) search engines. Coverage of those findings in The Wall Street Journal (1998) showed both the managerial interest and also the controversy generated by the findings. With significant advertising revenue at stake, those responsible for the engines are sensitive to assessments of their relative performance. Indeed, such assessments have loomed large in the business press discussion of the vast sums paid to acquire search engine sites.</p><p>In this study we will offer the following contributions. First, we present and validate a model for the performance of multiple Web search engines in finding URLs. We also analyze some natural, relatively simple models (Rasch-type ability/difficulty model and capture/recapture model) and find that they fail to represent key aspects of search engine performance (which the proposed model does contain). Second, we analyze the performance of six popular Web search engines in finding marketing/management phrases. <ref type="bibr" target="#b23">Selberg and Etzioni (1996)</ref> studied all queries submitted to MetaCrawler, and <ref type="bibr" target="#b16">Lawrence and</ref><ref type="bibr" target="#b16">Giles (1998, 1999)</ref> examined queries from the scientists at the NEC Research Institute in Princeton. Neither focused on management information. Third, we show how some characteristics of marketing/management phrases and of Web pages/URLs affect search engine performance. We also highlight the association between structural characteristics of a search engine (e.g., size of universe covered, depth of search) and that engine's success. Fourth, our empirical model application allows us to do more than just "rate the search engines," enabling us to describe the distinctive patterns of overlap and distinctiveness among them. Finally, for these kinds of management phrases, we are able to estimate the number of URLs not found by individual search engines, and indeed not by the collection of engines. We also can calculate the incremental benefit in adding a particular search engine's results to those URLs already found. The next section offers a description of the search process and search outcomes, some summary statistics regarding search engine performance, and a conceptualization of factors thought to affect that performance. The subsequent sections develop our model, validate it empirically, and use it to draw substantive conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Searching the Web for Marketing Information</head><p>A simple example will help illustrate the research issues of interest. In October 1998, we queried each of six popular Web search engines to find documents containing the phrase "mere exposure effect." Alta Vista found 99 documents. Northern Light located 83; of course, many of these duplicated the ones from Alta Vista. HotBot found fewer ( <ref type="formula">49</ref>), but some had not been discovered by either Alta Vista or Northern Light. Finally, engines Infoseek, Excite, and Lycos found fewer documents <ref type="bibr">(22, 21, and 9</ref>, respectively) but again some new pages were included. Together, all six engines located 172 documents, so even the "best" search engine (for this phrase) found less than 60% of this total (i.e., Alta Vista's 99 out of 172).</p><p>We should highlight that what we refer to simply as "search" (which is of course from the user's perspective) is really the result of a complex process. A search request does not directly cause a real-time search of the Web, but rather a (potentially complicated) lookup in a very large database. This database arises as the result of past webcrawling (i.e., proceeding from URL to URL and indexing the Web page contents) by the search engine and (less often) by specific requests from sites to be included in the engine's database. While any "search" request, then, produces only a search result from a database that is essentially static, a search request can affect the database for future searches, e.g., by causing certain URLs to be checked for viability or by influencing the future webcrawling pattern (by changing the engine's inferred popularity/importance for certain words or phrases). Our study simply examines the user's experience upon requesting URLs whose corresponding Web pages contain a particular phrase for these studied search engines.</p><p>We also acknowledge at the outset that this study will not attempt to assess the relative "value" of the individual sites found, and indeed one might well be skeptical of any mechanism that claimed to do so. Different searchers will no doubt have different interests or needs. Rather, thinking about this simple example leads directly to the five research issues that we do address:</p><p>1. Search Engine Performance Across Phrases. Would the search-result pattern above hold up for other marketing phrases? "Mere exposure effect" is relatively new to marketing and is associated more with academic research than with current marketing management practice. Perhaps some engines would do better for longer-established phrases, or those more prevalent among practitioners. Certainly, because Web crawlers proceed from document to document via the links provided, they may end up covering relatively separate, disparate parts of the space of URLs. Such a propensity can be exacerbated by, for instance, the inclination of academic sites to link to other academic sites (via connection to coauthors, references, etc.).</p><p>2. Factors Affecting Discovery of URLs. In the example above, several URLs were found by all of the engines, while others were located by only one. For a given phrase, what makes some URLs "easy" to locate? In light of the Web crawler process mentioned, the more sites that link to a URL, the easier finding that URL will be. Of course, this measure is essentially impossible to observe. It is also not directly controllable by a site that wishes to be found. Instead, we focus on two factors that are observable and (within limits) controllable: the number of links on a URL (to other documents), and the domain type <ref type="bibr">(.com, .edu, .org, etc.)</ref>. The former should be related to URL discoverability because it is an indicator of sophistication and connectedness and may also stimulate reciprocal linkage (i.e., a linked site electing to provide a link back). The latter factor (domain type) may matter through a propensity for sites to link within (rather than across) these types.</p><p>3. Search Engine Structural Characteristics. Although search engines' operating details are proprietary, they are known to differ with respect to some basic characteristics. We will summarize the apparent relationship between such structural properties and the engines' search performance.</p><p>4. Overlap and Sequential Search. We are also interested in the way in which patterns of overlap among the search engines determine their incremental benefit when combined. In our example above imagine that Alta Vista was the search engine used first. Would using a second engine be expected to add substantially to the number of documents found? What about a third? How many engines are needed to find the "lion's share" of relevant documents? Which particular engine would add most to, say, Alta Vista's results?</p><p>The proposed model will allow us to answer these questions.</p><p>5. How Much Information Did We Miss? Using all six search engines we found 172 documents mentioning "mere exposure effect." But how many documents did we fail to find? Note that any single URL's search results can be summarized by a binary six-vector, where the ith element is a "1" if search engine i found the URL in question and a "0" if it did not. There are, of course, 2 6 ‫ס‬ 64 such patterns, and for each phrase searched we can create the full frequency count among these 64 patterns-except for one. The number of URLs associated with the (0,0,0,0,0,0) vector is not available because this represents the number of URLs missed by all six search engines. However, after creating a model that represents well the engines' Web coverage and overlap (by fitting the 63 patterns above), we will forecast the frequency of this 64th pattern-as it indicates the size of the remaining "undiscovered" part of the Web.</p><p>To build a model that would address these five issues, we proceeded through four steps to build an appropriate database.</p><p>Step 1: Marketing Phrases for Search. The marketing phrases searched needed to be diverse enough to represent an interesting universe and also vary on the factors thought to affect search engine performance (i.e., research issue 1, above). Accordingly, phrases were selected via three criteria:</p><p>1. They are relatively central to marketing thought, appearing in popular reference works <ref type="bibr" target="#b4">(Bennett et al. 1995</ref><ref type="bibr" target="#b7">, Clemente 1992</ref>.</p><p>2. They are specific enough so that a Web search need not be refined further (e.g., "marketing management" was found on 44,432 Web pages by Alta Vistatoo many to be helpful without more detail).</p><p>3. They span the two phrase dimensions discussed earlier: managerial versus academic, and newer versus older. Five phrases were selected in each cell of the resulting 2 ‫ן‬ 2 design, leading to 20 phrases overall.</p><p>Step 2: Phrase Search Via Search Engine. The six search engines examined here (Alta Vista, HotBot, Excite, Infoseek, Northern Light, Lycos, located at http:// www.altavista.com, http://www.hotbot.com, http:// www.excite.com, http://infoseek.go.com, http:// www.northernlight.com, and http://www.lycos.com, respectively) are the most popular based on user awareness, popular press mentions, and inclusion in previous studies and in metasearch programs <ref type="bibr">(PC Magazine Online 1998</ref><ref type="bibr" target="#b3">, Beatty 1998</ref><ref type="bibr" target="#b9">, Feldman 1998</ref><ref type="bibr" target="#b16">, Lawrence and Giles 1998</ref><ref type="bibr" target="#b16">and 1999</ref>. Note that while Yahoo! is often mentioned by users as a "search engine," it is actually a directory, and at the time of our study Yahoo! additionally incorporated Inktomi, the same search engine used by HotBot. Thus, we did not include it. Although, as recently pointed out by <ref type="bibr" target="#b16">Lawrence and Giles (1999)</ref>, HotBot, Microsoft Snap, and Yahoo! do not return exactly the same information because of filtering and/or different underlying Inktomi databases. The 20 phrases were searched using each of the six engines during October 1998. During the search, two properties of each located URL were recorded: the number of links (0-5, 6-10, or ‫,)ם01‬ and the domain type (.com, .edu, .org, or "other") indicating whether the site was commercial, academic, an organization, or other (the latter including non-U.S. sites). This URL information will allow us to address research issue 2 above.</p><p>Step 3: Integrate Search Results. As noted earlier, the search result for any located URL can be summarized in a binary six-vector. However, meaningfully comparing these results across engines requires substantial care. The same document may be reached by different alphanumeric strings, requiring that the documents themselves be accessed and checked both for similarity across engines and for duplication within an engine. URLs were also checked to verify that they were active and in fact contained the phrase in question. (Both Excite and Infoseek use heuristics that may return URLs similar-but not identical-to the phrase searched. These instances were deleted.)</p><p>Step 4: Search Engine Characteristics. As in research issue 3, we want to link search engines' performance to their characteristics. Because the number of search engines is small, it would not be useful to formally incorporate these characteristics into the model itself, but we will be able to investigate an association between overall search performance and an engine's structural properties. The key properties of interest are engine size (the total number of pages indexed) and several binary indicators of capability. The latter includes Depth (whether an engine searches an entire site without a preset limit), Frame Support (ability to follow frame links), Image Maps (ability to follow image maps), and Learns Frequency (whether an engine estimates the frequency with which a page's content changes, and uses that information to determine visit frequency). Other search engine characteristics would be interesting to include (such as number of pages crawled per day) but do not appear to be reliably measured and available <ref type="bibr" target="#b24">(Sullivan 1998)</ref>. The search engine features above were taken from the Search Engine Watch site <ref type="bibr" target="#b24">(Sullivan 1998</ref>) and were measured as of August 4, 1998.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the 20 marketing phrases, their categorization regarding newness and academic/managerial, and the total number of URLs found by each engine for each phrase. Note that this table is not the complete data, but rather is a summary. For each of the 1588 located URLs, the data used in our modeldevelopment are a binary six-vector together with the two URL characteristics (number of links, domain type) and two phrase characteristics (as above).</p><p>As a further summary, Table <ref type="table" target="#tab_1">2</ref> shows how the URLs found are distributed across phrase and URL characteristics. The table entries provide for a given engine, the proportion of all URLs found (by any engine) hav-ing a particular characteristic. For instance, Alta Vista located 52.1% of all managerial-phrase URLs that were found. It did a little better (53.5%) finding academicphrase URLs. Relative to the engine's baseline level of performance across all phrases, Infoseek had the greatest skew toward locating academic-phrase URLs (0.163 academic versus 0.125 managerial), and Northern Light had the greatest inclination toward managerialphrase URLs (0.462 academic versus 0.529 managerial). Overall, Alta Vista had the best performance in finding academic-phrase URLs, while Northern Light had the greatest success finding marketing-managerial ones. Analogous conclusions for other phrase/URL characteristics are available via Table <ref type="table" target="#tab_1">2</ref>. Table <ref type="table">3</ref> provides the structural characteristics of the engines.</p><p>Before developing our model, it was useful to note what would happen if search outcomes for any given phrase were independent-i.e. if each URL had some probability of being located (possibly engine-specific) and one engine's finding the URL told us nothing about any other engine's. In such a situation, substantive research issues 1 and 2 (effect of URL and phrase characteristics) could be addressed by a separate simple model (e.g., logistic regression) for each search engine, and research issue 4 (overlap between engines) would have a very simple answer for any set of engines. The independence assumption is also the linchpin of the most careful model published so far for search engine performance <ref type="bibr" target="#b16">(Lawrence and Giles 1998)</ref>.</p><p>They consider a model with the top two engines assumed to be independent. Accordingly, we begin by considering the independence assumption in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Are Search Engine Outcomes Independent?</head><p>The simplest, and arguably the most natural, starting point for representing the URLs found by multiple Web search engines is the independent binomial model. It is based on two assumptions. First, for any given search phrase j, it imagines that any given search engine i finds any one of the URLs containing that phrase independently of its finding other such URLs, and with some probability p ij . Second, the model assumes that the probability p ij that search engine i finds any particular URL containing phrase j does not depend on the set of URLs found by any other search engine.</p><p>For a single URL containing phrase j, the data can be written simply as the binary six-vector (y 1jk , y 2jk , y 3jk , y 4jk , y 5jk , y 6jk ), where y ijk ‫ס‬ 1 if the kth URL for phrase j is found by search engine i, and is 0 otherwise. For URL k and phrase j the likelihood function is</p><formula xml:id="formula_0">L(y , y , y , y , y , y ) 1jk 2jk 3jk 4jk 5jk 6jk 6 y 1‫מ‬y ijk ijk ‫ס‬ p (1 ‫מ‬ p ) ,<label>(1)</label></formula><formula xml:id="formula_1">͟ ij ij i‫1ס‬</formula><p>where p ij is the probability that engine i finds any given URL containing phrase j. Because the URLs are exchangeable by assumption, the likelihood for the data for phrase j is the product of (1) across all URLs (in practice, a partial likelihood will be used, since the (0,0,0,0,0,0) vector will be missing).</p><p>This independent binomial model has much to recommend it. It is parsimonious: Each search engine i (for each phrase j) can be summarized by a single quantity-its search success probability p ij . The model can provide an estimate of the number of URLs not found. After any number of search engines have been used, the expected number of new URLs from another  search engine h is simply (N j ‫מ‬ m)p hj , where m is the cumulative number of URLs already found and N j is the (unknown) number of URLs containing phrase j.</p><p>Lawrence and Giles (1998) expressed concern about the independence assumption, and that concern was well founded. We report in Table <ref type="table" target="#tab_2">4</ref> the value of ‫מ‬logL for this model and the associated BIC statistic. Four particular versions of the independent binomial model were evaluated: (1) constant p, (2) different p for each engine but constant across phrases, (3) different p for each phrase but constant across engines, and (4) different p for each engine and phrase. A simple chisquare test on the value of ‫2מ‬logL rejects each of these four models. Naturally, with over 1,500 observations the power of such a test is very high and may not in itself present a strong case for substantial interdependence. Instead, two other considerations will argue for a model that relaxes the independence assumption. First, we will see later that relevant goodness-of-fit indicators can be improved substantially via a spatial interdependence model. Second, we note that the BIC criterion (which penalizes highly parameterized models for data overfitting) actually prefers, among independence models, the one where location probabilities differ only by search engine (and not by phrase). In other words, search is characterized simply by six p ivalues, one for each search engine (the relative magnitude of the p i are given by the total URL count by engine in Table <ref type="table" target="#tab_0">1</ref>).</p><p>It is easy to show that an estimate of the number of URLs found by all engines in any three-engine set (denoted 1,2, t for convenience) under this model is:</p><formula xml:id="formula_2">2 n 12 n ‫ס‬ n . (<label>2</label></formula><formula xml:id="formula_3">) 12t t n n 1 2</formula><p>Taking, for instance, Alta Vista and HotBot as engines "1" and "2," the actual three-way overlap n 12t and the overlap predicted by the independence model via ( <ref type="formula" target="#formula_2">2</ref> In short, looking across our 20 marketing phrases, the independence model substantially underpredicts the actual overlap for these triplets of search engines. These positive residuals suggest that two search engines with high coverage (Alta Vista and HotBot) are inclined to subsume the other four engines. This suggests the use of Rasch-type ability/difficulty models <ref type="bibr" target="#b19">(Rasch 1966</ref><ref type="bibr" target="#b1">, Andersen 1973</ref>, whereby the probability that a given URL is located is a function of both a URL "difficulty" parameter and an search engine "ability" parameter. In this kind of model the "easy" URLs will tend to be found by all search engines and the "hard" URLs only by the search engines that find many overall. In other words, Alta Vista and HotBot will overlap somewhat, but the other search engines will overlap even moreso with this pair (and hence produce positive residuals) because the URLs they find will tend to be the "easy" ones. Of course, other search engine triplets could show different discrepancies than those observed above. Our main point is the observation that independence does not appear to be a solidly supported assumption, and a model where spatial location of search engines determines patterns of overlap may have value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A General Proximity Model</head><p>We provide initially a heuristic description of our modeling approach for WWW data. This non-formal description is useful to describe our intuition, why we expect this class of models to improve on simpler ones, and the expected limitations and subsequent improvement in fit as our models become more complex. Needed notation and formal models are presented after.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Heuristic and Graphical Descriptions</head><p>We posit a general class of models for the ability of WWW search engines based on the proximity ("distance") from a specific engine to a given URL and the "reach" of an engine. Our basic model suggests that when an engine and URL are proximate, the engine is likely to find that URL, and unlikely when not. In particular, each engine and URL are hypothesized to "sit" at an unknown location in D-dimensional space. A URL's location is modeled to be centered around a mean location determined by both its phrase and covariates specific to the phrase and URL (e.g., type of phrase, URL domain extension, etc.). Then, from an engine's location, it "throws out a net" and probabilistically captures URLs within its reach. That is, there is a monotonically decreasing relationship between distance from engine to URL and the probability a URL is found. Pushing this analogy farther, inferences of interest under the model are then derived from: (1) the location of each engine (that is, do "weaker" engines find just a subset of those URLs found by the better engines, which would follow if all engines were located at the same place, or do engines "carve" out their own locations), (2) the size of the net for each engine (in our model this is the ability of the engine), (3) the shape of the net (are the underlying dimensions related), (4) the number of underlying dimensions D adequate to model the data, (5) the effects, if any, of phrase and URL covariates on URL's locations and hence their probability of being found, and (6) an exponent determining how fast the probability of an engine finding a URL drops off as a function of their proximity. We considered three specific cases of this general proximity model.</p><p>As a point of reference for describing the proximity models, consider the graphical representation of the independent binomial model ( § 3) shown in Figure <ref type="figure">1</ref>, panel A. The horizontal line represents the (D ‫ס‬ 1 dimensional) space of URL locations, and the various search engines differ in the degree to which they (probabilistically) cover this space, beginning at the origin. The graph can be interpreted as having each engine stand at the origin and throw out a line, capturing as many URLs ("fish") as possible. Because engines with longer fishing lines (i.e., more ability) reach out farther from the origin, they are likely to "catch" more URLs, although which URLs the better engine (engine 1) finds is unrelated to the specific URLs found by the weaker engine (engine 2). That is, via the independence assumption, it is as if the URLs randomly redistributed their locations in the time elapsed between the search by the two engines.</p><p>As an alternative to this independence model, we will examine a D ‫ס‬ 1 dimensional proximity model, depicted in Figure <ref type="figure">1</ref> panel B and denoted "Model 1" below. Here, each engine is again located at the origin and casts its probabilistic coverage of the line according to its "ability." But unlike the independence model, here the URL locations remain fixed. Accordingly, some URLs really are more difficult to locate (i.e., those labeled "D" and "E") than others (e.g., "A" and "B") as they lie far from the origin. As a result, it is unlikely that the search engines with lesser ability will find URLs not found by the better engines. As suggested in § 2 (and confirmed in § 5.1) this feature of Model 1 does not fit the data particularly well. (Even the weakest engine Lycos finds URLs not found by other engines). This suggested the extension of Model 1 in two ways under our general proximity model structure. First, in Model 2 (Figure <ref type="figure">1</ref> panel (C)) we extend to D ‫ס‬ 2 dimensions yet leave all of the engine locations at the origin. A more general version considered in Model 3 (Figure <ref type="figure">1</ref> panel (D)) also allows the engine locations to vary-i.e., as suggested earlier, a search engine may "stake out" a distinctive part of the URL space. As shown below, the results indicate that Model 3 is necessary to provide an adequate fit to the pattern of Web search results for marketing information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Notation, Development, and Computational Approach</head><p>We consider the case described in § 2 where each of i ‫ס‬ 1, . . . , I search engines is utilized on the WWW to locate URLs for each of j ‫ס‬ 1, . . . , J phrases. Let K j denote the total number of distinct URLs found for the jth phrase (by any of the engines) and y ijk a binary outcome where y ijk ‫ס‬ 1, k ‫ס‬ 1, . . . , K j , if the kth URL for the jth phrase is found by engine i, and 0 otherwise. The collection of all outcomes y ijk is denoted Y. In addition, for each URL we obtain covariate vector x jk ‫ס‬ Figure <ref type="figure">1</ref> Models for URL Discovery by Web Search Engines (x jk1 , . . . , x jkP ) to identify known characteristics of phrases and/or URLs that may make them harder or easier to find. The collection of all covariates is denoted X.</p><p>We posit a proximity model for p ijk ‫ס‬ Prob(y ijk ‫ס‬ 1) defined as a function of the following engine and URL specific parameters. Let ‫ס‬ (h i1 , . . . , h iD ) and</p><formula xml:id="formula_4">‫ס‬ t t h c i j k</formula><p>(c jk1 , . . . , c jkD ) denote the location of the ith engine and kth URL for phrase j in D-dimensional space. Additionally, define R i , a D ‫ן‬ D dimensional scaling matrix for engine i, and</p><formula xml:id="formula_5">d ijk ‫ס‬ d(h i , c jk ) ‫ס‬ (h i ‫מ‬ a squared Mahalanobis distance t ‫1מ‬ c ) R (h ‫מ‬ c ) jk i i jk</formula><p>between engine i and the k-th URL for phrase j. Thus, the diagonal elements of R i are the abilities ("reach") and the off-diagonal elements indicate the covariation of abilities for engine i in the D dimensions. We assert a model for p ijk as a function of d ijk given by</p><formula xml:id="formula_6">1 p ‫ס‬ , (<label>3</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">ijk u 1 ‫ם‬ d ijk</formula><p>where u defines the rate at which the probability an engine finds a given URL drops off. In general, spatial/ distance models have been utilized in other marketing contexts, especially brand choice <ref type="bibr">(Elrod 1988, Kamakura and</ref><ref type="bibr" target="#b15">Srivastava 1984)</ref>. We note that (3) is equivalent to logit(p ijk ) ‫ס‬ ‫מ‬u • log(d ijk ), a logistic link where u is the slope of regressor log(d ijk ). Assuming conditional independence of engines, phrases, and URLs within phrase this yields a product Bernoulli likelihood for parameters X 1 ‫ס‬ (h 1 , . . . , h I , c 11 , . . . ; , R 1 , . . . ,</p><formula xml:id="formula_9">R I , u) equal to c K J J y 1‫מ‬y u ijk ijk 1 d ijk p(Y|X ) ‫ס‬ . (4) 1 ͟ ͟ ͟ u u 1 ‫ם‬ d 1 ‫ם‬ d i j k ijk ijk</formula><p>Because commonalities are likely to exist among the engines, the phrases, and the URLs, we extend the model for Y given in (4) to include a set of prior distributions for X 1 , allowing for the sharing of information across units. The choice of priors for the components of X 1 were made in the following manner. Because the six engines that we consider represent the engines of interest, we treat the engine specific parameters as fixed effects and put non-informative priors on h i , R i , i ‫ס‬ 1, . . . , I. A non-informative prior is also adopted for u reflecting our lack of knowledge regarding this parameter. In contrast, it is of interest to summarize the location of phrase j for which we may regard c jk , k ‫ס‬ 1, . . . , K j as a random sample of URLs drawn from a population distribution. By convention and for computational convenience, we put a hierarchical multivariate normal-Inverse Wishart prior structure on the URL locations: We denote the prior level parameters by X 2 ‫ס‬ (␣ 1 , . . . , ␣ J , b, K 1 , . . . , K J , R ␣ ) and the prior ␣, distribution by p(X 1 |X 2 ).</p><formula xml:id="formula_10">c ϳ MVN (␣ ‫ם‬ bx , K ) jk D j jk j ␣ ϳ MVN (␣ , R )) j D ␣ ‫1מ‬ K ϳ W (S),<label>(5)</label></formula><p>Inferences for the model parameters X 1 and X 2 were derived by obtaining samples from the marginal posterior distributions p(X 1 |Y, X) and p(X 2 |Y, X) using a Markov chain Monte Carlo (MCMC) sampler <ref type="bibr" target="#b11">(Gelfand et al. 1990</ref><ref type="bibr" target="#b21">, Rossi et al. 1996</ref>. For each of Model 1, Model 2, and Model 3, we report results obtained by running three independent chains for 3000 draws from overdispersed starting positions, discarding the initial 500 draws of each chain after determining convergence (German and Rubin 1992) and estimating the quantities of interest using the remaining 7500 draws. Further details are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model 1: One-Dimensional Ability/Difficulty</head><p>Model We first considered a simple special case of the general proximity model defined by ( <ref type="formula" target="#formula_6">3</ref>), (4), and (5), which consisted of a D ‫ס‬ 1 dimensional model with all engines located at the origin h 1 ‫ס‬ . . . h I ‫ס‬ 0. To identify the model, we set as a reference point R 1 ‫ס‬ 1, the ability of Alta Vista indexed as i ‫ס‬ 1, and set the rate factor u ‫ס‬ 0.5. This model, in which each engine ("examinee") has a unidimensional ability R i and each URL has a unidimensional location c jk ("test item difficulty"), is similar in spirit to the <ref type="bibr" target="#b19">Rasch (1960)</ref> model commonly used in educational testing.</p><p>Model 1 was applied to the set of 20 phrases and 1588 URLs described in § 2. A summary of results for engine abilities, presented as R i is given in column 2 of Table <ref type="table" target="#tab_4">5</ref>. The ordering of engine abilities suggested (Alta Vista, Northern Light, HotBot, Excite ‫ס‬ Infoseek, Lycos) is unambiguous in all comparisons (true for all 7500 draws) except for the comparison (a) Alta Vista Ͼ Northern Light, p ‫ס‬ 0.78 and (b) Excite Ն Infoseek, p ‫ס‬ 0.48. The results from Models 2 and 3, better fitting models described later, will further refine these relations.</p><p>Inferences under Model 1 for the phrase and URL covariates (1) domain extension: .edu, .com, .org, other, (2) # of Links on the URL: 0-5, 6-10, ‫,ם01‬ (3) Type of Phrase: Managerial/Academic, (4) Age of Phrase: Newer/Older, and (5) the interaction between (3) and (4), on the mean of URL locations c jk and hence p ijk , are given in column 2 of Tables <ref type="table" target="#tab_5">6 and 7</ref>. In Table <ref type="table" target="#tab_5">6</ref> we report the posterior median, standard error, and probability of the effect being greater than 0 for each covariate. Table <ref type="table" target="#tab_6">7</ref> gives the adjusted phrase mean for URLs with a given covariate level. To interpret these findings, recall that all engines for Model 1 are located at the origin, thus any positive coefficient suggests that the covariate level makes URLs of that type harder to find, and vice-versa. We observe strong evidence that  Note: Reported are the posterior medians (standard deviations) and posterior probability of the effect being greater than 0. Int. is the interaction between managerial and newer.</p><p>URLs with fewer links are harder to find than those with the most number of links ‫)ם01(‬ and modest evidence that URLs having domain extensions .edu or .org are slightly easier to find. These results are also confirmed by <ref type="bibr" target="#b16">Lawrence and Giles (1999)</ref>. Other inferences were: (1) there was no significant difference in the phrase locations (posterior median of R ␣ ‫ס‬ 0.001), which is consistent with the stable hit rates for each engine by phrase reported in Table <ref type="table" target="#tab_0">1</ref> and the loglikelihoods reported in Table <ref type="table" target="#tab_2">4</ref>, and (2) URL variances R j were inversely related to the number of URLs found (r ‫ס‬ ‫.)58.0מ‬</p><p>A more detailed and informative look at the performance of Model 1 is presented in columns 3 through 6 of Table <ref type="table" target="#tab_7">8</ref>. Here we consider the number of URLs, showing each of the 2 6 ‫ס‬ 64 possible engine-hit patterns. The table provides the observed number n obs for each pattern (excluding (0,0,0,0,0,0)), as well as the 2.5%, 50%, and 97.5% percentiles for the predicted frequency. Some interesting residuals are evident. First, we note that Model 1 tends to underpredict the number of unique URLs found by each engine as seen in the unique engine-hit patterns 32, 48, 56, 60, and 62 (pattern 63 is slightly overpredicted). Second, and related to the underprediction in the number of uniques, Model 1 also tends to overpredict the number of URLs found by exactly two engines, as seen in patterns <ref type="bibr">16, 24, 28, 30, 44, 46, 47, 52, 54, 59, and 61 (patterns 31, 55,</ref>  and 58 are adequately fit, and pattern 40 is underpredicted). These results were not surprising because in Model 1 each engine is located at the origin and is casting its "fishing line" in the same direction.</p><p>One further inference that can be derived from the model is an estimate of the number of URLs not found by any of the engines. This question has managerial relevance from two perspectives: (1) A manager searching for URLs on a specific topic may wish to know the fraction of those related URLs he or she is likely to find by using these six engines; and (2) consider the owner of a URL wanting his or her Webpage to be found. Under the model, we can compute the posterior distribution of the number of URLs not found, K, by noting that P(all engines miss a URL)</p><formula xml:id="formula_11">‫ס‬ (1 ‫מ‬ p ) ⇒ ͟ ijk i P(at least one finds it) ‫ס‬ 1 ‫מ‬ (1 ‫מ‬ p ) ⇒ ͟ ijk i n ‫ס‬ 1 ‫מ‬ (1 ‫מ‬ p ) * K ⇒ obs ͟ ijk i n obs K ‫ס‬ . (<label>6</label></formula><formula xml:id="formula_12">) 1 ‫מ‬ (1 ‫מ‬ p ) ͟ ijk i</formula><p>These results are shown in pattern 64 and suggest that the 95% posterior interval for the number of missing URLs for the 20 phrases is (253.94, 330.30) with posterior median 283.43. This indicates that Model 1 predicts 283.43/(1588 ‫ם‬ 283.43) Ϸ 15% of the URLs are missed by using all six engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Model 2 and Model 3 Results</head><p>We considered two additional special cases of the general proximity model to improve on Model 1. Model 2 consisted of a D ‫ס‬ 2 dimensional version where each engine was located at the origin (h 11 ‫ס‬ h 12 ‫ס‬ . . . h I1 ‫ס‬ h I2 ‫ס‬ 0). As a scale identifiability constraint we set R 11 , the ability of Alta Vista on dimension 1, equal to 1. By definition, the addition of a second dimension would improve the fit; however, we suspected that locating each engine at the origin, as per a pure ability/difficulty model, would still provide an inadequate fit. In Model 3, we generalize Model 2 to allow individual search engines to carve out a distinctive portion of (two-dimensional) URL space, i.e., the engine locations (h i1 , h i2 ) were allowed to vary. In fitting Model 3, we set h 11 ‫ס‬ h 12 ‫ס‬ 0, h 21 ‫ס‬ 0, restricted h 31 Ͼ 0, and put R 11 ‫ס‬ 1 as shift, y-axis rotation, x-axis rotation, and scale identifiability constraints respectively.</p><p>Models 2 and 3 results for engine abilities R i11 , R i22 , and the correlation between dimensions q i12 is given in Table <ref type="table" target="#tab_4">5</ref> (columns 3-8). A graphical representation of the engine performances for Model 3 is given in Figure <ref type="figure">2</ref>, panels A and B. The results suggest that there are indeed two unique dimensions in which engines operate. Model 2 findings give the ordering in dimension 1 of Northern Light, Alta Vista, HotBot, Excite, Infoseek, and Lycos, whereas dimension 2 results give the ordering Alta Vista, Northern Light, HotBot, Infoseek, Excite, and Lycos. This is consistent with Model 1 findings of an ambiguous ordering of Alta Vista versus Northern Light and Excite versus Infoseek. However, we note that the total "area" covered by Northern Light is superior to that of Alta Vista because its posterior median abilities (2.670, 1.020) suggest greater coverage than Alta Vista's (1.000, 1.760). These findings are replicated in Model 3, in which Northern Light is far superior to Alta Vista on dimension 1 (3.720 versus 1.000) and almost equal on dimension 2 (1.870 versus 1.960). This is suggested by Northern Light's high number of unique finds (pattern 62), indicating its location far from the other engines, but still high hit rate 785/1588 (i.e., high ability to "compensate" for a distant location). The remaining ordering of engines for Model 3 are similar to those described for Model 2.</p><p>The engine locations for Model 3 are given in Table <ref type="table" target="#tab_8">9</ref> (also seen in Figure <ref type="figure">2</ref>) and suggest that the engines do carve out different locations. Northern Light, and HotBot are located the farthest distance from Alta Vista, indicating their abilities to have unique finds. Infoseek and Lycos are located "half-way" between Northern Light and Alta Vista and in a sense are "maximizing" their ability to find URLs that happen not to be found by either of the two best-performing engines. Excite's location near Alta Vista suggests, as described more fully in § 6.3, that the additional benefit of using Excite if Alta Vista has already been used is less than that for Infoseek, despite the fact that they are "equally able" engines.</p><p>The effects of the phrase and URL covariates on dimensions 1 and 2 for Models 2 and 3 are given in Tables 6 and 7. The posterior probabilities of the effects being greater than 0 (Table <ref type="table" target="#tab_5">6 columns</ref>  <ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">8,</ref><ref type="bibr">10)</ref> indicate that in fact domain extension, number of links, and type and age of phrase do have a significant impact on the mean phrase location. To interpret their effects on the probability that a given URL is found, consider Table <ref type="table" target="#tab_6">7</ref>, which gives the coordinates of the mean phrase location for a URL with each of the given covariate attribute levels, and that of a URL with each covariate level at the baseline condition. Because under Model 2 all engines are located at the origin, and the mean phrase under the baseline condition is at (0.130, ‫,)080.0מ‬ any covariate level that brings the phrase mean closer to the origin will increase the probability a URL is found, and vice-versa. The results indicate that fewer than ‫ם01‬ links and managerial phrases move the mean farther from the origin and hence lower the find probabilities. The domain extension .com, .org, and the interaction of new and managerial phrase condition move the mean phrase locations closer to the origin. The remaining covariate levels have results that depend on the ability of a given engine in each dimension. The covariate-effect results for Model 3 generally need to be examined separately for each search engine because the locations of the engines vary. This examination is straightforward, using the phrase/URL locations from Table <ref type="table" target="#tab_6">7</ref> and the search en-   method of <ref type="bibr" target="#b5">Bradlow and Zaslavsky (1997)</ref>, in which case deletion of URLs is implemented by importance reweighting the parameter draws from the full data posterior distribution. As a result of the conditional independence structure of the likelihood given in (4), the importance reweighting scheme is trivial and computationally cheap in that each parameter draw is reweighted for URL jk by the inverse of its contribution to the likelihood, i.e., p(y jk |X 1 , X 2 ) ‫1מ‬ . The total number of predictions made under this approach 9528 (1588 URLs by 6 engines) provides an adequate basis for validation. The results of the validation experiment indicated that Models 1 through 3, were able to predict 58%, 72%, and 81%, respectively, of the URL correctly (all results significant at the 0.05 level), suggesting an adequate predictive ability of the modeling approach and a substantial preference for Model 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusions</head><p>We set out to better understand the performance of popular Web search engines in finding marketing phrases. This required development of a model (Model 3) able to capture distinctive patterns of overlap and coverage among the engines. Furthermore, we wanted to understand how some characteristics of the phrase being searched, and of the URL being sought, would affect search outcomes. As discussed in § 5.2, two phrase characteristics (newer/older and managerial/ academic) and two URL characteristics (number of links, domain type) significantly affected search engine outcomes. The effect of number-of-links happens to be consistent across engines: The more links, the more likely the document will be located. Given the disparity in Web engine coverage patterns (as in Figure <ref type="figure">2</ref>), the other substantive effects differed by engine. For instance, a search for an academic phrase (as opposed to managerial) aided Infoseek's prospect for locating URLs, but hindered that of Northern Light.</p><p>To elaborate on our empirical and model-based results, we conclude by addressing four simple questions:</p><p>-What search engine "works best"? -Why do certain search engines find more URLs than other engines?</p><p>-What are the benefits to sequential search? -How much information is still unaccounted for?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">What Search Engine "Works Best?"</head><p>We again acknowledge that "best" here means simply locating more URLs containing the desired marketing phrase. Overall, based on the Model 3 estimates in Table 8 (and consistent with Table <ref type="table" target="#tab_0">1</ref>), we can make five simple statements concerning the "best engine question": 1. Overall, for a randomly chosen marketing phrase and URL, the search engine most likely to find it is Alta Vista.</p><p>2. BUT, Northern Light is a very close second and, in fact, does slightly better than Alta Vista in finding managerial phrases.</p><p>3. HotBot is a very respectable third, locating a little over 50%-60% as many URLs as Alta Vista or Northern Light.</p><p>4. Excite and Infoseek trail more substantially, locating 20%-30% as many documents as the two leading engines.</p><p>5. Lycos found 10%-15% as many documents as the two leaders. Of course, these findings pertain specifically to the time period of search (October 1998), the information domain of interest to us (marketing phrases), and the particular 20 phrases selected. With respect to this last restriction, however, we note that the variation in mean locations across phrases (after accounting for our covariates) was very small. (The variance across phrases in the baseline mean phrase location (␣ ,␣ )   <ref type="table">3</ref> chance to change our findings. We next consider possible explanations for the engines' differential performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Why Do Certain Engines Find More URLs?</head><p>Research issue 3 in § 2 addressed how structural characteristics of search engines would affect the search results. Recall that some fundamental measures of this sort were provided in Table <ref type="table">3</ref>. Because the number of popular search engines (here, six) is small relative to the information, it was not desirable to embed these features formally in our URL-location model. Armed, however, with overall performance statistics engineby-engine we can conduct an exploratory analysis linking search engine properties to overall search effectiveness.</p><p>Of course, the factor that looms largest in such an analysis is search engine size-i.e., the total number of Web pages indexed. Not only would it be extraordinary if "size did not matter," but it could be well argued that "size is everything," i.e., that the number of URLs found by search engine A relative to engine B is entirely predicted by their relative sizes. This last hypothesis was essentially tested with the independence model of search outcomes, and rejected, in § 3. In other words, our Model 3, with search engines that are somewhat distinct in the space that they cover, argues that structural characteristics beyond size may have an impact on search outcomes and motivated us to examine the full set of engine characteristics in Table <ref type="table">3</ref>. Accordingly, our profiling search outcomes based on engine characteristics was done in two sequential steps. The first examined the relationship between size and overall URLs found. The second looked at any deviations from a "size/total-URLs" connection to see if those deviations are associated with other engine properties from Table <ref type="table">3</ref>. Essentially, the factor size represents a very simple "par" model for engine performance, and we examine in step 2 engines that overperform (and underperform) relative to size.</p><p>Table <ref type="table" target="#tab_0">11</ref> reports the results of these analyses. Columns (a) and (b) show clearly that our marketing phrase search outcomes are correlated substantially with engine size (q ‫ס‬ 0.833). They also show that size is far from the only factor. Column (c) reports the ratio of URLs found to engine size. The variation in these values shows that much more is going on than simply engines indexing more pages. Based on column (c), three engines did substantially better in locating URLs than their size would indicate: Northern Light, Alta Vista, and Infoseek. At the other extreme, not only was Lycos tied for smallest size, but it also found fewer URLs relative to its size than any of the other engines. Taking the overperformance of Northern Light and Alta Vista alone, one might suggest a convex relationship between size and URLs found (increasing returns to size) as opposed to a linear one posited in column (c), but this explanation is inconsistent with HotBot's underperformance and Infoseek's overperformance.</p><p>Instead, we sought to understand the variation in column (c) via the other search engine characteristics. Specifically, we created a simple index of search sophistication from the characteristics Depth of Search, Frames Support, Image Maps, and Learns Frequency. For each engine, we summed the binary indicators for each of the four variables ("1" ‫ס‬ more sophisticated search, "0" ‫ס‬ less sophisticated) and report the resulting index in Table <ref type="table" target="#tab_0">11</ref> column <ref type="bibr">(d)</ref>.</p><p>Our measure of sophistication does a good job of explaining which engines overperform relative to their size. The three overperforming engines in column (c) are also leaders with respect to the sophistication index, although Infoseek and HotBot were admittedly tied. Overall, the correlation between overperformance in column (c) and the sophistication index in (d) is q ‫ס‬ 0.658, which shows that these structural properties of search engines are substantially related to engine performance, and in a way not reflected in the engine's size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Sequential Search</head><p>One practical question of managerial interest is "which search engine should I use?" We believe that the previous two subsections summarize what our data and modeling say about that. Another practical question is "Now that I have used search engine W should I do an additional search, and if so what engine Z should I use?" Let's examine the first part of this question. Based on the results for Model 3 (Table <ref type="table" target="#tab_7">8</ref>), Alta Vista would be one's best single search engine choice, expected to find 48% of the marketing/management phrase URLs that exist. This is pretty good, but there is still plenty to find. More to the point, there is still plenty that can readily be found. Now turning to the second part above, if one added a second search engine after using Alta Vista, which should it be? Figure <ref type="figure">2</ref> by itself does not provide a clear answer. Instead, this figure shows that a putative case could be made for four of the other engines. HotBot's coverage does not overlap much with Alta Vista's, but Northern Light also does not overlap completely and covers a great deal of the URL space. Alternatively, Alta Vista will not actually find all URLs in its Figure <ref type="figure">2</ref> coverage area as indicated by the probability values 0.5 and 0.33 for the iso-probability curves, and many URLs exist to be found close to the origin. Excite and Infoseek are centered near the origin and accordingly are wellpositioned to locate those residual URLs.</p><p>As it turns out, Northern Light is easily the best choice here for finding additional URLs. This can be established both by Table <ref type="table" target="#tab_7">8</ref> using the actual search pattern finds (column 3) or Model 3's predicted search pattern outcomes. For our purposes it will suffice to simply tally the incremental URLs (not found by Alta Vista) for each of the remaining five engines. These are, in order, Northern Light (actual incremental ‫ס‬ 443, predicted incremental using Model 3 ‫ס‬ 468), HotBot (actual ‫ס‬ 271, predicted ‫ס‬ 259), Infoseek (actual ‫ס‬ 136, predicted ‫ס‬ 124), Excite (actual ‫ס‬ 110, predicted ‫ס‬ 109), and Lycos (actual ‫ס‬ 35, predicted ‫ס‬ 42). Thus we conclude that in general it is important to consider both overall coverage ability and overlap in selecting combinations of search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">How Much Information Is Still Unaccounted</head><p>For? We have seen that combined search outcomes from multiple engines improves greatly on any one engine's performance. Yet, how much marketing information remains unlocated, even after using all six engines? For our 20 marketing phrases, the results in Table <ref type="table" target="#tab_7">8</ref> provide an answer to that question. Based on the estimate from Model 3, the fraction of total relevant URLs missed by all six search engines is just 10.8% <ref type="bibr">(192.704/ 1786.967)</ref>. Given the small variation in phrase location for our 20 marketing phrases searched, the reader should feel confident that the search engines cover about 90% of what exists to be found for these kind of phrases.</p><p>This is quite different-and much better-than the Web coverage estimated by <ref type="bibr" target="#b16">Lawrence and Giles (1998)</ref> for their scientific-phrase searches. There, the six search engines were estimated to cover about 60% of the indexable URLs. In their updated 1999 article, this figure is even lower and, as they suggest, states that "engines aren't keeping up." Two explanations for the discrepancy across studies suggest themselves readily. First, the estimated number of URLs not found could be highly sensitive to the particular model specification selected. As we have seen, our marketing data reject the independent binomial model used by Lawrence and Giles because that model does not effectively capture the patterns of overlap for sets of engines. So if we had to select one model to estimate the size of the Web, we would propose our Model 3 as a more appealing approach. Nonetheless, if the estimated Web size is so sensitive to model specification, one might well question the ability of any of these models to provide a reliable estimate-at least without exhaustive checking of individual assumptions. Fortunately, this situation has not arisen. While we do not recommend using the independent binomial model, its estimate of cumulative URL coverage by our six search engines (across all 20 phrases) is 89.6%-very close to the value found using our Model 3. In short, while the independent binomial model methodology is suspect, it too indicates high coverage of marketing information. Accordingly, the differences between our results gration of ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>). The approach taken here to solve these intractable integrals is iterative simulation via a Markov chain Monte Carlo (MCMC) sampler. This approach states that under certain regularity conditions, samples from ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>  <ref type="bibr" target="#b13">(Hastings 1970)</ref>, where for each parameter that was unconstrained, we utilized a symmetric Gaussian jumping distribution with mean at the previously drawn value , and variance set to provide a high acceptance rate.</p><p>(t)</p><p>X 1 For those parameters constrained to the positive real line (variances, u, and h 31 in Model 3), we utilized a Gamma distribution kernel with shape parameter and scale parameter , which has mean</p><formula xml:id="formula_13">(t) 2 (t) k(X ) kX 1 1</formula><p>equal to the previous draw and variance 1/k. The value of k was (t)</p><p>X 1 set differently for each parameter to obtain an adequate acceptance rate.</p><p>Three independent streams for each of the three models were run using overdispersed starting values obtained from an initial run. Computing times for Models 1 through 3 were 3, 12, and 14 seconds, respectively, per iteration on an HP7000 workstation using Fortran 77 code.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>j</head><label></label><figDesc>where MVN D (x, y) denotes a D-dimensional multivariate normal distribution with mean vector x and covariance matrix y, ␣ j ‫ס‬ (␣ j1 , . . . , ␣ jD ) the mean location of phrase j, b a D ‫ן‬ P dimensional coefficient matrix where b dp is the slope for the pth covariate in dimension d, ‫ס‬ the population mean of thē ␣ (␣ , . . . ,␣ )1 D phrase locations, K j and R ␣ are D ‫ן‬ D-dimensional covariance matrices for phrase j and the population of phrase means, and denotes an Inverse-Wishart ‫1מ‬ W (Q) j distribution with j degrees of freedom and scale matrix Q. The values of and S were chosen as uninformative, allowing the data to fully specify the values of K j . As well, a noninformative prior distribution was utilized for b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 URL Coverage by Six Popular Web Search Engines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Number of URLs Found By Search Engine and Marketing Phrase</head><label>1</label><figDesc></figDesc><table><row><cell>Search Engine*</cell></row></table><note>*AV ‫ס‬ Alta Vista, HB ‫ס‬ HotBot, EX ‫ס‬ Excite, IS ‫ס‬ Infoseek, NL ‫ס‬ Northern Light, LY ‫ס‬ Lycos Note: Manag. ‫ס‬ 1 indicates a managerial phrase, 0 an Academic phrase. Newer ‫ס‬ 1 a newer phrase, 0 an older phrase.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Search</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Age</cell><cell></cell><cell></cell><cell>Type</cell><cell></cell><cell></cell><cell>Links</cell><cell></cell><cell></cell><cell>Domain</cell></row><row><cell>Engine</cell><cell>New</cell><cell>Old</cell><cell></cell><cell>Manag.</cell><cell cols="2">Acad.</cell><cell>0-5</cell><cell>6-10</cell><cell>‫ם01‬</cell><cell>edu</cell><cell>com</cell><cell>org</cell><cell>other</cell></row><row><cell>AV</cell><cell>0.504</cell><cell cols="2">0.564</cell><cell>0.521</cell><cell cols="2">0.535</cell><cell>0.523</cell><cell>0.545</cell><cell>0.548</cell><cell>0.495</cell><cell>0.557</cell><cell>0.644</cell><cell>0.530</cell></row><row><cell>HB</cell><cell>0.304</cell><cell cols="2">0.280</cell><cell>0.297</cell><cell cols="2">0.293</cell><cell>0.284</cell><cell>0.288</cell><cell>0.328</cell><cell>0.312</cell><cell>0.269</cell><cell>0.328</cell><cell>0.288</cell></row><row><cell>Ex</cell><cell>0.155</cell><cell cols="2">0.125</cell><cell>0.140</cell><cell cols="2">0.144</cell><cell>0.146</cell><cell>0.138</cell><cell>0.137</cell><cell>0.140</cell><cell>0.142</cell><cell>0.164</cell><cell>0.143</cell></row><row><cell>IS</cell><cell>0.169</cell><cell cols="2">0.109</cell><cell>0.125</cell><cell cols="2">0.163</cell><cell>0.135</cell><cell>0.155</cell><cell>0.167</cell><cell>0.153</cell><cell>0.110</cell><cell>0.205</cell><cell>0.147</cell></row><row><cell>NL</cell><cell>0.506</cell><cell cols="2">0.478</cell><cell>0.529</cell><cell cols="2">0.462</cell><cell>0.481</cell><cell>0.551</cell><cell>0.505</cell><cell>0.502</cell><cell>0.526</cell><cell>0.521</cell><cell>0.458</cell></row><row><cell>LY</cell><cell>0.050</cell><cell cols="2">0.058</cell><cell>0.056</cell><cell cols="2">0.050</cell><cell>0.044</cell><cell>0.080</cell><cell>0.066</cell><cell>0.056</cell><cell>0.088</cell><cell>0.041</cell><cell>0.026</cell></row><row><cell>Table 3</cell><cell cols="5">Structural Characteristics of Search Engines*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Search Engine</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Characteristics</cell><cell>AV</cell><cell>HB</cell><cell>EX</cell><cell>IS</cell><cell>NL</cell><cell>LY</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Size (million pages) 140</cell><cell>110</cell><cell>55</cell><cell>30</cell><cell>80</cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Depth of Search</cell><cell cols="6">No Limit No Limit No Limit Sample No Limit Sample</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Frames Support</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image Maps</cell><cell></cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Learns Frequency</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Engine Results by Phrase Age, Phrase Type, URL Number of Links, and Domain Extension *Source Search Engine Watch (Sullivan 1998) BRADLOW AND SCHMITTLEIN The Little Engines That Could: Modeling Performance of WWW Search Engines Marketing Science/Vol. 19, No. 1, Winter 2000 49</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 Global</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell># Parameters</cell><cell>‫*2מ‬LL</cell><cell>BIC</cell></row><row><cell>Constant p</cell><cell>1</cell><cell>11236.72</cell><cell>11245.88</cell></row><row><cell>Different p by engine</cell><cell>6</cell><cell>9602.83</cell><cell>9657.80</cell></row><row><cell>Different p by phrase</cell><cell>20</cell><cell>11197.58</cell><cell>11380.82</cell></row><row><cell>Different p engine by phrase</cell><cell>120</cell><cell>9276.17</cell><cell>10375.60</cell></row></table><note>Goodness-of-Fit for Independence ModelsNote: Reported are ‫*2מ‬ Log-Likelihood, and the BIC criterion.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 Posterior</head><label>5</label><figDesc></figDesc><table><row><cell>, 2, and 3</cell></row></table><note>Medium Engine Abilities on Dimensions 1 and 2 (R 11 , R 22 ), Correlation (q 12 ), and Distance Factor u for Models 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 Phrase</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell>Model 1</cell><cell></cell><cell></cell><cell cols="2">Model 2</cell><cell></cell><cell></cell><cell cols="2">Model 3</cell><cell></cell></row><row><cell>Cov.</cell><cell>Dim 1</cell><cell></cell><cell>Dim 2</cell><cell></cell><cell>Dim 1</cell><cell></cell><cell>Dim 2</cell><cell></cell><cell>Dim 2</cell><cell></cell></row><row><cell>.edu</cell><cell>‫250.0מ‬ (0.06)</cell><cell>0.177</cell><cell>‫401.0מ‬ (0.05)</cell><cell>0.050</cell><cell>‫541.0מ‬ (0.07)</cell><cell>0.008</cell><cell>‫802.0מ‬ (0.06)</cell><cell>0.000</cell><cell>0.017 (0.04)</cell><cell>0.653</cell></row><row><cell>.com</cell><cell>0.018 (0.07)</cell><cell>0.610</cell><cell>‫322.0מ‬ (0.09)</cell><cell>0.003</cell><cell>0.008 (0.08)</cell><cell>0.423</cell><cell>‫312.0מ‬ (0.09)</cell><cell>0.003</cell><cell>‫281.0מ‬ (0.04)</cell><cell>0.000</cell></row><row><cell>.org</cell><cell>‫090.0מ‬ (0.12)</cell><cell>0.117</cell><cell>‫431.0מ‬ (0.11)</cell><cell>0.005</cell><cell>0.020 (0.10)</cell><cell>0.633</cell><cell>‫222.0מ‬ (0.10)</cell><cell>0.018</cell><cell>0.057 (0.05)</cell><cell>0.998</cell></row><row><cell>0L-5L</cell><cell>0.099 (0.05)</cell><cell>0.957</cell><cell>‫082.0מ‬ (0.06)</cell><cell>0.005</cell><cell>‫030.0מ‬ (0.08)</cell><cell>0.470</cell><cell>0.138 (0.06)</cell><cell>0.990</cell><cell>‫641.0מ‬ (0.03)</cell><cell>0.000</cell></row><row><cell>6L-10L</cell><cell>0.136 (0.09)</cell><cell>0.947</cell><cell>0.090 (0.10)</cell><cell>0.733</cell><cell>‫710.0מ‬ (0.09)</cell><cell>0.360</cell><cell>0.042 (0.09)</cell><cell>0.673</cell><cell>‫231.0מ‬ (0.05)</cell><cell>0.005</cell></row><row><cell>Man.</cell><cell>0.067 (0.11)</cell><cell>0.733</cell><cell>0.198 (0.11)</cell><cell>0.990</cell><cell>0.370 (0.12)</cell><cell>1.000</cell><cell>‫411.0מ‬ (0.10)</cell><cell>0.148</cell><cell>0.148 (0.10)</cell><cell>0.913</cell></row><row><cell>Newer</cell><cell>0.106 (0.11)</cell><cell>0.807</cell><cell>0.137 (0.08)</cell><cell>0.930</cell><cell>0.082 (0.15)</cell><cell>0.635</cell><cell>‫032.0מ‬ (0.08)</cell><cell>0.000</cell><cell>0.280 (0.04)</cell><cell>1.000</cell></row><row><cell>Int.</cell><cell>‫211.0מ‬ (0.15)</cell><cell>0.237</cell><cell>‫754.0מ‬ (0.15)</cell><cell>0.000</cell><cell>‫883.0מ‬ (0.22)</cell><cell>0.010</cell><cell>0.026 (0.13)</cell><cell>0.560</cell><cell>‫473.0מ‬ (0.10)</cell><cell>0.000</cell></row></table><note>and URL Covariate Slopes (b) for Models 1, 2, and 3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 Effect of Phrase and URL Covariates x jk on the Mean Phrase Location</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Model 1</cell><cell>Model 2</cell><cell>Model 3</cell></row><row><cell>x jk</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>l 1</cell><cell></cell><cell>(l 1 , l 2 )</cell><cell>(l 1 , l 2 )</cell></row><row><cell>.edu</cell><cell></cell><cell>0.968</cell><cell></cell><cell>(0.026, ‫)522.0מ‬</cell><cell>(0.017, 0.085)</cell></row><row><cell>.com</cell><cell></cell><cell>1.038</cell><cell></cell><cell>‫,390.0מ(‬ ‫)270.0מ‬</cell><cell>(0.012, ‫)411.0מ‬</cell></row><row><cell>.org</cell><cell></cell><cell>1.011</cell><cell></cell><cell>(0.004, ‫)060.0מ‬</cell><cell>(0.003, 0.125)</cell></row><row><cell>0L-5L</cell><cell></cell><cell>1.119</cell><cell></cell><cell>‫,501.0מ(‬ ‫)011.0מ‬</cell><cell>(0.363, ‫)870.0מ‬</cell></row><row><cell>6L-10L</cell><cell></cell><cell>1.156</cell><cell></cell><cell>(0.220, ‫)790.0מ‬</cell><cell>(0.268, ‫)460.0מ‬</cell></row><row><cell>Man.</cell><cell></cell><cell>1.087</cell><cell></cell><cell>(0.328, 0.290)</cell><cell>(0.111, 0.216)</cell></row><row><cell>Newer</cell><cell></cell><cell>1.126</cell><cell></cell><cell>(0.267, 0.000)</cell><cell>‫,500.0מ(‬ 0.348)</cell></row><row><cell cols="2">New ‫ם‬ Man.</cell><cell>1.081</cell><cell></cell><cell>‫,800.0מ(‬ ‫)610.0מ‬</cell><cell>‫,311.0מ(‬ 0.112)</cell></row><row><cell>Note:</cell><cell>(␣ ,␣ ) 1 2</cell><cell cols="3">is the mean phrase location with all covariates at baseline</cell></row><row><cell cols="5">levels. (l 1 , l 2 ) ‫ס‬ (␣ 1 ‫ם‬ b 1 x jk , ‫ם‬ b 2 x jk ) are the new coordinates includinḡ ␣ 2</cell></row><row><cell cols="5">the covariate effects. Model 1: ‫ס‬ 1.020, Model 2: ␣</cell><cell>(␣ ,␣ ) 1 2</cell><cell>‫ס‬ (0.130,-</cell></row><row><cell cols="2">0.080), Model 3:</cell><cell>(␣ ,¯␣ ) 1 2</cell><cell cols="2">‫ס‬ (0.225, 0.068)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 . Table of Web Engine Patterns and 95% Confidence Intervals for Models 1 through 3Table of Web Engine Patterns and 95% Confidence Intervals for Models 1 through 3</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model 1</cell><cell></cell><cell></cell><cell>Model 2</cell><cell></cell><cell></cell><cell>Model 3</cell><cell></cell></row><row><cell>Number</cell><cell>Pattern</cell><cell>n obs</cell><cell>2.5%</cell><cell>50%</cell><cell>97.5%</cell><cell>2.5%</cell><cell>50%</cell><cell>97.5%</cell><cell>2.5%</cell><cell>50%</cell><cell>97.5%</cell></row><row><cell>1</cell><cell>111111</cell><cell>2</cell><cell>0.020</cell><cell>0.032</cell><cell>0.052</cell><cell>0.019</cell><cell>0.113</cell><cell>0.825</cell><cell>0.017</cell><cell>0.104</cell><cell>0.623</cell></row><row><cell>2</cell><cell>111110</cell><cell>3</cell><cell>0.573</cell><cell>0.901</cell><cell>1.262</cell><cell>0.453</cell><cell>2.009</cell><cell>9.105</cell><cell>0.385</cell><cell>1.796</cell><cell>11.195</cell></row><row><cell>3</cell><cell>111101</cell><cell>1</cell><cell>0.024</cell><cell>0.041</cell><cell>0.060</cell><cell>0.016</cell><cell>0.096</cell><cell>1.009</cell><cell>0.021</cell><cell>0.115</cell><cell>0.843</cell></row><row><cell>4</cell><cell>111100</cell><cell>2</cell><cell>0.809</cell><cell>1.120</cell><cell>1.415</cell><cell>0.372</cell><cell>2.006</cell><cell>11.473</cell><cell>0.467</cell><cell>2.067</cell><cell>9.218</cell></row><row><cell>5</cell><cell>111011</cell><cell>3</cell><cell>0.175</cell><cell>0.284</cell><cell>0.450</cell><cell>0.153</cell><cell>0.805</cell><cell>3.599</cell><cell>0.155</cell><cell>0.616</cell><cell>3.553</cell></row><row><cell>6</cell><cell>111010</cell><cell>21</cell><cell>5.641</cell><cell>7.762</cell><cell>10.218</cell><cell>3.647</cell><cell>13.798</cell><cell>38.908</cell><cell>3.794</cell><cell>10.159</cell><cell>46.521</cell></row><row><cell>7</cell><cell>111001</cell><cell>0</cell><cell>0.225</cell><cell>0.353</cell><cell>0.509</cell><cell>0.148</cell><cell>0.740</cell><cell>4.358</cell><cell>0.162</cell><cell>0.706</cell><cell>4.677</cell></row><row><cell>8</cell><cell>111000</cell><cell>18</cell><cell>7.946</cell><cell>9.418</cell><cell>11.372</cell><cell>3.569</cell><cell>13.459</cell><cell>55.313</cell><cell>4.115</cell><cell>10.819</cell><cell>43.796</cell></row><row><cell>9</cell><cell>110111</cell><cell>3</cell><cell>0.179</cell><cell>0.289</cell><cell>0.425</cell><cell>0.190</cell><cell>0.625</cell><cell>6.061</cell><cell>0.213</cell><cell>0.788</cell><cell>3.075</cell></row><row><cell>10</cell><cell>110110</cell><cell>16</cell><cell>5.911</cell><cell>7.831</cell><cell>9.669</cell><cell>5.174</cell><cell>11.833</cell><cell>44.923</cell><cell>4.465</cell><cell>13.090</cell><cell>38.602</cell></row><row><cell>11</cell><cell>110101</cell><cell>1</cell><cell>0.246</cell><cell>0.357</cell><cell>0.509</cell><cell>0.121</cell><cell>0.619</cell><cell>4.927</cell><cell>0.263</cell><cell>0.794</cell><cell>3.828</cell></row><row><cell>12</cell><cell>110100</cell><cell>9</cell><cell>8.188</cell><cell>9.518</cell><cell>11.848</cell><cell>3.874</cell><cell>11.461</cell><cell>45.196</cell><cell>4.783</cell><cell>13.902</cell><cell>42.832</cell></row><row><cell>13</cell><cell>110011</cell><cell>7</cell><cell>1.721</cell><cell>2.556</cell><cell>3.427</cell><cell>1.243</cell><cell>4.902</cell><cell>20.107</cell><cell>1.478</cell><cell>4.284</cell><cell>16.198</cell></row><row><cell>14</cell><cell>110010</cell><cell>45</cell><cell>56.218</cell><cell>68.057</cell><cell>76.816</cell><cell>39.428</cell><cell>89.492</cell><cell>227.409</cell><cell>33.747</cell><cell>80.826</cell><cell>149.755</cell></row><row><cell>15</cell><cell>110001</cell><cell>2</cell><cell>2.220</cell><cell>3.086</cell><cell>4.035</cell><cell>1.198</cell><cell>4.844</cell><cell>18.233</cell><cell>1.495</cell><cell>5.245</cell><cell>19.813</cell></row><row><cell>16</cell><cell>110000</cell><cell>64</cell><cell>77.743</cell><cell>82.908</cell><cell>94.274</cell><cell>36.453</cell><cell>83.855</cell><cell>203.724</cell><cell>38.321</cell><cell>89.130</cell><cell>174.093</cell></row><row><cell>17</cell><cell>101111</cell><cell>3</cell><cell>0.071</cell><cell>0.111</cell><cell>0.176</cell><cell>0.045</cell><cell>0.220</cell><cell>1.527</cell><cell>0.053</cell><cell>0.252</cell><cell>1.727</cell></row><row><cell>18</cell><cell>101110</cell><cell>5</cell><cell>2.090</cell><cell>2.957</cell><cell>3.983</cell><cell>0.807</cell><cell>4.139</cell><cell>19.556</cell><cell>0.944</cell><cell>4.334</cell><cell>25.004</cell></row><row><cell>19</cell><cell>101101</cell><cell>0</cell><cell>0.089</cell><cell>0.139</cell><cell>0.212</cell><cell>0.040</cell><cell>0.195</cell><cell>1.360</cell><cell>0.057</cell><cell>0.264</cell><cell>1.962</cell></row><row><cell>20</cell><cell>101100</cell><cell>4</cell><cell>2.947</cell><cell>3.648</cell><cell>4.577</cell><cell>0.751</cell><cell>3.936</cell><cell>20.847</cell><cell>1.097</cell><cell>4.850</cell><cell>20.581</cell></row><row><cell>21</cell><cell>101011</cell><cell>4</cell><cell>0.631</cell><cell>0.973</cell><cell>1.431</cell><cell>0.253</cell><cell>1.671</cell><cell>7.866</cell><cell>0.400</cell><cell>1.569</cell><cell>8.107</cell></row><row><cell>22</cell><cell>101010</cell><cell>25</cell><cell>20.300</cell><cell>25.408</cell><cell>32.607</cell><cell>6.517</cell><cell>27.677</cell><cell>91.151</cell><cell>9.312</cell><cell>26.298</cell><cell>123.853</cell></row><row><cell>23</cell><cell>101001</cell><cell>1</cell><cell>0.825</cell><cell>1.210</cell><cell>1.782</cell><cell>0.371</cell><cell>1.481</cell><cell>7.023</cell><cell>0.423</cell><cell>1.669</cell><cell>9.798</cell></row><row><cell>24</cell><cell>101000</cell><cell>25</cell><cell>28.132</cell><cell>31.949</cell><cell>36.674</cell><cell>5.151</cell><cell>27.985</cell><cell>109.350</cell><cell>11.617</cell><cell>28.970</cell><cell>106.308</cell></row><row><cell>25</cell><cell>100111</cell><cell>2</cell><cell>0.681</cell><cell>0.971</cell><cell>1.415</cell><cell>0.412</cell><cell>1.388</cell><cell>11.372</cell><cell>0.539</cell><cell>1.893</cell><cell>8.642</cell></row><row><cell>26</cell><cell>100110</cell><cell>20</cell><cell>21.908</cell><cell>26.016</cell><cell>31.929</cell><cell>8.630</cell><cell>24.931</cell><cell>92.002</cell><cell>13.482</cell><cell>32.703</cell><cell>98.609</cell></row><row><cell>27</cell><cell>100101</cell><cell>4</cell><cell>0.883</cell><cell>1.176</cell><cell>1.710</cell><cell>0.343</cell><cell>1.290</cell><cell>8.354</cell><cell>0.545</cell><cell>2.091</cell><cell>9.668</cell></row><row><cell>28</cell><cell>100100</cell><cell>19</cell><cell>28.950</cell><cell>32.011</cell><cell>37.390</cell><cell>6.627</cell><cell>24.896</cell><cell>115.699</cell><cell>12.236</cell><cell>35.924</cell><cell>129.168</cell></row><row><cell>29</cell><cell>100011</cell><cell>9</cell><cell>6.364</cell><cell>8.431</cell><cell>11.262</cell><cell>3.053</cell><cell>10.262</cell><cell>41.608</cell><cell>3.704</cell><cell>12.416</cell><cell>38.137</cell></row><row><cell>30</cell><cell>100010</cell><cell>174</cell><cell>206.952</cell><cell>225.715</cell><cell>244.697</cell><cell>89.316</cell><cell>194.995</cell><cell>394.634</cell><cell>102.703</cell><cell>209.021</cell><cell>379.458</cell></row><row><cell>31</cell><cell>100001</cell><cell>8</cell><cell>7.843</cell><cell>10.398</cell><cell>14.566</cell><cell>2.678</cell><cell>9.751</cell><cell>30.043</cell><cell>4.200</cell><cell>13.789</cell><cell>42.763</cell></row><row><cell>32</cell><cell>100000</cell><cell>340</cell><cell>259.575</cell><cell>279.628</cell><cell>303.729</cell><cell>47.639</cell><cell>186.828</cell><cell>347.876</cell><cell>197.822</cell><cell>290.697</cell><cell>378.992</cell></row><row><cell>33</cell><cell>011111</cell><cell>2</cell><cell>0.024</cell><cell>0.040</cell><cell>0.059</cell><cell>0.019</cell><cell>0.115</cell><cell>0.716</cell><cell>0.017</cell><cell>0.091</cell><cell>0.435</cell></row><row><cell>34</cell><cell>011110</cell><cell>1</cell><cell>0.747</cell><cell>1.097</cell><cell>1.440</cell><cell>0.688</cell><cell>2.015</cell><cell>7.967</cell><cell>0.386</cell><cell>1.451</cell><cell>8.018</cell></row><row><cell>35</cell><cell>011101</cell><cell>0</cell><cell>0.031</cell><cell>0.050</cell><cell>0.075</cell><cell>0.014</cell><cell>0.107</cell><cell>0.703</cell><cell>0.019</cell><cell>0.096</cell><cell>0.587</cell></row><row><cell>36</cell><cell>011100</cell><cell>3</cell><cell>1.025</cell><cell>1.331</cell><cell>1.661</cell><cell>0.479</cell><cell>1.970</cell><cell>11.391</cell><cell>0.414</cell><cell>1.675</cell><cell>7.726</cell></row><row><cell>37</cell><cell>011011</cell><cell>0</cell><cell>0.217</cell><cell>0.348</cell><cell>0.512</cell><cell>0.146</cell><cell>0.822</cell><cell>3.502</cell><cell>0.101</cell><cell>0.566</cell><cell>3.300</cell></row><row><cell>38</cell><cell>011010</cell><cell>9</cell><cell>7.311</cell><cell>9.215</cell><cell>11.637</cell><cell>4.358</cell><cell>15.006</cell><cell>51.108</cell><cell>3.175</cell><cell>9.224</cell><cell>33.472</cell></row><row><cell>39</cell><cell>011001</cell><cell>0</cell><cell>0.289</cell><cell>0.424</cell><cell>0.631</cell><cell>0.176</cell><cell>0.718</cell><cell>2.900</cell><cell>0.098</cell><cell>0.600</cell><cell>4.482</cell></row><row><cell>40</cell><cell>011000</cell><cell>24</cell><cell>9.909</cell><cell>11.462</cell><cell>13.131</cell><cell>3.415</cell><cell>13.234</cell><cell>60.630</cell><cell>2.877</cell><cell>10.249</cell><cell>36.014</cell></row><row><cell>41</cell><cell>010111</cell><cell>3</cell><cell>0.235</cell><cell>0.351</cell><cell>0.495</cell><cell>0.190</cell><cell>0.752</cell><cell>4.655</cell><cell>0.229</cell><cell>0.723</cell><cell>2.877</cell></row><row><cell>42</cell><cell>010110</cell><cell>11</cell><cell>7.567</cell><cell>9.521</cell><cell>11.487</cell><cell>4.499</cell><cell>12.636</cell><cell>45.393</cell><cell>3.630</cell><cell>11.113</cell><cell>40.041</cell></row><row><cell>43</cell><cell>010101</cell><cell>2</cell><cell>0.303</cell><cell>0.432</cell><cell>0.624</cell><cell>0.147</cell><cell>0.666</cell><cell>3.678</cell><cell>0.140</cell><cell>0.799</cell><cell>3.564</cell></row><row><cell>44</cell><cell>010100</cell><cell>8</cell><cell>9.834</cell><cell>11.614</cell><cell>14.324</cell><cell>3.495</cell><cell>11.751</cell><cell>58.152</cell><cell>2.893</cell><cell>13.000</cell><cell>40.329</cell></row><row><cell>45</cell><cell>010011</cell><cell>3</cell><cell>2.199</cell><cell>3.041</cell><cell>3.941</cell><cell>1.410</cell><cell>5.082</cell><cell>22.546</cell><cell>1.281</cell><cell>4.111</cell><cell>14.191</cell></row><row><cell>46</cell><cell>010010</cell><cell>54</cell><cell>73.643</cell><cell>81.158</cell><cell>91.576</cell><cell>31.988</cell><cell>99.461</cell><cell>192.606</cell><cell>26.370</cell><cell>76.380</cell><cell>144.021</cell></row><row><cell>47</cell><cell>010001</cell><cell>2</cell><cell>2.703</cell><cell>3.684</cell><cell>5.172</cell><cell>3.182</cell><cell>5.192</cell><cell>17.191</cell><cell>1.211</cell><cell>4.632</cell><cell>19.091</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>.</cell><cell>2.5%, 50%, and 97.5% Posterior Percentiles for Model 3</cell></row><row><cell></cell><cell>Engine Locations (h 1 , h 2 )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 . Global Goodness-of-Fit for Various Models</head><label>10</label><figDesc></figDesc><table><row><cell>Model</cell><cell># Parameters</cell><cell>log (Bayes Factors)</cell></row><row><cell>Constant p</cell><cell>1</cell><cell>0</cell></row><row><cell>Different p by engine</cell><cell>6</cell><cell>816.90</cell></row><row><cell>Different p by phrase</cell><cell>20</cell><cell>19.50</cell></row><row><cell>Different p engine by phrase</cell><cell>120</cell><cell>980.28</cell></row><row><cell>Model 1</cell><cell>63</cell><cell>253.46</cell></row><row><cell>Model 2</cell><cell>140</cell><cell>1501.04</cell></row><row><cell>Model 3</cell><cell>149</cell><cell>1564.96</cell></row><row><cell cols="3">Note: Reported are log(Bayes Factor) comparing each model in turn to the</cell></row><row><cell>Constant p model.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>is only 0.0027 for and 0.002 for ␣␣.) 1</cell><cell>That</cell></row><row><cell cols="2">is, another set of 20 phrases drawn at random from our</cell></row><row><cell cols="2">marketing-phrase universe would have essentially no</cell></row></table><note>*Sum of indicators for high performance in Depth of Search, Frames Support, Image Maps, and Learns Frequency from Table</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Marketing Science/Vol. 19, No. 1, Winter 2000</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>gine locations from Table <ref type="table">9</ref>. The results for Model 3 do, however, indicate one consistent finding across search engines: The 0-5 and 6-10 link conditions move the mean phrase locations further away from the locations of the engines, decreasing the predicted probability that they are found. For the remaining cases, the results depend on the covariate and the specific engine.</p><p>A more detailed analysis for Models 2 (columns 7-9) and 3 (columns 10-12) of the 2 6 engine-hit patterns with observed counts n obs and 2.5%, 50%, 97.5% quantiles is provided in Table <ref type="table">8</ref>. We observe a significant improvement in Model 3 fit for the uniques <ref type="bibr">(patterns 32, 48, 56, 60, 62, 63)</ref> relative to Models 1 and 2. We also note that for 14 of the 15 engine pairs (excluding pattern 59), the 95% interval for Model 3 contains the observed value compared to 3 out of 15 for Model 1 and 12 out of 15 for Model 2. An estimate of the fraction of URLs not found is also obtained in pattern 64. The estimates under Model 2 (174.756/(1588 ‫ם‬ 174.756) Ϸ 10% and that for Model 3 <ref type="bibr">(192.704/(1588 ‫ם‬ 192.704</ref>) Ϸ 11% are consistent with each other and suggest that these six engines as a whole, for these 20 phrases, cover a significant proportion of the Web. A global comparison of model fit is presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Model Comparison and Cross-Validation</head><p>A global goodness-of-fit comparison was performed for each of Models 1 through 3 against the simple "strawman" independence models described earlier:</p><p>(1) constant p, (2) different p for each engine but constant across phrases, (3) different p for each phrase but constant across engines, and (4) different p for each engine and phrase. Table <ref type="table">10</ref> presents the number of parameters and the natural log of the Bayes Factor log(p(M i |Y, X)/p(M 1 |Y, X)), as described in <ref type="bibr" target="#b17">Newton and Raftery (1994)</ref>, comparing each models marginal likelihood p(M i |Y, X) to the constant p model, p(M 1 |Y, X) in turn. For the independence models, p(M i |Y, X) is evaluated at the MLE, for Models 1 through 3 p(M i | Y, X) is computed as the harmonic mean of the loglikelihood evaluated at the 7500 MCMC draws. Larger values of the Bayes factor indicate model superiority.</p><p>In the end, Model 3 is selected as superior. Interestingly, we note that Model 1 does not defeat the simple model of constant p for each engine and phrase or a different p by engine.</p><p>To assess the predictive ability of our model, we employed a version of Bayesian cross-validation <ref type="bibr" target="#b22">(Rust and Schmittlein 1985)</ref> where we dropped out in turn each of the 1588 URLs, re-estimated the model, and predicted the engine find pattern for the left out URL. To make this approach computationally feasible under a MCMC simulation structure, we employed the do not stem from hypersensitivity to model assumptions. This brings us to the second explanation: namely, that these kinds of marketing/management documents are relatively easy to locate. While we cannot prove this, it is a reasonable hypothesis. Parts of the Web are of course much more "active" than others, with respect to both availability of hyperlinks from one document to another, and the degree of use of those links. This interconnectedness is the key to a search engine's performance. Documents containing our marketing research and marketing management phrases may well be relatively active in this respect. That is, other Web documents may be particularly likely to link to the commercial sites, educational sites, or organizations' sites that contain the information. While our results do not say that Web-based marketing information providers can simply count on search engines bringing multitudes to their location, they do indicate that much of the marketing information currently on the Web can be located readily-if one uses multiple search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Limitations and Future Research</head><p>This study is limited in that it used six specific search engines (the ones discussed most often in the popular press and examined in other systematic studies) during one specific time period (October 1998) to search for Web pages containing each of 20 specific marketing/management phrases (obtained by surveying common marketing reference sources). In addition, our analysis treats each Web page containing the search phrase as fully and equally valued, i.e., we do not judgmentally assess how "good" a page is (for an unspecified search purpose). To be sure, we are skeptical of attempts to do this assessment. In this area, we essentially assume that the searcher is able to articulate what is in fact being sought. Accordingly, we also do not evaluate the heuristics used by search engines to rank URLs reported in a search.</p><p>Changing any of these study design elements may materially affect the empirical results. We note in particular that the relative performance of search engines has been observed to vary over time <ref type="bibr" target="#b16">(Lawrence and Giles 1999)</ref>. We are less concerned about selection of the search phrases because search phrase locations did not vary substantially across the 20 examined here. Our investigation of the role played by the search phrase characteristics and search engine characteristics is limited by judgmental coding of the former and the need to rely on nonproprietary factors for the latter. The study found significant effects for each despite these limitations.</p><p>We hope that this paper has provided some useful data, and some insight, concerning use of Web search engines to find managerial information. Our proposed (and validated) spatial coverage model provides both a "snapshot summary" of the search engines vis-a-vis each other (as in Figure <ref type="figure">2</ref>), and also yields predictions regarding cumulative performance of engine combinations. We have shown that certain characteristics of search engines, search phrases, and URL locations affect the probability that a given engine will locate a given URL. Of course, the search engines themselves will evolve, and patterns of coverage and overlap can change accordingly. This evolution (and its causes) will be interesting to explore in future research. We are hopeful that our model framework will continue to provide a basis for summarizing these patterns. The marketing information base on the Web is evolvingand expanding-very rapidly. For many purposes it has (and will continue to) outstrip the ability of managed directories, lists, and the like to provide focused useful direction, or even to keep up with change. The Web search engines are well positioned to meet this challenge in the future, and currently they collectively-if not individually-can do so for the kind of marketing information examined here. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Inferences for parameters X 1 and X 2 are obtained from the marginal posterior distributions p(X |Y) ϰ p(Y|X )p(X |X )p(X )dX , and Ύ defined by the likelihood and priors given in (4) and (5). The nonconjugate likelihood and prior structure prevent closed-form inte-</p><note type="other">1</note><p>The authors thank the Special Issue editor, area editor, and three anonymous reviewers for useful suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BRADLOW AND SCHMITTLEIN The Little Engines That Could: Modeling Performance of WWW Search Engines</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive home shopping: incentives for consumers, retailers, and manufactures to participate in electronic marketplaces</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Alba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barton</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Janiszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacy</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="38" to="53" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conditional inference for multiple-choice questionnaires</title>
		<author>
			<persName><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Erling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British J. Math. Statist. Psycho</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="31" to="44" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing buyer search costs: implications for electronic marketplaces</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Bakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1676" to="1692" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">NBC puts its firepower behind snap! The Wall Street Journal</title>
		<author>
			<persName><forename type="first">Sally</forename><surname>Beatty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-09-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Bennett</surname></persName>
		</author>
		<title level="m">Dictionary of Marketing Terms</title>
				<meeting><address><addrLine>Lincolnwood, IL</addrLine></address></meeting>
		<imprint>
			<publisher>NTC Business Books</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Case influence analysis in Bayesian inference</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graphical Statist</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="314" to="331" />
			<date type="published" when="1997-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Virtual shopping: breakthrough in marketing research</title>
		<author>
			<persName><forename type="first">Ray</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Bus. Review</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="131" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">N</forename><surname>Clemente</surname></persName>
		</author>
		<title level="m">The Marketing Glossary. AMACOM (American Management Association)</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Choice map: inferring a product-market map from panel data</title>
		<author>
			<persName><forename type="first">Terry</forename><surname>Elrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="21" to="40" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Web search services in 1998: trends and challenges</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Feldman</surname></persName>
		</author>
		<ptr target="http://www.infotoday.com/searcher/jun/story2.htm#chart" />
	</analytic>
	<monogr>
		<title level="j">Searcher</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An Introduction to Probability Theory and Its Applications</title>
		<author>
			<persName><forename type="first">William</forename><surname>Feller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Illustration of Bayesian inference in normal data models using Gibbs sampling</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Racine-Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="972" to="985" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inference from iterative simulation using multiple sequences</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="457" to="511" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monte Carlo sampling methods using Markov chains and their applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Hastings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Internet and Web use in the United States: baselines for commercial development</title>
		<author>
			<persName><forename type="first">Donna</forename><forename type="middle">L</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Kalsbeek</surname></persName>
		</author>
		<author>
			<persName><surname>Novak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="50" to="68" />
			<date type="published" when="1996-07" />
			<publisher>Thomas P. Novak</publisher>
		</imprint>
	</monogr>
	<note>Comm. ACM</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting choice shares under conditions of brand interdependence</title>
		<author>
			<persName><forename type="first">Wagner</forename><forename type="middle">A</forename><surname>Kamakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajendra</surname></persName>
		</author>
		<author>
			<persName><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="420" to="434" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accessibility of information on the Web</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="109" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Approximate Bayesian inference with the weighted likelihood bootstrap</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statist. Soc. Series B</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="48" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Web search sites: metasearch sites</title>
		<author>
			<persName><surname>Pc Magazine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-12-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic Models for Some Intelligence and Attainment Tests. Nielson and Lydiche (for Danmarks Paedagogiske Institut)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British J. Math. Statist. Psych</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
	<note>An item analysis which takes individual differences into account. Part 1</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facilitating the Gibbs sampler: the Gibbs stopper and the griddy-Gibbs sampler</title>
		<author>
			<persName><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="861" to="868" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The value of purchase history data in target marketing</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="340" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Bayesian crossvalidated likelihood method for comparing alternative specifications of quantitative models</title>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">T</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Schmittlein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="40" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-engine search and comparison using the MetaCrawler</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Selberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International World Wide Web Conference</title>
				<meeting>the Fourth International World Wide Web Conference<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">195</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Web document address http://searchenginewatch. interest.com/webmasters/features.html. The Wall Street Journal. 1998. Web&apos;s vastness foils even best search engines</title>
		<author>
			<persName><forename type="first">Danny</forename><surname>Sullivan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-04-03" />
		</imprint>
	</monogr>
	<note>Search engine watch: search engines feature chart</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m">and has been with the authors 7 months for 4 revisions; processed by Greg Allenby</title>
				<imprint>
			<date type="published" when="1998-06-18" />
		</imprint>
	</monogr>
	<note>This paper was received</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
