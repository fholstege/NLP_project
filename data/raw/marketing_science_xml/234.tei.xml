<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Random Assignment Is Not Enough: Accounting for Item Selectivity in Experimental Research</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-08-16">August 16, 2016.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fred</forename><forename type="middle">M</forename><surname>Feinberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ross School of Business and Department of Statistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linda</forename><forename type="middle">Court</forename><surname>Salisbury</surname></persName>
							<email>salisbli@bc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Carroll School of Management</orgName>
								<address>
									<addrLine>Boston College, Chestnut Hill</addrLine>
									<postCode>02467</postCode>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanping</forename><surname>Ying</surname></persName>
							<email>yuanping.ying@yum.com</email>
							<affiliation key="aff2">
								<orgName type="department">Yum! Brands</orgName>
								<address>
									<postCode>75024</postCode>
									<settlement>Plano</settlement>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When Random Assignment Is Not Enough: Accounting for Item Selectivity in Experimental Research</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2016-08-16">August 16, 2016.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2016.0991</idno>
					<note type="submission">Received: December 13, 2014; accepted: February 1, 2016; Preyas Desai served as the editor-in-chief and Eric Bradlow served as associate editor for this article.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>choice models</term>
					<term>consumer behavior</term>
					<term>decision making</term>
					<term>econometric models</term>
					<term>sample selection</term>
					<term>Heckman model</term>
					<term>Markov chain Monte Carlo</term>
					<term>hierarchical Bayes</term>
					<term>variety seeking</term>
					<term>assortment size</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>E xperimental methods are critical tools in marketing, psychology, and economics to isolate the effects of key variables from vagaries intrinsic to field data. As such, they are often considered exempt from the sort of sample selectivity artifacts widely documented in empirical research, in part because participants are randomly assigned to experimental conditions. To conserve time and resources, experiments often focus on items participants have chosen or are familiar with, for example, postchoice satisfaction ratings, certain free recall tasks, or specifying consideration sets preceding brand choice. When consumer input even partially influences the items about which researchers request subsequent data, the potential for item selectivity arises. In such situations, analyses are contingent on both the choice context(s) of the experiment and the alternatives participants elect to evaluate, potentially leading to substantial item selectivity overall and to differing degrees across conditions. We examine situations in which a nonignorable "choose one of many" (polytomous) selection process limits which items offer up subsequent information, and develop methods to allow substantive results to pertain to the full set of items, not only those selected. The framework is illustrated via two experiments in which participants choose and then evaluate a frequently purchased consumer good as well as data first examined by Ratner et al. <ref type="bibr" target="#b28">[Ratner RK, Kahn BE, Kahneman D (1999)</ref> Choosing less-preferred experiences for the sake of variety. J. Consumer Res. 26(1):1-15]. Results indicate substantial item selectivity that, when corrected for, can lead to markedly different interpretations of focal variable effects, such as large effect size changes and even sign reversal. Moreover, failing to flexibly account for item selectivity across experimental conditions, even in well-designed experimental settings, can lead to inaccurate substantive inferences about consumers' evaluative criteria. We further demonstrate robustness to theoretically driven (but not overtly misspecified) selection rules and provide researchers with a simple, "two-step" exploratory procedure akin to a "control function" approach-involving just one additional variable added to standard models-to determine whether and to what degree item selectivity may be affecting their substantive results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Field research faces the formidable task of making valid inferences from samples that are often far from random. People who fill out surveys, firms that are publicly traded, and industries characterized by heavy advertising are seldom representative of the larger groups that marketers, economists, and social scientists wish to understand. Making such inferences relies on statistical methods that correct for a sample's being selected in some manner. Such methods date back to the classic articles of <ref type="bibr" target="#b34">Tobin (1958)</ref> and <ref type="bibr" target="#b17">Heckman (1979)</ref>, and have become core tools in cognate disciplines (e.g., <ref type="bibr" target="#b40">Winship and Mare 1992)</ref> as well as economics proper <ref type="bibr" target="#b18">(Heckman 1990</ref><ref type="bibr" target="#b27">, Puhani 2000</ref>.</p><p>A similar path was forged in quantitative marketing, where selectivity on which consumers participate or provide key information has been addressed in a wide variety of settings, including models accounting for panel attrition <ref type="bibr" target="#b12">(Danaher 2002)</ref> and underreporting <ref type="bibr" target="#b41">(Yang et al. 2010)</ref>, assessing long-run promotion effects using recency, frequency, and monetary (RFM) variables <ref type="bibr" target="#b1">(Anderson and Simester 2004)</ref>, of online banking <ref type="bibr">Feinberg, Salisbury, and Ying: Item Selectivity in Experimental Research Marketing Science 35(6), pp. 976-994, © 2016 INFORMS 977</ref> decisions conditional on having signed up and logged in <ref type="bibr" target="#b20">(Lambrecht et al. 2011)</ref>, for correlations in incidence and strength of online product opinions <ref type="bibr">Schweidel 2012, Ying et al. 2006)</ref> or ad impressions and visit behavior in online display advertising <ref type="bibr" target="#b5">(Braun and Moe 2013)</ref>, to interrelate content creation and purchase decisions <ref type="bibr" target="#b0">(Albuquerque et al. 2012)</ref>, and culminating in <ref type="bibr" target="#b36">Wachtel and Otter's (2013)</ref> comprehensive framework to account for multiple waves of selectivity (e.g., scoring and targeting) enacted deliberately by marketers.</p><p>Two distinct sorts of selectivity, one common and widely addressed, the other the subject of the present article, are often unwittingly conflated. The common form is where participants (consumers or respondents) self-select into "conditions," that is, those that buy a specific car, see a target film, or subscribe to a particular website. Random assignment (of participants) into experimental settings is often taken as sufficient to control for selectivity artifacts that can arise. While in many cases it does, in others it does not. A second, distinct type of selectivity can and does occur, in both field and experimental data, one that random assignment alone cannot fully overcome. It occurs when researchers observe feedback-ratings, evaluations, or any sort of contingent information-about items whose selection is even partly influenced by consumer input. In this case, then the potential for item selectivity arises. For example, if participants are asked to evaluate items with which they are most familiar (i.e., items compete with one another on familiarity for inclusion), then selectivity takes place on characteristics those items share-characteristics that can never be fully captured by researchers-potentially altering the estimated effects of experimental manipulations. Moreover, the degree of selectivity exhibited by a consumer can vary, depending on the manipulation used in the condition into which she has randomly been assigned. The point we hope to make is that random assignment alone does not ensure that experimental results are free from selectivity artifacts, so long as the information provided by consumers is in any way contingent on overt (i.e., explicitly required by the experiment) or covert (allowed in the experiment) choices they have made. In our empirical illustrations, we have limited our purview for comparability purposes to one specific but widely used paradigm, where (randomly assigned to conditions) consumers make a series of choices and then provide feedback on each. Yet the general point applies to any experiment where the participants are not required to provide an equal degree of feedback on all items used in the study, or where the items evaluated are themselves not selected randomly by the researcher. We find striking evidence of selectivity artifacts in our empirical applications, including some large changes in key effect sizes and even effect sign reversal.</p><p>The method we introduce later falls under the rubric of a nonignorable missing data mechanism: evaluative data are "missing" on items participants did not select. A review of such methods is beyond the scope of this article and have been treated extensively for survey research by, for example, <ref type="bibr">Rubin (2004, Chapter 6)</ref>. Specifically, the framework developed by <ref type="bibr" target="#b17">Heckman (1979)</ref> can be viewed as "bivariate normal stochastic censoring" (see <ref type="bibr">Little and Rubin 2002, p. 322;</ref><ref type="bibr">Enders 2010, p. 293)</ref>, where values on one latent variable ("selection" utility) determine the likelihood of observing an outcome ("prediction," e.g., postconsumption satisfaction).</p><p>A hallmark of nonignorable selectivity artifacts is culling the set based on anything related to the dependent variable over and above what is attributable to (available) covariates. As described nontechnically in the review article of <ref type="bibr">Schafer and Graham (2002, p. 151)</ref>, nonignorability stems from the data being "missing not at random" (MNAR; contrasted with "missing at random" (MAR), which is generally "ignorable"). Specifically, given covariates X and dependent variable Y, "under MAR, there could be a relationship between missingness and Y induced by their mutual relationships to X, but there must be no residual relationship between them once X is taken into account. Under MNAR, some residual dependence between missingness and Y remains after accounting for X." 1 This is rarely if ever the case in behavioral research: specifically, which items a subject chooses cannot be presumed unrelated to her eventual evaluation of those items, even after available covariate effects have been regressed out. This will be true regardless of whether subjects are randomly assigned to conditions. In our experiments, this lack of relation is provably false: in each data setting, there is significant and meaningful residual explanatory power in formally "linking" the selection and evaluation processes. <ref type="bibr" target="#b43">Zanutto and Bradlow (2006)</ref> provide an extended discussion of ignorability in marketing applications in the context of "data pruning"removing alternatives, observations, households, etc., from the data-including an example of parameter bias based on (nonignorable) selection of top brands. Using their framework, <ref type="bibr" target="#b2">Andrews and Currim (2005)</ref> conduct a detailed simulation study for scanner data applications, verifying substantial parameter bias and suggesting "best practices" for empirical research, but do not provide a statistical approach for correcting for selectivity artifacts, our main goal here.</p><p>The classic <ref type="bibr" target="#b17">Heckman (1979)</ref> model allows a researcher to use a nonrandomly selected sample of individuals to make inferences about the entire population.</p><p>Marketing Science 35(6), pp. 976-994, © 2016 INFORMS By way of analogy, our framework uses information about items consumers select from some set to make inferences about all items in that set. This distinction has been noted in educational testing: , for example, pointed out that if students can at least partly select which questions to answer on a multiquestion exam, the unanswered ones are nonignorable nonresponses; a student's success on answered questions cannot be simply extrapolated to the others. 2 <ref type="bibr">Wainer and Thissen (1994, p. 159)</ref>, in a general review of "examinee choice" in testing, highlight the difficulty of comparing performance on self-selected activities, asking, "Is your French better than my calculus?" That is, what students have not selected (or avoided) is nonignorable. We stress here and subsequently that if a researcher wishes to make inferences about selected items only-e.g., how price affects ratings of only items you purchased, as opposed to all items available-the present modeling framework may be of limited benefit. Yet, when the missing data (evaluations of items consumers did not choose) are nonignorable, our modeling framework allows researchers to establish, measure, and correct for parametric estimation artifacts stemming from item selectivity in the wider scope of understanding choice processes, consumer evaluations, and the full array of available products.</p><p>In the <ref type="bibr" target="#b17">Heckman (1979)</ref> framework, sample inclusion is binary: something is either "in or out," evaluated for inclusion irrespective of other available items. This is justified when items (or individuals) are considered on their own merits alone, e.g., having a positive net profit for being sent a catalog is not affected by how many other customers "make the cut" <ref type="bibr" target="#b8">(Bult and Wansbeek 1995)</ref>. In most marketing contexts, however, items do compete for inclusion. Consider choosing an entrée from a menu. Whether the restaurant is of high quality (many entrées are appealing) or poor, we do not choose multiple items in the first case or zero in the second, but one in each. We stress here that item selectivity can occur even when "selection" is not overt: for example, participants provide more informative evaluations for items with which they are knowledgeable or fervent. <ref type="bibr">3</ref> Such selection or screening processes are frequently a critical, if unheralded, part of experimental research in marketing and decision theory. For example, research participants are often instructed to choose from a set of alternatives before reporting relevant measures about the chosen item(s), such as satisfaction (e.g., <ref type="bibr" target="#b13">Diehl and Poynor 2010</ref><ref type="bibr" target="#b16">, Gu et al. 2013</ref><ref type="bibr" target="#b21">, Litt and Tormala 2010</ref><ref type="bibr" target="#b28">, Ratner et al. 1999</ref> or willingness to pay (e.g., <ref type="bibr">Carmon et al. 2003, Pham and</ref><ref type="bibr" target="#b26">Chang 2010)</ref>. These diverse experimental settings share one crucial point of commonality: consumer input determines which items "survive" to be reported on.</p><p>Our main goals in this article, therefore, are to introduce and explore the notion of item selectivity; to develop models more directly applicable to the type of "choose one from many" item selectivity problems typically encountered in marketing, choice theory, and empirical behavioral research; to extend these models to allow for varying degrees of selectivity across experimental conditions; to provide "exact" Bayesian methods for their estimation; to provide a simple, two-step approximation to these methods to enable exploratory data analysis (EDA) for detecting item selectivity; to present varied empirical evidence of their importance; and to demonstrate a sufficient degree of substantive robustness to the specification of the selection mechanism.</p><p>We start by briefly reviewing standard selectivity models and then showing how to extend and estimate them. The importance of carefully modeling selectivity is then demonstrated in three data settings: one a new analysis of data examined in <ref type="bibr" target="#b28">Ratner et al. (1999)</ref> and two involving postchoice satisfaction for a frequently purchased consumer good. In the online appendix (available as supplemental material at https://doi.org/10.1287/mksc.2016.0991), we use a RESET-based testing procedure <ref type="bibr" target="#b24">(Peters 2000)</ref> to rule out omitted regressor bias for our results, demonstrate the importance of accounting for observed preference heterogeneity in our model specification, and test for evidence of error covariance (finding none). We also develop an approximate method for behavioral researchers engaged in EDA, requiring only the addition of one simple new covariate to their analyses. <ref type="bibr" target="#b17">Heckman's (1979)</ref> original model for selectivity can be written as a two-equation system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary Selection and Extension to Polytomies</head><formula xml:id="formula_0">Y s = X s s + s (1) Y p = X p p + p if Y s &gt; 0 (2) s p ∼ N 0 0 1 1 (3)</formula><p>The outcome or "prediction" variable, Y p , is observed only in cases where the (binary) selection variable, Y s , </p><formula xml:id="formula_1">covariates X s interval a Y p covariates X p 1 2 1 X s 1 1 2 9 X p 1 1 2 1 2 0 X s 1 2 2 2 3 1 X s 2 1 3 6 X p 2 1 3 2 3 0 X s 2 2 3 2 3 0 X s 2 3 3 a</formula><p>The dependent measure in the prediction submodel, Y p , is interval in our applications, although extensions to ordinal and other dependent variable (DV) types are possible with relatively minor adjustments (e.g., <ref type="bibr" target="#b42">Ying et al. 2006)</ref>.</p><p>is positive. Selectivity is absent when the correlation, , is near 0. Given regressors X s X p , the system (1)-( <ref type="formula">3</ref>) is ordinarily estimated by maximum likelihood techniques, or "two-step" estimation approaches, which can be inaccurate for large <ref type="bibr" target="#b27">(Puhani 2000)</ref> and sensitive to specification of X s X p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation, Model Likelihood, and Estimation</head><p>Let us first consider data for two specific respondents, whose choice sets may differ in composition, size, experimental manipulation, or otherwise, as shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We denote selection and prediction estimates for respondent r as V s r i k r = X s r i k r s and V p r i k r = X p r i k r p , respectively. The r subscript can be suppressed for clarity, and it is convenient to number the item chosen (i.e., selected) by each respondent as 1 k r or simply as 1. We therefore observe secondstage values only for i = 1, that is, associated with deterministic utility portion V p 1 k r or V p 1 .</p><p>Given the joint error distribution, s p ∼ N 0 0 1 1 , the joint density for a particular observation (that is, suppressing k r ) is 4</p><formula xml:id="formula_2">P V s 1 + s 1 &gt; V s i + s i i&gt;1 and Y p = V p 1 + p 1 (4)</formula><p>This yields a "mixed" likelihood, where selection is a discrete probability mass function and prediction a continuous probability density function, for which is an estimated dispersion parameter. If s i are multinormal with zero mean and identity covariance, 5 (4) can be evaluated by isolating s 1 , decomposing p 1 = s 1 +¯ z, for z a standard normal draw and 2 = 1 − 2 . We can therefore rewrite (4)</p><formula xml:id="formula_3">P s 1 &gt; V s i − V s 1 + s i i&gt;1 and Y p = V p 1 + s 1 +¯ z 4</formula><p>We will eventually allow to vary by experimental condition, but leave it unsubscripted here. For conciseness, we use x ≥ y i to mean x ≥ max i y i . <ref type="bibr">5</ref> We shall test this empirically for our data, finding support in all three experiments (see the online appendix).</p><p>This can be further simplified by fixing = s 1 , so that</p><formula xml:id="formula_4">P &gt; V s i −V s 1 + s i i&gt;1 and z = Y p −V p 1 − ¯</formula><p>cleaves into two probabilistic statements: one about s i and one about z, all of which are standard normal by construction. We can therefore simply integrate across to complete the likelihood statement</p><formula xml:id="formula_5">∈R i&gt;1 − V s i − V s 1 • Y p − V p 1 − ¯ d<label>(5)</label></formula><p>When = 0, the portion of the integrand in (5) from the prediction model reduces to Y p − V p 1 / ; this no longer depends on = s 1 and can therefore come outside the integral. In other words, when selection is "ignorable," the likelihood factors into its selection and prediction component parts.</p><p>We estimate all parameters in the likelihood built up from (5)-</p><p>, and the coefficients within V s and V pacross observations and participants, using both standard classical (e.g., gradient search, quadrature) and Bayesian (Markov chain Monte Carlo (MCMC)) methods, which agreed closely in all cases. Because some of our key tests will involve bounded parameters like , which cannot have a limiting normal or t density, we report Bayesian highest density regions (HDRs) for such quantities. Bayesian estimates are based on a burn-in of 20,000 iterations, inference on an additional 20,000, and convergence checked by trace plots and typical diagnostics (e.g., Gelman-Rubin, Geweke) using multiple chains. Model comparisons (and significance levels) are carried out for nested models and parametric restrictions via likelihood ratio tests, and for nonnested models using Deviance Information Criterion (DIC) <ref type="bibr" target="#b33">(Spiegelhalter et al. 2002)</ref>. We report posterior means for all parameters using highly diffuse conjugate priors for all parameters but , which has a flat prior.</p><p>Our data are typical of many behavioral studies, where the number of observations per participant can be far fewer than potential explanatory covariates, particularly when interaction effects are incorporated. Because we have few choices per participant (only 3 each in Studies 2 and 3), particularly so compared with potentially heterogeneous coefficients (e.g., over 20 in Study 3), as per <ref type="bibr" target="#b3">Andrews et al. (2008)</ref> we do not attempt to recover "unobserved" heterogeneity. <ref type="bibr">6</ref> Instead, as we stress throughout and test for, it is critical to account for "observed" heterogeneity, which in all three data sets takes the form of "prior ratings" (and related measures like self-stated "favorite") for each item by each participant; results will consistently show that these prior ratings are by far the statistically strongest effects of all measured.</p><p>We note that Bayesian estimation of the model can be slow, and searching the model space in this way, which we take up later, may not be feasible for behavioral researchers focused on theory testing or simply exploring their data. To that end, we develop a "two-step" approximation method that allows for such exploration and involves adding a new covariate in the prediction model, X = −1 exp −p 1 , where p 1 is the (logit or probit) probability for the chosen item from the selection model. This merely involves running the selection and prediction models separately, in order, using standard software; details appear in the appendix along with derivations and a comparison of results versus the "correct" estimates from the empirical applications in the online appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Applications</head><p>We illustrate the importance of accounting for item selectivity by applying models accounting for it to data from three studies. A critical commonality across the three studies is the random assignment of participants to experimental conditions, so any observed selectivity artifacts cannot stem from this assignment alone. However, in all experimental conditions, participants will choose items, which they will subsequently evaluate. If differences in experimental conditions thereby influence selection, the existence and degree of item selectivity can systematically vary across conditions. For example, characteristics of the choice set from which research participants choose-such as the number of available alternatives or the relative attractiveness of alternatives-likely influence selection and, consequently, degree of item selectivity. We examine effects of choice set composition on item selectivity in our applications, allowing for the possibility that item selectivity varies across conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of the Three Studies</head><p>Our first study is a reanalysis of data from <ref type="bibr">Ratner et al. (1999, Experiment 5)</ref>, which suggested that when the number of available alternatives increases, consumers choose more varied sequences of items "for the sake of variety" rather than choosing items that are more preferred a priori. Other researchers have associated larger assortments with higher product expectations and lower evaluations of chosen items <ref type="bibr" target="#b19">(Iyengar and Lepper 2000</ref><ref type="bibr" target="#b6">, Broniarczyk 2008</ref><ref type="bibr" target="#b13">, Diehl and Poynor 2010</ref>. We test for item selectivity and explore whether the degree of selectivity varies by choice set size condition. Study 2 explores a similar question about differing selectivity by choice set size, but for a different well-known and well-documented choice contextthe choice task typically employed to examine the socalled diversification bias <ref type="bibr">(Simonson 1990, Read and</ref><ref type="bibr" target="#b29">Loewenstein 1995)</ref>. Study 3 explores whether varying attractiveness of options in the choice set, without increasing the number of alternatives, can also impact the degree of item selectivity across conditions. We present each participant with a choice set of the same size, individualized to contain her most-and leastfavored items, and manipulate the relative attractiveness, the "bunchiness" of the internal items (as defined subsequently). A summary of the commonalities and differences across the studies is shown in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparison and Theory-Based Specification of the Selection Process</head><p>For each of our three data sets, we will refer to a particular model as "best." This designation is based on a near-exhaustive search of the model space that included all possible combinations of available covariates theoretically relevant to the variety-seeking contexts we examine: a priori item rating, choice lag, choice frequency, time since last chosen, a binary indicator of which item is most preferred a priori, and indicator variables representing the experimental choice set conditions in each study. <ref type="bibr">7</ref> For the prediction submodels, which are linear regressions, we run all possible combinations of covariates; for the selection submodels, we begin with a standard stepwise probit, run both "forward" and "backward," as well as Least Angle Regression (LARS) procedures <ref type="bibr" target="#b14">(Efron et al. 2004</ref>). When the best-fitting prediction and selection submodels are determined, we estimate the conjoined (full) model including selectivity and all "nearby" models (i.e., adding in/subtracting out covariates one at a time) with the crucial error correlation parameter(s) (values of in each condition) freely estimated; that is, the resulting "best" model is such that (1) all its covariates are significant, (2) no excluded covariates are significant, and (3) if a higher-order interaction appears, all lower-order interactions (significant or not) for those same covariates do as well, to allow for appropriate interpretation of higher-order effects. (As such, for example, we do not include all possible two-way interaction effects, only those identified as significantly improving fit.) Before any models are estimated, we standardize all covariates, except for binary variables, which are mean centered, to aid in the estimation and interpretation of models with interaction terms. In a later section, we examine the robustness of our estimated effects to the selection submodel specification.</p><p>In comparing models in any of our data settings, we stress one point: the prediction submodel does not itself change. Rather, our main concern is how the presence of and type of error correlation ( ) codified by the selection model affect deductions (from the prediction model); that is, our focus is on the proper specification, and informativeness, of the selection model. Across the three data sets, we will address two questions primarily: Is there evidence of item selectivity overall? Is the degree of item selectivity substantially different across experimental conditions? We also examine whether, and how, estimated effects are impacted by two common procedures: failing to account for selectivity at all and specifying a theorydriven account of the selection process (as opposed to searching for the "best" model). Findings indicate that substantive results can be strongly altered by the former (failing to account for selectivity), but are moderately (but not "infinitely") robust to the latter.</p><p>The key primary check in all three studies will be on the significance of , the error correlation. Although the literature on selectivity offers many discussions of the "meaning" of , it cautions researchers against pinning specific interpretations onto it, as it can arise from several sources (including omitted regressors, which we test for in the online appendix). It is, however, uncontroversial regarding the effects of omitting specific hypotheses require testing, the proposed model need be run only for each hypothesized relationship. They can also first avail of the "two-step" procedure detailed in the appendix.</p><p>this correlation: when the selection process and evaluation process are based on similar underlying criteria that cannot be fully accounted for via covariates-such as finding superior options for purchase-we would expect a positive value of . This is what we find in each of our studies, and we also document substantively relevant coefficient sign changes.</p><p>The goal across our studies is to go beyond a substantive examination of these data per se, toward demonstrating the need to statistically account for selectivity by experimental condition when the experimental design setup allows. Because we present three studies, descriptions of the experimental procedures and data are deliberately concise, allowing more emphasis on the role of item selectivity in the results proper. Full details for all studies are available from the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 1: Reexamination of Ratner, Kahn, and Kahneman Data</head><p>A number of studies (e.g., <ref type="bibr" target="#b32">Simonson 1990)</ref> suggest that consumers can behave as if varied sequencesof products or other experiences-were intrinsically superior to repetitive ones. <ref type="bibr" target="#b28">Ratner et al. (1999;</ref><ref type="bibr"></ref> henceforth, RKK) concluded that more varied sequences of popular songs resulted in diminished enjoyment during consumption (even though the authors ruled out satiation with top-ranked songs). RKK's analysis made use of established statistical methods and, because their article dealt with many topics substantively unrelated to selectivity, no Heckman-type corrections were applied (this would not have been possible in any case because their analysis was performed at the aggregate level). By contrast, we conduct our analyses at the individual item level, and so we can specify a model that allows for item selectivity and compare the analysis results with and without constraining the error correlation parameter(s), , to be zero. <ref type="bibr">8</ref> RKK (Experiment 5) studied the effects of the number of available alternatives on the selection and ratings of popular songs using a two-cell, within-subjects experimental design. <ref type="bibr">9</ref> Participants first provided a priori ratings, on a 100-point scale, for 12 popular songs that were presented to them through a computer program. Idiosyncratic preference rankings were constructed based on these initial ratings. Participants</p><p>Marketing Science 35(6), pp. 976-994, © 2016 INFORMS were next presented with either a set of three items or a set of six items. They chose, listened to, and rated the song of their choice, also on a 100-point scale; this was done 10 times. For an additional 10 occasions, participants were presented with the othersized choice set and completed similar choice and rating tasks. The {small; large} pair of choice sets from which each participant chose was constructed using prior preference rankings and randomly assigned: either ranks 1 3 6 2 4 7 10 11 12 or ranks 2 4 7 1 3 6 8 9 10 . Because these data involve two phases-choice (of which song one listens to) from sets of varied sizes and ratings (of satisfaction or liking of the chosen song)-it is well suited to the models developed here and to evaluating the substantive implications of choice-based selectivity.</p><p>Our re-examination is narrowly focused on whether limiting analysis only to items chosen by participants (selection) affects estimates related to satisfaction with one's choice (prediction). To this end, we predict posterior ratings (Rating) using prior ratings (Prior Rating), how frequently an item was chosen in past occasions (Frequency), whether the song was chosen last time (Choice Lag), and the size of the choice set (Set Size, either three or six songs), while accounting for selectivity effects arising from the choice process by accounting for its Prior Rating, how often it was chosen (Frequency), and again whether the song was chosen last time (Choice Lag). 10 These regressors were drawn from among those examined by RKK, and the Frequency and Choice Lag covariates were also included because they have long been examined in the varietyseeking literature (e.g., <ref type="bibr" target="#b35">Van Trijp et al. 1996)</ref>. Model estimates are given in Table <ref type="table" target="#tab_2">3</ref>.</p><p>We do not engage in a substantive reinterpretation of the very rich RKK data from this experiment, nor link it to the numerous companion studies in that article. We can, however, consider model implications strictly from the vantage point of item selectivity, which are indicated for these data, as we discuss next. In a later section, we examine substantive findings in terms of robustness to the specification of the selection submodel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We discuss three models, in order of their appearance in Table <ref type="table" target="#tab_2">3:</ref> (1) assuming no selectivity (restricting = 0); (2) allowing for an identical degree of selectivity across conditions (free ); and (3) allowing selectivity to vary across choice set conditions ( SetSize . In this way, one can "decompose" the influences of the various modeling constructs systematically. Because all parameters are estimated via Bayesian techniques, we assess significance via HDRs for posteriors. Thus, "significant" denotes zero lies outside a specific HDR, usually at the 0.05 level, although for convenience we list traditional means and standard errors.</p><p>Item Selectivity. The model estimates reveal an intriguing and, to our knowledge, novel pattern of selection effects across conditions. Allowing for selection, but assuming equal across (choice set size) conditions, yields a estimate that is not significantly different from zero (ˆ = 0 043, HDR = −0 289 0 332 ). This would appear to suggest there are no selection effects for these data. However, allowing to vary by condition reveals significant selectivity, but only for the larger choice set condition:ˆ Small = −0 125, HDR = −0 404 0 138 ;ˆ Large = 0 571, HDR = 0 353 0 727 . Thus, degree of selectivity increases with choice set size; later, we revisit this finding in light of analogous ones from Studies 2 and 3. Estimated Effects. Importantly, we also find that allowing for selectivity by condition impacts estimated effect sizes. Substantively, the pattern of effects for the selection model is of lesser interest, as coefficients across the various models (in Table <ref type="table" target="#tab_2">3</ref>) are very close; HDRs overlap to a degree that render them statistically indistinguishable. It is worth noting, however, that the estimated negative effects of Choice Lag and its interaction with Prior Rating (e.g.,ˆ S Lag = −0 481, S Lag×PrRate = −0 226, multiple ) are consistent with previous research examining variety seeking and preference (under)weighting in repeated sequential choice (RKK; <ref type="bibr" target="#b32">Simonson 1990)</ref>.</p><p>A very different pattern emerges across the prediction models. The most striking result is that the effects of Set Size are vastly different across models. When there is no selection ( = 0) or equal selection across choice set size conditions (common ), the effects of Set Size are significantly positive and not statistically distinguishable (ˆ P SetSize = 0 126, no ; P SetSize = 0 119, common ); that is, if one analyzed only the Rating (i.e., prediction) data, choice set size could be confidently claimed as positively affecting evaluation. However, when selectivity is accounted for across set size conditions ( SetSize ), we see that Set Size has a strongly negative effect (ˆ P SetSize = −0 322, p &lt; 0 003, multiple ). This is consistent with extant literature demonstrating that consumers tend to be less satisfied with items chosen from relatively larger assortments (e.g., <ref type="bibr" target="#b19">Iyengar and</ref><ref type="bibr">Lepper 2000, Diehl and</ref><ref type="bibr" target="#b13">Poynor 2010)</ref>. A posteriori, in comparing a choice set of size 3 to one of size 6 (as RKK did), chosen items are rated about one-third standard deviation lower, on average, when chosen from the larger set, net of other covariate effects. The valence of an important main effect  is therefore reversed when the Ratings data are analyzed in the absence of an associated model for choice that not only allows for selectivity, but that also does not restrict selectivity to be fixed across experimental conditions.</p><p>The interaction between Set Size and how frequently an item has been chosen (Frequency) also has strongly differing effects. When selectivity is not accounted for by condition, the interaction effect is not significantly different from zero (ˆ P Freq×SetSize = 0 026, p &gt; 0 27, = 0). However, when selectivity is allowed to vary across conditions ( SetSize , the interaction between Frequency and Set Size becomes larger and significant (ˆ P Freq×SetSize = 0 099, p &lt; 0 05). In other words, frequently selecting the same item from a larger set is a stronger indicator of enjoyment, compared to repeated choice in a smaller set. The model without selection ( = 0) does not suggest this more nuanced insight into repeated choice.</p><p>Model Fit. Finally, one is left with the question of which model represents the data best, which can be assessed via both Bayesian and classical metrics. DIC speaks clearly for multiple , the values of which, for {no , common , and multiple }, are <ref type="bibr">{4,253, 4,255, 4,244}. 11</ref> Likelihood ratio tests corroborate the DIC comparison, while allowing statistical tests for nested models (like those in Table <ref type="table" target="#tab_2">3</ref>): the model allowing selection to vary by choice set size (multiple ) offers a better fit for the data than both the model with no selection (no ; difference in log likelihood (LL diff = 6 58, df = 2, p &lt; 0 002) and the model restricting selection to be equal across set size conditions (common ; LL diff = 6 51, df = 1, p &lt; 0 001). The models with no selection versus equal selection across set size conditions exhibit no difference in fit (no versus common ; LL diff = 0 07, df = 1, p &gt; 0 7); this is consistent with the nonsignificance of when it is <ref type="bibr">11</ref>  restricted to be constant across experimental conditions. The slightly less parsimonious model thus more than compensates for its additional complexity, and allowing the correlation between selection and prediction to differ across conditions not only improves fit, but affects interpretation of focal substantive effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2: Choice Set Size</head><p>The previous analysis, based on data collected in a classic prior experimental study, demonstrated the potential for selection effect strength to vary across experimental conditions with differing choice set sizes.</p><p>Study 2 was designed and conducted with an explicit goal in mind: to examine whether the same potential exists when the time between each item selection is much greater than that examined in Study 1. To do so, we examine another well-known repeated choice phenomenon widely documented in the marketing and psychology literatures. Numerous prior studies have observed that people choosing multiple items at once now, to consume later, tend to choose a more varied set of items than if they had chosen the items one at a time, just before consuming each one (e.g., <ref type="bibr">Simonson 1990, Read and</ref><ref type="bibr" target="#b29">Loewenstein 1995)</ref>. While prior research has focused on the impact of these two choice modes on variety seeking, we will instead focus our analysis on the influence of the size of the available choice set on evaluation of chosen items and potential selectivity artifacts.</p><p>Participants chose three snacks, either from a set of 6 snacks or from a set of 12. Half the participants chose all three snacks at once ("simultaneous choice"); the remaining participants chose each snack one at a time across three choice occasions ("sequential choice"). A 2 (simultaneous choice versus sequential choice) × 2 (small choice set versus large choice set) betweensubjects design was employed.</p><p>Snacks included well-known brands of crackers, chips, candy bars, cookies, and nuts. The small set condition included six snack options, and the large set condition included those six snacks plus six more. The small set condition stimuli and task replicate experiments found in <ref type="bibr" target="#b32">Simonson (1990)</ref> and <ref type="bibr" target="#b29">Read and Loewenstein (1995)</ref>. The six additional snacks in the large set were chosen to mirror the six snacks in the small choice set, in terms of both product attributes and market share, so as not to increase perceptions of attribute variety or general product desirability. One hundred four undergraduate students participated in the study to earn course credit in an introductory marketing course.</p><p>The study was composed of four sessions spaced one week apart. In Session 1, participants' prior preferences were measured; participants rated how much they liked each snack using an 11-point Likert scale</p><p>(1 = dislike very much, 11 = like very much). Participants also ranked the snacks from their most favorite to their least favorite. The choice tasks took place during Sessions 2, 3, and 4; we refer to these as Choice Weeks 1, 2, and 3, respectively. Participants in the sequential condition chose and ate one snack in Choice Week 1, chose and ate a second snack during Week 2, and chose and ate a third snack in Week 3. Participants in the simultaneous condition selected all three snacks in Choice Week 1, designating the first snack to be eaten in Choice Week 1, the second snack to be eaten in Choice Week 2, and the third snack in Choice Week 3. Immediately after participants ate each of their chosen snacks, they rated how much they liked the snack using an 11-point Likert scale (1 = dislike very much, 11 = like very much).</p><p>The snack evaluation rating measured immediately after a participant ate her chosen snack is the dependent variable, and it is observed only for the single item chosen for that time period. As in Study 1, we considered potential regressors from published research analyses examining this particular task <ref type="bibr" target="#b32">(Simonson 1990</ref>) and variety seeking in general, as well as a covariate to represent our choice set size manipulation. Regressors for the selection model (for which item is chosen) are Prior Rating (the a priori item rating), Favorite (whether the item was designated the favorite; a priori rank equals one), Choice Lag (whether the item had been chosen in the prior time period), Choice Lag × Favorite interaction, and Choice Lag × SEQ, where SEQ represents the choice mode manipulation (equals one for sequential choice, zero for simultaneous choice). The regressors for the prediction model (for the single brand chosen) include Prior Rating, Favorite, Choice Lag, and Choice Lag × Favorite interaction as well as Set Size (equals one for the large set, zero for the small set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Item Selectivity. We find a pattern of selection effects mirroring those found in Study 1, again revealing evidence that ignoring item selectivity risks the possibility of drawing incorrect substantive conclusions. Table <ref type="table" target="#tab_5">4</ref> summarizes model estimation results in a manner similar to Study 1, featuring three candidate models that differ in how flexibly they allow for selection: no selectivity ( = 0), selectivity common across conditions (free ), and selectivity varying across choice set conditions ( SetSize ). The pattern of results in Table <ref type="table" target="#tab_5">4</ref> shows strong evidence of selectivity in the large choice set condition (ˆ Large = 0 568, HDR = 0 280 0 787 ; multiple , but not in the small choice set condition (ˆ Small = −0 273, HDR = −0 731 0 433 ; multiple ). This is consistent with the results in Study 1, and the estimated values of Small and Large are remarkably similar across the two studies. The model allowing for selectivity, but restricting to be constant    across conditions (common ), yields an estimated value that is significantly positive (ˆ = 0 413, HDR = 0 055 0 703 ). Thus, selection effects are clearly evident in the data; however, presuming that degree of selectivity is the same across all choice set conditions would be erroneous in this case, just as in Study 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimated Effects.</head><p>A key substantive implication of failing to appropriately account for selectivity is that, just as in Study 1, the estimated effect of choice set size on evaluation changes dramatically. The estimated effects of Set Size are not distinguishable from zero when there is no selection (ˆ P SetSize = 0 034, p &gt; 0 37; no ) or equal selection across choice set size conditions (ˆ P SetSize = −0 065, p &gt; 0 28; common ). However, when selectivity is accounted for across set size conditions, choice set size has a very large negative effect (ˆ P SetSize = −0 881, p &lt; 0 004; multiple ). Again we find that when a model allowing varying degrees of selectivity across choice set conditions is employed, the results reveal that participants tend to be less satisfied with items chosen from the larger choice set. This pattern of results is remarkably concordant with Study 1 (RKK), even though that experiment was run by other researchers, using a within-subjects design and different stimuli, and with many more repetitions.</p><p>Differences in selectivity also reveal their substantive importance when comparing the estimated Favorite effect strengths between the = 0 prediction submodel (i.e., no selection effects) and its more flexible variants. "Common" selectivity yieldsˆ P Fav = 0 340 (p &lt; 0 008; common ), a significant effect; presuming there is no selectivity yieldsˆ P Fav = 0 193 (p &gt; 0 05; = 0), a nonsignificant value about half the size. Allowing selectivity to differ across conditions also yields a positive effect of most-favored status,ˆ P Fav = 0 273 (p &lt; 0 03; multiple ). Thus, allowing for item selectivity reveals a crucial role of the favorite option in evaluation: the most-favored option gets a "boost" in evaluation, over and above that accounted for by its higher prior rating. Note that a significant negative interaction between Favorite and Choice Lag is observed in all three models-ˆ P Lag×Fav = −0 675 (p &lt; 0 02) when = 0,ˆ P Lag×Fav = −0 808 (p &lt; 0 005) when is unrestricted, andˆ P Lag×Fav = −0 711 (p &lt; 0 02) when is allowed to vary across set size conditions-suggesting that, in the case where the Marketing Science 35(6), pp. 976-994, © 2016 INFORMS favorite was chosen in the prior period, the favorite item's evaluation is discounted. Thus, for these data, a modeling approach that allows for selectivity reveals that the favorite option almost always "gets a boost" in evaluation (except in the case when it was chosen last time).</p><p>Model Fit. In addition to the substantive insights gained from allowing for selectivity, we find better model fits for both models with unrestricted , as measured using DIC: 1,655, 1,652, and 1,646 for no , common , and multiple , respectively. This evidence is bolstered by likelihood ratio tests: the model with free common offers a better fit than one restricting to zero (LL diff = 2 67, df = 1, p &lt; 0 03), and the model allowing selectivity to vary across choice set conditions fits the data better than both one restricting to be constant across conditions (LL diff = 4 23, df = 1, p &lt; 0 005) and the model restricting to be zero (LL diff = 6 90, df = 2, p &lt; 0 002). 12 Overall, analysis of these data provides evidence that appropriately accounting for selectivity adds substantially to both model fit and interpretation of effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3: Choice Set "Bunchiness"</head><p>The prior two studies demonstrated that degree of item selectivity can vary with the number of alternatives in a choice set. This study assesses whether selection effects can vary across choice set conditions even when the number of selection alternatives stays constant. We explore this question with the same choice task employed in Study 2, and we examine the impact of varying the relative attractiveness of items in the choice set on degree of selectivity and the value of .</p><p>"Bunchiness" refers to the degree to which a choice set contains items that are perceived to be equally attractive to one another (i.e., the extent to which items are "bunched" together), from the decision maker's perspective. For example, a choice set comprising six equally attractive items or one with six equally unattractive items would be high in bunchiness. <ref type="bibr">13</ref> Note that bunchiness is a characteristic of the choice set itself, rather than of any one item in the set. We expect that bunchiness will have a negative relationship with degree of selectivity (and ) because, for experimental procedures in which participants choose the items <ref type="bibr">12</ref> Additional tests of model fit (Table <ref type="table" target="#tab_5">4</ref>, bottom) demonstrate that adding Prior Rating × Set Size to the prediction model is nonsignificant (p &gt; 0 2), while removing Set Size from the selection model significantly reduces model fit (p &lt; 0 01). <ref type="bibr">13</ref> We operationalize bunchiness here based on how attractive the items are to individuals; there may be other ways, including perceived variety in a particular set (e.g., it is possible to like all items in a set, while also finding them to have important differences). However, because we focus on the degree to which respondents may be indifferent between two randomly chosen items in the set, attractiveness seems an appropriate criterion. they evaluate, items from bunchy choice sets have similar probabilities of sample inclusion, so analyses of bunchy sets may be less prone to selection artifacts. In other words, everything in a bunch is similar in overall attractiveness, so there is not much to be gained (or lost) by the consumer's choosing one over another, suggesting at best modest item selectivity.</p><p>One hundred twenty-six participants followed a four-week procedure analogous to that in Study 2. They were asked to rate and rank 12 snacks in Week 1 (the same as those in the Study 2 large set condition). The number of available items was held constant across conditions, at six, and we manipulated choice set bunchiness. To this end, the items available in the choice set varied across three bunchiness conditions based on the idiosyncratic rankings provided by each participant: bunchy attractive (ranks 1, 2, 3, 4, 5, and 12), bunchy unattractive (ranks 1, 8, 9, 10, 11, and 12), and not bunchy <ref type="bibr">(ranks 1, 4, 6, 8, 10, and 12)</ref>. We include the bunchy-unattractive condition in the experimental design to assess whether any potential impact of bunchiness is conditional on the bunched alternatives being perceived as (more) attractive; we will allow separate measures of selectivity for all three conditions to determine whether, empirically, estimates for bunchy attractive and bunchy unattractive are close in magnitude. Note that all choice sets include both the most-favored item (rank = 1) and the leastfavored item (rank = 12), so that the range of relative attractiveness of all items in the set is consistent across conditions. The three conditions are reminiscent of the experimental design in <ref type="bibr" target="#b39">Wang et al. (1995)</ref>, where three pairs of exam questions were varied on both mean difficulty and difficulty difference. By asking test takers to indicate a preference in each pair, but still answer all six questions (so there is no "missing" evaluation data), they were able to verify that post hoc correction for selectivity is critical.</p><p>We include two binary variables to represent bunchiness: Bunchy Attractive (equals 1 for the bunchy-attractive condition, 0 for not-bunchy and bunchy-unattractive conditions) and Bunchy Unattractive (1 for bunchy-unattractive, 0 for not-bunchy and bunchyattractive conditions). The selection submodel regressors are a priori rating (Prior Rating), a priori mostfavored item (Favorite), whether an item was chosen last time (Choice Lag), choice lag interacted with sequential/simultaneous choice mode (Choice Lag × SEQ), choice lag interacted with prior rating (Choice Lag × Prior Rating), a priori rating interacted with choice mode (Prior Rating × SEQ), and most-favored item interacted with the two choice set condition indicator variables (Bunchy {Attractive, Unattractive} × Favorite). Prediction submodel regressors are Prior Rating, Favorite, Choice Lag, SEQ, Bunchy {Attractive, Unattractive} (i.e., two main effects), Prior Rating × SEQ,    Choice Lag × SEQ, Bunchy {Attractive, Unattractive} × Prior Rating, and Bunchy {Attractive, Unattractive} × Favorite. As in the two previous studies, we standardize all variables, except binary (dummy) variables, which are mean centered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Item Selectivity. We again find strong evidence of selectivity, this time for all choice set conditions, with the value of systematically varying with bunchiness. Table <ref type="table" target="#tab_8">5</ref> presents estimation results for three models differing in how flexibly selectivity is modeled: no selection ( = 0), restricting selectivity to be constant across choice set conditions (common ), and allowing selectivity to differ across choice set conditions (multiple ; Bunchiness ). Restrict-ing to be constant across conditions yields an estimated value that is significantly positive (ˆ = 0 543, HDR = 0 190 0 788 ; common ). Allowing to differ across conditions ( Bunchiness ) reveals that degree of selectivity varies with bunchiness: the not-bunchy choice set produced the greatest degree of selectivity (ˆ NotBunchy = 0 835, HDR = 0 578 0 959 ), while the bunchy-attractive and bunchy-unattractive choice sets generated lower, and nearly identical, degrees of selectivity (ˆ BunchyAttr = 0 450, HDR = 0 028 0 740 ; BunchyUnattr = 0 470, HDR = 0 040 0 823 ). Consistent with both Studies 1 and 2, we find clear evidence of selection effects in the data, but they vary in degree across choice set conditions, in this case, without varying choice set size; decreasing the degree to which available options are perceived as equally attractive</p><p>Marketing Science 35(6), pp. 976-994, © 2016 INFORMS (i.e., a "less bunchy" choice set) increases degree of item selectivity. Estimated Effects. Comparing the prediction submodels with increasing flexibility of accounting for selection reveals striking differences in several of the prediction covariates. First, consistent with our finding in Study 2, the estimated effect of Favorite doubles in size when selectivity is accounted for in the model (ˆ p Fav = 0 315, p &lt; 0 005 when = 0;ˆ p Fav = 0 601, p &lt; 0 001 with common ;ˆ p Fav = 0 640, p &lt; 0 001 with multiple ). Similarly, the negative interaction effect of Bunchy Attractive × Favorite without selectivity (ˆ P BunchyAttr×Fav = −0 721, p &lt; 0 003, = 0) grows increasingly negative as selectivity is introduced into the model (ˆ P BunchyAttr×Fav = −0 905, p &lt; 0 001, common ) and then allowed to vary across conditions (ˆ P BunchyAttr×Fav = −1 093, p &lt; 0 001, multiple ). The combined result of these two covariate estimate changes is that failing to account for selectivity across conditions would lead a researcher to underestimate the size of the postchoice evaluation "boost" for mostfavored options in the bunchy-unattractive and notbunchy set conditions.</p><p>Second, when selectivity is restricted to be zero ( = 0) or common across choice set conditions (free ), Bunchy Unattractive has a negative effect on evaluation (ˆ P BunchyUnattr = −0 407, p &lt; 0 001 when = 0; P BunchyUnattr = −0 390, p &lt; 0 001 with common ). However, when is allowed to vary across conditions, the estimated effect of Bunchy Unattractive shrinks dramatically to nonsignificance (ˆ P BunchyUnattr = −0 093, p &gt; 0 32 with multiple ). Similar to Studies 1 and 2, we find a stark change in the estimated effect of choice set condition on evaluation, but with one critical twist: in Studies 1 and 2, modeling selectivity (across conditions) led to enhanced effects or overt sign reversals, but here, an effect that would otherwise appear significant recedes to nonsignificance. This highlights that modeling selectivity does not always lead to a particular conclusion-e.g., effects get stronger or reversebut rather depends on the nature of the data and bestfitting model.</p><p>Third, we find that the estimated effect of Choice Lag × SEQ without selection (ˆ P Lag×SEQ = 0 279, p &gt; 0 08; = 0) nearly doubles and becomes statistically significant when selectivity is accounted for in the model (ˆ P Lag×SEQ = 0 488, p &lt; 0 02 with common ; P Lag×SEQ = 0 536, p &lt; 0 008 with multiple ). Thus, for these data, failing to account for selectivity would lead the analyst to erroneously conclude that inertial choices have no distinct effect on evaluation, when they do, even after accounting for prior preference rating (Prior Rating).</p><p>Model Fit. Finally, we assess model fit and find that it consistently improves as selection is more flexibly accounted for in the model. Model fit, as measured using DIC, improves when is assumed constant across choice set conditions (common ; 1,885 versus 1,892), and improves further when is allowed to vary across choice set conditions (multiple ; 1,883 versus 1,885). Likelihood ratio tests also indicate that model fit improves: presuming common improves fit versus restricting to zero (LL diff = 5 01, df = 1, p &lt; 0 003), allowing to vary across choice set conditions improves fit versus presuming zero (LL diff = 8 00, df = 2, p &lt; 0 002), and allowing to vary across choice set conditions marginally improves model fit versus presuming common (LL diff = 2 99, df = 1, p ≈ 0 05). <ref type="bibr">14</ref> In conclusion, the findings from this study offer further support for the importance of accounting for selectivity by experimental condition (when warranted) as well as its potential impact on both model fit and the substantive interpretation of effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common Findings Across Studies 1-3 and Using the Model in Practice</head><p>We find evidence for and substantive implications of item selectivity across all three studies. First, across the board, the key footprint of item selectivity-a significant value of in at least one condition-is observed. Second, assuming the degree of selectivity is the same in all conditions is strongly rejected: in Studies 1 and 2, assuming is constant across conditions leads to poorer fit and substantively altered results, and even in Study 3, when all conditions show significant positive , there is evidence of being higher in the "not-bunchy" condition. Last, in all cases, allowing for item selectivity alters the substantive interpretation of the study in question, strikingly so in the case of Set Size for Studies 1 and 2, where the selectivitybased model indicates very significantly diminished evaluations in the larger set. By contrast, the results of Studies 2 and 3 show enhanced effects of the mostfavored option on evaluation when selectivity is fully accounted for in the analysis.</p><p>One element uniting our results is that the conditions showing the greatest selectivity in all three studies are those with arguably the greatest "informativeness." For example, the larger sets in Studies 1 and 2 contain more items, and therefore more information on which consumers can base their judgments; note that in RKK, there is also a greater variation in the judged quality of the items available. In Study 3, the nonbunchy condition is the one lacking a large "lump" of similarly rated items, and therefore presents a more diverse collection of item differences to judge in selection.</p><p>Having detected substantive artifacts associated with item selectivity in each of the three studies, we feel justified in advocating its being explicitly accounted for by behavioral researchers. However, we recognize two practical impediments to doing so: the complex and time-consuming nature of estimation (for even one model) and the need to explore a very large model space for most applications. We address these in turn, providing practical solutions explored further in the appendix and the online appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to Selection Model Specification</head><p>As mentioned earlier, researchers often have provisional theories they wish to test, as opposed to engaging in an automated, exhaustive search of the model space. Because this article is about carefully modeling selection, one might reasonably question whether the results here are "robust" (versus "brittle") with regard to the specification of the selection model. In one obvious sense, no: in each case, failing to allow for selectivity fit the data worse and altered a substantively important measurement. However, what if researchers had used theory to guide them and prespecified the selection model, rather than searching widely for the best-fitting one? For each study, we examine this issue, as well as examine what happens if a covariate of known substantive importance-Prior Rating in the selection model-is deliberately left out.</p><p>The choice of available covariates in each of the three studies was guided by prior/anticipated findings, although in each case there were additional covariates collected as controls, confluence with other research, etc., that were not directly theoretically implicated; this is especially so for the large number of potential interaction terms. For each data set, we examine what a substantively oriented researcher might positbased solely on prior findings-to be a strongly plausible selection model, to assess robustness (of findings in the prediction model) to not searching the entire model space. We also assess how far this can be "stretched" by deliberately leaving out a common covariate of known importance to selection: Prior Rating. Table <ref type="table">A</ref>.2 summarizes our approach and key results, as described here. (Full estimation results for all models appear in the online appendix.) Study 1 (RKK). While alternating choices (e.g., variety) and repeated consumption of an item have each been examined separately as drivers of varied choice behavior, their interaction (i.e., Choice Lag × Frequency) has not. Doing so (model M1a) entails a significant reduction in model fit (LL diff = 35 1, df diff = 1, p &lt; 0 001) but no substantive difference in estimates of (by condition) or prediction covariate effects. RKK further ruled out satiation as an explanation for their findings (in their Experiment 5); model M1b excludes Frequency with similar results: far worse overall fit (LL diff = 56 4, df diff = 2, p &lt; 0 001; model M1b), but no substantive differences in (by condition) or covariates. The same is not true if Prior Rating is excluded: model fit is far worse (LL diff = 80 084, df diff = 1, p &lt; 0 001; model M1c), but there are substantive differences in both ( Small = −0 448 is significantly negative) and covariate effects (most notably, Set Size erroneously doubles). We next replicate this discussion for Studies 2 and 3.</p><p>Study 2. Prior diversification bias theory <ref type="bibr" target="#b29">(Read and Loewenstein 1995)</ref> suggests consumers' a priori favorite item may influence choice, but does not suggest it interacts with switching behavior. Removing Choice Lag × Favorite from the selection model degrades fit (LL diff = 3 511, df diff = 1, p &lt; 0 01; M2a), but entails no substantive differences in by condition, nor in substantive prediction covariate effects. Removing Prior Rating, however, causes both loss of fit (LL diff = 69 6, df diff = 1, p &lt; 0 001; M2b) and two important substantive changes in the prediction model: Set Size and Favorite both become nonsignificant (and Favorite becomes three times more important in the selection model, standing in for Prior Ratings). Study 3. Although <ref type="bibr" target="#b39">Wang et al. (1995)</ref> examined a type of "bunchiness" for question pairs in scholastic testing, no prior research has focused on it as a choice predictor. So it is natural to consider dropping bunchiness condition dummies, which we include as interaction effects with the favorite item in the selection model. Doing so leads to worse fit (LL diff = 3 157, df diff = 2, p &lt; 0 05), with neither substantive differences in (by bunchiness condition) nor in prediction coefficients. The same is true for further dropping Favorite: poorer fit (L diff = 11 388, df diff = 3, p &lt; 0 001) and similar values of both and prediction coefficients. However, dropping Prior Rating (LL diff = 49 4, df diff = 1, p &lt; 0 001) causes to recede to nonsignificance in both the Bunchy Attractive and Unattractive conditions, and the theoretically implicated Choice Lag × SEQ interaction to become nonsignificant as well.</p><p>In conclusion, all three data sets tell an identical story: substituting a theoretically driven selection model specification for the "best" one arrived by exhaustive search did degrade overall model fit, but had no real substantive implications for focal effects ( by condition, prediction coefficients). This robustness was not boundless: doing the same with Prior Rating led not only to losses in overall fit, but important substantive differences in values and focal prediction coefficients. We conclude that although researchers need not always search the entire model space as we have here,</p><p>Marketing Science 35(6), pp. 976-994, © 2016 INFORMS due diligence in specifying covariates included in the selection model is likely to secure similar substantive conclusions in the prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-</head><p>Step Approach for EDA Estimating the model using Bayesian tools can be slow, and of course requires a familiarity with interpreting MCMC output. One might reasonably ask, "How can I determine whether my data require this additional step?" We describe (in the appendix) and both derive and test (in Tables B1-B3 in the online appendix) the performance of a simple, "two-step" procedure that broadly replicates the results of the "exact" Bayesian analysis in this article. This twostep procedure has much in common with the "control function" approach described by <ref type="bibr" target="#b25">Petrin and Train (2010)</ref>, in which a researcher wishes to estimate a choice model including endogenous covariates and so first regresses these on other available covariates (including instruments), and then includes the residuals from that regression among the covariates in the choice model. This is practicable whenever residuals in the first-stage model (for the endogenous regressors) can be directly observed, as in ordinary least squares (OLS). In our set-up and applications, the firststage model is (discrete) choice, and residuals from a choice model can only be estimated, not observed.</p><p>The gist of the procedure is running the selection (choice) model-which can be multinomial logit or probit-retaining the probabilities for the selected items, p 1 , and then creating a new covariate (for each experimental condition), X = −1 exp −p 1 . This is simply added to the prediction model along with all other covariates, and thenˆ = b/ b 2 + MSE/ 2 X is calculated for each condition, where b is the estimated coefficient of X. Table <ref type="table">A</ref>.1 summarizes the results of doing so for all three studies versus the "correct" estimates. The magnitude and significance ofˆ by condition are quite close to the correct values, with the exception of the large set size in RKK. Notably, all of the "common " cases are well within a 95% confidence interval (CI) of their correct values, although the procedure can produce CIs somewhat smaller than the "correct" Bayesian method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantifying the Informativeness of the Selection Model</head><p>It is reasonable to ask, to what degree does carefully accounting for correlations "help" in forecasting? That is, how informative is the selection model? When = 0, this is simple: the selection likelihood is just over half the total (57.4% in RKK; 57.2% in Study 2; 51.8% in Study 3). When = 0, this is complicated by the fact that the likelihood in (5) cannot be neatly cleaved into selection and prediction portions, but rather is integrated over the latent error distribution. However, standard output provides a direct answer: the residual error variance is found to beˆ 2 1 −ˆ 2 . Summing these for each observation in each condition provides a clear indication of the "informativeness" of the selection model, as a percentage reduction in mean squared error (MSE) (ˆ 2 ). These vary quite a bit by study and whether is common or multiple: 0.0% (RKK, common), 9.0% (RKK, multiple), 7.0% (Study 2, common), 8.9% (Study 2, multiple), 17.2% (Study 3, common), 23.1% (Study 3, multiple). The poor informativeness of the RKK common stems from the erroneous conclusion that ≈ 0 when it was forced to be identical across conditions.</p><p>A simpler (though only approximate) measure is to follow the two-step procedure above and look directly at the MSE reduction when the "residual" from the selection model is entered into the prediction model: respectively, 0.0% and 0.6% (RKK), 2.5% and 7.8% (Study 2), 6.5% and 9.3% (Study 3). These are proportionally consistent with the exact proportions above, and suggest that even when the selection model coefficients are not optimally adjusted through joint estimation, the reduction in MSE due to the "informativeness" of the selection model can be substantial and readily assessed via ordinary regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Potential Extensions</head><p>Model frameworks developed by <ref type="bibr" target="#b17">Heckman (1979)</ref>, <ref type="bibr" target="#b34">Tobin (1958)</ref>, and others address analysis of field data with selectivity not amenable to researcher control. Although similar corrections have been applied in field data studies in marketing, their use in laboratory experiments, which typically offer the luxury of random assignment of participants, has been tacitly seen as less pressing, or perhaps nonexistent. It is also possible that the "in or out" selectivity of the classic <ref type="bibr" target="#b17">Heckman (1979)</ref> model may have limited its applicability in choice-based behavioral research.</p><p>Our intent was to demonstrate that item selectivity can arise in a broad class of decision problems in consumer and psychological research, even when participants are randomly assigned to conditions, and to show how to account for it in a fairly general setting. Three studies-one a reanalysis of an influential, classic data set and two theory-driven experiments designed specifically for this purpose-converged on similar conclusions, namely, that selectivity effects can be substantial even in controlled, randomized laboratory studies; ignoring selectivity can alter focal substantive results, including effect sizes and even sign reversals; and allowing for different degrees of selectivity across experimental conditions can be crucial. We also show that substantive results are broadly (but boundedly) robust to a reasonable variety of selection model specifications, and provide researchers with a readily applied, "two-step" method to determine whether selectivity corrections may impact their results.</p><p>Although we have not reported on them in this article, we have successfully extended the model to different types of selection and prediction outcomes, including binary and "pick-k-of-n" selection and both ordinal (i.e., on a discrete, ordered scale) and discrete choice prediction (i.e., we observe only what was finally chosen, not any evaluation of it). <ref type="bibr">15</ref> A common example of such extensions is discussed by <ref type="bibr" target="#b36">Wachtel and Otter (2013)</ref>, where only some consumers receive a catalog, some of whom purchase, and then some of those repurchase, with selectivity at each stage. The information we receive from consumers differs by how far they are in this winnowing process, so that item selectivity can result. Researchers ignoring the individual-specific selection phase(s) may be led astray in gauging market drivers: for example, price may influence which items are eliminated early on, and may thus appear relatively unimportant if only later stages of the purchase process are analyzed.</p><p>We can envision several fruitful extensions of the basic methodology. For example, "selection" in our model requires full knowledge of available options and item covariates, which are rarely available in field data and require foresight in experimental settings. Although we underscored the importance of observed heterogeneity, the incorporation of (unobserved) parametric heterogeneity when there are few observations per consumer (relative to the number of estimated coefficients) remains a challenge in choice modeling. It is possible, for example, that such unaccounted for unobserved heterogeneity can mask, attenuate, or falsely imply selectivity artifacts. Studies 2 and 3, with just three choices per respondent, were too "shallow" to investigate this, but the RKK data did allow a preliminary investigation. Using hierarchical Bayes techniques and estimating such a heterogeneous model for all reported coefficients yielded strongly overlapping 95% HDRs for both "common " and "multiple " models, <ref type="bibr">16</ref> but obviously these limited findings cannot claim to generalize to other settings.</p><p>Among other extensions for practical model applications, one might similarly wish to rule out omitted regressors as a formal part of the model, without post hoc RESET-based procedures. Similarly, we presented robustness checks on the specific form of the selection model, but this required prior theory to determine reasonable alternatives; future work might explore an automated or disintermediated method to explore only "theoretically guided" selectivity specifications. Avoiding exhaustive search (e.g., stepwise or LARS methods on the selection and prediction submodels separately) to determine the best regressor set for the "full" conjoined model would enhance usability for practical applications; the covariate space for the conjoined model can be vast, especially when, as in our applications, interaction effects are considered. This was merely when linearity was presumed; allowing nonlinearities, in the form of power or other transforms of selection or prediction covariates, would expand the model space dramatically further. We view these as primarily issues of implementation and processing speed, and to be prohibitive mainly when researchers require a full search of the model space. The methods used to estimate the "full" model are especially attractive when multiple selection models have been run-based on theory or prior findingswhich thereby allow for greater predictive stability via Bayesian model averaging.</p><p>Researchers often employ models (without selectivity) to test specific relationships between variables, as opposed to prediction per se; as such, they may be erroneously finding support (or lack thereof) for key hypotheses because estimates critical to testing them are not corrected for item selectivity. The models presented here can be readily estimated using a variety of available software platforms with modest run times, and can be first explored using the derived two-step approximation. As such, behavioral researchers could apply them "out of the box" to determine whether item selectivity substantively altered conclusions when initial consumer choice, screening, or input was a critical feature of their experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Supplemental material to this paper is available at https:// doi.org/10.1287/mksc.2016.0991.</p><p>Marketing Science 35(6), pp. 976-994, © 2016 INFORMS possible instead to estimate (1) as a multinomial logit model, in which case the (unconditional) distribution of s is i.i.d. Gumbel, with mode 0 and mean the Euler-Mascheroni constant, ≈ 0 5772 In this case, we can ask about the mean, median, or modal value of s for the selected item. In the online appendix, we show that these have closed-form solutions, since the conditional density for error of the chosen item is again Gumbel, with location parameter ln p 1 , where the "chosen" item is denoted by subscript 1. We further show that the researcher can avail of any of the following as a new covariate, "X," in the prediction equation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean:</head><p>−1 exp −p 1 exp − Median: −1 exp −p 1 ln 2 Mode:</p><formula xml:id="formula_6">−1 exp −p 1</formula><p>These new potential covariates (compared empirically below) are available from any program estimating the (conditional) logit model, since they are simple functions of the predicted probabilities (p 1 ) for the chosen items. We also note that, because X depends only on predicted probabilities, it is also possible to use a multinomial probit model to compute p 1 ; results doing so were extremely similar to those using the logit for selection, as shown in detail in the online appendix. For all three data sets, conclusions about "common " were identical (to the fully Bayesian approach) using the two-step method; they were nearly so when varied by condition, failing only for the large set size in RKK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating</head><p>To estimate , we augment the original prediction model with the new variable, X, obtained as above from the selection model Y p = X p p + X + new</p><p>The key insight is that is just the correlation between the error in the selection model, represented by the new variable (X), and p , which has been decomposed into X + new ; that is ≈ corr X X + new </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Add</head><label></label><figDesc>0.100) 0 043 [−0 289 0 332] Small , small set size −0 125 [−0 404 0 138] Large , large set size 0 571 [0.353, 0.727] Frequency × Set Size in selection (df diff = 1; LL = −2 106 836; DIC = 4 248 798) 0.987 Add Favorite in selection and prediction (df diff = 2; LL = −2 105 106; DIC = 4,244.383) 0.177 Add Favorite and all possible two-way Set Size interactions in selection and prediction 0.342 (df diff = 9; LL = −2 101 782; DIC = 4,251.811) Notes. Bold denotes statistical significance. Numbers in brackets represent the 95% Bayesian HDR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Data Structure and Notation for Two Respondents under Item Selectivity</figDesc><table><row><cell>Respondent</cell><cell>Number of</cell><cell>Selection:</cell><cell>Selection</cell><cell>Prediction:</cell><cell>Prediction</cell></row><row><cell>number r</cell><cell>alternatives k r</cell><cell>polytomous Y s</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Overview of Data Set Structures for Three Applications</figDesc><table><row><cell></cell><cell>Study 1 (RKK)</cell><cell>Study 2</cell><cell>Study 3</cell></row><row><cell>Set size</cell><cell>3 or 6</cell><cell>6 or 12</cell><cell>6</cell></row><row><cell>(No. of items)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>No. of choices</cell><cell>10</cell><cell>3</cell><cell>3</cell></row><row><cell>Individualized</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell>sets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stimuli</cell><cell>Songs</cell><cell>Snacks</cell><cell>Snacks</cell></row><row><cell>Time between</cell><cell>1 minute</cell><cell>&lt;1 minute</cell><cell>&lt;1 minute</cell></row><row><cell>choices</cell><cell></cell><cell>or 1 week</cell><cell>or 1 week</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Study 1 (RKK Data) Model Comparisons: Posterior Means for Selection, Prediction, , and Parameter estimates (std. err.)</figDesc><table><row><cell>Model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Marketing Science 35(6), pp. 976-994, © 2016 INFORMS</cell></row></table><note>(bottom)  summarizes whether other potentially relevant covariates had been excluded from the focal model: adding a Frequency × Set Size interaction to the selection model is nonsignificant (p &gt; 0 98), as are adding a Favorite item indicator main effect (p &gt; 0 15) and all possible two-way interactions in selection and prediction (p &gt; 0 3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Feinberg, Salisbury, and Ying: Item Selectivity in Experimental Research</figDesc><table><row><cell>Marketing Science 35(6), pp. 976-994, © 2016 INFORMS</cell><cell>985</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Study 2 Model Comparisons: Posterior Means for Selection, Prediction, , and Parameter estimates (std. err.)</figDesc><table><row><cell>Model</cell><cell>No</cell><cell>Common</cell><cell>Multiple</cell></row><row><cell>Selectivity/</cell><cell>= 0</cell><cell>Free</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Notes. Bold denotes statistical significance. Numbers in brackets represent the 95% Bayesian HDR.</figDesc><table><row><cell>Number of parameters</cell><cell>12</cell><cell>13</cell><cell>14</cell></row><row><cell>DIC</cell><cell>1,655.320</cell><cell>1,652.189</cell><cell>1,645.995</cell></row><row><cell>Log likelihood</cell><cell>−815 604</cell><cell>−812 930</cell><cell>−808 703</cell></row><row><cell>Likelihood ratio tests: p-values</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Common vs. no</cell><cell></cell><cell>0.021</cell><cell></cell></row><row><cell>Multiple vs. no</cell><cell></cell><cell></cell><cell>0.001</cell></row><row><cell>Multiple vs. common</cell><cell></cell><cell></cell><cell>0.004</cell></row><row><cell>Add Prior Rating × Set Size to prediction</cell><cell></cell><cell></cell><cell>0.212</cell></row><row><cell>(df diff = 1; LL = − 807 926; DIC = 1 646 697)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Remove Set Size from prediction</cell><cell></cell><cell></cell><cell>0.004</cell></row><row><cell>(df diff = 1; LL = −812 956; DIC = 1,651.920)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Feinberg, Salisbury, and Ying: Item Selectivity in Experimental Research</figDesc><table><row><cell>Marketing Science 35(6), pp. 976-994, © 2016 INFORMS</cell><cell>987</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>Study 3 Model Comparisons: Posterior Means for Selection, Prediction, , and Parameter estimates (std. err.)</figDesc><table><row><cell>Model</cell><cell>No</cell><cell>Common</cell><cell>Multiple</cell></row><row><cell>Selectivity/</cell><cell>= 0</cell><cell>Free</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table A.1 Bayesian Estimation of the Full Model vs. Two-Step Approaches</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mean of selected</cell><cell cols="2">Median of selected</cell><cell cols="2">Mode of selected</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">item residual</cell><cell cols="2">item residual</cell><cell cols="2">item residual</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Probit selection</cell><cell cols="2">Probit selection</cell><cell cols="2">Probit selection</cell></row><row><cell>Study</cell><cell>Condition</cell><cell>Correct Rho</cell><cell>OLS</cell><cell>Bayes</cell><cell>OLS</cell><cell>Bayes</cell><cell>OLS</cell><cell>Bayes</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For additional information regarding selection (of participants) on observables versus unobservables, see<ref type="bibr" target="#b7">Bronnenberg et al. (2010</ref>Bronnenberg et al.  ( , p. 1006).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This line of work addresses how which items one selects (an observable) is informative about overall capability (test performance), with methodology focused on rescoring methods, via item response theory, to correct for selectivity stemming from test designs that allow examinees to choose the question(s) they answer. Here, we focus on how the unmodeled portion of selectivity (a latent residual) can improve predictions (of postchoice evaluation). We later assess the informativeness of this unmodeled part of selectivity.3  <ref type="bibr" target="#b4">Bradlow and Zaslavsky (1999)</ref> provide one of the first examinations of this phenomenon, using hierarchical Bayesian techniques. In their data, "no answer" on a satisfaction survey suggested customers who may have been relatively uninformed about product features, and so entailed a nonignorable "item nonresponse" model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Marketing Science 35(6), pp.976-994, © 2016 INFORMS   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Model convergence was poor (particularly so for Studies 2 and 3) for the traditional random-coefficients specification, and in some cases Bayesian measures of model fit yielded implausible values. Mean effects, however, were broadly consistent with the "observed heterogeneity only" effects reported.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We did an exhaustive search to ensure that reported substantive conclusions were truly based on the "best" model for each empirical application. Researchers in consumer behavior and behavioral decision theory rarely require an exhaustive search; however, when</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">RKK performed a repeated measures analysis of covariance, with average real-time satisfaction ratings and choice set size predicting average retrospective evaluations of participants' chosen sequences. They found a significant effect of set size, with participants in the larger set condition reporting global evaluations greater than their satisfaction ratings. Our analyses are conducted at the individual item level and thus cannot be directly compared to the RKK results.9  We thank the authors for allowing us to reanalyze their data. Forty-eight study participants are included in our analysis.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Including the same covariates in selection and prediction is permissible, particularly so when, as here, theory suggests doing so. An additional covariate representing the highest ranked item in each set (Favorite) was also tested in the selection and prediction submodels; it was not significant and is not discussed further.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Additional tests confirmed that including Bunchy {Attractive, Unattractive} × Prior Rating in the selection model does not improve fit (p &gt; 0 20), while removing any of the two-way interactions between Bunchy {Attractive, Unattractive} and Prior Rating or Favorite significantly reduces model fit (all p &lt; 0 001).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">Using Bayesian estimation tools, any of these outcome types can be back-sampled from to draw the "underlying" latent intervalscaled variable(s) that gave rise to it, rendering the remaining estimation equivalent (or nearly so) to a standard Heckman or a seemingly unrelated regression.16  Values for homogeneous versus heterogeneous, respectively, are as follows: common , (−0 289 0 331) versus (−0 029 0 491); multiple , small set size, (−0 404 0 138) versus (−0 213 0 425); multiple , large set size, (0 353 0 727) versus (0 042 0 598).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Feinberg, Salisbury, and Ying: Item Selectivity in Experimental Research</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank Greg Allenby, Christie Brown, Pierre Chandon, Clint Cummins, Terry Elrod, Gabor Kezdi, Mike Palazzolo, and Carolyn Yoon for their assistance and suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. Simple Test and "Two-Step" Estimation Procedure for Multinomial Selectivity</head><p>Recall that the researcher wishes to estimate the system (1)-(3). One way to do so is to estimate the residuals, s , for the chosen items and introduce them as regressors in (2). When s are (unconditionally) normal, there is no closedform solution, when conditioned on their being for the chosen items, for their expectation, median, or mode. It is</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is simple to calculate, because X and new are uncorrelated, since they are in the same regression. In the online appendix, we show that</p><p>where b is the estimated coefficient for X, 2 X is the variance of X, and MSE is the estimated mean square error of the regression. Note that this ensures that lies in (−1 1) and that b ≈ 0 leads to ≈ 0.</p><p>Significance levels for can be gauged by those for b; CIs can be computed using the upper and lower estimates for b itself. When is estimated separately by conditionone of the main points of this article-one can simply put in separate "X" variables for each condition, obtain separate estimates of b, and thereby , being sure to use the MSE for the overall regression. Because b and MSE are not necessarily uncorrelated, one can run a Bayesian regression instead of a classical one; the quantity b/ b 2 + MSE/ 2 X is calculated for each draw of b and MSE, giving a distribution for requiring no asymptotic assumptions, from which tests and CIs (highest density regions) arise.</p><p>We compare the accuracy of these methods-using the residual mean, median, and mode; logit versus probit selection; and classical versus Bayesian regression-in the online appendix for all three data sets in the article. The results are summarized in Table <ref type="table">A</ref>.1. We find the proposed method to be reasonably accurate and to work (marginally) best by using the following for inference: the mean residual, probit selection, and Bayesian regression.</p><p>To summarize, the entire procedure, which can be used in an "exploratory" manner, entails four straightforward steps:</p><p>(1) Run a multinomial logit or probit selection model; retain the estimated probabilities for the chosen items, p 1 .</p><p>(2) Compute the new covariate, X, using the mean, median, or mode (e.g., for the mode, −1 exp −p 1 . This should be done separately for each condition; e.g., three conditions requires three separate "X" variables, placed in the next step simultaneously.   (3) Run the prediction model, including X: Y p = X p p + X + new .</p><p>(4) Computeˆ = b/ b 2 + MSE/ 2 X , either once (if the regression was classical) or at each draw of the sampler (if it was Bayesian), with CIs computed accordingly.</p><p>We summarize the Bayesian and OLS procedures for a probit model on all three data sets; complete estimation results, including logit and CIs, appear in the online appendix.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating promotional activities in an online two-sided market of user-generated content</title>
		<author>
			<persName><forename type="first">P</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Chatow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jamal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="406" to="432" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long-run effects of promotion depth on new versus established customers: Three field studies</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Simester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="20" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An experimental investigation of scanner data preparation strategies for consumer choice models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Currim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Res. Marketing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="331" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the recoverability of choice behaviors with random coefficients choice models in the context of limited data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ainslie</forename><forename type="middle">A</forename><surname>Currim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="99" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable model for ordinal data from a customer satisfaction survey with &quot;no answer</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">responses. J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">445</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online display advertising: Modeling the effects of multiple creatives and individual impression histories</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Moe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="753" to="767" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Product assortment</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Broniarczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Consumer Psychology</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Haugtvedt</surname></persName>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Herr</surname></persName>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Kardes</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Laurence Erlbaum Associates</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="755" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Do digital video recorders influence sales?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Bronnenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Dubé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Mela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="998" to="1010" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal selection for direct mail</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wansbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="378" to="394" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Salisbury</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Item Selectivity in Experimental Research 994</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="994" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Option attachment: When deliberating makes choosing feel like losing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wertenbroch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeelenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal pricing of new subscription services: Analysis of a market experiment</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Danaher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="138" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Great expectations?! Assortment size, expectations and satisfaction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poynor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="312" to="322" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Applied Missing Data Analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Enders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Guilford Publications</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Turning the page: The impact of choice closure on satisfaction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Botti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Faro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="283" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Varieties of selection bias</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Econom. Rev</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">When choice is demotivating: Can one desire too much of a good thing?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality Soc. Psych</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="995" to="1006" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stuck in the adoption funnel: The effect of interruptions in the adoption process on usage</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lambrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="355" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fragile enhancement of attitudes and intentions following difficult decisions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Litt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Tormala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="584" to="598" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Rja</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Statistical Analysis with Missing Data</title>
				<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online product opinions: Incidence, evaluation, and evolution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Moe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schweidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="386" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the use of the RESET test in microeconometric models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Econom. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="361" to="365" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A control function approach to endogeneity in consumer choice models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Petrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Train</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regulatory focus, regulatory fit, and the search and consideration of choice alternatives</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="640" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Heckman correction for sample selection and its critique</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Puhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econom. Surveys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Choosing less-preferred experiences for the sake of variety</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diversification bias: Explaining the discrepancy in variety seeking between combined and separated choices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loewenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psych.: Appl</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="49" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiple Imputation for Nonresponse in Surveys</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Missing data: Our view of the state of the art</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="177" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The effect of purchase quantity and timing on variety-seeking behavior</title>
		<author>
			<persName><forename type="first">I</forename><surname>Simonson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian measures of model complexity and fit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc., Ser. B</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="639" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimation of relationships among limited dependent variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Why switch? Product category-level explanations for true variety-seeking behavior</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Van Trijp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Inman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Successive sample selection and its relevance for management decisions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Otter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="170" to="185" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On examinee choice in educational testing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Res. Report Ser</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="195" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How well can we compare scores on test forms that are constructed by examinees choice?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="199" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the viability of some untestable assumptions in equating exams that allow examinee choice</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">App. Measurement Ed</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Models for sample selection bias</title>
		<author>
			<persName><forename type="first">C</forename><surname>Winship</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Mare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Rev. Soc</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="327" to="350" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling the underreporting bias in panel survey data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="525" to="539" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Leveraging missing ratings to improve online recommendation systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="365" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Data pruning in consumer choice models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Zanutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Marketing Econom</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
