<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Model-Based Purchase Predictions for Large Assortments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-04-18">April 18, 2016.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bruno</forename><forename type="middle">J D</forename><surname>Jacobs</surname></persName>
							<email>jacobs@ese.eur.nl</email>
						</author>
						<author>
							<persName><forename type="first">Bas</forename><surname>Donkers</surname></persName>
							<email>donkers@ese.eur.nl</email>
						</author>
						<author>
							<persName><forename type="first">Dennis</forename><surname>Fok</surname></persName>
							<email>dfok@ese.eur.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Business Economics and Econometric Institute</orgName>
								<orgName type="institution" key="instit1">Erasmus School of Economics</orgName>
								<orgName type="institution" key="instit2">Erasmus University Rotterdam</orgName>
								<address>
									<postCode>3000 DR</postCode>
									<settlement>Rotterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Business Economics</orgName>
								<orgName type="department" key="dep2">Erasmus School of Economics</orgName>
								<orgName type="institution">Erasmus University Rotterdam</orgName>
								<address>
									<postCode>3000 DR</postCode>
									<settlement>Rotterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Erasmus School of Economics</orgName>
								<orgName type="institution" key="instit1">Econometric Institute</orgName>
								<orgName type="institution" key="instit2">Erasmus University Rotterdam</orgName>
								<address>
									<postCode>3000 DR</postCode>
									<settlement>Rotterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Model-Based Purchase Predictions for Large Assortments</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2016-04-18">April 18, 2016.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2016.0985</idno>
					<note type="submission">Received: December 23, 2013; accepted: January 27, 2016; Pradeep Chintagunta, Dominique Hanssens, and John Hauser served as the special issue editors and Daniel Goldstein served as associate editor for this article.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>model-based predictions</term>
					<term>large scale purchase prediction</term>
					<term>scalability</term>
					<term>purchase history data</term>
					<term>latent Dirichlet allocation</term>
					<term>mixture of Dirichlet-Multinomials</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to predict what a customer will purchase next is valuable in many marketing applications. This is especially true for online retailing. Adequate predictions for the next products to be purchased enable online retailers to implement a product recommendation system, determine the positions of products in the result of a customer's search query, optimize the collection of products to be displayed on a personalized landing page or suggest products to complement the contents of a customer's shopping basket.</p><p>Examples of personalization in practice are Amazon's "Customers Who Bought This Item Also Bought" section, Apple's iTunes Genius, and the Netflix recommendation system. There is also clear evidence that such personalized configurations influence behavior <ref type="bibr" target="#b8">(Ghose et al. 2014</ref><ref type="bibr" target="#b23">, Pan et al. 2007</ref><ref type="bibr" target="#b25">, Salganik et al. 2006</ref>). These applications have in common that they require a personalized selection of products from a potentially large product assortment. Ideally, the selection contains those products that are most likely to be of interest to the customer. Moreover, the selection should be relatively small as the available space to show products is often limited.</p><p>The effectiveness of personalization attempts crucially depends on the accuracy of the predictions. A complicating factor in purchase prediction is the fact that the typical online retailer sells items from a very broad assortment to an even larger customer base. Hence predictions should not only be accurate, but the prediction method should scale to large applications as well <ref type="bibr" target="#b20">(Naik et al. 2008)</ref>. Additionally, to be useful in an online setting predictions should be available in near real-time. Obtaining predictions, and updating them as new information comes in, should therefore be fast.</p><p>The typical data available at an online retailer for purchase prediction are the customer purchase histories. In some cases additional customer characteristics (e.g., demographics) are also available. However, on the product level characteristics are often absent and if descriptions are available, it is not obvious how to extract useful predictors from this information. In this Many online retailers predict a customer's next purchase using collaborative filtering algorithms, for example, by relying on counts of the co-occurrence of items in purchase history data <ref type="bibr" target="#b14">(Jannach et al. 2011</ref><ref type="bibr" target="#b16">, Liu et al. 2009</ref>). In such a count-based approach a decision must be made on how to measure the co-occurrence of items, as one can count pairs, triplets or even higherorder product combinations. A choice for small sets of items results in information loss, i.e., purchase patterns that span many products might not be easily identified. On the other hand, for large combinations of products the matrix of co-occurrence counts becomes sparse, resulting in predictions based on just a few matches in the customer base. Another challenge in collaborative filtering algorithms is incorporating customer characteristics. One possible approach is to partition the customer base using such characteristics. However, this can only be done for a few variables with a limited number of levels; otherwise sample sizes per subgroup become too small.</p><p>By contrast, model-based approaches to predict individuals' choices have a long history in marketing <ref type="bibr" target="#b12">(Guadagni and Little 1983</ref><ref type="bibr" target="#b18">, McFadden 1986</ref><ref type="bibr" target="#b28">, Wagner and Taudes 1986</ref><ref type="bibr" target="#b5">, Fader and Hardie 1996</ref> and such methods are well suited to include customer characteristics. However, the usual implementations of these models tend to break down in the typical online retail setting, where a wide variety of products is sold to a large number of customers <ref type="bibr" target="#b20">(Naik et al. 2008)</ref>. One way to make methods more scalable is to consider only a subset of the data in terms of customers and/or products <ref type="bibr" target="#b32">(Zanutto and Bradlow 2006)</ref>. Clearly, this is not a viable solution if the aim is to predict purchase behavior for each customer across the entire product assortment.</p><p>In this paper we try to bridge the gap between retail practice and marketing academia by discussing modelbased prediction methods that do work in the context of large assortments. By developing such methods we open an avenue for future research on marketing interventions in large scale assortments, for example on the effectiveness of product recommendations, extending the work of <ref type="bibr" target="#b2">Bodapati (2008)</ref>. Note that this would not be feasible with the heuristic, count-based approaches currently used in practice. We consider two model-based approaches. In addition, we present (an implementation of) a count-based collaborative filter and a scalable version of a discrete choice model that will serve as benchmarks. We compare the methods on their (i) heterogeneity assumptions, (ii) estimation complexity, (iii) memory requirements for real-time online predictions, and (iv) predictive performance.</p><p>The first method we present is a novel approach inspired by topic models used in the text modeling literature. Traditionally, a topic model describes a document by relating the words in the text to latent topics. We adapt this class of models to the purchase prediction context: Words become product purchases, a document is a customer's purchase history, and a topic represents a certain preference for products in the assortment. Given that the word "topic" is incongruous in a retailing context, we refer to topics as motivations. <ref type="bibr">1</ref> Naturally, customers can have more than one motivation, just as a document can cover multiple topics. This idea leads to a class of models that can describe and predict customer purchase behavior in large assortments.</p><p>The most frequently used topic model is latent Dirichlet allocation (LDA) by <ref type="bibr" target="#b1">Blei et al. (2003)</ref>. This model has been used to analyze very large text corpora <ref type="bibr" target="#b24">(Ramage et al. 2010</ref><ref type="bibr" target="#b19">, Mimno et al. 2012</ref>, showing that LDA provides the necessary scalability. By contrast to the text modeling literature, where documents tend to have many words, customers often have only a couple of purchases or they might even be new to the retailer. Given such limited information per customer, we need to formally estimate the population-level a priori probabilities of particular motivations. This extends the LDA text modeling implementation, where these probabilities are typically considered to be known or at best calibrated using heuristics <ref type="bibr" target="#b29">(Wallach et al. 2009</ref><ref type="bibr" target="#b0">, Asuncion et al. 2009</ref>. To account for observed heterogeneity, we extend LDA by relating customer characteristics to the a priori motivation probabilities. This can capture heterogeneity related to variables such as referrer site, demographics or other customer characteristics. Most likely this increases the model's predictive power in particular for customers with few or no observed purchases. We refer to this model as LDA-X.</p><p>The next method we consider is a mixture of Dirichlet-Multinomials (MDM) <ref type="bibr" target="#b13">(Jain et al. 1990</ref>). MDM specifies individual-specific probability vectors that contain a customer's purchase probabilities over all products in the assortment. In turn, these probability vectors follow a discrete mixture of Dirichlet distributions. MDM has previously been applied in marketing <ref type="bibr" target="#b13">(Jain et al. 1990</ref>), but to our knowledge never to a large product assortment. Although, in theory, customer characteristics can also be included in MDM we will argue that the resulting model will no longer be feasible in terms of estimation complexity, given the setting of our application.</p><p>The predictive performance of LDA(-X) and MDM is compared to that of a count-based collaborative filter and a discrete choice model. We assess predictive performance using data from an online retailer. For each method, we create customer-specific prediction sets that contain the products most likely to be purchased. These sets are next matched with hold-out purchase data. For further insight into the differences between the methods, we also consider the predictive performance for groups of customers who differ in the length of their observed purchase history. Furthermore, in a setting where repeat purchases are frequent, e.g., fast moving consumer goods, performing well by correctly predicting frequently purchased products or repeat purchases might not be too difficult. Such recommendations might even be perceived as trivial or boring <ref type="bibr" target="#b7">(Fleder and Hosanagar 2009)</ref>. Therefore we also study the predictive performance for unexpected products, which we define as products that have not previously been purchased by the customer and which are in the tail of the assortment.</p><p>The remainder of this article proceeds as follows: In §2 we present the methods used in this research and discuss their heterogeneity assumptions and scalability. Technical details are available in the appendices. Subsequently, we apply the methods to data of an online retailer. An overview of this data is provided in §3 and the results are reported in §4. To conclude, we summarize our findings and suggest directions for future research in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>In this section we present our prediction methods. First, we introduce two model-based prediction methods, LDA(-X) and MDM, that infer latent customer traits from purchase data. We compare these methods on their heterogeneity assumptions and estimation complexity. Next, the two benchmark methods are introduced, i.e., a set of collaborative filters (CF) and a discrete choice model (DCM) that captures customer heterogeneity through constructed, but observed predictor variables. Subsequently, all methods are compared on their suitability to update predictions in a realtime setting. Finally, we discuss our assessment of the quality of predictions.</p><p>All methods share the following notation: The products from the J -dimensional assortment are numbered j = 1 J . For each customer i = 1 I we observe n i product purchases from this assortment. The purchase history of customer i is denoted by the vector y i = y i1 y in i , where y in ∈ 1 J represents the nth purchase of customer i. In addition we have customerlevel characteristics coded in the K-dimensional column vector</p><formula xml:id="formula_0">x i = x i1</formula><p>x iK . We combine the purchase histories in Y = y 1 y I and the predictor variables in X = x 1</p><p>x I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Latent Dirichlet Allocation</head><p>Our first model is inspired on topic models. The key idea underlying our application of these models to the context of purchase history data is that customer purchases are driven by a (small) set of latent motivations (the topics). Each motivation then drives preferences for a subset of products in the assortment, for example, a preference for eco-friendly products, for low-fat products or for products for sensitive skin.</p><p>In general, customers are likely to be driven by different motivations over time and even within a single purchase occasion. Additionally, the same product purchased by different customers may be driven by different underlying motivations: A movie can be purchased by a fan of the lead actor or by a customer that is fond of the movie's genre. These features are embedded in topic models, in which customers may have multiple motivations, and products may be associated with more than one motivation.</p><p>The basis for our method is latent Dirichlet allocation (LDA) introduced by <ref type="bibr" target="#b1">Blei et al. (2003)</ref>. LDA has been proven to scale to applications well beyond the dimensions of a typical online retailer. For example, it has been used to analyze over eight million posts on Twitter that contain words from a vocabulary of more than five million entries <ref type="bibr" target="#b24">(Ramage et al. 2010)</ref>, or for the analysis of 1.2 million out-of-copyright books <ref type="bibr" target="#b19">(Mimno et al. 2012)</ref>. Below, we first present the details of our adaptation of LDA in the context of predicting customer purchase behavior. Next, we extend LDA by including customer-level predictor variables.</p><p>In LDA each latent motivation m = 1 M is represented by a probability vector m over the complete J -dimensional assortment. Given that a purchase is driven by motivation m, the probability of buying product j is simply mj . The motivation-specific probability vectors are distributed as m ∼ Dirichlet J . A priori there is no reason to favor one product over another in a motivation. This is reflected in the parameterization of , where we set each element equal to a common value 0 . This value determines whether the Dirichlet distribution tends to favor more narrow ( 0 close to zero) or more broad (large 0 ) motivations <ref type="bibr" target="#b29">(Wallach et al. 2009)</ref>. Even though each purchase is driven by a single motivation, a customer's entire purchase history may be driven by multiple motivations. This variation is described by an individual-specific discrete mixture i over the M motivations. The probability that a product purchase of customer i is driven by motivation m is then given by im . These probabilities differ across customers and are modeled as i ∼ Dirichlet M . Here, is an M-dimensional vector that captures the relevance of each motivation across the customer base. The expected value of the probability that motivation m drives a purchase equals</p><formula xml:id="formula_1">Ɛ im = m M l=1 l (1)</formula><p>Therefore, the larger the value of m , the more likely it is that a customer will make a purchase driven by motivation m.</p><p>The last step is to link motivations to actual purchases. We denote by z in ∈ 1 M the actual motivation that drives purchase y in . As motivations are latent, we must account for all possible motivations to obtain the marginal probability that customer i will purchase product j, resulting in</p><formula xml:id="formula_2">Pr y in = j l M l=1 i = M m=1 Pr y in = j z in = m l M l=1 Pr z in = m i = M m=1 mj im<label>(2)</label></formula><p>In the topic modeling literature it is common practice to determine the parameters of the Dirichlet distributions (for i ) and 0 (for m ) by means of heuristics, rather than formally inferring their values from available data <ref type="bibr" target="#b29">(Wallach et al. 2009)</ref>, for example, imposing = 50/M <ref type="bibr" target="#b10">(Griffiths and Steyvers 2004)</ref> and 0 = 0 01 <ref type="bibr" target="#b26">(Steyvers and Griffiths 2007)</ref>, or by applying a grid search for and 0 <ref type="bibr" target="#b0">(Asuncion et al. 2009</ref>). These heuristics are not directly applicable in our setting as they have been designed for text modeling. Given that purchase histories tend to be much shorter than documents, we expect the LDA predictions to be more sensitive to the values of and (to a lesser degree) of 0 . We therefore extend the common LDA model and place proper prior distributions on both parameters and formally estimate and 0 in a Bayesian setting.</p><p>We specify a log-normal distribution for m , that is, we define log m = m , and set a normal prior for m . We set the mode of the log-normal distribution equal to M −1 , which is within the range of values frequently used in the text modeling literature, and place 10% of its probability mass above 1. 2 This prior specification favors i -vectors that allocate the majority of the probability mass to a small number of motivations, while still allowing for more uniformly distributed i -vectors. Similarly, we place a log-normal distribution on 0 with its mode equal to 0 01, and 10% of its probability mass above 1. This specification supports m -vectors where only a few products from the assortment receive significant probability mass, representing fairly specific 2 These two conditions implicitly identify the two parameters of the log-normal distribution. motivations. Still, this prior is rather uninformative; broader motivations that spread the probability mass more equally over the assortment remain quite likely.</p><p>These prior specifications also allow us to easily extend LDA by including customer characteristics, coded in x i . Such variables are likely to improve the predictive performance of the model. We extend the loglinear specification for m to im as follows: log im = m + x i m . This links customer preferences, represented by the likelihood of each of the motivations, to the additional customer-level information, resulting in LDA-X. To illustrate the effect of this specification on the distribution of i consider the expected value of im , which gives the probability that a typical customer with characteristics x i makes a purchase driven by motivation m</p><formula xml:id="formula_3">Ɛ im i = im M l=1 il = exp m + x i m M l=1 exp l + x i l (3)</formula><p>The m parameters capture the dependence of the probability that motivation m is used, on the customerspecific variables x i . The prior distribution of m and m can only be sensibly determined if the level and scale of the x i variables are known. We therefore standardize the customer-level variables such that they have mean zero and unit variance. Given this scale, we assume that all elements in m are normally distributed with zero mean and variance equal to 0 04. This corresponds to a prior 95% confidence interval that is approximately equal to −0 4 +0 4 . Note that this prior distribution is chosen to be relatively narrow on purpose, as the effect of m on im is exponential. As x i is mean-centered, we use the same prior for m as in LDA.</p><p>To obtain customer-specific predictive distributions, we condition on the model structure of LDA. In particular, given the model parameters , 0 and the latent purchase assignments Z, the predictive distribution for a new purchaseỹ in can be shown to equal <ref type="bibr" target="#b10">(Griffiths and Steyvers 2004)</ref> Pr</p><formula xml:id="formula_4">ỹ in = j Z 0 Y = M m=1 Pr ỹ in = j z in = m Z 0 Y Pr z in = m z i = M m=1 Ɛ mj Z 0 Y Ɛ im z i = M m=1 0 + c MJ mj J 0 + J p=1 c MJ mp • m + c IM im M l=1 l + c IM il (4)</formula><p>where c MJ mj is the number of times a purchase of product j is driven by motivation m and c IM im is the number of purchases made by customer i that are driven by motivation m. To obtain the predictive distribution for the LDA-X model one simply replaces with i in (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dirichlet-Multinomial Models</head><p>The Dirichlet-Multinomial (DM) model <ref type="bibr" target="#b15">(Jeuland et al. 1980</ref><ref type="bibr" target="#b9">, Goodhardt et al. 1984</ref>) is a known model-based approach to capture heterogeneity in purchase behavior. Applications of this model can be found in <ref type="bibr" target="#b11">Grover and</ref><ref type="bibr">Srinivasan (1987), Fader (1993)</ref>, and <ref type="bibr" target="#b6">Fader and Schmittlein (1993)</ref>. In this model, each customer is given an individual-specific vector i containing the purchase probabilities for each product in the J -dimensional assortment, where J p=1 ip = 1. The probability that customer i purchases product j at a specific purchase occasion n is given by Pr y in = j i = ij .</p><p>Large values for the purchase probability ij imply that customer i is likely to buy product j. In the DM model the customer-specific i -vectors are assumed to arise from a single Dirichlet distribution: i ∼ Dirichlet J</p><p>. The -vector describes the overall purchase behavior in the customer base: If product j is frequently purchased, j will have a large value relative to the other values in and vice versa.</p><p>The original DM model has been extended such that the probability vectors i originate from a finite mixture of Dirichlet distributions <ref type="bibr" target="#b13">(Jain et al. 1990)</ref>, not from a single Dirichlet distribution. This extension is known as a mixture of Dirichlet-Multinomials (MDM). In MDM, each customer is assigned to one of M segments and each segment is characterized by its own Dirichlet distribution. Given that customer i is allocated to segment m, denoted by s i = m, the customer's purchase probabilities i are distributed as Dirichlet J m . The m -vectors are segment specific, describing the distribution of the purchase probability vectors for customers in segment m. Customers are hence expected to be similar, though not identical, within a segment, but different across segments.</p><p>Segment membership in MDM is described by an M-dimensional categorical distribution with probability vector . The element m gives the a priori probability that a customer is a member of segment m, that is, Pr s i = m = m . As we consider MDM in the Bayesian paradigm we also specify prior distributions over and the m -vectors. For it is natural to favor no segment over any other a priori, therefore we use a uniform distribution over the M − 1 -dimensional simplex. For each mj we use a log-normal prior distribution with its mode at 0 01 and 10% of the probability mass above 1. This specification allows for i -vectors that allow many products to be purchased with a large probability, but it favors customer segments who purchase from a more limited subset of the assortment.</p><p>Similar to the approach in LDA(-X), we obtain customer-specific predictive distributions of a new purchaseỹ in conditional on the data, parameters, and segment allocations. In MDM this requires a prediction of segment membership of the customer, combined with the purchase probabilities, conditional on segment membership</p><formula xml:id="formula_5">Pr ỹ in = j s \i l M l=1 y i = M m=1 Pr ỹ in = j s i = m m y i Pr s i = m s \i l M l=1 y i = M m=1 Ɛ ij s i = m m y i Pr s i = m s \i l M l=1 y i = M m=1 mj +c IJ ij J p=1 mp +c IJ ip Pr s i = m s \i l M l=1 y i (5)</formula><p>where</p><formula xml:id="formula_6">Pr s i = m s \i l M</formula><p>l=1 y i is specified in Online Appendix 1 (available as supplemental material at http:// dx.doi.org/10.1287/mksc.2016.0985) (see Equation (A.9)) and c IJ ij equals the number of times customer i has purchased product j. If i is a new customer c IJ ij = 0 for all j by definition. Note that both components in (5) depend on the customer's purchase history, unlike LDA(-X) where only the motivation probabilities are customer specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Inference</head><p>The predictive distributions specified above are conditional on the number of segments/motivations M, the model parameters, and segment/motivation allocations to customers/purchases. For a given number of M, we rely on Bayesian methodology to infer the model parameters and latent variables of the models. Direct inference on the posterior distribution is not tractable; therefore, we derive Markov Chain Monte Carlo (MCMC) methods to generate samples from the posterior distribution. Specifically, we use a random walk Metropolis-Hastings within Gibbs sampler to draw samples from the target posterior distribution. The predictive distributions can then be obtained by averaging over these draws.</p><p>The full posterior of LDA(-X) is given by</p><formula xml:id="formula_7">p Z l M l=1 0 i I i=1 l M l=1 Y X</formula><p>where l M l=1 is only relevant when customer characteristics X are included. Straightforward use of a Gibbs sampler for this posterior distribution is very inefficient. This is the result of a strong dependence between the latent motivation assignments Z on one hand and the parameters m and i on the other hand. A Gibbs sampler would therefore require an excessive number of draws to properly explore this posterior. Instead, we take advantage of the fact that the Dirichlet distribution is the conjugate prior for a categorical random variable. This allows us to marginalize over the m and i parameters, while retaining closed-form expressions for the conditional distributions of the other parameters in LDA(-X). By doing so we substantially improve the mixing properties of the Gibbs sampler <ref type="bibr" target="#b10">(Griffiths and Steyvers 2004)</ref>. Hence, we examine the so-called collapsed posterior distribution of LDA(-X), defined as:</p><formula xml:id="formula_8">p Z 0 l M l=1 Y X .</formula><p>The elements of Z are sampled using a Gibbs sampler, while for the other parameters we implement a random walk Metropolis-Hastings sampler.</p><p>The set-up for inference in MDM is very similar to LDA(-X). The complete posterior distribution is given by: p s</p><formula xml:id="formula_9">i I i=1 l M l=1</formula><p>Y . Again, we marginalize over the discrete distributions i and , resulting in a collapsed posterior distribution of MDM:</p><formula xml:id="formula_10">p s l M l=1 Y .</formula><p>Here the segment allocations s can be sampled in a Gibbs step, while the l parameters require a random walk Metropolis-Hastings sampler.</p><p>LDA(-X) and MDM are members of the general class of mixture models. This class of models is well known to be susceptible to end up in an area around a local maximum of the posterior distribution. As is common in this literature, this risk is reduced by using multiple random starts <ref type="bibr">Kamakura 2000, Train 2009</ref>). For each value of M, we consider 250 different random starts. We reduce the computational burden of this approach by evaluating each random start at several intermediate steps of the estimation routine. At each step, we continue only with the best performing candidates. Performance is measured by the likelihood that results from the model's predictive distributions, averaged over purchases in a modelselection data set. This measure is closely related to the goal of predicting a new purchase as accurately as possible.</p><p>The same performance measure is also used to determine the number of motivations (for LDA(-X)) or segments (for MDM). In particular, for each model we increase the value of M until we find a decrease in the average predictive likelihood of the model-selection data. <ref type="bibr">3</ref> More details on the estimation routines, including pseudo-code, are provided in Online Appendix 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Comparison of LDA(-X) and MDM</head><p>Although the structures of LDA(-X) and MDM might appear quite similar at first, these models differ fundamentally on various grounds. In this subsection we first discuss this difference in terms of customer heterogeneity. Next, we consider the estimation complexity of the LDA(-X) and MDM models.</p><p>2.4.1. Heterogeneity Assumption. MDM assumes that heterogeneity in purchase behavior can be described by segmenting the customer base into groups of customers. Customers across segments are expected to be dissimilar, while customers in a segment are expected to be rather similar. Hence, similarity between customers is mainly driven by segment membership. In LDA(-X) purchase behavior is described by motivations, where each motivation represents a preference for certain products in the assortment. Heterogeneity in purchase behavior is described by customer-specific probabilities for these motivations. This leads to a model where the purchases of a single customer are driven by multiple motivations. Here similarity between customers is motivation specific. Customers can have very similar purchase behavior for one set of products, corresponding to a shared motivation, and be very different for a set of products that belong to another motivation.</p><p>Which heterogeneity structure fits best depends on the specific situation. If customers typically have one or very few motivations, grouping customers in segments might be beneficial. If many combinations of motivations are present, the continuous mixture of motivations in LDA(-X) will be more parsimonious. Therefore, if a retailer has many different (latent) subcategories in its product assortment, and preferences across these subcategories vary independently across individuals, it is likely that the heterogeneity can be specified more parsimoniously by LDA(-X).</p><p>Although MDM assumes a hard clustering of customers into segments, one will use posterior segment probabilities to eventually make predictions. This will typically lead to a form of soft clustering, where a weighted combination of different segments is used. This brings the heterogeneity structure of MDM closer to that of LDA(-X). As we observe more purchases, the posterior segment probabilities in MDM will of course become more and more extreme. In the end this converges to strictly assigning a customer to a single segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Estimation Complexity.</head><p>The different heterogeneity assumptions underlying LDA(-X) and MDM have a large impact on estimation complexity through the number of customer-specific parameters. In MDM each customer is given a distribution over the J -dimensional assortment, while in LDA(-X) a customer is described by a probability distribution over the M motivations, where M is generally much smaller than J . Even though we marginalize over these customerspecific distributions, this still affects the scalability of the models. Table <ref type="table" target="#tab_0">1</ref> summarizes, for each model, the parameters that must be sampled to infer the model structure after marginalization. We differentiate between the sampling technique required, as Gibbs steps tend to be much faster and have better mixing properties than Metropolis-Hastings steps <ref type="bibr" target="#b3">(Damien et al. 1999)</ref>.</p><p>In LDA(-X) we need as many motivation allocations as purchases (N in total), whereas for MDM we only need to sample one segment allocation per customer </p><formula xml:id="formula_11">l M l=1 M × J LDA Z N 0 , 1 + M LDA-X Z N 0 , , l M l=1 1 + M × 1 + K</formula><p>Notes. I, number of customers; M, number of segments/motivations; J, assortment size; K , number of predictor variables in x i ; N, total number of purchases.</p><p>(I in total). Although the number of allocations is larger in LDA(-X), this does not imply that the total allocation in LDA(-X) is computationally more demanding.</p><p>The sampling step for each motivation assignment in LDA(-X) involves only elementary arithmetic operations, while for each segment allocation in MDM we have to evaluate complex Gamma functions. It is difficult to quantify the difference in computational complexity as it also depends on the (latent) structure in the data, but we anticipate that MDM will be slightly more complex for these Gibbs sampling steps. <ref type="bibr">4</ref> The remaining model parameters are sampled using Metropolis-Hastings steps, each of which is computationally demanding. For LDA we sample 1 + M parameters; for LDA-X this increases to 1 + M × 1 + K parameters. These numbers are in sharp contrast to MDM in which M × J parameters are sampled. This renders MDM much more demanding in terms of estimation time, as the assortment size J is large. This is the price that must be paid for the many degrees of freedom per customer. The number of Metropolis-Hastings steps in LDA(-X) is largely insensitive to the size of the assortment, the number of customers, and the number of purchases. In MDM, on the other hand, the number of Metropolis-Hastings steps linearly increases with the assortment size. This limits the scalability of MDM, which is why we can only extend LDA by including observed heterogeneity through x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Benchmark Methods</head><p>In this section we present the main ideas underlying the two benchmark methods to which we will compare the predictive performance of LDA(-X) and MDM. Details are available in Appendix A. The first benchmark is a collaborative filter; the second is built on standard discrete choice modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">Collaborative Filtering.</head><p>A collaborative filter is a deterministic algorithm that predicts purchases by matching customers to each other based on purchase histories. There are many ways to implement a collaborative filter. Details of the actual implementations used in industry are not common knowledge. Below we develop our own implementation of a collaborative filter.</p><p>Ideally, a focal customer is matched to customers who purchased the focal customer's previously purchased products and at least one additional item. However, such a matching on the complete purchase history is in general not feasible due to the curse of dimensionality; the larger the purchase history, the less likely it matches other customers' histories.</p><p>We alleviate this curse of dimensionality by matching on parts of the purchase history. First, for each customer i we replace the complete purchase history vector y i by the set of unique sorted subvectors of length k that can be created from y i . We denote this set of vectors by H k i . For example, for k = 2 a customer's purchase history is replaced by all of the unique sorted pairs that can be formed using the purchase history, so y i = 1 1 1 2 3 would be reduced to the set H 2 i containing the pairs 1 1 , 1 2 , 1 3 , and 2 3 . <ref type="bibr">5</ref> Next, for each subvector in this set we match the focal customer against all customers. If k is relatively small, this will result in many more matches compared to a matching on the complete purchase history. This solves the curse of dimensionality problem at the cost of a loss of information.</p><p>Using these matches for a customer, we create individual-specific product scores that can be used to construct rankings over the product assortment. This ranking obviously depends on k. In our application we consider collaborative filters with two combination sizes, k = 1 and k = 2, denoted by CF-1 and CF-2, respectively. Using k = 1, customers are matched on the presence of single products in their purchase history. For k = 2 customers are matched on the presence of pairs of products in their purchase history. Larger product combinations are not desirable in our application, in terms of computational feasibility and the degree of sparseness in these larger combinations. <ref type="bibr" target="#b17">(Maddala 1983</ref><ref type="bibr">, Mc-Fadden 1986</ref>) have been extensively used in marketing to model discrete choices from a set of given alternatives. Implementing a traditional discrete choice model that directly uses purchase history data from a large assortment as predictor variables, however, is infeasible. Such a model would have to predict purchases for J products based on J predictor variables, where each predictor variable describes whether a product was purchased by the customer in the past. This model specification would require simultaneous estimation of J J − 1 parameters, which is infeasible from a computational perspective and will also likely result in Marketing Science 35(3), pp. 389-404, © 2016 INFORMS identification issues due to sparse data. Hence, traditional discrete choice models do not scale well when the number of products J becomes large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">Discrete Choice Models. Random utilitybased multinomial choice models</head><p>The benchmark discrete choice model that we propose resolves these problems by smartly constructing predictor variables, thus enabling a huge reduction in the number of parameters to be estimated. We build on the idea that in the binary logit model, the probability that customer i purchases product j is specified by</p><formula xml:id="formula_12">Pr y in = j = exp ij 1 + exp ij (6)</formula><p>where ij represents the log odds of customer i having purchased product j. Our goal is to capture the variation of ij across customers and products through predictor variables, without introducing customer-or product-specific parameters.</p><p>In the simplest model we could relate ij to a constant, the log of the number of products purchased by customer i, and the log of the observed odds ratio for product j. However, in this model the implied product ranking will not differ across customers. To obtain personalized rankings we need to replace the observed population odds by odds ratios that are specifically relevant to customer i.</p><p>To this end we first apply k-means clustering to the purchase history data to obtain clusters of customers. Using these clusters, we can define cluster-specific odds ratios for each product and add these as predictor variables to our model. To capture the heterogeneity across customers we also include interaction between the cluster-specific odds ratios and a measure of the similarity between customer i and the cluster. As in LDA(-X) and MDM we can vary the number of identified clusters. For similarity with these methods we denote the number of clusters in this method by M. Notes. I, number of customers; M, number of segments/motivations/clusters; J, assortment size; n i , number of purchases made by customer i; N, total number of purchases.</p><p>Finally, the model can straightforwardly be extended using customer characteristics. We label this model as DCM, short for discrete choice model. Technical details are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Real-Time Online Predictions</head><p>For each of the prediction methods, it is straightforward to construct a product ranking over the assortment for each customer. In the context of online retailing it is important to continuously update this ranking based on the customer's new purchases. Re-estimating the (population-level) parameters can be done offline after a substantial amount of new data has been collected. However, updating the predictions for a specific customer should be feasible online. This allows the retailer to update predictions while customers select products during a shopping trip. For all methods, the real-time update step itself consists of simple arithmetic operations using the details provided in Online Appendix 2. A possible bottleneck could be the amount of data that must be available, retrieved, and processed to enable the updates. In the top half of Table <ref type="table" target="#tab_1">2</ref> we display the number of elements needed to update a single customer's product ranking in real-time, for each new product purchase that is observed. The bottom half of the table provides information on the amount of data that must be stored for the entire customer base to enable the aforementioned real-time update step.</p><p>The first row in Table <ref type="table" target="#tab_1">2</ref> mimics the context of our application, i.e., a medium-sized online retailer with an assortment of 500 products, 10,000 customers, and on average 10 purchases per customer. The number of segments/motivations/clusters (M) is set to 20, which is slightly larger than our empirical findings in this paper. We consider our implementation of a collaborative filter with combination size k = 2. In this context, the number of elements that must be selected for the real-time update step is of the same order of magnitude across the prediction methods. The storage requirements, on the other hand, are of a different order of magnitude, i.e., millions for the collaborative filter versus thousands for the model-based approaches. However, for these settings all methods can easily be used in practice.</p><p>To illustrate the scalability of the various methods we increase the size of the assortment and customer base by a factor of 10 and we double the average purchase history size and M. Naturally, all memory requirements increase in this setting, but the rate of growth differs significantly. For the collaborative filter the storage requirements increase approximately by a factor of 1,000, while the model-based approaches only increase by a factor of 20. The same holds for the third context, in which we again increase the dimensions. This illustrates that the dimension reduction achieved by the modelbased approaches ensures that they are suitable for real-time predictions in large scale applications, even if the number of underlying dimensions grows with the amount of available data. In addition, it is infeasible to use a combination size larger than k = 2 in our implementation of a collaborative filter, as in that case the storage requirements would increase even faster. For very large applications, one might even need to rely on the simpler CF-1, which only matches purchase histories on the presence of single products. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Performance Measures</head><p>To evaluate the methods for a range of different customization applications, we consider prediction sets of different sizes. A prediction set of size S contains the S highest ranked products for a customer. To recommend a single product, the prediction set of size 1 is most relevant. However, when customizing a page with search results the prediction set of size 10 may be more relevant. We assess the quality of a prediction set by matching its contents against hold-out purchase data. These purchases are denoted by y i for customer i; the number of unique purchased products in y i is given by u i .</p><p>We denote a complete ranking of all J products for customer i by the vector r i . The first element, r i1 , is the product with the highest predicted purchase probability for the model-based rankings, the highest product score for the collaborative filters, and the highest odds for DCM. The quality of a prediction set of size S can be measured by the number of products in the prediction set that overlap with the hold-out purchases: S s=1 I r is ∈ y i . This number should be seen relative to the maximum number of hits possible to obtain a hit rate that may be compared across prediction sets of different sizes. This maximum is bounded by S, the size of the prediction set, and the number of unique hold-out purchases u i . Hence, the hit rate for customer i could be defined as: S s=1 I r is ∈ y i / min S u i . If a prediction set is presented to a customer in an application, such as a recommendation list, the positions within the set are also of importance <ref type="bibr" target="#b31">(Xu and Kim 2008)</ref>. We incorporate this notion in our hit rate by weighing the hits according to their ranks. For the sth ranked product in a prediction set of size S this weight is specified as: w s S = 1 − s − 1 /S. Combining the above, we obtain our final performance measure, the weighted hit rate</p><formula xml:id="formula_13">h i r i S = S s=1 I r is ∈ y i w s S min S u i s=1 w s S (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head><p>We apply the prediction methods to purchase data from a medium-sized online retailer in the Netherlands. <ref type="bibr">7</ref> The data starts at the launch of the retailing platform and covers a period of approximately 67 weeks. The product assortment primarily consists of nonfood fast-moving consumer goods, such as detergents, deodorants, and shampoo. The assortment is complemented with a small selection of high turnover products for infants and toddlers, such as diapers and baby food. Consequently, the data contains many repeat purchases.</p><p>Initially, the data contains 3,226 unique product IDs. These IDs correspond to a very fine-grained classification, e.g., different package sizes of the same product each receive a unique ID. We opt for a more coarsegrained classification and combine products on the category-brand level. For example, different fragrances of the same deodorant brand are aggregated to one category-brand combination. This approach results in a total of 440 unique category-brand combinations. Additionally, this aggregation step is applied to the customer orders: If an order contains multiple products from the same category-brand, we consider this a single purchase from this category-brand. Finally, the category-brands that are purchased five times or fewer across all purchases are removed from the data. Below we refer to the category-brand combinations as "products." After the aggregation steps the data contains 95,208 product purchases of 394 products made by 11,783 distinct customers.</p><p>We chronologically split the data in two parts: The first 80% of the purchases are used as in-sample data; the hold-out data comprises the last 20% of the purchases. We use the hold-out data to assess the predictive  performance of the methods. This division mimics the setting of predicting future purchase behavior. Subsequently, we split the in-sample data into an estimation and a model-selection subset. We randomly select half of the customers from the in-sample data; for each of these customers, a single product purchase is randomly selected as model-selection data. We use the remaining in-sample data to estimate LDA(-X), MDM, and DCM, and to create the collaborative filters. The model-selection data is used to determine the number of motivations/segments/clusters (M) in LDA(-X), MDM, and DCM, respectively. Table <ref type="table" target="#tab_3">3</ref> summarizes the three data subsets in terms of number of customers, unique products, and number of product purchases.</p><p>It is likely that the type of customer acquired by the retailer changes over time, for example due to (a shift in) brand awareness or the mix of advertising channels used. Therefore, we investigate whether the customer's time of adoption at the retailer systematically shifts customer preferences. We define the time of adoption as the number of days between a customer's first order, and the starting date of the retailing platform. We take the natural logarithm of this variable to allow for larger shifts in the preferences of customers acquired during the early stages of the retailing platform. Finally, this variable is standardized using the mean and variance of the in-sample data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Predictive Performance</head><p>In this section we report on the predictive performance of the methods considered in this paper. First, for LDA(-X), MDM, and DCM we determine M, the number of motivations, segments, and clusters, respectively. For each model we select M using the model's average predictive likelihood for the model-selection data. For details refer to Appendix B. We select M = 13 for LDA, M = 15 for LDA-X, M = 11 for MDM, and M = 14 for DCM.</p><p>To assess a method's predictive performance we evaluate its weighted hit rate for the hold-out data, see (7). In the weighted hit rate, each hit receives a weight that depends on the rank assigned to the prediction. A better (numerical lower) rank receives a larger weight than a worse (numerical higher) rank.</p><p>Figure <ref type="figure" target="#fig_2">1</ref> presents the average weighted hit rate for each method, obtained by averaging the individualspecific hit rates across all customers in the hold-out data. In case we predict only a single product for each customer, i.e., a prediction set of size one, LDA-X has the best performance with a hit rate close to 0 45. For most prediction set sizes, LDA(-X) and MDM outperform the collaborative filters. The best performing collaborative filter is CF-2, which matches customers on the presence of pairs of products in their purchase history. Given the decent predictive likelihoods generated by DCM (see Appendix B), its performance in terms of ranking the purchased products is unexpectedly poor. Note that the average hit rate declines for the first few prediction set sizes. This is a direct consequence of the denominator in the definition of the hit rate in (7), which divides the total number of hits by the maximum number of hits possible for a given customer and prediction set size. This number increases with the size of the prediction set until it reaches the number of unique products purchased by the customer. As the average number of unique purchases per customer in the hold-out data is almost 5, the hit rates increase beyond that value for most methods.</p><p>We study the difference in performance for the prediction methods in more detail by separately considering specific groups of customers and products. In particular, we first divide the customers in the holdout data into three groups based on the number of purchases in the estimation data: (i) 2,185 customers with no prior observed purchases (Figure <ref type="figure" target="#fig_4">2</ref> The most apparent performance difference between these groups is visible in the range of the y-axis. If we observe many purchases for a customer the average hit rates are twice as large for the smaller prediction sets, compared to those for customers with no purchases in the estimation data. This is exactly according to our expectations, and provides empirical evidence that purchase history data is very informative about a customer's future purchases. By examining Figure <ref type="figure" target="#fig_4">2</ref>(a) we see that for customers without previous purchases the collaborative filters perform very well (particularly for moderate-sized prediction sets). Note that for this specific group of customers the collaborative filters rank the products according to their market penetration in the customer base. Also for LDA and MDM there is no information that can be used to make a personalized prediction. LDA-X uses the time of adoption, although this does not seem to shift the baseline predictions a lot. Hence, the performance differences between LDA(-X) and MDM are small.</p><p>In the absence of a purchase history, the similarity of a customer to each of the M clusters, used to create predictor variables in DCM, is meaningless. As a result, the DCM's predictive power is low for these customers. In fact, a large part of the performance gap on the complete hold-out data between DCM and the other prediction methods is driven by the poor performance for the group of customers without a purchase history.</p><p>We observe a different pattern for customers with a moderate number of past purchases in Figure <ref type="figure" target="#fig_4">2</ref>(b), where LDA(-X) and MDM consistently outperform the collaborative filters. This indicates that these modelbased methods are better able to learn from a customer's previous purchases than the collaborative filters. Comparing the methods, LDA(-X) attains the highest overall performance and performs best when we predict only a single product; MDM performs better for larger prediction sets. The performance of DCM is competitive for the smaller prediction sets, although its relative performance drops substantially for larger prediction set sizes.</p><p>The final group of customers that we consider are those who made many purchases, displayed in <ref type="bibr">Figure 2(c)</ref>. The general conclusion is similar to that of the customers with a moderate number of purchases. However, in this case MDM obtains the highest performance for prediction sets with more than one product. This result, combined with the previous findings, may be explained by the flexibility of the customer-level heterogeneity structure. In MDM preferences are modeled by a customer-specific probability vector over the product assortment. On the other hand, in LDA(-X) a customer's individual preferences are described by a lower-dimensional probability vector over the M motivations. Both models learn from previous purchases, but in MDM this learning is directly incorporated in the preferences over the assortment, while in LDA(-X) it is done indirectly through the probabilities for the motivations. Consequently, MDM has more degrees of freedom at the level of the individual customer as the assortment size J is much larger than the number of motivations M. This additional flexibility pays off when many purchases are observed for a customer.</p><p>The results above highlight the performance of the methods for the complete assortment. However, many of the highly ranked products are frequently purchased or products that have been previously purchased by the focal customer. Customers can easily anticipate such recommendations and might even be bored by them <ref type="bibr" target="#b7">(Fleder and Hosanagar 2009)</ref>. It is therefore interesting to evaluate the performance of the methods when predicting products that may be more unexpected.</p><p>To assess the performance of the methods for predicting such unexpected products, we evaluate the predictive performance for a restricted subset of the product assortment. This subset is constructed as follows: First, we remove 20% of the products in the assortment that are most frequently purchased in the estimation data. Second, we create a customer-specific restriction by removing the products that have previously been purchased by this customer. Subsequently, for each customer, we only consider the predictions and hold-out purchases for products in this restricted subset of the assortment. As customers are less likely to be aware of these products, performing well on this aspect could increase the cross-selling performance of marketing actions that are based on such predictions.</p><p>The predictive performance for the restricted set of products is displayed in Figure <ref type="figure" target="#fig_4">2</ref>(d). LDA and MDM perform better than the collaborative filters and DCM, but LDA-X clearly outperforms all of the other prediction methods. This remarkable performance difference primarily arises for the highly ranked products. By examining these products, we find that the product "Slimming nutrition-Weight Care" appears in the top of many of the LDA-X customer-level prediction sets. The prediction sets resulting from the other methods, however, do not contain this product. In fact, "Slimming nutrition-Weight Care" is the most frequently purchased product in the hold-out data. Its purchase frequency has shifted from 0 04% in the estimation data to 4 88% in the hold-out data. LDA-X captures this shift through the time of adoption variable. 8 This shows that the inclusion of predictor variables has merit in the context of purchase prediction, even though the time of adoption variable in general does not add much explanatory power. The reason that we do not see a similar shift for DCM can be explained by the way the predictor variables enter the model. In LDA-X, it directly influences the likelihood of a certain motivation, in effect boosting a motivation that is relevant for customers who adopted later in time. In this case, it boosts the motivation with products that are purchased more frequently later in the observation period, including the period of the hold-out predictions. By contrast, in DCM the clusters are determined "outside" the model, using the k-means algorithm. The performance of the clustering algorithm does not benefit from selecting a cluster that is linked to the other prediction variables, as the predictor variables are not included when constructing the clusters. In the absence of such clusters of customers, inclusion of the predictor variables cannot shift the importance of these products, as they are not in a separate cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have evaluated several methods for purchase prediction in large product assortments using purchase history data. Inspired by the text modeling literature, we have introduced a novel model-based approach that uses latent Dirichlet allocation (LDA(-X)) to predict purchases. In addition, we have considered mixtures of Dirichlet-Multinomials (MDM), a framework well known in the brand-choice modeling literature. The performance of these model-based approaches has been contrasted against two benchmarks, i.e., a set of count-based collaborative filters, in which customers are matched on the contents of their purchase history, and a scalable implementation of a discrete choice model (DCM), that does not break down when used with a large product assortment. All methods can construct customer-specific product rankings over the assortment that can be used for purchase prediction.</p><p>Naturally, the prediction methods differ in their heterogeneity assumptions, estimation complexity, and memory requirements. In MDM purchase heterogeneity is specified at the customer level by segmenting the customer base. In LDA(-X), on the other hand, this heterogeneity is specified at the motivation level, which groups products, not customers. These heterogeneity assumptions also affect the estimation complexity of the models. MDM has more flexibility to model a customer's purchase behavior than LDA(-X), but this comes at the price of increased estimation complexity as more parameters must be estimated. The estimation complexity of the logit part of the DCM is relatively low, but it does depend on customer clusters from an external method (i.e., the k-means algorithm). The collaborative filter has as advantage that no (latent) model structure must be estimated, but its storage requirements for generating real-time online predictions rapidly increase for large applications. By contrast, the model-based approaches require less storage and this grows much slower with the size of the application.</p><p>The performance of the methods was assessed based on purchase prediction sets derived from the product rankings, and comparing these sets to actual hold-out purchases. In general, LDA(-X) and MDM perform best and, even though these two models are conceptually different, their predictive performance is comparable. In addition, we have considered the setting where we focus on the predictive performance for products in the tail of the assortment that have not yet been purchased by the customer. In this case LDA-X clearly outperforms the other methods, which can be attributed to the time of adoption variable included in LDA-X. Although DCM also includes this predictor variable, its dependence on the k-means algorithm prevents it from effectively using the additional information to generate better predictions.</p><p>In summary the LDA(-X) prediction method that we have introduced in this paper is the most promising approach to purchase prediction, particularly in the context of large online retailers. Its predictive performance is very competitive compared with the other methods and it scales well with the size of the application. Finally, it is a self-contained prediction method that can readily accommodate additional information available to the retailer. In our application we only had access to a weak predictor, but the potential benefits of including stronger predictors of customer preferences into the model could be great.</p><p>To conclude, LDA(-X) can be readily used as a stepping stone for further model-based research that quantifies and optimizes the impact of marketing interventions in large scale retailing environments. For example, one could optimize a recommendation system that differentiates between the likelihood of purchasing a product and the added benefits from recommending that product <ref type="bibr">(Bodapati 2008, Wagner and</ref><ref type="bibr" target="#b28">Taudes 1986)</ref>; this is difficult to implement in a count-based method such as a collaborative filter. Such extensions are an interesting avenue for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Supplemental material to this paper is available at http://dx .doi.org/10.1287/mksc.2016.0985.</p><p>Marketing Science 35(3), pp. 389-404, © 2016 INFORMS the summary of all purchase histories reduced to product combinations of size k + 1. So, matching customers on pairs of products requires counts over triplets of products as input for the purchase predictions. The product ranking for each customer is constructed by sorting the products on the product score defined above. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Discrete Choice Models</head><p>In the binary logit model, the probability that customer i purchases product j is specified by Pr y in = j = exp ij</p><formula xml:id="formula_14">1 + exp ij (A3)</formula><p>Here, ij represents the log odds of having purchased product j. Ignoring heterogeneity among customers for the moment, these odds will largely be driven by the log of the number of (unique) products purchased by customer i, denoted by u i , and the relative attractiveness of product j. We capture the relative attractiveness of product j using the log odds of this product based on the observed product-purchase frequencies in the purchase data at the customer-base level. This leads to the following expression for the log odds of customer i buying product j:</p><formula xml:id="formula_15">ij = + log u i + log odds j (A4)</formula><p>The product ranking resulting from this specification will be the same for all customers as the product attractiveness is defined at the customer-base level, not the customer level. To obtain predictions that differ across customers, we introduce heterogeneity in the model. To achieve this without resorting to a model with unobserved heterogeneity, as in LDA(-X) or in MDM, or requiring an excessive number of parameters, as in a regular choice model implementation, we construct variables at the customer-product level that characterize the attractiveness of product j for customer i using the available purchase history data.</p><p>The first step is to characterize customers based on their purchase history. We describe each customer's purchases by v i , a J -dimensional vector containing the proportions of each product in the customer's purchase history, with J p=1 v ip = 1. <ref type="bibr">11</ref> We then perform k-means clustering on these proportion vectors using M clusters. Customer heterogeneity can now be characterized by a customer's similarity with respect to each of the cluster means. We define the similarity of customer i with cluster m as</p><formula xml:id="formula_16">w im = 1 1 + v i −v m</formula><p>where v i −v m measures the Euclidean distance between customer i's proportion vector and the mth cluster meanv m . <ref type="bibr">10</ref> In the rare case that two or more products receive the same score, they are ranked according to their order in the data set, which is alphabetical. <ref type="bibr">11</ref> For smoothing purposes we add one pseudo observation to each customer's purchase history that is equal to the relative market shares of each product.</p><p>We can now parsimoniously introduce customer-level heterogeneity by combining the cluster-level product attractiveness and the similarity measures w im that capture the relevance of each cluster for each customer</p><formula xml:id="formula_17">ij = + log u i + M m=1 log o mj 1m + 2m w im (A5)</formula><p>where o mj denotes the odds for product j that corresponds to the purchase proportions in cluster meanv m . Note that in this model specification, the parameters are not product specific, as the relative attractiveness of each product is captured through the summary of the purchase behavior of the various clusters. 12 Maximum likelihood estimation of this parsimonious discrete choice model (DCM) is relatively straightforward and including the other available predictor variables is therefore feasible. To do so, we extend the specification in (A5) to include interactions with the customer-specific predictor variables in x i , resulting in</p><formula xml:id="formula_18">ij = + log u i + M m=1 log o mj • 1m + 2m w im + K k=1</formula><p>x ik 1km + 2km w im (A6) fluctuates across values of M. Note that the average predictive likelihood is merely an indicator for the actual predictive performance in our application, as we will consider the rank assigned to purchased products to evaluate the predictive performance and not actual purchase likelihoods.</p><p>To determine the number of motivations and segments in LDA(-X) and MDM, we select the first value of M for which the average predictive likelihood decreases when M is increased by 1, i.e., we select the first local maximum. As the graphs in Figure <ref type="figure">B</ref>.1(a) stabilize after their first local maximum, this approach results in a parsimonious, yet high performing model specification. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Jacobs, Donkers, and Fok: Model-Based  Purchase Predictions for Large Assortments 390 Marketing Science 35(3), pp. 389-404, © 2016 INFORMS paper we focus on predicting purchase behavior based on purchase history data, possibly complemented with customer characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Marketing</head><label></label><figDesc>Science 35(3), pp. 389-404, © 2016 INFORMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1 (Color online) Predictive Performance for the Complete Hold-Out Data, as a Function of Prediction Set Size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a)); (ii) 809 customers with a moderate amount (1-9) of purchases (Figure 2(b)); and (iii) 751 customers with many (10 or more) purchases (Figure 2(c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 (</head><label>2</label><figDesc>Figure 2 (Color online) Predictive Performance for Different Groups of Customers/Products in the Hold-Out Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure B. 1 (</head><label>1</label><figDesc>Figure B.1 (Color online) Average Predictive Likelihood for the Model-Selection Data as a Function of M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure B.1(b) shows the differences in performance between subsequent values of M. The first negative value, corresponding to a decrease in performance, is obtained at M = 14 for LDA, M = 16 for LDA-X, and M = 12 for MDM. Hence, we select M = 13 for LDA, M = 15 for LDA-X, and M = 11 for MDM. The average predictive likelihood is more volatile across values of M for DCM, resulting in the first local maximum for M = 4. In the spirit of our M selection criterion for LDA(-X) and MDM, we instead select the smallest value of M that corresponds to a local maximum in the range of the values of M where the predictive likelihood has leveled off. For DCM, this happens at M = 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Parameters to Sample in the MCMC Estimation Procedures Across Different Models</figDesc><table><row><cell></cell><cell></cell><cell>Gibbs sampler</cell><cell>Metropolis-Hastings sampler</cell></row><row><cell cols="3">Model Parameters No. parameters</cell><cell>Parameters</cell><cell>No. parameters</cell></row><row><cell>MDM</cell><cell>s</cell><cell>I</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="6">Comparison of Memory Requirements for Real-Time Updating</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">No. of selected data elements for each real-time update step</cell><cell></cell></row><row><cell>Retailer context</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I</cell><cell>J</cell><cell>n i</cell><cell>M</cell><cell>LDA(-X)</cell><cell>MDM</cell><cell>CF-2</cell><cell>DCM</cell></row><row><cell>10,000</cell><cell>500</cell><cell>10</cell><cell>20</cell><cell>1 00 • 10 4</cell><cell>1 00 • 10 4</cell><cell>5 51 • 10 3</cell><cell>1 01 • 10 4</cell></row><row><cell>100,000</cell><cell>5 000</cell><cell>20</cell><cell>40</cell><cell>2 00 • 10 5</cell><cell>2 00 • 10 5</cell><cell>1 05 • 10 5</cell><cell>2 00 • 10 5</cell></row><row><cell>1,000,000</cell><cell>50 000</cell><cell>40</cell><cell>80</cell><cell>4 00 • 10 6</cell><cell>4 00 • 10 6</cell><cell>2 05 • 10 6</cell><cell>4 00 • 10 6</cell></row><row><cell></cell><cell></cell><cell cols="4">No. of stored data elements for the real-time update step</cell><cell></cell><cell></cell></row><row><cell>Retailer context</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I</cell><cell>J</cell><cell>N/I</cell><cell>M</cell><cell>LDA(-X)</cell><cell>MDM</cell><cell>CF-2</cell><cell>DCM</cell></row><row><cell>10,000</cell><cell>500</cell><cell>10</cell><cell>20</cell><cell>2 10 • 10 5</cell><cell>3 10 • 10 5</cell><cell>6 77 • 10 7</cell><cell>1 10 • 10 5</cell></row><row><cell>100,000</cell><cell>5 000</cell><cell>20</cell><cell>40</cell><cell>4 20 • 10 6</cell><cell>6 20 • 10 6</cell><cell>6 30 • 10 10</cell><cell>2 20 • 10 6</cell></row><row><cell>1,000,000</cell><cell>50 000</cell><cell>40</cell><cell>80</cell><cell>8 40 • 10 7</cell><cell>1 24 • 10 8</cell><cell>6 26 • 10 13</cell><cell>4 40 • 10 7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Characteristics of the Three Subsets of the Purchase Data</figDesc><table><row><cell>Subset</cell><cell>Customers</cell><cell>Unique products</cell><cell>Purchases</cell></row><row><cell>Full data</cell><cell>11,783</cell><cell>394</cell><cell>95,208</cell></row><row><cell>Estimation data</cell><cell>8,831</cell><cell>393</cell><cell>71,346</cell></row><row><cell>Model-selection data</cell><cell>4,820</cell><cell>323</cell><cell>4,820</cell></row><row><cell>Hold-out data</cell><cell>3,745</cell><cell>369</cell><cell>19,042</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While intuitively plausible, we do not claim that the actual decision process is driven by these motivations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Jacobs, Donkers, and Fok: Model-Based Purchase Predictions for Large Assortments</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Marketing Science 35(3), pp.389-404, © 2016 INFORMS   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To validate this approach we also consider the models for larger values of M. The predictive performance stabilized at the values obtained with the selected value of M.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">More details on the required sampling steps can be found in Online Appendix 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Use of unique sorted pairs implies that 1 1 occurs in H 2 i only once and that H 2 i contains the pair 1 2 and not 2 1 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In our application, this simpler collaborative filter performs systematically worse than CF-2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The authors wish to thank Christian van Someren, former Managing Director of Truus.nl, for kindly providing us this data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We acknowledge that there can be many external influences that drive this shift in purchase behavior. Our predictor variable (time of adoption) likely serves as a proxy for the actual causes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">For k &gt; 0 it is possible that a product combination h is never purchased with another product, i.e., for all p we have J p=1 c h p = 0 in (A2). If a customer's purchase history contains such a combination, we regress to a lower value of k for this customer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Model specifications where the coefficients were allowed to be product specific suffered from severe identification problems in our application as the number of parameters is increased by a factor J .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the review team for their suggestions and remarks that helped us to improve this paper. The computations for this work were carried out on the Dutch national e-infrastructure with the support of SURF Cooperative.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Technical Details for the Benchmark Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Collaborative Filtering</head><p>We refer to a subvector of a customer's purchase history as a product combination, denoted by h, and c h gives the number of customers who purchased product combination h, that is</p><p>where dim h denotes the dimension of h and I A equals 1 if condition A is true and 0 otherwise. To obtain purchase predictions for customer i, using product combinations of size k, we score all products in the assortment based on their co-occurrence with each of the product combinations in H k i . For product combination h ∈ H k i the prediction score for product j equals the number of customers who purchased j and the products in h, normalized by the sum of the score for h and any product p = 1 J . This normalization ensures that each product combination h ∈ H k i receives the same weight, independent of the prevalence of h in other customers' purchase histories. The final product score is the sum of the normalized scores across all h ∈ H k i . Formally, for combination size k, the overall score of product j for customer i equals</p><p>where the arguments between angle brackets represent a single product combination of size k + 1. 9 Hence, to obtain product scores s k ij , by matching customers based on purchase histories reduced to combinations of size k, we need</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Model Selection</head><p>In all model-based approaches we must determine M the number of motivations, segments, and clusters. We evaluate LDA(-X) for M = 3 30 and MDM for M = 1 30, where MDM with M = 1 corresponds to the DM model. For each of these model configurations (choice of model plus a value of M) we use 250 different random starts to avoid local maxima. Throughout the estimation procedure the performance of each random start is measured by the average predictive likelihood for the model-selection data. As discussed in §2.3, at several points during the procedure we drop the worst-performing starting values (see Online Appendix 1). At the end of the estimation routine we use the parameter estimates that result from the random start with the highest average predictive likelihood. We evaluate DCM for M = 2 30. To avoid local maxima in the k-means algorithm used in DCM, we use 1,000 different random cluster initializations. For each value of M, the clustering that obtains the lowest in cluster sum-of-squares is selected.</p><p>The average predictive likelihoods for the model-based approaches are shown in <ref type="bibr">Figure B.1(a)</ref>. We find that for each method the average predictive likelihood steeply increases for the first few values of M and then levels off for larger values of M. This result indicates that choosing M too small likely impedes performance more than choosing M too large. The average predictive likelihood of LDA and LDA-X is similar, reaching a value of approximately 0.05 for the larger values of M. MDM performs slightly better, reaching a value close to 0.055. DCM performs similarly and in between LDA(-X) and MDM, although its performance</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On smoothing and inference for topic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Twenty-Fifth Conf. Uncertainty Artificial Intelligence</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</editor>
		<meeting>Twenty-Fifth Conf. Uncertainty Artificial Intelligence<address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recommendation systems with purchase data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Bodapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gibbs sampling for Bayesian non-conjugate and hierarchical models by using auxiliary variables</title>
		<author>
			<persName><forename type="first">P</forename><surname>Damien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wakefield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B Statist. Methodology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="331" to="344" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrating the Dirichlet-multinomial and multinomial logit models of brand choice</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="112" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling consumer choice among SKUs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bgs</forename><surname>Hardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="452" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Excess behavioral loyalty for highshare brands: Deviations from the Dirichlet model for repeat purchasing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Schmittlein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="478" to="493" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blockbuster culture&apos;s next rise or fall: The impact of recommender systems on sales diversity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fleder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hosanagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="697" to="712" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Examining the impact of ranking on consumer behavior and search engine revenue</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1632" to="1654" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Dirichlet: A comprehensive model of buying behaviour</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Goodhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asc</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chatfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. A General</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="621" to="655" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<title level="m">Finding scientific topics. Proc. Natl. Acad. Sci. USA</title>
				<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simultaneous approach to market segmentation and market structuring</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="153" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A logit model of brand choice calibrated on scanner data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Guadagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jdc</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="238" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of latent class models with heterogeneous choice probabilities: An application to market structuring</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="101" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zanker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Felfernig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedrich</surname></persName>
		</author>
		<title level="m">Recommender Systems: An Introduction</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A multibrand stochastic model compounding heterogeneous Erlang timing and multinomial choice processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Jeuland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="277" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hybrid of sequential rules and collaborative filtering for product recommendation</title>
		<author>
			<persName><forename type="first">D-R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="3505" to="3519" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Maddala</surname></persName>
		</author>
		<title level="m">Limited-Dependent and Qualitative Variables in Econometrics</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The choice theory approach to market research</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfadden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="297" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse stochastic inference for latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Twenty-Ninth Internat. Conf. Machine Learn</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</editor>
		<meeting>Twenty-Ninth Internat. Conf. Machine Learn<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1599" to="1606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bodapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kamakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kreulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montgomery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Model-Based Purchase Predictions for Large Assortments 404</title>
		<author>
			<persName><forename type="first">Donkers</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><surname>Fok</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">2016 INFORMS Challenges and opportunities in high-dimensional choice data analyses</title>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="213" />
		</imprint>
	</monogr>
	<note>Marketing Lett.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Granka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Google we trust: Users decisions on rank, position, and relevance</title>
				<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="801" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Characterizing microblogs with topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liebling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Internat. AAAI Conf. Weblogs Soc. Media</title>
				<editor>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
			<persName><forename type="first">S</forename><surname>Gosling</surname></persName>
		</editor>
		<meeting>Fourth Internat. AAAI Conf. Weblogs Soc. Media<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Experimental study of inequality and unpredictability in an artificial cultural market</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Salganik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="issue">5762</biblScope>
			<biblScope unit="page" from="854" to="856" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Latent Semantic Analysis</title>
				<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Mcnamara</surname></persName>
			<persName><forename type="first">S</forename><surname>Dennis</surname></persName>
			<persName><forename type="first">W</forename><surname>Kintsch</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="424" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Train</surname></persName>
		</author>
		<title level="m">Discrete Choice Methods with Simulation</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A multivariate Polya model of brand choice and purchase incidence</title>
		<author>
			<persName><forename type="first">U</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taudes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="244" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking LDA: Why priors matter</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cki</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Market Segmentation: Conceptual and Methodological Foundations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Kamakura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Order effect and vendor inspection in online comparison shopping</title>
		<author>
			<persName><forename type="first">Y(c)</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Retailing</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="486" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data pruning in consumer choice models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Zanutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Marketing Econom</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
