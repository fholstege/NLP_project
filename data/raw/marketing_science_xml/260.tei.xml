<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org A Video-Based Automated Recommender (VAR) System for Garments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-04-25">April 25, 2016.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shasha</forename><surname>Lu</surname></persName>
							<email>s.lu@jbs.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Cambridge Judge Business School</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1AG</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Management</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Xiao</surname></persName>
							<email>lixiao@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cambridge Judge Business School</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1AG</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Smeal College of Business</orgName>
								<orgName type="institution">Pennsylvania State University</orgName>
								<address>
									<postCode>16802</postCode>
									<settlement>University Park</settlement>
									<region>Pennsylvania</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Ding</surname></persName>
							<email>minding@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Cambridge Judge Business School</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB2 1AG</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Management</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org A Video-Based Automated Recommender (VAR) System for Garments</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2016-04-25">April 25, 2016.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2016.0984</idno>
					<note type="submission">Received: December 31, 2013; accepted: January 27, 2016; Pradeep Chintagunta, Dominique Hanssens, and John Hauser served as the special issue editors and Olivier Toubia served as associate editor for this article.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>retailing</term>
					<term>video analysis</term>
					<term>collaborative filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clothing and accessories retailer Zara launches 11,000 designs on store shelves every year. Although a retail chain like Zara usually maintains a wide variety of products at its warehouses, each store only displays a smaller set of items because of space constraints. Even so, evaluating all of the items in a single store would require a significant time commitment and cognitive effort from customers. Since they only evaluate a subset of the available inventory, shoppers may leave the store without finding anything they like, even when the retailer may carry products that appeal to their preferences-at that store, a different store, or the warehouse.</p><p>To address this problem in retail stores, it is crucial for retailers to understand customer preferences and recommend products accordingly. To this end, retailers usually adopt two strategies: (a) train salespeople to discern customer preferences based on solicited and/or unsolicited feedback from individuals and provide appropriate recommendations; and (b) use marketing research tools such as conjoint analysis to understand customer preferences for a variety of product features and make recommendations accordingly.</p><p>However, these two strategies have several limitations. The first strategy is expensive because an experienced salesperson must be paid well and more employees must be hired in order for the strategy to be effective. In addition, salespeople have a limited ability to remember every product in a company's available inventory and the quality of recommendations from salespeople may vary widely. Furthermore, some <ref type="bibr">Lu, Xiao, and Ding: Video-Based Automated Recommender (VAR)</ref> System for Garments Marketing Science 35(3), pp. 484-510, Â© 2016 INFORMS 485 customers may not like being observed by salespeople and may even be suspicious of their recommendations. The second strategy is rarely used to provide personalized customer recommendations in the real world, because the costs associated with asking every customer in a store to complete a conjoint task during the shopping process would be significant. The recommendations are thus based on average preferences; this is clearly a suboptimal strategy, since individuals are very likely to have heterogeneous preferences.</p><p>To address these problems in retail stores, we have created an automated and scalable garment recommender system using real-time in-store videos that can improve the experiences of garment shoppers and increase product sales while requiring minimal extra effort from customers. The video-based automated recommender (VAR) system is based on observations that garment shoppers tend to try on garments and evaluate themselves in front of store mirrors. Combining state-of-the-art computer vision techniques and marketing models of consumer preferences, the system automatically identifies shoppers' preferences based on their reactions and uses that information to make meaningful personalized recommendations. First, the system uses a camera to capture a shopper's behavior in front of the mirror to make inferences about her preferences based on her facial expressions and the part of the garment she is examining (region of interest) at each time point. Second, the system matches this shopper with a database of shoppers whose preferences, purchasing, and/or consideration decisions are known, and identifies a small set of shoppers with similar preferences to the focal customer. Finally, recommendations are made to the focal customer based on the preferences, purchasing, and/or consideration decisions of these like-minded shoppers (referred to as nearest neighbors in this study). Each of the three steps can be implemented with several variations, and a retailing chain can choose the specific configuration that best serves its purpose.</p><p>The system can be used mainly for making real-time individualized recommendations, and it also provides useful self-evaluation tools for customers (e.g., allowing prior try-on videos and/or frames to be played and compared later). The information extracted from try-on videos across customers and garments may also provide valuable insights in the big data framework. To our knowledge, this is one of the first attempts to integrate video analysis (real-time facial expression recognition and hand detection) at the individual customer level with extant marketing research methods to create useful managerial tools in the retail context.</p><p>Our aim is to demonstrate a proof of concept that the information inferred from video (automated detection of customers' facial expressions and regions of interest, in our case) has value in predicting customers' individual preferences toward different garments with minimal extra effort on their part. Although the potential benefits of our model to retailers and customers are promising, we believe that many possible variations could be implemented. We hope that the present study will serve as a meaningful starting point for future work on the automated extraction of information from videos to help both consumers and retailers.</p><p>The rest of this article is organized as follows. First, we review literature on video data in marketing and relevant practices in the retailing context. Second, we describe the system, its general structure and variations, and how each step is implemented. We then describe an empirical study (Study 1) designed to test the validity of the system by comparing one specific implementation of the system with two nonautomated individual level recommender systems. The feasibility and value of the system in the real world is demonstrated in a second empirical study (Study 2). This is followed by a section on building a scalable VAR system for retail chains, and a section on mining valuable information from large-scale try-on video data. We conclude with a general discussion on the use of video analysis in marketing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review</head><p>In this section, we begin by describing challenges associated with inferring customer preferences in retail stores. Then, we provide a brief review of extant video-based research in marketing, as well as potential uses of video data. Although we do not review the vast literature on video analysis from the computer science discipline in this section, we do discuss relevant literature in our methods and implementation sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Challenges Associated with Determining</head><p>Customer Preferences in Retail A typical customer has neither the time nor the mental capacity to evaluate even a small subset of store displays <ref type="bibr">Lee 2004, Scheibehenne et al. 2010)</ref>. To reduce customer effort and increase sales, one possible approach is to provide individualized recommendations <ref type="bibr" target="#b4">(Aljukhadar et al. 2012</ref>) based on customer preferences. Yet, identifying customer preferences in retail stores is challenging. Customers not only have different preferences, they also generally do not voluntarily provide structured preference information to retailers. Moreover, customers only evaluate a small subset of displayed products and partial product features while shopping, so preferences must be inferred based on just a few products.</p><p>To address these challenges, retailers typically use two strategies. First, retailers hire salespeople to communicate with customers about their product preferences and assist them while they shop. Salespeople usually make recommendations based on subjective judgments Marketing Science 35(3), pp. 484-510, Â© 2016 INFORMS about customer preferences, the effectiveness of which depends greatly on their skills and experiences, and the quality of their interactions with customers <ref type="bibr" target="#b25">(Franke and</ref><ref type="bibr">Park 2006, Weitz et al. 1986)</ref>. It is also very difficult for a salesperson to memorize information about every item in stock to make informative recommendations <ref type="bibr" target="#b101">(Weitz et al. 1986)</ref>. Customers are at risk of experiencing undesirable outcomes if a salesperson is unable to provide useful information or is not motivated to protect a customer's interests <ref type="bibr" target="#b91">(Swan et al. 1999)</ref>. Second, practitioners use preference measurement models such as conjoint analysis to infer customer preferences about product features <ref type="bibr" target="#b32">(Green and Srinivasan 1990</ref>). The typical practice in business is to estimate conjoint models at the aggregate level, given individual-level part-worth estimates <ref type="bibr" target="#b15">(DeSarbo et al. 1995)</ref>. In this way, retailers can recommend popular items with the features customers prefer most and display them in prominent places. However, this is a one-size-fits-all strategy; the same products are always recommended, regardless of a customer's preferences. Although providing individualized recommendations is preferable <ref type="bibr" target="#b5">(Ariely et al. 2004)</ref>, customers would need to indicate their preferences as they shop, which is rarely done in practice because of the extra effort involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Using Video Data in Marketing</head><p>Videos can be used to record and analyze behavior in situational contexts, which means such data can potentially reveal both qualitative and quantitative insights <ref type="bibr">(Basil 2011, Belk and</ref><ref type="bibr" target="#b8">Kozinets 2005)</ref>. As video data and inexpensive video editing hardware and software become more prevalent, marketing researchers and practitioners are harnessing the potential of video data to generate insights for business practice <ref type="bibr" target="#b8">(Belk and Kozinets 2005)</ref>.</p><p>With the rapid increase in computation power, we are now able to capture, store, and analyze video data for various purposes, such as face recognition <ref type="bibr" target="#b110">(Zhao et al. 2003)</ref>, facial expression recognition <ref type="bibr" target="#b24">(Fasel and Luettin 2003</ref><ref type="bibr" target="#b78">, Russell and Fernandez-Dols 1997</ref><ref type="bibr" target="#b87">, Shergill et al. 2008</ref>, and gesture detection <ref type="bibr" target="#b51">(Kuch and</ref><ref type="bibr">Huang 1995, Yoruk et al. 2006</ref>), among others. Video analysis has been applied in various disciplines such as artificial intelligence, human-computer interaction, biometrics, and marketing. Commercial tools 1 have also been developed using video analysis to help marketers identify customer demographics.</p><p>In marketing, video data provide rich information that can be useful to both consumers and managers if properly used <ref type="bibr" target="#b7">(Belk and</ref><ref type="bibr">Kozinets 2005, Lee and</ref><ref type="bibr" target="#b53">Broderick 2007)</ref>. <ref type="bibr" target="#b87">Shergill et al. (2008)</ref> proposed a framework for using video data to allocate salespeople to customers, and <ref type="bibr" target="#b7">Belk (2011)</ref> proposed using documentary videos to examine consumer behavior. Given its potential to yield rich insights into consumer behavior, video analysis is being increasingly used in retail contexts (e.g., <ref type="bibr" target="#b38">Hui et al. 2009a</ref><ref type="bibr">Hui et al. , b, 2013</ref><ref type="bibr" target="#b97">Valizade et al. 2014;</ref><ref type="bibr" target="#b108">Zhang et al. 2014)</ref>. <ref type="bibr" target="#b40">Hui et al. (2013)</ref> first used in-store video tracking to collect data about customers' in-store shopping paths, shedding light on customers' product consideration processes.</p><p>Video data can provide information on the temporal, spatial, and social dimensions of objects, as well as psychological information about customers <ref type="bibr" target="#b48">(Kozinets and Belk 2006)</ref>, such as emotional reactions, associated stimulating factors, and other simultaneous behavioral responses without interrupting the normal shopping process. Affective responses play an important role in information processing and product evaluation <ref type="bibr" target="#b69">(Pham 1998</ref>; see also <ref type="bibr">Clore 1983, 1988;</ref><ref type="bibr" target="#b102">Wyer and Carlston 1979)</ref>, thereby driving customer behavior <ref type="bibr">(Roseman et al. 1996, Zeelenberg and</ref><ref type="bibr" target="#b107">Pieters 2004)</ref>. Positive affective responses result in positive evaluations of the focal product, whereas negative affective responses result in negative evaluations <ref type="bibr" target="#b10">(Bloch 1995)</ref>. The emotion theory literature <ref type="bibr" target="#b27">(Frijda 1986</ref><ref type="bibr" target="#b28">, Frijda and Zeelenberg 2001</ref><ref type="bibr" target="#b76">, Roseman et al. 1996</ref> shows that different emotions can lead to "different behavioral tendencies (action tendencies or patterns of action readiness) and behavioral consequences" <ref type="bibr">(Zeelenberg and Pieters 2004, p. 446)</ref>.</p><p>Customers' behavioral responses also serve as important clues in evaluating their preferences. For example, touching (conscious or unconscious) plays a prominent role in garment evaluation <ref type="bibr">(Grohmann et al. 2007, McCabe and</ref><ref type="bibr" target="#b59">Nowlis 2003)</ref>. The first sense that humans develop, touching, is a form of analytical or systematic (versus relational) processing in which one feature is evaluated at a time <ref type="bibr" target="#b105">(Yazdanparast and Spears 2012)</ref>. People use their hands to acquire and process information about objects, sometimes simply for the sake of sensation <ref type="bibr">(Klatzky et al. 1993, Peck and</ref><ref type="bibr" target="#b66">Childers 2003)</ref>. Evidence shows that tactile cues are more influential than visual cues in customers' evaluations of clothing products, which have diverse material properties, such as texture and stretch <ref type="bibr" target="#b33">(Grohmann et al. 2007</ref><ref type="bibr" target="#b37">, Holbrook 1983</ref>).</p><p>Very few marketing scholars have developed models to apply automatic video analysis to infer customer preferences in real business contexts. In the few existing empirical marketing studies, human judgment was used to analyze video or image data, which is labor intensive and time consuming for large data sets and cannot be scaled up or accomplished in real time. To our knowledge, we are among the first to automatically infer individual preference information from video data in the retailing context and to develop managerial tools that combine video technology from computer science with current standard marketing research methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">General Design of a VAR System</head><p>In this section, we describe the general design of a VAR system. The system is designed to satisfy several key criteria that must be met in a retailing environment: (a) make recommendations for individuals based on their individualized preferences; (b) require minimal customer effort and not interfere with a customer's normal shopping experience; (c) address a customer's need for privacy; and (d) be easy to implement in a retail store and scale well to large retail chains. We first describe the system setup and the initiation and termination of each recommendation. We then discuss each of the three analytic steps involved in making a recommendation in detail. The general structure of the system is illustrated in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">System Setup and Recommendation Process</head><p>The system's hardware includes a central computing unit connected to many pairs of decentralized in-store user interface devices. Each pair is comprised of an input device (i.e., webcam) and an output device (i.e., display). The central computing unit can be either onsite or virtual in the form of rented servers (including database servers) in the cloud computing environment. The system's software resides on the central computing unit, and includes codes for the three analytic steps as well as two databases: an inventory database and a customer database. The inventory database is updated dynamically as new items are added, and the customer database expands as more customers use the system. The inventory database may be comprised of an existing store database (e.g., item ID/barcode and a photo), or may include additional information on each garment (e.g., garment regions, feature descriptions). The customer database includes an organically growing set of information about which items a customer has tried and her associated reactions as captured via video analysis during her try-on experiences; these data are linked with eventual purchase decisions (if made) during a shopping trip. We call this a try-on customer database.</p><p>There are two caveats regarding the try-on customer database. First, depending on the opt-in level a customer selects, entries in the database may be (a) independent (i.e., when a customer chooses not to be tracked across different try-ons during a shopping trip); (b) linked to the same customer during one shopping trip only (i.e., if a customer chooses to be tracked across different try-ons during a shopping trip but wants her identifier, such as face image, to be erased at the end of the business day); or (c) linked to the same customer over multiple shopping trips (possibly to different stores of the same chain, if she allows her identifier, either her face or an ID number, to be permanently stored). Independent entries are generally not useful in helping the system make recommendations to <ref type="bibr">Marketing Science 35(3), pp. 484-510, Â© 2016 INFORMS</ref> other customers. Second, the purchase decisions made by a customer during a given shopping trip may be connected to the try-on data in several different ways with potentially varied precision. For example, one way to link a try-on to a purchase is to assign each garment a unique ID, which is recorded both while the customer is trying on the item and at purchase. Another way is to identify a customer's identity at the checkout counter using face recognition software and to match it against customers who tried garments on that particular business day in that store (assuming she has opted-in to tracking).</p><p>The VAR system is initiated once a customer opts-in by scanning the product barcode/ID on the system's barcode reader. This mechanism also enables the system to identify which product the customer is evaluating. By scanning the garment ID, the customer activates the camera to capture her reactions. As the customer evaluates the garment, the video camera records the process and sends information to the central computing unit for analysis. The system terminates the analysis when the customer leaves the evaluation area (i.e., when the frontal upper body cannot be detected for a certain continuous period of time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Step 1: Infer Preferences from</head><p>Try-On Video Data The purpose of the first analytic step is to use video data to infer some preference information about a focal customer based on her reactions toward a garment she is trying on. As described in detail in the literature review, two visual cues can be obtained from customers in a retailing context: affective responses and behavioral responses.</p><p>Facial expression is a good and natural indication of a customer's internal emotions and mental activities <ref type="bibr" target="#b78">(Russell and Fernandez-Dols 1997)</ref> and thus is often used as a proxy for a person's affective state in both practice and research. For example, companies such as Procter &amp; Gamble and Unilever collect high-frequency data on facial expressions to understand their influence on consumer behavior <ref type="bibr" target="#b94">(Teixeira et al. 2012)</ref>, and GfK tracks viewers' facial expressions for copy testing <ref type="bibr" target="#b60">(Miller 2013</ref>). In research contexts, facial expressions have been used to study viewers' preferences toward Internet video commercials <ref type="bibr" target="#b94">(Teixeira et al. 2012</ref><ref type="bibr" target="#b93">(Teixeira et al. , 2014</ref>, and how customers react to different online shopping contexts in virtual stores <ref type="bibr" target="#b70">(Raouzaiou et al. 2002)</ref>.</p><p>Behavioral responses (e.g., touching) are also crucial in the evaluation of products, especially apparel <ref type="bibr" target="#b66">Childers 2003, Peck and</ref><ref type="bibr" target="#b68">Wiggins 2006)</ref>. It has been well documented in the literature that hand movements are associated with customers' exploratory and evaluative perceptions <ref type="bibr">(Krishna 2009, Peck and</ref><ref type="bibr" target="#b66">Childers 2003)</ref>; in fact, touching (haptic perception) is the dominant input for determining product quality and can increase perceptions of ownership <ref type="bibr" target="#b67">(Peck and Shu 2009)</ref>. When a customer wants to assess a fashion element on a garment, she may touch the corresponding area repeatedly. We call this a region of interest in the present paper.</p><p>The VAR system infers customers' preferences by simultaneously analyzing affective responses (facial expressions) and behavioral responses (region of interest being touched) captured on video as they evaluate garments in front of a mirror. Thanks to advanced computer vision techniques, a customer's facial expressions and the areas of a garment that a customer touches can both be automatically inferred from video data with reasonable accuracy (see a recent review by <ref type="bibr" target="#b104">Xiao et al. 2013)</ref>. We describe the analysis process below (see Figure <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Preprocessing.</head><p>Once the process is initiated (i.e., a customer opts in), the actual video analysis process begins when the system detects a person (more specifically, the frontal upper body) in the scene using the local context 2 detector <ref type="bibr" target="#b50">(Kruppa et al. 2003)</ref>. The detector uses the differences between the sum of the pixels within two rectangular regions (Haar-like features 3 ) to encode the details of the head, neck, and shoulder area, and Adaboost to select features and train the classifier. This upper body detector 4 is trained with images that contain a person's head, neck, and shoulder area. After it detects the location of the human body, the system crops the image so it contains only the focal customer and the garment the customer is evaluating, for further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Face Recognition and Facial Expression</head><p>Recognition. <ref type="bibr">5</ref> Face recognition and facial expression recognition serve different purposes and are performed at different time points in the VAR system but are achieved with similar techniques <ref type="bibr" target="#b12">(Chavan and</ref><ref type="bibr">Kulkarni 2013, Fasel and</ref><ref type="bibr" target="#b24">Luettin 2003)</ref> and thus are discussed here in the same subsection. Face recognition allows the VAR system to link a customer's try-on experiences by matching faces in the video data (this only needs to be done at the beginning of each try-on video). Facial expression recognition is used to capture the affective state of a customer in each frame analyzed. Since video is actually a temporal sequence of still images (called frames) representing scenes in motion, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region of interest detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial expression recognition</head><p>Infer preferences toward the try-on garment</p><p>Frame extracted from try-on video</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identify neighborhood</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Make recommendations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face recognition</head><p>Note. We did not use face recognition to track customers in our empirical studies. However, we have checked our codes and the VAR system recognizes faces with high accuracy (see discussion in Â§6.1.1). facial expression recognition can be regarded as recognizing facial expression in each frame in the video. <ref type="bibr">6</ref> The general process involves three steps: face detection, feature extraction, and face/expression classification (see <ref type="bibr" target="#b12">Chavan and</ref><ref type="bibr">Kulkarni 2013, Fasel and</ref><ref type="bibr" target="#b24">Luettin 2003</ref> for literature reviews on these steps).</p><p>Face detection. There are three main methods for automatic face detection-template matching, feature based (e.g., skin color), and image based-that train machine systems on large numbers of samples (i.e., images labeled as face or nonface). Among these, image-based methods perform the best and achieve good accuracy in detecting faces in images <ref type="bibr">(Rowley et al. 1998, Sung and</ref><ref type="bibr" target="#b90">Poggio 1998)</ref>. The most popular computer vision software applications (e.g., MATLAB and OpenCV) include pretrained face classifiers and toolboxes that can be incorporated into the VAR system. After the face is detected, the face image is typically normalized by size.</p><p>Feature extraction. The human face is complex, so decomposing it into an effective set of features is critical to the success of face/facial expression recognition. Feature extraction mainly involves three types of features: geometric based, appearance based, and a combination of both. Geometric-based features measure the displacements of certain face regions such as the eyebrows or corners of the mouth, and appearance-based features are concerned with face texture. Appearancebased features may be extracted either holistically or locally. Holistic features are determined by processing the face as a whole, for example, eigenface features <ref type="bibr" target="#b0">(Abboud et al. 2004</ref><ref type="bibr" target="#b96">, Turk and Pentland 1991</ref><ref type="bibr" target="#b103">, Xiao and Ding 2014</ref>. Local features are specific facial features or areas that are prone to change with facial expressions, and are extracted by, for example, detecting local binary patterns (LBP, first introduced by <ref type="bibr" target="#b64">Ojala et al. 1996)</ref> associated with the eye and mouth regions <ref type="bibr" target="#b85">(Shan et al. 2009)</ref>. Because of illumination variations common in retail settings, techniques must be incorporated to reduce noise and make the information required for recognition more salient. Gamma correction, a nonlinear graylevel transformation, enhances the local dynamic range of the image in dark and shadowed regions while compressing the bright regions <ref type="bibr">(Shan et al. 2003, Tan and</ref><ref type="bibr" target="#b92">Triggs 2010)</ref>. Rotation invariant LBP (RI-LBP) 7 is then used to represent the various features. The RI-LBP technique is widely used in computer vision; it has been shown to be invariant to monotonic global illumination changes <ref type="bibr" target="#b64">(Ojala et al. 1996</ref><ref type="bibr" target="#b85">, Shan et al. 2009</ref><ref type="bibr" target="#b92">, Tan and Triggs 2010</ref>, and its computational simplicity makes it suitable for real-time applications. The RI-LBP features are then used in the classification task.</p><p>Classification. Face/facial expression recognition is essentially a classification problem in supervised learning that involves assigning face/expression labels to focal face images. A large variety of algorithms are available for this step. For example, support vector machines (SVM), hidden Markov models (HMM), neural networks, and k-nearest neighbors (KNN) have been demonstrated to perform well in the literature <ref type="bibr">Khorasani 2004, Oliver et al. 2000)</ref>. To achieve facial expression recognition, a set of training images for facial expressions needs to be collected and incorporated into the VAR system, generally using either a seven-expression scheme (i.e., neutral, happiness, sadness, fear, disgust, surprise, and anger; <ref type="bibr" target="#b21">Ekman 1994, Ekman and</ref><ref type="bibr" target="#b22">Friesen 1971)</ref> or a three-expression scheme (i.e., positive, negative, and neutral; <ref type="bibr" target="#b107">Zeelenberg and Pieters 2004)</ref>. The first few frontal face images detected from each try-on video can be used to train the system for face recognition in the future if matches are not found in the existing database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Region of Interest Detection.</head><p>The region of interest provides an estimate of a customer's focus of attention when evaluating a garment. The system first detects the hand position in the video frame, then relates it to a specific region on the garment (and the corresponding feature if a region-to-feature relationship has been precoded for that garment in the inventory database). Hand detection. A key step in gesture recognition, hand tracking, and human-computer interaction applications, hand detection is an active research area in the computer vision field <ref type="bibr" target="#b61">(Mitra and Acharya 2007)</ref>. Approaches to hand detection include color-based detection (using local skin color), appearance-based detection (using predefined geometric hand templates), and motion-based detection (tracking hand movement by assuming different motion features for hand and the background regions). As a classical approach for hand detection, color-based detection has proven to be effective and robust <ref type="bibr">Kitani 2013, Saxe and</ref><ref type="bibr" target="#b80">Foulds 1996)</ref> since skin color is fairly uniform and an individual's hands and face are typically the same color <ref type="bibr">Rehg 2002, Zhu et al. 2000)</ref>.</p><p>Garment region map. After detecting the location of the garment in each frame, the system identifies the specific region of interest by determining where the hand is positioned on the garment. In Figure <ref type="figure">3</ref>, we present a typical 3 Ã 5 garment region map to specify the locations of design features on a garment. This follows common practice in garment design (see <ref type="bibr" target="#b14">Cordier et al. 2003</ref><ref type="bibr" target="#b55">, Liu et al. 2010</ref>.</p><p>There are various ways to construct a garment region map for the VAR system. At one extreme, a retail store can code each garment to best capture its design features. Such an item-level garment region map enables more conceptually meaningful matching later, since each region corresponds to one particular feature; however, this approach requires extra work to code every new garment added to the inventory database. At the other extreme, a VAR system may use a generic garment region map (see Figure <ref type="figure">3</ref>) for all garments. Somewhere in between, a VAR system may include several garment region map templates to reflect general design categories (e.g., t-shirt, pants), and each garment could be coded to a template. (This could be automatically determined if the inventory database included such general classification information.) A VAR system based on collaborative filtering (CF) does not require a 1:1 correspondence between regions and features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Preference Inference.</head><p>Preferences are inferred by matching the facial expression to the region of interest in each frame and aggregating similar facial expressions for the same region of interest among all frames analyzed throughout the try-on process. (Typically, the system will not analyze every frame in the video to reduce computational burden; for example, it may analyze 10 frames from each second in a 30 frames-per-second (fps) recording, i.e., it will skip every two frames.) Assuming a three-expression (positive, negative, neutral) scheme, a preference score for a given region (or feature, if the garment region map is precoded with features) is calculated as the total number of detected positive expressions minus the total number of detected negative expressions when a customer evaluates a particular region/feature of the garment throughout the try-on video. The total number of three expressions (positive, negative, neutral) detected, respectively, throughout the try-on video could be used as proxies for the overall garment preference. To help reduce computational burden, we used one summary element for each region/feature and three separate elements for overall item-level preference in our calculation. With F representing the number of regions/features on a garment, we used an F + 3 Ã 1 vector, denoted as P u g , to represent customer u's preference toward garment g, where the first F elements represent customer u's preferences (the total number of detected positive expressions minus the total number of detected negative expressions when the focal customer evaluated the particular region/feature throughout the try-on video) toward F regions/features of garment g, and the last three elements represent customer u's overall preferences (the total number of positive, negative, and neutral expressions, respectively, detected throughout the try-on video) toward garment g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Step 2: Identify Neighborhood for</head><p>Focal Customer Once the VAR system obtains the information about the focal customer's reactions to various regions/features of a particular garment, the next step is to match her to a set of customers with similar preferences from the try-on customer database. The widely used matching and recommendation method is CF. In general, CF is the process of filtering items using the collaborative opinions/preferences of other people (see <ref type="bibr" target="#b30">Goldberg et al. 1992</ref><ref type="bibr" target="#b31">Goldberg et al. , 2001</ref> for some applications in marketing). The fundamental assumption of the CF approach is that if users A and B behave similarly when evaluating certain products, they are more likely to behave similarly toward other products. The CF approach has been widely deployed by firms such as Amazon and MovieLens to make recommendations to customers because it is highly effective and easy to implement <ref type="bibr" target="#b89">(Su and Khoshgoftaar 2009)</ref>. The basic idea is, even when limited information exists about a focal customer, useful recommendations can be made based on what is known about other customers with similar preferences (e.g., what other items they have tried on or purchased). Matching generally can be accomplished by evaluating the similarities between the observed focal customer's preferences and those in the database, and selecting those whose similarities with the focal customer exceed a certain threshold.</p><p>In the garment retailing context, each try-on incident (one customer trying on one garment in front of the mirror) represents a customer-garment pair. For each try-on incident, the system first finds a set of customers in the database who have tried (i.e., evaluated) the same garment. These customers are called candidate neighbors for the target customer-garment pair. The system then selects a subset of these candidate neighbors who have similar preferences for that garment with the focal customer. These people, called nearest neighbors, form a neighborhood for the target customer-garment pair.</p><p>In the garment retailing context, the CF algorithm must satisfy three criteria. First, it must be able to make recommendations to first-time users. <ref type="bibr">8</ref> Second, it must be able to deal with highly sparse data since customers typically only evaluate (try on) a small subset of the garments in the database, and purchase even fewer. Third, it must scale well with an increasing number of customers and items. We discuss our choice and implementation of the CF algorithm based on these three criteria.</p><p>CF can be implemented using one of several approaches: content based, neighborhood based, or model based. We chose to use the neighborhood-based CF algorithm because it is easy to implement, is widely used in practice, and new data (about customers as well as garments) can be added easily and incrementally <ref type="bibr" target="#b89">(Su and Khoshgoftaar 2009)</ref>, which is crucial for scalability in the garment retailing context.</p><p>Two types of response data can be potentially used for the CF-based recommender system in the garment retailing context: customers' implicit/explicit responses to items considered (try-on data) and items purchased (choice data). We recommend the use of try-on data in the CF algorithm in the VAR system for two reasons: (a) there are a lot more try-on data than choice data, which better addresses the data sparsity issue, one of the most important factors affecting the performance of CF-based recommender systems <ref type="bibr">Marketing Science 35(3), pp. 484-510, Â© 2016 INFORMS Khoshgoftaar 2009)</ref>; and (b) since the purpose of the VAR is to suggest items for customers to consider/try on, it makes more sense to use the same type of data in the recommendation process. In addition, choice data might be influenced by factors other than customers' item preferences (e.g., prices), which may not be relevant to other customers.</p><p>The neighborhood selection and similarity weighting mechanisms are key components of a neighborhoodbased recommendation method. The algorithm employed to construct a neighborhood for recommendation purposes depends on the ratio between the number of customers and the number of items <ref type="bibr" target="#b89">(Su and Khoshgoftaar 2009)</ref>. If the number of customers is much larger than the number of items, it is likely that an item would be rated by a large number of customers. In this case, it is optimal to use a mechanism to select a subset of high-confidence neighbors to form the neighborhood <ref type="bibr" target="#b16">(Desrosiers and Karypis 2011)</ref>, such as a correlation-based similarity weighting mechanism <ref type="bibr" target="#b36">(Herlocker et al. 2004</ref>). Correlation-based similarity measures have been widely used in commercial recommender systems by firms such as MovieLens <ref type="bibr" target="#b36">(Herlocker et al. 2004</ref><ref type="bibr" target="#b73">, Resnick et al. 1994</ref><ref type="bibr" target="#b72">, Resnick and Varian 1997</ref>. However, if the number of items is much larger than the number of customers, an algorithm that utilizes information from all people who have rated the same item is preferable, because there will only be a few such neighbors for each focal customer <ref type="bibr" target="#b16">(Desrosiers and Karypis 2011)</ref>. In the garment retailing context, since there are typically a lot more customers than items, correlation-based similarity weighting is more suitable for selecting the neighborhood.</p><p>It is worth noting that although the traditional CF approach uses ratings with only one component (i.e., the overall item-level rating), there has been growing interest in using customers' responses to multiple aspects of items to generate more accurate recommendations <ref type="bibr" target="#b2">(Adomavicius and Kwon 2007</ref><ref type="bibr" target="#b3">, Adomavicius et al. 2011</ref><ref type="bibr" target="#b79">, Sahoo et al. 2006</ref>. A CF method based on multiple-component responses can potentially solve the first-time user problem by learning customers' attribute-level preferences <ref type="bibr" target="#b71">(Rashid et al. 2002)</ref>. The multiple-component CF method has proven to be effective and outperforms traditional single-rating CF methods in many applications where ratings on individual features carry meaningful information <ref type="bibr">Kwon 2007, Manouselis and</ref><ref type="bibr" target="#b58">Costopoulou 2007)</ref>. We implemented a multiple-component CF algorithm in this study since the VAR system can infer customers' responses to multiple features/regions of the garment.</p><p>The neighborhood-based CF method used in the VAR system operates as follows. First, given a focal customer's current try-on incident, all customers in the try-on customer database who have tried on the same garment g as the focal customer u are identified.</p><p>These candidate neighbors are represented as M u g . Second, a similarity score is calculated between the focal customer and each candidate neighbor in M u g . This similarity score is used to form a proximity-based neighborhood between the focal customer and her likeminded neighbors. The purpose here is to rank order the candidate neighbors based on how similar they are to the focal customer. Similarity can be calculated based on correlations, distance, and cosine, among others <ref type="bibr" target="#b89">(Su and Khoshgoftaar 2009)</ref>. As explained previously, we use a correlation-based similarity weighting mechanism here.</p><p>We can calculate the similarity score between two customers who have both tried on the same garment g using their video-inferred preferences about g. As described in Â§3.2, we use P u g , an F + 3 Ã 1 vector, to represent the focal customer u's preferences (i.e., the feature-level and item-level preferences inferred from the try-on video) toward garment g, and P v g , another F + 3 Ã 1 vector to represent a candidate neighbor v's preferences toward the same garment g. The similarity score between the preferences of focal customer u and her candidate neighbor v based on their reactions to trying on garment g is given by</p><formula xml:id="formula_0">sim g u v = P u g âP u g P v g âP v g P u g âP u g P u g âP u g P v g âP v g P v g âP v g (1)</formula><p>whereP u g is an F + 3 Ã 1 vector, calculated as</p><formula xml:id="formula_1">P u g = ï£® ï£¯ ï£° 1 1 ï£¹ ï£º ï£» Ã 1 F + 3 P u g</formula><p>andP v g is also an F + 3 Ã 1 vector, calculated as <ref type="bibr" target="#b89">(Su and Khoshgoftaar 2009)</ref>. Third, nearest neighbors are selected to form a neighborhood for the target customer-garment pair. One method used to select (or filter) candidates to form the neighborhood is threshold filtering <ref type="bibr" target="#b16">(Desrosiers and</ref><ref type="bibr">Karypis 2011, Schafer et al. 2007</ref>). When a correlationbased similarity mechanism is used, a common practice is to use threshold filtering with 0 as a threshold <ref type="bibr" target="#b36">(Herlocker et al. 2004</ref><ref type="bibr" target="#b81">, Schafer et al. 2007</ref>), since negative correlations "are generally believed to not be valuable in increasing prediction accuracy" <ref type="bibr">(Schafer et al. 2007, p. 302</ref>). However, the number of candidate neighbors who share a nonnegative similarity score with a focal customer u on garment g could be huge for large data sets. In such cases, top-K filtering can be further applied, where only the top K candidate neighbors with the highest similarity scores with focal customer u on garment g are selected. A general rule is to choose a K between 20 and 50, depending on the size and sparsity of the data set <ref type="bibr" target="#b16">(Desrosiers and Karypis 2011)</ref>. These nearest neighbors are represented by N u g .</p><formula xml:id="formula_2">P v g = ï£® ï£¯ ï£° 1 1 ï£¹ ï£º ï£» Ã 1 F + 3 P v g</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4.</head><p>Step 3: Make Recommendations Recommendations can then be made to the focal customer based on her nearest neighbors' preferences and/or behaviors. A try-on customer database has information on what other garments the nearest neighbors have tried on and their reactions (to each garment region/feature, and the overall garment) from the video analysis, as well as the garments they have purchased.</p><p>The system can use a heuristic to rank order these garments and then recommend those ranked highest. Although heuristic variations exist, they typically are based on some or all three types of information: how similar a nearest neighbor is to the focal customer, how often a particular garment is tried and/or bought by her nearest neighbors, and whether a garment is only tried, or tried and bought (and if one garment is only tried, the similarities between the focal customer's reactions to it and to another item that is tried and bought, which can be used as a proxy to represent how close this item is to being bought). A retailing chain, in practice, may need to test out various heuristics and find the one that works best.</p><p>One widely used heuristic in recommender systems is to compute predictions for a focal customer using nearest neighbors' responses (usually a rating-based response) to the items they have evaluated and their similarities to the focal customer by taking a weighted average of all responses from nearest neighbors (Adomavicius and Kwon 2007). The basic idea for the heuristic is that recommendations can be made by rank ordering all garments that were tried by at least one nearest neighbor (excluding the garments that the focal customer has already tried on), according to the weighted average of all nearest neighbors.</p><p>We implemented this heuristic in our VAR system by inferring responses to try-on items based on facial expressions and regions of interest. The try-on customer database contains information on which garments nearest neighbors have tried on and/or bought in the past. The system first estimates the purchase likelihood for the tried-but-not-purchased garments, assuming the customer's affective responses to the try-on garment signal how much she wants to buy that garment. The system can calculate the similarity between nearest neighbor n's tried-but-not-purchased garments and purchased garments using the overall preferences (a three-dimensional vector, i.e., the total number of frames for three expressions-positive, negative, and neutral, respectively-throughout the try-on video across all garment regions) inferred from her try-on videos. If nearest neighbor n has bought more than one garment, the system calculates the average similarity score for all purchased garments. The average similarity score for a tried-but-not-purchased item is treated as a proxy for nearest neighbor n's likelihood to purchase that garment.</p><p>Using A n to represent the set of garments purchased by nearest neighbor n, a â A n , where the total number of purchased garments is denoted as TA; and B n to represent the set of garments that were tried but not purchased by n, b â B n ; then s n a b represents the similarity between n's overall preference toward garments a and b, and is calculated as <ref type="bibr">(Karypis 2001, Su and</ref><ref type="bibr" target="#b89">Khoshgoftaar 2009)</ref> </p><formula xml:id="formula_3">s n a b = R n a R n a R n a R n a R n b R n b a â A n b â B n (2)</formula><p>where R n a is a 3 Ã 1 vector referring to the overall preferences of nearest neighbor n toward garment a, R n b is a also a 3 Ã 1 vector referring to the overall preferences of nearest neighbor n toward garment b, and the purchase likelihood of nearest neighbor n toward garment b is given by</p><formula xml:id="formula_4">PL n b = A n S n a b TA (3)</formula><p>Once the purchase likelihoods for both considered and purchased items of all nearest neighbors have been calculated, the system determines which items to recommend to the focal customer. First, it calculates a weighted summation of all nearest neighbors who have considered a particular garment i; this is treated as the predicted purchase likelihood of the focal customer u toward garment i, given by</p><formula xml:id="formula_5">Q u i = nâN u g w g u n Q n i (4)</formula><p>where w g u n refers to the normalized similarity between focal customer u and nearest neighbor n, given by</p><formula xml:id="formula_6">w g u n = sim g u n nâN u g sim g u n (5)</formula><p>and Q n i refers to the nearest neighbor n's purchase likelihood toward garment i, given by</p><formula xml:id="formula_7">Q n i = PL n i if i â B n 1 if i â A n (6)</formula><p>After rank ordering all items according to the weighted summations (predicted purchase likelihood for the focal customer), the top-ranked items are recommended to the focal customer for consideration.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Study 1: Validation</head><p>As discussed in the previous section, different VAR system variations may be implemented based on a retailer's preferred trade-off between accuracy and cost, as well as specific sets of garments and customers.</p><p>In this section, we demonstrate the feasibility of one such implementation for exemplary purposes. More importantly, we do not claim that this is the best possible implementation of VAR, but a proof of concept. We also compare this particular VAR system with two state-of-the-art benchmark models: a self-explicated conjoint (SEC) model and a self-evaluation after tryon (SET) model. Although we do not claim to have compared our system to all benchmarks, our objective is to demonstrate the relative usefulness of this particular VAR implementation. In this section, we describe the three models, explain the design and procedure of the empirical test, and present the analysis and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">VAR System and Benchmark Models</head><p>The VAR model uses customers' affective and behavioral responses (i.e., facial expressions and regions of interest touched with their hands) to try-on items inferred from video clips in combination with the neighborhood-based CF algorithm described in Â§3. The VAR model relies on a try-on customer database, which contains customers' try-on and purchase data. In our VAR system implementation, we closely followed the analytic steps described in Â§3. Specifically, to infer preferences from the try-on video, we used RI-LBP to extract features and SVM as our classification tool during the face/facial expression recognition step. We used a three-expression scheme (positive, negative, and neutral) for the training set of facial expressions. The garments in the inventory database were coded at the individual garment level to create a 1:1 correspondence between features and regions, which were then used to detect regions of interest. 9</p><p>We implemented two individual level, nonautomated recommendation models-SEC and SET-as benchmark models. Although both models make individual level recommendations, their applications require additional customer effort. We compare the three models in Table <ref type="table" target="#tab_1">1</ref>.</p><p>The SEC model estimates each customer's preferences (i.e., part-worth data) for all garment features and then recommends the items ranked highest in utility. This model is widely used by marketers to understand customers' preferences and predict their purchases, especially when there are a large number of attributes with many variations <ref type="bibr" target="#b32">(Green and</ref><ref type="bibr">Srinivasan 1990, Netzer et al. 2008)</ref>, as is the case for garments. The implementation of the SEC model relies on customers' self-explicated conjoint preference data. The design of the SEC model in the study follows a procedure described in <ref type="bibr" target="#b88">Srinivasan and Park (1997)</ref>.</p><p>The SET model uses customers' feedback after evaluating the try-on garment in combination with the same neighborhood-based CF algorithm used in the VAR system. The implementation of the SET model relies on a customer-item database containing the overall ratings, and possibly feature-level ratings, for the items they have evaluated. This database is constructed by asking each customer to answer a mandatory question on her overall rating of each try-on garment (i.e., purchase likelihood on a scale of 0-100) and optional questions related to feature preferences using seven-point scales. This is then supplemented by the observed purchase decision for these items. The rest of the model (CFbased recommendation) is essentially the same as that described for the VAR system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Study Design and Procedures</head><p>Following recent preference measurement literature in marketing (e.g., <ref type="bibr" target="#b43">Kim et al. 2014</ref><ref type="bibr" target="#b62">, Narayan et al. 2011</ref>, we conducted our empirical study in two stages: a calibration stage followed by a validation stage about a week later. This two-stage design provided a good test and platform for comparing our model to the benchmark models. The delay is typically used to ensure that participants "forget" about the calibration tasks, so that the validation results are free of any potential interference due to the calibration. This design was particularly useful to us since we were constrained by a typical academic research budget and thus collected calibration data for all three methods from the same individuals.</p><p>For the VAR and SET models, this also enabled us to use the calibration stage to construct the try-on customer database needed to make recommendations.</p><p>In the second empirical study, we demonstrate and evaluate the real-time recommendations of the VAR system using the existing try-on customer database obtained from the first empirical study and a different group of participants; thus, participants in the second study were not burdened with three different tasks.</p><p>In this first empirical study, we tested recommendations for women's casual tops. Based on literature related to apparel consumption behaviors <ref type="bibr" target="#b1">(Abraham-Murali and Littrell 1995</ref><ref type="bibr" target="#b20">, Eckman et al. 1990</ref><ref type="bibr">, Fang et al. 2003</ref><ref type="bibr" target="#b109">, Zhang et al. 2002</ref>, interviews with female apparel consumers, and current online apparel retailers' business practices, we selected nine garment attributes that are most relevant for purchases of women's casual tops and constructed the garment database in a 2 2 Ã 3 3 Ã 4 2 Ã 6 2 attribute space (see Table <ref type="table" target="#tab_2">2</ref>). Using SAS fractional factorial design, we generated 160 garment profiles that closely resemble the product assortments offered in a small garment store, and purchased real garments corresponding to these profiles from almost 100 online stores (typically, each store only had one or two garments that fit our profiles). From the 160 garments, we randomly selected 140 garments for the calibration stage and used the remaining 20 garments for the validation stage. Because of budget constraints, all garments were the same size (medium), a reasonable, but not ideal offering, given the physical characteristics of the participants. We recruited 127 female garment shoppers to participate in the study. Their average age was 21 (range: 18 27 ; SD = 2.2), average height was 163 cm (range: 154 177 ; SD = 4.8), average weight was 52 kg (range: 40 67 ; SD = 5.4), and average body mass index (BMI) was 19.53 (range: 15 06 24 01 ; SD = 1.78). This indicates the participant sample reasonably represented garment shoppers in that age group in the real world, and medium was the preferred size in most cases.</p><p>To mimic the real-life experience of apparel shopping, we set up a room to resemble a small garment store (mock store) with real garments displayed on racks. Based on feedback from participants, the shopping process in this environment was similar to the experience in a real garment store. To eliminate the impact of brand and price, tags and brand labels were removed and participants were told that all garments were from the same brand and in the same price range. A fitting room was provided for participants to change into the garments they wanted to try on. A mirror was placed outside of the fitting room with a camera mounted on top (Logitech C920, recording at 30 fps at a resolution of 1920 Ã 1080 pixels). The participants consented to being videotaped for research purposes, but did not know how the data would be analyzed (e.g., facial expressions, hand movements, etc.). To ensure that participants behaved as they would when shopping for clothes in a real garment store, each participant received a particular garment based on her preferences revealed during the empirical study as compensation based on the procedures described in <ref type="bibr" target="#b18">Dong et al. (2010)</ref>.</p><p>At the calibration stage, we collected preference data using each of the three models. Participants came to the mock store one at a time. Each participant was asked to browse the 140 garments on the racks and choose any items that she would like to try on. A mock store assistant (research assistant) then put all of the chosen garments onto a separate rack outside the fitting room as in a real store. After putting on a garment in the private fitting room, the participant came out to evaluate herself in front of the mirror; this process was repeated for each garment selected. Each evaluation process was videotaped by the webcam mounted on top of the mirror. The video data were used for the VAR model.</p><p>After evaluating all chosen items, each participant was asked to provide feedback on a tablet for each item she had tried on. (The garments were still hanging on the rack outside the fitting room and participants were encouraged to respond to the questions while reevaluating each garment.) Each participant was asked to answer a mandatory question on her item-level rating of each try-on garment (i.e., purchase likelihood on a scale of 0-100) and optional questions related to her feature-level preferences 10 using seven-point scales, followed by a question about her purchase decision. These data were used for the SET model. Finally, participants were asked to complete the questionnaires for the SEC model. After excluding feature levels (see Table <ref type="table" target="#tab_2">2</ref>) that they would never accept (noncompensatory), they followed the standard twostep procedure of rating levels within attributes, and then assigned the relative importance across attributes. Recommendations were made by rank ordering the estimated utilities of all garments.</p><p>At the validation stage, participants came back after about a week to evaluate 20 different garments and make purchase decisions. Participants were free to evaluate and try on any of the 20 garments without any guidance. The data collected during the calibration stages were used to test the ability of the three models (VAR, SET, and SEC) to predict each participant's choices/considerations from the 20 garments in the validation stage.</p><p>The calibration stage was comprised of 1,184 try-on incidents, with an average of nine items per participant (ranging from 3 to 26). The validation stage was comprised of 450 try-on incidents, with an average of four items per participant (ranging from one to nine). On average, it took each participant around 20 seconds to evaluate a garment in front of the mirror, with a range from 3 seconds to 50 seconds, and a standard deviation of 12 seconds (excluding the time used to try on the garment in the fitting room). The average total evaluation time per customer was three minutes, varying from less than one minute to more than 10 minutes. The distributions of evaluation time are shown in Figures <ref type="figure">4(a</ref>) and 4(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis and Results</head><p>We report the analysis and results of the VAR model following the three general analytic steps described in Â§3. We discuss each of the two benchmark models where relevant: SET in Â§ Â§4.3.2 (neighborhood identification) and 4.3.3 (predictive performance) and SEC in Â§4.3.3 (predictive performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Infer Preferences from Video Analysis (VAR Model).</head><p>We analyzed each video clip by extracting frames at 10 fps. Following the procedures described in Â§3, for each frame extracted the model (a) recognized the facial expression in the frame, (b) detected the region of interest by identifying the customer's hand position on the garment region map, and (c) inferred the participant's preference for each region (or feature) and overall preference for each item.</p><p>Face detection. We used the Viola-Jones face detector 11 <ref type="bibr">(2001,</ref><ref type="bibr">2004)</ref>, a face classifier that has already been trained and embedded into MATLAB, to detect the face in each frame. The Viola-Jones algorithm 12 makes it possible to process images rapidly while achieving high detection rates. It is a robust and rapid face classifier constructed by selecting a small number of important features using Adaboost.</p><p>It is worth noting that in natural settings, faces are usually viewed from various perspectives. The viewpoints (angle and position) of a face image have a great impact on the accuracy of facial expression recognition <ref type="bibr" target="#b100">(Wang and Ahuja 2003)</ref>. To achieve reasonable accuracy, only the frames in which the frontal view of the face could be detected were considered valid and processed for further facial expression recognition. Among the 234,639 frames extracted from the 1,184 try-on videos in the calibration stage, 185,855 frames were valid (79%). Figure <ref type="figure">5</ref> shows the distribution of the percentage of valid frames for each try-on video clip.</p><p>Feature extraction and expression classification. Each detected face was then normalized in terms of size <ref type="bibr" target="#b29">(Georghiades et al. 2001</ref>) and illumination (using gamma correction), and prepared for feature extraction. We extracted RI-LBP features from the detected face images.</p><p>We then used the SVM classifier for the expression classification step. <ref type="bibr">13</ref> For expression classification, we constructed a training data set with 540 facial images from 45 female participants following the three-expression scheme <ref type="bibr" target="#b107">(Zeelenberg and Pieters 2004)</ref>: positive (e.g., smile), negative (e.g., frown), and neutral (i.e., expressions that have no effect on determining whether the customer liked or disliked the garment). We adopted Zeenlenberg and Pieters' (2004) three-expression scheme rather than <ref type="bibr" target="#b22">Ekman and Friesen's (1971)</ref> seven-expression scheme because several of Ekman and Friesen's primary expressions rarely occur in a garment shopping context (e.g., sadness and fear), and the latter system performs well only when the intensity of expressions is high.</p><p>The facial expression training data set was constructed following standard procedures in the computer science literature for collecting posed facial expression data <ref type="bibr" target="#b13">(Cohen et al. 2003)</ref>. The facial expression data were collected from 45 participants using a desktop computer with a camera mounted on top of the screen. The participants were asked to make each type of facial expression three times for the camera. First, they were asked to make a facial expression reflecting an emotion (e.g., happy, unhappy, neutral); then they were asked to read a sentence that was intentionally picked to trigger a certain emotion, causing them to make the facial expression naturally. Finally, the participants were asked to make the specific facial expression again. The expression sequences were randomized for each subject and the entire process was video recorded for each participant. After the training videos were collected, frames were extracted from each video. For each participant, four images were manually selected by three human judges from the video frames that best represented each of the three facial expressions (positive, negative, and neutral) for each participant. Altogether, we collected 540 training images (4 facial <ref type="bibr">13</ref> See Online Appendix A for technical details. images Ã 3 facial expressions Ã 45 participants) for the facial expression training data set.</p><p>To test the performance of our model in correctly recognizing expressions from frames extracted from videos, we collected a test set of 100 facial expression videos from a different set of participants using the previously described procedure; each participant posed one of the three facial expressions (happy, unhappy, or neutral). <ref type="bibr">14</ref> We randomly selected one frame from each of the 100 video clips and discarded 21 because they captured participants before or after the posed expression. The three human judges manually labeled each of the 79 frames as positive, negative, or neutral, and their joint judgments were used as ground truth. We analyzed the 79 test frames and compared the facial expression recognition result with the ground truth. Table <ref type="table" target="#tab_3">3</ref> shows the confusion matrix of our method for the 79 valid frames; the overall accuracy is 79%. The (correct) recognition rates for positive, neutral, and negative expressions are 88%, 81%, and 68%, respectively.</p><p>Region of interest detection. Region of interest detection involves detecting a hand in a frame and then matching it with the garment region map to identify which region is being touched. When the inventory database contains information that relates a region to a feature (as in this case), the feature of interest can be inferred.</p><p>In hand detection, the hand is differentiated from other objects in the scene to determine its location. Because skin color is uniformly distributed in a small region of color space, it can serve as a strong cue for vision-based hand tracking <ref type="bibr" target="#b41">(Jones and Rehg 2002</ref><ref type="bibr" target="#b106">, Yoruk et al. 2006</ref><ref type="bibr" target="#b111">, Zhu et al. 2000</ref>. This detection involves three steps. <ref type="bibr">15</ref> First, the garment area is segmented from each frame using upper body detection <ref type="bibr" target="#b50">(Kruppa et al. 2003)</ref>, and a gray-world algorithm for color correction <ref type="bibr" target="#b47">(KovaÄ et al. 2003</ref>) is applied to the detected garment area image to eliminate noise associated with different illuminant conditions. Second, color segmentation is employed to differentiate skin-color blobs from the background. Finally, assuming the colors of the hands and face are similar within the same image, the skin </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6</head><p>An Example of Hand Position Detection color of the face is used as a cue to find the hand blobs on the garment region. Figure <ref type="figure">6</ref> shows an example of the detected skin blob using color clustering and the hand position detected. Preference inference. As described in detail in Â§3.2.4, we used a simple heuristic to develop inferences of preference toward each garment region/feature and the overall preference toward the garment. Since we had a 1:1 correspondence between features and regions coded in the inventory database, a customer's preference could be inferred for each region (or corresponding feature, as shown in Table <ref type="table" target="#tab_2">2</ref> in our implementation) of the try-on garment. The system uses the number of frames in which the focal customer displayed positive (coded as +1), negative (coded as â1), and neutral (coded as 0) expressions when touching a particular region/feature of a garment to infer her preferences toward that particular region/feature. The system then sums the scores for each type of expression to estimate her overall preference toward the garment in each specific video clip (try-on incident).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Identifying Neighborhood (VAR and SET Models).</head><p>In the VAR and SET models, a neighborhood is identified for the customer-garment pair in each customer's first try-on incident. Using only the first try-on incident data helped us better evaluate model performance, since the number of garments tried on by participants varied. Moreover, in real life, meaningful recommendations may be based on only one try-on incident since customers are encouraged but not required to be tracked across different try-on incidents (see our discussion on first-time users in Â§3.3). <ref type="bibr">16</ref> In both the SET and VAR models, we implemented a neighborhood-based CF method using a correlationbased similarity weighting mechanism as described in Â§ Â§3.3 and 4.1. In the SET model, participants' itemlevel and feature-level ratings were used to calculate the similarity between customers. Both models make recommendations based on nearest neighbors' response data for considered (try-on) garments. A leave-one-out cross validation approach is employed; that is, for each customer, we use the remaining 126 participants' try-on and choice data to make the CF-based recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Predictive Performance (VAR, SEC, and SET Models)</head><p>. We adopted metrics typically used in CF applications to compare the three models. Specifically, we used recall and precision because they are widely used for ranking-based systems in information retrieval research <ref type="bibr">(Herlocker et al. 2004, Su and</ref><ref type="bibr" target="#b89">Khoshgoftaar 2009)</ref>. If a model is allowed to recommend t items (t ranges from one to four in the present study), 17 recall (or hit rate) is the percentage of the considered (tried-on)/purchased items that are recommended by the model (a higher recall/hit rate indicates better model performance), and precision is the percentage of model-recommended items that are actually considered/bought by the customers (higher precision indicates better model performance).</p><p>In addition to these two metrics, we used Kullback-Leibler (K-L) divergence-a relative entropy that "measures the expected divergence in Shannon's information measure between the validation data and a model's predictions" <ref type="bibr">(Ding et al. 2011, p. 121)</ref>-to investigate the model performance, since K-L measures discriminate among models even when hit rates are the same <ref type="bibr" target="#b17">(Ding et al. 2011</ref><ref type="bibr" target="#b35">, Hauser et al. 2010</ref>. Initially, we calculated divergence from perfect prediction, in which a smaller K-L indicates better performance. To better interpret the K-L natural bits, we calculated a K-L metric (termed K-L percentage) relative to the K-L divergence of the null model (random recommendation). <ref type="bibr">18</ref> The K-L percentage is 0% for the null model and 100% for perfect prediction. Hence, a larger K-L percentage indicates better performance.</p><p>We evaluated the three models using these four metrics. The results are reported in Table <ref type="table" target="#tab_6">4(a)</ref>. Table <ref type="table" target="#tab_6">4</ref>(a) summarizes the ability of each model to predict considerations (i.e., try-ons) for the validation task, and Table <ref type="table" target="#tab_6">4</ref>(b) focuses on predicting choices (purchases). Comparing the VAR model to benchmark models, namely, SEC and SET, VAR model-based predictions are the best on all measures, including recall, precision, K-L, and K-L percentage, across four recommendation set sizes. <ref type="bibr">19</ref> More specifically, when predicting considerations, the VAR model performs significantly better than the SEC model across all recommendation set sizes on recall/hit rate and precision measures with p &lt; 0 10, and better than the SET model across all recommendation set sizes on recall/hit rate and precision measures, but not significant. On the K-L measure, the VAR model performs better than the SEC and SET models, but not significant. On the K-L percentage measure, the VAR model performs significantly better than the SEC model when the recommendation set size is 2 or 3 with p &lt; 0 05, and better than the SET model, but not significant except when the recommendation set size is 2 (p &lt; 0 05). The differences between the SEC and SET models are not significant across all recommendation set sizes on all four metrics.</p><p>When predicting choices, the VAR model performs better than the SEC model on recall/hit rate and precision measures, but not significant, except when the recommendation set size is 3 (p &lt; 0 10). The VAR model performs better than the SET model on recall/hit rate and precision measures across all recommendation set sizes, but not significant. The VAR model performs better than the SEC and SET models on the K-L measure, but not significant. On the K-L percentage measure, the VAR model performs significantly better than the SEC model when the recommendation set size is greater than 2 with p &lt; 0 10. The differences between the SEC and SET models are not significant across all recommendation set sizes on recall/hit rate, precision, and K-L measures. In general, the VAR system appears to not predict choices as well as it predicts considerations, possibly because the VAR system relies on a try-on customer database and recommends items <ref type="bibr">18</ref> A random recommendation model is used as the baseline in the calculation of K-L percentage, where recommendations are made by randomly selecting from the subset of garments that have been tried by at least one participant. In our case, all 20 items in the validation task had been tried by at least one participant. <ref type="bibr">19</ref> We ran additional VAR models using a generic garment region map instead of a feature map. The results are similar. that are considered, but not necessarily purchased by customers.</p><p>These results seem to indicate that implementing and using the VAR is feasible, and that the system provides valuable recommendations. Moreover, the system appears to perform better than the two benchmark models. To investigate the value of the additional complexity associated with feature-level information in the VAR system, we also compared its performance against three simpler benchmark models (results available on request): (a) a simple CF model based only on purchase history (which can be tracked through a loyalty card or credit card number), 20 (b) a SET model that relies on overall preference only, and (c) a VAR model that relies on overall preference only. The model comparison result again shows that the VAR model reported here is the best or not significantly different from the best on all measures, including recall, precision, K-L, and K-L percentage, across four recommendation set sizes. All of the evidence supports the use of a multiple-component approach that captures feature-level information in the neighborhood CF-based recommender system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Empirical Study 2: A Real-Time Implementation of a VAR System</head><p>In Study 1, we demonstrated that the VAR model performs better than the nonautomated benchmark models. The predictions in Study 1, however, were not made in real time. We now describe a second study demonstrating a real-time implementation of the VAR system and investigate its feasibility and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Study Setup</head><p>We used the mock store from the first empirical study for this study with some additional decorations consistent with a real store. Our inventory consisted of the same 160 garment styles used in the first empirical study (thus allowing the VAR system to use the try-on customer database comprised of data from the 127 participants from the first empirical study), with brand labels removed and all offered at the same "on-sale" price ($16, close to our procurement cost). When a garment style sold out in the store, it became a catalogue item. Catalogue items were recommended by the VAR system; although participants were unable to try them on, they indicated whether they liked the recommended catalogue items and would try them on if available. Following the general structure in Figure <ref type="figure">1</ref>, the VAR system used in Study 2 was comprised of one central   a Recall (or hit rate) is the number of items predicted correctly divided by the total number of considered items for all 127 participants (i.e., 450). b Precision is the number of items predicted correctly divided by the number of recommendations made. c K-L refers to the Kullback-Leibler divergence with natural scaling, a commonly used information theoretic measure that balances false positives and false negatives. Here we calculate divergence from perfect prediction, so a smaller K-L is better. The detailed formulae for calculating K-L can be found in the online appendix of <ref type="bibr" target="#b17">Ding et al. (2011)</ref> and <ref type="bibr" target="#b19">Dzyabura and Hauser (2011)</ref>.</p><p>d K-L percentage is a metric calculated relative to the K-L divergence of a null model (random recommendation). The K-L percentage is 0% for the null model and 100% for perfect prediction. Hence, a larger K-L percentage is better. The detailed formulae for calculating K-L percentage can be found in the online appendix of <ref type="bibr" target="#b17">Ding et al. (2011)</ref>.</p><p>* Best in models with the same recommendation set size.  computing unit and one pair of input/output devices. The central computing unit was a desktop computer (3.6 GHz CPU with 16 GB RAM) located offsite from the mock store, which contained the inventory database (of the 160 garments), the try-on customer database (with data from the 127 participants from the first empirical study), and the codes for the VAR model. The computer was used to process video captured and generate appropriate recommendations in real time.</p><p>Video analysis began when the frontal upper body was detected in the frame, and ended when either the frontal upper body could not be detected consecutively for 10 frames or the evaluation time had exceeded 50 seconds. (Note that the average evaluation time was 20 seconds per try-on incident in the first empirical study.) The average time it took the system to analyze a video and make recommendations was about three minutes. (We discuss how we can substantially reduce this processing time in Â§6.)</p><p>The input/output devices were located in the mock store, and consisted of a webcam connected to a laptop. <ref type="bibr">21</ref> The laptop transmitted the video captured by the webcam to the central computing unit via the Internet and displayed recommendations (garment images and IDs) sent by the central computing unit. We did not have a barcode reader (which would be used to identify the try-on garment in real life) in our mock store, so each participant was asked to manually type in the garment ID before she began to evaluate it in front of the mirror (thus activating the VAR system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Procedure and Results</head><p>We kept the mock store open for three days, Wednesday through Friday, 9:00 a.m. to 5:30 p.m. To attract participants, we offered them the equivalent of $8 each for stopping by, which they could use to purchase garments in the store. A total of 25 female participants visited the store; none of them had participated in the first empirical study and they were demographically similar to participants in the first study.</p><p>When a participant came in, the store assistant (research assistant) provided a brief explanation about how the VAR system works, and explained that she would be filling out a short survey at the end of the shopping experience. The participant then selected a garment to try on from the rack, and activated the VAR system. Once the offsite central computing unit identified a set of four garments to recommend, their images and ID numbers were sent via the Internet and displayed on the laptop screen in the mock store. The participant then viewed the recommended garments, indicated which of the four recommended garments she would like to try on, and selected one garment to try on in the next round. If she did not want to try on any of the recommended garments, or if the garment she wanted to try on next from the recommendation set was out of stock, she went back to the rack and selected a different garment to try on. The process was repeated until the participant did not want to try on any more garments and made a purchase decision (or not, if she found nothing she liked).</p><p>The 25 participants tried on 101 garments in total, an average of four garments per person (ranging from three to six). The VAR system made 94 real-time recommendations (each time recommending a set of four garments), but failed to make recommendations for seven try-on incidents because it either failed to detect a face in the video (e.g., hair covered the face, or the participant bowed her head too much) or no suitable nearest neighbors could be identified for the focal customer because she had negative similarity scores with all candidate neighbors. During each participant's last round of recommendations, shoppers were asked to indicate which of the recommended garments (or none) they would like to try on if they had the time and energy to try on more.</p><p>The results of the second empirical study provide further support for the usefulness of the VAR system in real time. Figure <ref type="figure">7</ref> shows the distribution of the number of would-have-tried garments 22 across all recommendation sets. In almost all cases, participants indicated that they would like to try on one or two garments from the set of four recommended garments. In just five of the 94 cases, participants indicated no interest in any of the four garments recommended. Since some items recommended and liked by participants were out of stock (catalogue items), 56 items recommended by the VAR model were actually tried on by participants.</p><p>A simple CF recommendation system based on purchase histories tracked via loyalty cards or credit card numbers seems like a natural first choice for garment retailers. Using actual try-ons (excluding the first garments selected by the participants) as the ground truth, we compared the performance of the VAR model against a simple CF model 23 based on purchase history. With a recommendation set size of four, the VAR model predicts considerations significantly better than the simple CF model on all measures, namely, recall/hit rate, precision, K-L, and K-L percentage with p = 0 00. At the end of their shopping trips, 64% of participants indicated that they liked the garments recommended by the VAR system. Seven of the 25 participants bought a total of eight garments, and four (50%) of these garments were VAR recommendations. Four additional participants told us that they would have liked to have purchased the garments recommended by the VAR system, but were not able to because they were catalogue items (i.e., we did not have the actual garments in the mock store), and one other participant would have bought a garment recommended by the system if we would of had a larger size. With a recommendation set size of four, the VAR model predicts choices significantly better than the simple CF model based on K-L (p = 0 08) and K-L percentage (p = 0 08) measures with p &lt; 0 10, and better based on recall/hit rate and precision measures, but not significant (p = 0 34).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Feedback on the VAR System</head><p>Each participant filled out a short survey at the end of her shopping trip on the overall shopping experience and provided reactions to the real-time VAR system she had just used. All participants indicated that their privacy had been well protected during the shopping experience. Most participants also thought that their experiences had been very similar (20%) or similar (44%) to their regular shopping experiences. Some participants said, "It looks quite like a real fashion store, very comfortable and relaxing;" and "I feel no difference from a regular shopping trip."</p><p>Regarding the VAR system in particular, 56% of participants thought the system was easy or very easy to use; only 16% thought the system was somewhat difficult to use, mostly because of the long wait time (three minutes for each recommendation) in the current implementation; and none thought it was difficult or very difficult to use. Over 70% of participants indicated that they would like to continue using the recommender system the next time they go shopping, and they would recommend the system to their friends. If a similar store (with only an automated recommender system and no salesperson following participants around) existed in the market, 89% of participants indicated that they would like to shop there.</p><p>Several participants described why they liked the VAR system: "The system is like magic! It made recommendations on some clothes that I would never pick myself, but when I follow the recommendations and try them on, they look amazingly good on me"; "There are too many clothes displayed in a store. I don't have the time or effort to carefully check every piece of clothing. The system saves me a lot of time and effort by finding what I like in several rounds"; "I trust the recommendations from a computer much more than a salesperson, because the computer is more objective, scientific and systematic"; "I feel the system knows my preferences better than a regular salesperson"; "I don't trust a salesperson's recommendations since their recommendations are often very subjective, and they make recommendations based on what they would like to sell, rather than how I look."</p><p>Participants also identified a few aspects of the VAR system that should be improved. The biggest concern was the wait time. They indicated that the ideal wait time for recommendations should be one to five seconds; they would be willing to wait up to 30 seconds after the system had proven itself to be very helpful to them. Some participants stated that the quality of the recommendations after the first try-on would likely determine whether they would use the system in the future. They also suggested that recommendations could be presented better. Instead of the web-quality garment photo used in the second study, high-resolution photos, 3-D photos, and/or images of garments on models would be more desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Implementation and Scaling Up in the Real World</head><p>In this section, we discuss a few key implementation concerns from the perspective of both customers and retailers, followed by a detailed discussion on scaling up the VAR system to large garment retailing chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Issues</head><p>Some implementation challenges relate to customers' attitudes toward the VAR system. We surveyed 417 people about their attitudes and willingness to use the SET and VAR recommender systems based on descriptions of the purpose, procedure, and customer effort required for each. We refer to this survey as the attitude survey in the rest of this paper, and discuss some of the relevant survey results below where appropriate.</p><p>6.1.1. Opt-In and Linked Try-On Data. The VAR system does not need to store the raw video data for each try-on incident. It analyzes customers' try-on videos in real time, and then stores customers' reactions to various regions (features) of interest for each garment in the database. Each try-on experience is thus simply a numeric record in the database, with variables identifying the garment, customer, and preferences inferred from each try-on video. Nonetheless, customers must be willing to make some small privacy sacrifices in order for the VAR system to perform well. Specifically, (a) customers must be willing to be videotaped while evaluating new garments to receive personalized recommendations; and (b) some customers must be willing to be tracked across multiple try-on experiences to build the try-on customer database for CF-based recommendations. It is worth noting that once data across different try-on incidents are linked, the VAR system does not need to maintain customer identifiers such as face images (thus ensuring privacy) in order for CF to work.</p><p>The general environment nowadays has made it much easier for customers to accept being videotaped. Retailers, for example, have been using surveillance video to monitor customers' purchase processes for quite a while <ref type="bibr" target="#b56">(Lyon 2001)</ref>; thus, being videotaped during the shopping process is not novel to customers. In our studies, the participants seemed to be open to being videotaped during the evaluation process. Yet retailers must exercise caution and investigate their target customers' acceptance of the practice before widespread implementation.</p><p>The bigger challenge, we believe, is convincing customers to be tracked across multiple try-on experiences. Two possible approaches to help address this challenge are allowing multiple levels of opt-in and providing additional incentives for tracking. A retailer could offer multiple levels of opt-in such as (a) no tracking; (b) tracking during a single shopping trip, but deleting all personal identifiers after a day; (c) tracking during all shopping trips at the store; and (d) tracking during all shopping trips at all stores in the chain. The customer can also be given the option to save certain try-on videos in the system for later viewing. Since almost all retail stores use surveillance cameras to record customers' activities and then erase the videos after a few days, we suspect many customers will be amenable to the idea of being tracked during a single shopping trip (i.e., having their identifiers such as face images erased at the end of the business day). All opt-in levels (except no tracking) will contribute useful records to the database.</p><p>There is an inherent opt-in rate, but it is also possible to entice customers to allow the system to keep their face images and track them across try-ons by promoting the benefits of doing so: customers can not only play back prior try-on videos but also view extracted frames from the videos so as to easily compare how they look in different garments. In the attitude survey, we asked participants to allocate 100 points across eight potential benefits of a recommender system; the average weight that they assigned to playing back their prior try-on videos was 13.67 (SD = 8.76), and the average weight that they assigned to being able to compare frames from prior try-on videos was 19.36 (SD = 10.29). When asked to tell us their acceptance of "allow the automatic system to record my try-on history" on a scale of 1 (completely unacceptable) to 7 (completely acceptable), they provided an average rating of 5.30 (SD = 1.47). These numbers seem to indicate that people are generally open to being tracked in this way. Furthermore, the additional benefits are important to participants, and may help convince more individuals to opt-in (at least during the same shopping trip).</p><p>On the technical side, some type of customer tracking technology must be implemented in the VAR system, such as face recognition (see Â§3). Although we did not use face recognition to track customers in our empirical studies, the system's face recognition function is very accurate. 24 6.1.2. Customer Effort. A customer who uses the VAR system must exert extra effort. Specifically, a customer must (a) step out of the fitting room to look in the mirror outside the fitting room; and (b) scan the garment's bar code to opt-in and activate the system. In the attitude survey, we asked participants to respond to the following statements using a seven-point scale (1 equals completely disagree, 7 equals completely agree): "It is not inconvenient to step out of the fitting room to use the video based recommender system" (mean = 5.02; SD = 1.71); and "It is not inconvenient to step out of the fitting room to use the mirror" (mean = 5.15; SD = 1.72). Their responses seem to indicate that although additional effort must be expended, on average, most people do not see it as a major hurdle.</p><p>Customers can use a handheld scanner to scan a garment ID and opt-in to use the system. Using the same seven-point scale, we asked customers whether they would accept scanning the garment ID themselves. Although responses were positive (mean = 5.54, SD = 1.38), this may still be a challenge if the tag is on the back of the garment or in a hard-to-reach place. This may require extra work from the retailers' side to attach tags in locations that are easy for customers to scan while wearing a garment. 6.1.3. Addition of New Garments or New Garment Features. The VAR system only works if a garment (identified in a try-on video or a possible candidate for recommendation) is in both the inventory and the try-on customer database. Adding new garments with potentially new features to an inventory database can <ref type="bibr">Marketing Science 35(3), pp. 484-510, Â© 2016 INFORMS</ref> be costly if the retailer wants to use garment-specific region maps, even if it only needs to do it once for all stores in the chain. To address this issue, a retailer may choose to use a generic garment region map when adding new garments to the inventory database, since such a map requires no additional coding effort and has been shown to generate consistently accurate recommendations similar to a system using item-level garment region maps (which enable a 1:1 correspondence between a region and a feature) in our first empirical study (see footnote 19). As a middle-ground solution, several garment region map templates could be coded first, and a worker would only need to assign the appropriate one to a new garment. For the first few customers who try a recently added garment, a VAR system based on a try-on customer database would be unable to make recommendations since no other try-on data for that garment would be available, making it impossible to find neighbors (referred to as new item problem in the CF literature) <ref type="bibr" target="#b11">(Bobadilla et al. 2012</ref>). To fill this initial recommendation void, a common practice is to ask a set of motivated customers to rate new items <ref type="bibr" target="#b11">(Bobadilla et al. 2012</ref>). The try-on customer database could also be seeded by recommending recently added garments to randomly selected customers (e.g., one out of four in each recommendation set is a new garment with minimal try-on data in the customer database).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Scalability of the VAR System</head><p>Large garment retailers usually have hundreds of retail stores (typically with multiple fitting rooms in each) and offer thousands of products. Several challenges are associated with implementing a VAR system in such contexts: (a) processing video as video length increases; (b) making CF-based recommendations as the numbers of features, customers, and garments increase; (c) running the system in a large number of stores where many customers may be using the system simultaneously; and (d) the costs of achieving these computation goals. 6.2.1. Video Processing. Since the VAR system analyzes video information at the frame level, video processing time grows linearly with video length. For each frame, the VAR system performs two main tasks: facial expression recognition and hand detection. As a result, video processing computing time is approximately linear with the length of the try-on video. Note that with current video streaming technology, the latency of real-time video transmission is usually less than 150 ms and is independent of the length.</p><p>To investigate scalability regarding the video processing time, we explored whether we could reduce the three-minute average processing time reported in Study 2 on a remotely located desktop equipped with a 3.6 GHz CPU and 16 GB RAM. The video processing time can be improved through at least Total processing time (in seconds)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video length (in seconds)</head><p>Total-par4</p><p>Total-nopar Total-before two approaches: code optimization, which requires no hardware upgrades or technology investments; and parallel computing, which invokes some hardware requirements and potentially costs more. We investigated the potential to improve video processing time using these two strategies.</p><p>We performed an extensive analysis on the video processing time and found that the most time was spent reading unstructured video information (i.e., frames from the video file); this can be optimized by substantially reducing the input/output (I/O) time required. We also investigated the benefit of using parallel computing; since video processing is done at the frame level, each frame of video can be analyzed on a different CPU core. To gauge the reduction in video processing time using the two strategies, we randomly selected two video clips for each of the five lengths (10 sec, 20 sec, 30 sec, 40 sec, 50 sec) and calculated the average processing time for each pair. We then compared the total video processing time of the three models: (a) the original code we used in Study 2 (total-before), (b) optimized code without parallel computing (totalnopar), and (c) optimized code with parallel computing (total-par4). We analyzed all videos on a desktop with four Intel 3.6 GHz i7-4790 CPU cores and 16 GB RAM. The processing was split across all four cores in the third model only. The results are reported in Figure <ref type="figure">8</ref>. For a 20 sec try-on video from our empirical studies, processing time was reduced from 264 sec in the original model to about 40 sec when both code optimization and parallel processing were employed. In a real-life implementation, one can rent even a 40 vCPU at a very affordable price from the Amazon cloud computing service, thus further decreasing the processing time substantially. These results provide good evidence that the video processing time can be shortened sufficiently to avoid unacceptable delays in real-life applications. to the number of garment features involved, because the similarity score between two customers is calculated only once in our model, regardless of the number of features. The increase in computing time is due to the increase in the vector size involved in the calculation.</p><p>Although the computational complexity of neighborhood-based CF is not NP-hard <ref type="bibr" target="#b89">(Su and Khoshgoftaar 2009)</ref>, scholars have shown that a naÃ¯ve neighborhoodbased CF algorithm could have limited scalability for large data sets. With millions of customers and millions of items, even a CF algorithm with the complexity of O(n) 25 could not react immediately to real-time requirements <ref type="bibr" target="#b89">(Su and Khoshgoftaar 2009)</ref>. Thus, like other CF-based recommender systems used in practice, two key modifications must be made to our model to ensure that the system can generate real-time responses.</p><p>The first modification is to adopt top-K filtering and use only a small set of "best" neighbors when the database is large, instead of using all neighbors that satisfy a set of rules (the latter strategy tends to yield a huge number of nearest neighbors as the number of customers and garments increases). A typical number suggested in the literature is 20 to 50 <ref type="bibr" target="#b9">(Bell and Koren 2007)</ref>. Our simulations show that the computing time for a database with 500,000 customers, 3,000 garments, and try-on data for 30 garments for each customer can be reduced from over 2,400 sec when using all nonnegatively correlated candidate neighbors to about 30 sec when using the top 50 nearest neighbors on a 2.6 GHz PC with 16 GB RAM. According to our simulation results, 26 when the top-K filtering method is used for the recommender system, the computing time increases in an approximately linear fashion with both the number of customers and the number of garments.</p><p>The second modification is to implement the recommender system in a computing environment via a distributed data processing framework, which one can rent from Amazon (see example in Â§6.2.4). Most of the computing time is spent searching for nearest neighbors and garments tried for a specific user, which are processes that can be parallelized. Thus, according to Amdahl's law, using parallel computing-specifically, a cloud computing platform-would help to reduce the computing time required to make one recommendation to an acceptable level, even when huge data sets are used. For example, large Web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, and most computations are performed on machines with large amounts of memory (e.g., 1 TB RAM) <ref type="bibr" target="#b34">(Gupta et al. 2013</ref>). 6.2.3. Concurrent Recommendations. The ability to provide concurrent recommendations also needs to scale up easily as the number of stores (i.e., the number of end-user devices) increases, since more customers will be using the system simultaneously. Fortunately, this is a general problem that has already been addressed by major commercial firms, typically through the use of cloud computing and database services for data distribution and replication, enabling the demand for computing power to be scaled up or down in real time. For example, Cassandra (NoSQL Database) is used by firms such as Netflix, Twitter, and Cisco to achieve this objective <ref type="bibr" target="#b45">(Klein et al. 2015</ref><ref type="bibr" target="#b46">, Klems et al. 2012</ref>. Note that normally a retail chain would rent such services from firms such as Amazon (see Â§6.2.4), instead of building and owning them in-house. The former is a lot more cost effective and creates much more flexibility to respond to demand fluctuations (scale up and down). To ensure fast recommendations, the retailing chain should reserve sufficient instances from service providers like Amazon in accordance with a predicted number of concurrent uses of the VAR system at different time periods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4.">Cost of Achieving Desired Computation</head><p>Capacity. System costs are comprised mainly of hardware costs (i.e., user interfaces and central computing unit devices) and human costs. The cost of a user interface mainly amounts to the cost of a webcam and an LCD screen. Assuming the average cost for such a system is $400 (based on current prices listed on the Internet), the total cost of the user interface would be $400 Ã X Ã Y , where X is the number of stores and Y is the average number of fitting room interfaces per store. A chain with 1,000 stores and 10 fitting rooms per store would spend $4 million on user interfaces. The cost of a central computing unit depends on whether a company decides to buy (establish internal computing capability) or rent (use computing capability from a provider). In the rent model, which currently is very popular, a firm would rent a cloud computing engine from a provider such as Amazon (e.g., Amazon EMR (Elastic MapReduce) service combined with its Amazon Elastic Compute Cloud (Amazon EC2)) cloud database engine and storage space (e.g., Amazon Relational Database Service (Amazon RDS) for Amazon Aurora). Based on a back-of-the-envelope calculation, 27 the variable cost of generating one recommendation would likely be a few cents. There would also be fixed costs associated <ref type="bibr">Marketing Science 35(3), pp. 484-510, Â© 2016 INFORMS</ref> with renting cloud database storage space (10 TB of data storage costs about $1,000/month at Amazon). Finally, experienced engineers must be employed to develop, deploy, and maintain the central computing unit, and people must be hired to set up and maintain the user interfaces.</p><p>These costs are manageable, but not trivial. It is thus important for firms to perform careful cost benefit analyses for their specific situations before deciding whether or not to deploy a VAR system. It is also advisable that firms perform initial small-scale tests of the system to gauge their own specific benefits and costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Mining Information from</head><p>Try-On Video Data</p><p>In addition to providing individualized recommendations, try-on videos enable firms to mine valuable information when pooled across individuals and garments. Video data from the retail industry are a main source of (unstructured) big data, because in-store data are rich in volume, variety, and velocity (i.e., changing in real time). At least two types of valuable insights can be mined from large-scale try-on video data, which cannot be obtained from other data sources: (a) haptic evaluation patterns (i.e., customers' hand movements when evaluating a particular garment or garment type); and (b) reasonably accurate demographic (e.g., age) and physical attributes (e.g., height, body shape) of customers who have tried each garment. Useful information can be mined from a large number of try-on videos with minimal privacy sacrifices from customers.</p><p>In the rest of this section, we illustrate how haptic pattern data can be mined from try-on videos and discuss how they might be used. For ease of interpretation, we present haptic pattern data as superimposed images that we call hand heat maps. A hand heat map is customer normalized and centered, with dots representing the hand positions in all frames examined, and different dot colors representing the density of dots in a particular position. Figure <ref type="figure">9</ref> shows a simple example with four hand heat maps (2 customers Ã 2 garments). The first column presents a garment image (ID 5), followed by the hand heat maps of two different customers from their try-on videos for this garment. The second column presents a second garment image (ID 76), followed by two similarly obtained hand heat maps. Each dot indicates the hand position in a frame. We use three colors (red for one observation, purple for multiple observations, and blue for the highest concentration of observations) to represent different dot densities, but a continuous scale can be employed if needed.</p><p>A comparison of the hand heat maps reveals several useful pieces of information. First, the density measurements highlight regions of a garment that seem to attract the most attention. Second, a particular customer's evaluation patterns can potentially be discerned by comparing her hand heat maps for different garments. For example, comparing the two hand heat maps of the first customer across two different garments (row 2), we notice that she touched the hemline quite often, indicating that this region of the garment was important to her evaluation. Third, we might be able to infer the important regions of a particular garment for a group of customers by comparing hand heat maps of multiple people (possibly from the same target segment) for the same garment. Comparing the two hand heat maps of the second garment across two different customers (column 2), we notice that the first customer evaluated how the garment looked on her when she stretched out completely (hand positions scattered around), and the second customer was concerned about how the garment flowed around her shoulder (hand positions concentrated in the shoulder region). In addition, it appears that both customers cared about how the outer part of the hemline looked on them.</p><p>In real-life implementations, these individual-level (customer Ã garment) hand heat maps can be pooled for further data mining, for example, across all garments tried by a single customer or a segment of customers, or across all customers who have tried a particular garment or garment type, to provide more robust insights about a customer or customer segment, a garment or garment type, and various combinations of customers and garments. Such insights mined from large-scale try-on videos could be quite useful for designers and retailers. For example, when hand heat maps are combined with the respective purchase data, designers and retailers may be able to identify additional underlying reasons for the (un)popularity of items, and adjust inventories, displays, promotions, and future designs accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>In this paper, we proposed a video-based automated recommendation system for use in garment retail stores, with potential benefits to both customers and retailers. The VAR system may be implemented with several variations. We demonstrated one such implementation and compared its usefulness to two state-of-the-art (individual level but not automated) benchmark product recommendation models in an empirical study. We also demonstrated the system's real-life feasibility in a second empirical study in which we provided real-time recommendations to shoppers.</p><p>We contribute to the marketing literature in two ways. First, to our knowledge, we are among the first to create a system based on video data (as opposed to survey and scanner data) to estimate customer preferences in a Figure <ref type="figure">9</ref> An Example of a Hand Heat Map retail context. Second, unlike existing marketing studies in which commercial software packages were used for data analysis, we are among the first to write and test algorithms for video analysis in marketing, thus opening the black box of existing commercial software. This should help pave the way for new modeling innovations in marketing related to video data. Through two empirical studies, we demonstrated a proof of concept for the VAR system in garment retailing contexts. We did not exhaustively compare our model to all possible benchmark models, nor do we claim that our implementations of benchmark models are the best ways to implement. It is worth it to further test other types and implementations of benchmarks using larger data sets collected from real-life retailing contexts. Although potential benefits of the VAR system include generating individualized recommendations, decision aids for consumers and insights from rich and big data, a firm should evaluate the trade-offs between specific benefits and costs when comparing the VAR to other recommendation systems. Since this is the first paper in this domain, much work needs to be done to further improve the model prior to widespread implementation, and many promising extensions can be made in future research. First, it is worthwhile to explore and test alternative implementations of the VAR system. Model performance also can be improved by identifying and removing irrelevant information from video data, for example, habitual or random facial expressions not related to the garment itself, or behaviors (facial expressions and hand positions) related to interactions with other people (e.g., friends). Furthermore, customer preferences could be inferred more accurately by utilizing other information in the videos, such as an individual's weight, height, age, skin tone, etc. This information can be incorporated into the VAR system to help improve the system's performance. It may also be worthwhile to explore other methods of detecting regions of interest, such as eye tracking <ref type="bibr" target="#b40">(Hui et al. 2013</ref>). Moreover, researchers can explore whether a VAR system can be used in other retailing contexts.</p><p>Video data are a new source of information in the retailing context. Applications of video analysis in marketing may include detecting gender, skin color, and eye gaze; recognizing facial expressions and body or hand gestures; tracking trajectories; and counting people (see <ref type="bibr" target="#b104">Xiao et al. 2013)</ref>. As a first attempt to use video analysis to infer customers' individual preferences, the VAR system may yield benefits such as reducing customer searching effort, increasing retail sales by recommending garments that customers are likely to purchase, and helping companies adjust designs or inventory to match customer preferences. As discussed in Â§6, the costs associated with implementing a large-scale VAR system are nontrivial. A firm should carefully weigh the expected benefits and costs before deploying such a system. We hope that this new method can become a valuable tool to retailers and create benefits for both customers and retail stores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Supplemental material to this paper is available at http://dx .doi.org/10.1287/mksc.2016.0984.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1General Design of the VAR System</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2General Process of Video Analysis in the VAR System</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3 (Color online) An Example of a Garment Region Map</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc>Figure 4(a) (Color online) Evaluation Time per Try-On Item</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure 4(b) (Color online) Total Evaluation Time per Participant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 5 (Color online) Percentage of Valid Frames per Video Clip</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>a</head><label></label><figDesc>p-value (VAR vs. SEC/SET) 0 p-value (VAR vs. SEC/SET) 0 p-value (VAR vs. SEC/SET) 0 Recall (or hit rate) is the number of items predicted correctly, divided by the total number of chosen items for 127 participants (i.e., 207). * Best in models with the same recommendation set size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 7 (Color online) Distribution of Number of Would-Have-Tried Garments Across All Recommendation Sets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 8 (Color online) A Comparison of Video Processing Time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Comparison of the Three Models</figDesc><table><row><cell>Model</cell><cell>SEC</cell><cell>SET</cell><cell>VAR</cell></row><row><cell>Data source</cell><cell>Customer</cell><cell>Customer</cell><cell>Camera</cell></row><row><cell>Preference inference</cell><cell>Self-reported, one-time</cell><cell>Item-based, self-stated</cell><cell>Item-based, video-inferred</cell></row><row><cell></cell><cell>measurement</cell><cell>feedback</cell><cell>preference</cell></row><row><cell>Recommendation</cell><cell>Utility ranking</cell><cell>Collaborative filtering</cell><cell>Collaborative filtering</cell></row><row><cell>mechanism</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cost to retailers</cell><cell>Computer/tablet</cell><cell>Computer/tablet</cell><cell>Computer/tablet with camera</cell></row><row><cell>Cost to customers</cell><cell>Time and effort to answer</cell><cell>Time and effort to answer the</cell><cell>Leaving the fitting room to evaluate</cell></row><row><cell></cell><cell>the questions</cell><cell>questions; scan bar code</cell><cell>garments; scan bar code</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Garment Design Attribute Space</figDesc><table><row><cell>Attribute</cell><cell>Level 1</cell><cell>Level 2</cell><cell>Level 3</cell><cell>Level 4</cell><cell>Level 5</cell><cell>Level 6</cell></row><row><cell>Fabric</cell><cell>Cotton</cell><cell>Not cotton</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Color</cell><cell>White</cell><cell>Red</cell><cell>Yellow</cell><cell>Green</cell><cell>Blue</cell><cell>Black</cell></row><row><cell>Silhouette</cell><cell>A-type</cell><cell>H-type</cell><cell>X-type</cell><cell>S-type</cell><cell>-</cell><cell>-</cell></row><row><cell>Stripes</cell><cell>No</cell><cell>Yes</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Fashion element</cell><cell>None</cell><cell>Lace</cell><cell>Ruffle</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Neckline</cell><cell>Scoop</cell><cell>Crew</cell><cell>V-neck</cell><cell>Boat neck</cell><cell>Flat collar</cell><cell>Shirt collar</cell></row><row><cell>Sleeve design</cell><cell>No</cell><cell>Puff</cell><cell>Bat wing</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Sleeve length</cell><cell>No</cell><cell>Short</cell><cell>Medium</cell><cell>Long</cell><cell>-</cell><cell>-</cell></row><row><cell>Hemline length</cell><cell>Short</cell><cell>Medium</cell><cell>Long</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Confusion Matrix for Facial Expression Recognition</figDesc><table><row><cell></cell><cell></cell><cell>Recognized</cell><cell></cell><cell>Correct</cell></row><row><cell>Ground truth</cell><cell>Positive</cell><cell>Neutral</cell><cell>Negative</cell><cell>recognition rate (%)</cell></row><row><cell>Positive (25)</cell><cell>22</cell><cell>1</cell><cell>2</cell><cell>88</cell></row><row><cell>Neutral (26)</cell><cell>0</cell><cell>21</cell><cell>5</cell><cell>81</cell></row><row><cell>Negative (28)</cell><cell>4</cell><cell>5</cell><cell>19</cell><cell>68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>(a)</cell><cell>Empirical Comparison of Model Performance for Predicting Considerations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>(b)</cell><cell>Empirical Comparison of Model Performance for Predicting Choices</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Lu, Xiao, and Ding: Video-Based Automated Recommender (VAR)  System for Garments Neighborhood-Based CF Algorithm. Two factors may affect the computing time of the CF recommendation algorithm: the number of customers and the number of garments. Computing time is only marginally related</figDesc><table><row><cell>Marketing Science 35(3), pp. 484-510, Â© 2016 INFORMS</cell><cell>505</cell></row><row><cell>6.2.2.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>It will be interesting to see what type of stores and contexts Marketing Science 35(3), pp. 484-510, Â© 2016 INFORMS are more suitable for the implementation of the VAR system.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See http://www.videomining.com for an example.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Local context is defined as a local area surrounding the face, i.e., the head, neck, and shoulder area.3  Haar-like features are weighted differences of integrals over rectangular subregions (see<ref type="bibr" target="#b98">Viola and Jones 2001</ref>).4  The upper-body detector is a modified version of the Viola-Jones detector Jones 2001, 2004)  and is available through the Open Computer Visions Library and MATLAB.5  Details on face and facial expression recognition can be found in Online Appendix A (available as supplemental material at http:// dx.doi.org/10.1287/mksc.2016.0984).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Facial expression can be recognized either by classifying facial expressions in each single frame or by tracking facial points of interest in a temporal sequence of images<ref type="bibr" target="#b24">(Fasel and Luettin 2003)</ref>. In this paper, we use the former since we want to match the facial expression to the hand position in each frame.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Detailed information on the RI-LBP operator can be found in Online Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Here, a first-time user is a person who has no past try-on data stored in the database because the customer either is using the system for the first time or has chosen not to be tracked across different try-on incidents.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We also tested an implementation using a generic garment region map shown in Figure3(see footnote 19), where customers' preferences related to the 15 generic regions (instead of garment features) were detected from the try-on video.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Features of each garment were determined by the garment design generated from the attribute space in Table2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">The MATLAB computer vision toolbox was used to detect faces in the video frames.12  We describe the procedure in more detail in Online Appendix A (see also Jones 2001, 2004).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Using posed expressions to evaluate the performance of an expression recognition algorithm is a standard practice in the computer vision literature<ref type="bibr" target="#b95">(Tian et al. 2011).</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">In Online Appendix B we describe an alternative VAR implementation; the VAR model performance is quite consistent across a customer's different try-on incidents (and comparable to combining inferences from multiple try-on incidents).17  We examined a recommendation set including up to four items because the average consideration set observed in the validation stage was approximately four items (3.5). Furthermore, it would be difficult to present a larger set to a customer on a computer screen in real life. We also chose to recommend four items each time in the second empirical study.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">In the simple CF model, we select customers from the purchase history database who have purchased the garment that the focal customer is currently trying on as the neighborhood of the focal customer, and then use neighbors' past purchases to make recommendations to the focal customer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">The laptop would not be required if an IP address-enabled webcam and display screen were used.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">Would-have-tried items are the items that customers were interested in trying on, but could not because the recommended items were catalogue items.23  See footnote 20 for the recommendation mechanism of a simple CF model. Like the VAR model, the recommendation set size for the simple CF model was set as four.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24">We randomly selected 27 frames with frontal faces of 27 different participants described in Â§4, and the system was able to correctly match each with one of the 127 faces in the database.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25">O(n) is a general notation representing the time complexity of an algorithm. For an algorithm with a time complexity of O(n), as input size increases to infinity, the computing time increases linearly with the size of the input.26  Simulation results are available on request.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Marketing Science 35(3), pp.484-510, Â© 2016 INFORMS   </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the participants in presentations given by the authors, in the College of Business at City University of Hong Kong and Cambridge Judge Business School, for their feedback; as well as the editor, the area editor, and two anonymous reviewers for their insightful comments. This research was supported by the National Natural Science Foundation of China Fund [Grants 71232008 and 71502039], and the Institute for Sustainable Innovation and Growth (iSIG) at the School of Management, Fudan University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facial expression recognition and synthesis based on an appearance model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Comm</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="723" to="740" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Consumers&apos; conceptualization of apparel attributes. Clothing Textiles Res</title>
		<author>
			<persName><forename type="first">L</forename><surname>Abraham-Murali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Littrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">New recommendation techniques for multicriteria rating systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="48" to="55" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-criteria recommender systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Manouselis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Ricci</surname></persName>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
			<persName><forename type="first">B</forename><surname>Shapira</surname></persName>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="769" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using recommendation agents to cope with information overload</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aljukhadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Senecal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Daoust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Electronic Commerce</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="70" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning by collaborative and individual-based recommendation agents</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ariely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Lynch</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aparicio</surname></persName>
		</author>
		<author>
			<persName><surname>Iv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Psych</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Use of photography and video in observational research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Basil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Qualitative Market Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Examining markets, marketing, consumers, and society through documentary films</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Macromarketing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="409" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Videography in marketing and consumer research</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Belk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Kozinets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Qualitative Market Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="128" to="141" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable collaborative filtering with jointly derived neighborhood interpolation weights</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh IEEE Internat. Conf. Data Mining</title>
				<meeting>Seventh IEEE Internat. Conf. Data Mining<address><addrLine>Omaha, NE</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Seeking the ideal form: Product design and consumer response</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Bloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="29" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A collaborative filtering approach to mitigate the new user cold start problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="225" to="238" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Facial expression recognition-Review</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">B</forename><surname>Chavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Latest Trends Engrg. Tech</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="237" to="243" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facial expression recognition from video sequences: Temporal and static modeling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="187" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Made-to-measure technologies for an online clothing store</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cordier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graphics Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="48" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Market segmentation with choice-based conjoint analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Desarbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="147" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comprehensive survey of neighborhood-based recommendation methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Ricci</surname></persName>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
			<persName><forename type="first">B</forename><surname>Shapira</surname></persName>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="107" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unstructured direct elicitation of decision rules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzyabura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gaskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="127" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple mechanism to incentivealign conjoint experiments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Res. Marketing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active machine learning for consideration heuristics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dzyabura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="801" to="819" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward a model of the in-store purchase decision process: Consumer use of criteria for evaluating women&apos;s apparel. Clothing Textiles Res</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Damhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kadolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="13" to="22" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Strong evidence for universals in facial expression: A reply to Russell&apos;s mistaken critique</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="287" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality Soc. Psych</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="129" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Research on building the preference model of seamless badminton sportswear based on conjoint analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Materials Res</title>
		<imprint>
			<biblScope unit="volume">796</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Salesperson adaptive selling behavior and customer orientation: A meta-analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="693" to="702" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video-Based Automated Recommender (VAR) System for</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename></persName>
		</author>
		<idno>Â© 2016 INFORMS 509</idno>
	</analytic>
	<monogr>
		<title level="j">Garments Marketing Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="484" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Frijda</surname></persName>
		</author>
		<title level="m">The Emotions</title>
				<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Appraisal: What is the dependent?</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Frijda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeelenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Appraisal Processes in Emotion Theory, Methods, Research</title>
				<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
			<persName><forename type="first">A</forename><surname>Schorr</surname></persName>
			<persName><forename type="first">T</forename><surname>Johnstone</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="141" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using collaborative filtering to weave an information tapestry</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Oki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eigentaste: A constant time collaborative filtering algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="151" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conjoint analysis in marketing: New developments with implications for research and practice</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The influence of tactile input on the evaluation of retail product offerings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grohmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Spangenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Sprott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Retailing</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="245" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">WTF: The who-to-follow system at</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twitter. Proc. 22nd Internat. Conf. World Wide Web</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disjunctions of conjunctions, cognitive simplicity, and consideration sets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Befurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzyabura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inform. Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="53" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using a structural model of halo effect to assess perceptual distortion due to affective overtones</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Holbrook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="252" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Path data in marketing: An integrative framework and prospectus for model-building</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="320" to="335" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The traveling salesman goes shopping: The systematic deviations of grocery paths from TSP-optimality</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="566" to="572" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding unplanned consideration and purchase conversion using in-store video tracking</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Inman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="445" to="462" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Deconstructing the &quot;first moment of truth</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Statistical color models with application to skin detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="96" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluation of item-based top-n recommendation algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Tenth Internat. Conf. Inform. Knowledge Management</title>
				<meeting>Tenth Internat. Conf. Inform. Knowledge Management<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PIE: A holistic preference concept and measurement model</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="351" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Haptic identification of objects and their depictions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Klatzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception Psychophysics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="178" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Performance evaluation of NoSQL databases: A case study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gorton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Donohoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Workshop on Performance Anal. Big Data Systems</title>
				<meeting>1st Workshop on Performance Anal. Big Data Systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A runtime quality measurement framework for cloud database service systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Klems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bermbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weinert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth Internat. Conf. Quality Inform</title>
				<meeting>Eighth Internat. Conf. Quality Inform<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Human skin color clustering for face detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>KovaÄ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Solina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Tool, IEEE Region</title>
		<editor>Zajc B, TkalÄiÄ M</editor>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="144" to="148" />
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Kozinets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Belk</surname></persName>
		</author>
		<title level="m">The Sage Dictionary of Social Research Methods</title>
				<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Sage Publications</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="318" to="320" />
		</imprint>
	</monogr>
	<note>Videography. Jupp V</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Krishna</surname></persName>
		</author>
		<title level="m">Sensory Marketing: Research on the Sensuality of Products</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast and robust face finding via local context</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kruppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castrillon-Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint IEEE Internat. Workshop Visual Surveillance Performance Evaluation Tracking Surveillance</title>
				<meeting>Joint IEEE Internat. Workshop Visual Surveillance Performance Evaluation Tracking Surveillance<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vision-based hand modeling and tracking for virtual teleconferencing and telecollaboration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Internat. Conf. Comput. Vision</title>
				<meeting>Internat. Conf. Comput. Vision<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="666" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The effect of information overload on consumer choice quality in an online environment</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Marketing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="183" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The past, present and future of observational research in marketing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Broderick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Qualitative Market Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pixel-level hand detection in ego-centric videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2013 IEEE Conf. Comput. Vision Pattern Recognition</title>
				<meeting>2013 IEEE Conf. Comput. Vision Pattern Recognition<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A survey on CAD methods in 3D garment design</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mmf</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Indust</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="576" to="593" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Lyon</surname></persName>
		</author>
		<title level="m">Surveillance Society: Monitoring Everyday Life</title>
				<meeting><address><addrLine>Maidenhead, UK</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill Education</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Facial expression recognition using constructive feed forward neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khorasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1588" to="1595" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Experimental analysis of design choices in multiattribute utility collaborative filtering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Manouselis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Costopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Pattern Recognition Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="332" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The effect of examining actual products or product descriptions on consumer preference</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mccabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Nowlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Psych</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="431" to="439" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GfK adds facial coding to ad testing system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="http://www.research-live.com/news/technology/gfk-adds-facial-coding-to-ad-testing-system/4009252.article" />
	</analytic>
	<monogr>
		<title level="j">ResearchLive</title>
		<imprint>
			<date type="published" when="2013-02-25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gesture recognition: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, Cybernetics, Part C: Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="324" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">How peer influence affects attribute preferences: A Bayesian updating mechanism</title>
		<author>
			<persName><forename type="first">V</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="368" to="384" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Beyond conjoint analysis: Advances in preference measurement</title>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Feit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="337" to="354" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A comparative study of texture measures with classification based on featured distribution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">LAFTER: A real-time face and lips tracker with facial expression recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1369" to="1382" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">To have and to hold: The influence of haptic information on product judgments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Childers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="35" to="48" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The effect of mere touch on perceived ownership</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="434" to="447" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">It just feels good: Customers&apos; affective response to touch and its influence on persuasion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiggins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="56" to="69" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Representativeness, relevance, and the use of feelings in decision making</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="159" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A hybrid intelligence system for facial expression recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raouzaiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsapatsoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tzouvaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stamou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Sympos. Intelligent Technologies, Hybrid Systems Implementation Smart Adaptive Systems</title>
				<meeting>Eur. Sympos. Intelligent Technologies, Hybrid Systems Implementation Smart Adaptive Systems<address><addrLine>Mainz, Albufeira, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Verlag</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Getting to know you: Learning new user preferences in recommender systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cosley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mcnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Internat. Conf. Intelligent User Interfaces</title>
				<meeting>7th Internat. Conf. Intelligent User Interfaces<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Recommender systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Varian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="56" to="58" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Grou-pLens: An open architecture for collaborative filtering of netnews</title>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Iacovou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Comput. Supported Cooperative Work</title>
				<meeting>ACM Conf. Comput. Supported Cooperative Work<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename></persName>
		</author>
		<title level="m">Video-Based Automated Recommender (VAR) System for Garments 510</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="484" to="510" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Appraisal determinants of emotions: Constructing a more accurate and comprehensive theory</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Roseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Emotion</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="77" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fernandez-Dols</surname></persName>
		</author>
		<title level="m">The Psychology of Facial Expression</title>
				<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Collaborative filtering with multi-component rating for recommender systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Workshop Inform. Technologies Systems</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
			<persName><forename type="first">R</forename><surname>Venkataraman</surname></persName>
		</editor>
		<meeting>16th Workshop Inform. Technologies Systems<address><addrLine>Milwaukee</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Toward robust skin identification in video images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Foulds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Internat. Conf. Automatic Face Gesture Recognition</title>
				<meeting>Internat. Conf. Automatic Face Gesture Recognition<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frankowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Adaptive Web</title>
		<title level="s">Lecture Notes Comput. Sci.</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Brusilovsky</surname></persName>
			<persName><forename type="first">A</forename><surname>Kobsa</surname></persName>
			<persName><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page" from="291" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Can there ever be too many options? A meta-analytic review of choice overload</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scheibehenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greifeneder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="425" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Mood, misattribution, and judgments of well-being: Informative and directive functions of affective states</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Clore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality Soc. Psych</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">How do I feel about it? The informative function of affective states</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Clore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affect, Cognition, and Social Behavior</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Fiedler</surname></persName>
			<persName><forename type="first">J</forename><surname>Forgas</surname></persName>
		</editor>
		<meeting><address><addrLine>Hogrefe, Zurich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="44" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Illumination normalization for robust face recognition against varying lighting conditions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Internat. Workshop Anal. Modeling Faces Gestures</title>
				<meeting>IEEE Internat. Workshop Anal. Modeling Faces Gestures<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Computerized sales assistants: The application of computer technology to measure consumer interest-A conceptual framework</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Shergill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarrafzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Diegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electronic Commerce Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="191" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Surprising robustness of the selfexplicated approach to customer preference structure measurement</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="286" to="291" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A survey of collaborative filtering techniques</title>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">421425</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Example-based learning for view-based human face detection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Customer trust in the salesperson: An integrative review and meta-analysis of the empirical literature</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Swan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bus. Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Enhanced local texture feature sets for face recognition under difficult lighting conditions. Image Processing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biometrics Compendium</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1635" to="1650" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Why, when, and how much to entertain consumers in advertisements? A web-based facial tracking field study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El Kaliouby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="809" to="827" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Emotion-induced engagement in Internet video advertisements</title>
		<author>
			<persName><forename type="first">T</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="159" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Facial expression recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Face Recognition</title>
				<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="487" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognitive Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">The impact of retailer promotional activities on store traffic-A video-based technology. Working paper</title>
		<author>
			<persName><forename type="first">S</forename><surname>Valizade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jedidi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>New York</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vision Pattern Recognition</title>
				<meeting>IEEE Comput. Soc. Conf. Comput. Vision Pattern Recognition<address><addrLine>Kauai, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Facial expression decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth IEEE Internat. Conf., Comput. Vision</title>
				<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="958" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Knowledge, motivation, and adaptive behavior: A framework for improving selling effectiveness</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sujan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="174" to="191" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Wyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlston</surname></persName>
		</author>
		<title level="m">Social Cognition, Inference and Attribution</title>
				<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Just the faces: Exploring the effects of facial features in print advertising</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="352" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">An introduction to audio and visual research and applications in marketing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="213" to="253" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Need for touch and information processing strategies: An empirical examination</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdanparast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Spears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Behav</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="415" to="421" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Shape-based hand recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yoruk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1803" to="1815" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Beyond valence in customer dissatisfaction: A review and new findings on behavioral responses to regret and disappointment in failed services</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeelenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bus. Res</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="445" to="455" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">An examination of social influence on shopper behavior using video tracking data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Casual wear product attributes: A Chinese consumers&apos; perspective</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fashion Marketing Management</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Segmenting hands of arbitrary color</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Internat. Conf. Automatic Face Gesture Recognition</title>
				<meeting>IEEE Internat. Conf. Automatic Face Gesture Recognition<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="446" to="453" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
