<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Polyhedral Methods for Adaptive Choice-Based Conjoint Analysis: Theory and Application</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Toubia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Hauser</surname></persName>
							<email>jhauser@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rosanna</forename><surname>Garcia</surname></persName>
							<email>r.garcia@neu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Bradlow</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Columbia Business School</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<addrLine>522 Uris Hall</addrLine>
									<postCode>3022, 10027</postCode>
									<settlement>Broadway, New York</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">MIT Sloan School of Management, Massachusetts Institute of Technology</orgName>
								<address>
									<addrLine>40-179, 1 Amherst Street</addrLine>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Business Administration</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<addrLine>202 HA</addrLine>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Polyhedral Methods for Adaptive Choice-Based Conjoint Analysis: Theory and Application</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 e 1526-548X 07 2605 0596</idno>
					</monogr>
					<idno type="DOI">10.1287/mksc.1060.0257</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>P olyhedral methods for choice-based conjoint analysis provide a means to adapt choice-based questions at the individual-respondent level and provide an alternative means to estimate partworths when there are relatively few questions per respondent, as in a Web-based questionnaire. However, these methods are deterministic and are susceptible to the propagation of response errors. They also assume, implicitly, a uniform prior on the partworths. In this paper we provide a probabilistic interpretation of polyhedral methods and propose improvements that incorporate response error and/or informative priors into individual-level question selection and estimation.</p><p>Monte Carlo simulations suggest that response-error modeling and informative priors improve polyhedral question-selection methods in the domains where they were previously weak. A field experiment with over 2,200 leading-edge wine consumers in the United States, Australia, and New Zealand suggests that the new question-selection methods show promise relative to existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head> <ref type="bibr" target="#b16">Toubia et al. (2003)</ref> <p>demonstrated that polyhedral methods for adaptively selecting questions in metric conjoint analysis could improve accuracy when partworths are either homogeneous or heterogeneous, and could do so whether response errors are large or small. <ref type="bibr">Toubia et al. (THS 2004)</ref> extended polyhedral methods to choice-based questions, but with mixed success. Polyhedral choice-based questions improved accuracy when response errors were low, but not when they were high. Furthermore, although polyhedral methods for metric paired-comparison questions predict well for empirical data, there have been no empirical validity tests for choice-based polyhedral methods despite the growing interest among practitioners for adaptive choice-based methods.</p><p>In this paper we propose and test a generalization of THS that takes response error into account for choicebased questions and has the potential to improve accuracy in high response-error domains. We do so by recasting the polyhedral heuristic into a Bayesian framework. This framework also includes prior information in a natural, conjugate manner. After verifying the methods with simulations, we undertake a large-scale, multicountry study in which each respondent completes two separate conjoint tasks. This design enables us to compare question selection with a within-respondent design that implies greater statistical power to distinguish methods. We compare methods on the ability to predict actual choices. We examine whether the methods lead to different managerial implications by comparing forecasts of willingness to pay as well as the optimal product lines implied by each method.</p><p>This paper is organized as follows. Section 2 briefly reviews the published choice-based polyhedral methods and discusses two key limitations. Sections 3 and 4 propose solutions to these limitations. Sections 5 examines the methods with Monte Carlo simulations. Section 6 describes the methodological results of the field experiment. Section 7 concludes and offers directions for future research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Review and Critique of Polyhedral Choice-Based Methods</head><p>Choice-based polyhedral question selection selects each choice question to learn as much as possible about a respondent's preferences. The conceptual idea is to recognize that the set of choice questions and their corresponding answers define a polyhedron, i.e., a set of "feasible" partworth vectors that perfectly fit previous observations. Each choice narrows the range of feasible partworths making the range smaller and smaller until it converges toward a single partworth vector. The method works well when the respondent makes no errors, but can be highly sensitive to errors, particularly in the early choices.</p><p>We now provide a brief technical review to establish both notation and conceptual reasoning for the generalizations.</p><p>Answers to Choice-Based Questions Interpreted as Constraints on the Partworths Without loss of generality, we use binary vectors in the theoretical development to simplify notation and exposition. Multilevel features are used in both the simulations and the application. Let x qjf indicate that the jth alternative in the qth choice set contains the f th feature, and let x qj be the binary row vector describing the jth alternative in the qth choice set. Define x qk similarly for the kth profile. Let u be the l-dimensional vector of partworths for a given respondent. Let qj and qk be error terms such that the respondent's utility for profile j in choice set q is x qj u + qj . The utility-maximizing respondent will choose profile j * over profile k if and only if x qj * − x qk u + qj * − qk ≥ 0. Each choice among J alternatives implies J − 1 such inequality constraints, indicating that the utility of the chosen profile is higher than that of the other J − 1 alternatives in the choice set.</p><p>Let X 1 q be the matrix of the x qj * − x qk s for all J −1 inequality constraints stacked for the first q questions. Note that the respondent's q choices are coded in X 1 q by the selection of j * for each question. Let be the corresponding vector of error differences and, without loss of generality, scale all partworths to be nonnegative and normalize the partworths so that they sum to 100. <ref type="bibr">1</ref> Then, if e is a vector of 1s and 0 is a vector of 0s (of length l , the answers to the choicebased questions imply the following constraints on the respondent's partworths:</p><p>P1 X 1 q u + ≥ 0 u ≥ 0 e u = 100</p><p>1 Nonnegativity assumes that we know a priori which level of the partworth vector is preferred. This simplifies notation. We address empirical issues in later sections. The selection of 100 is arbitrary and implicitly rescales the error vector, .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Deterministic Polyhedral Question Selection</head><p>Polyhedron Ω {1,…,q} = set of estimates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Selection</head><p>For any given vector , the set of vectors u satisfying the constraints in (P1) is a mathematical object called a polyhedron. THS select questions such that the polyhedron corresponding to = 0 never becomes empty, and effectively assume that = 0. Let 1 q be the polyhedron obtained after q questions. The q + 1st question imposes new constraints on the partworths, giving rise to a new polyhedron 1 q+1 that is a subset of the previous polyhedron 1 q . For a linear compensatory utility model, each point in 1 q is consistent with the respondent choosing one and only one of the alternatives in choice set (q + 1) (except for a set of points of measure 0 for which the respondent is indifferent between at least two profiles). Hence, the q + 1st question divides 1 q into J collectively exhaustive (smaller) polyhedra that are of roughly equal size. The region corresponding to the respondent's choice becomes the starting polyhedron for the next question. See Figure <ref type="figure">1</ref> for a choice set of two alternatives. If there were no response errors, the sequence of polyhedra would shrink toward the respondent's true partworth vector.</p><p>Question selection (choice set selection) obeys two principles: (1) choice balance and (2) postchoice symmetry. Choice balance minimizes the expected size of 1 q+1 and is implemented by ensuring that a respondent who uses the working estimate of the partworths,ˆ u q , would be approximately indifferent between all the alternatives in the choice set. Choice balance is common in the literature and, for choice questions, typically increases the efficiency of the questions <ref type="bibr" target="#b2">(Arora and Huber 2001</ref><ref type="bibr" target="#b6">, Hauser and Toubia 2005</ref><ref type="bibr" target="#b7">, Huber and Zwerina 1996</ref><ref type="bibr" target="#b8">, Kanninen 2002</ref>. <ref type="bibr">2</ref> Postchoice symmetry minimizes the maximum Marketing Science 26(5), pp. 596-610, © 2007 INFORMS uncertainty on any combination of partworths, and is implemented by constructing choice sets that divide the polyhedron 1 q perpendicularly to its longest axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation</head><p>Because choice questions are chosen such that the polyhedron 1 q never becomes empty, all points in 1 q are consistent with all of the respondent's choices. Thus, THS use the analytic center of 1 q , u q , as the working estimate of u after q questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Critique</head><p>Choice-based polyhedral question selection and estimation are promising. Empirically, choice balance is achieved and the polyhedra shrink rapidly (although there is no published data on the ability to predict actual choices). Compared to randomly generated questions, orthogonal designs, and aggregate customization <ref type="bibr" target="#b2">Huber 2001, Huber and</ref><ref type="bibr" target="#b7">Zwerina 1996)</ref>, deterministic choice-based polyhedral questions improve accuracy when response error is low, but not when response error is high.</p><p>The poor performance for high response errors is likely due to response-error propagation, as illustrated in Figure <ref type="figure">2</ref>. In this example, the respondent's true partworth values are as indicated by a star ( ). With no response error, the respondent would choose Profile 2, corresponding to the lower polyhedron, and the set of feasible partworths (new polyhedron) would converge toward the true value. However, with response errors the respondent might choose Profile 1, corresponding to the upper polyhedron. Once such a choice is made, the partworths can never converge to the true value. The closest estimate would be on the border, as indicated by the small diamond ( ). Moreover, without a formal probabilistic structure, there is no easy way to incorporate prior Polyhedron Ω {1,…,q} = set of estimates Set of estimates consistent with x (q + 1)1 being chosen Set of estimates consistent with</p><formula xml:id="formula_0">x (q + 1)2 being</formula><p>Estimates caught on the "wrong" side of the polyhedron due to response error.</p><p>True partworths chosen. consistent with the first q choices information on the likely distribution of partworths. We next address both response error and prior information with a Bayesian interpretation of choice-based polyhedral methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bayesian Interpretation for Choice-Based Polyhedral Methods</head><p>We can interpret the analytic center as a working estimate if we assume a prior distribution on the partworth vector u that is uniformly distributed on the initial polyhedron, 0 = u u ≥ 0 e u = 100 . Denote this distribution as P 0 : defined by</p><formula xml:id="formula_1">P 0 u = 0 if u 0 ; P 0 u = 0 −1 if u ∈ 0 .</formula><p>( 0 is the measure of the set 0 .) Denote the data provided by the respondent through the first q choices with D 1 q . (D 1 q is encoded in X 1 q .) The deterministic algorithm implicitly assumes a likelihood function of the form:</p><formula xml:id="formula_2">P D 1 q u = 1 if u ∈ 1 q and P D 1 q u = 0 if u 1 q . Applying Bayes rule, P u D 1 q ∝ P D 1 q u P 0 u ∝ P 1 q u .</formula><p>In other words, the posterior distribution is the uniform distribution with support 1 q .</p><p>Once the method is viewed in a Bayesian framework, the two implicit assumptions of the absence of response error and uniform priors may easily be relaxed by generalizing, respectively, the likelihood function and the prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Probabilistic Polyhedral Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizing the Likelihood Function</head><p>In the deterministic algorithm = 0, and the respondent chooses the profile with the highest deterministic utility with probability 1. All posterior distributions are uniform distributions supported by polyhedra. We generalize the algorithm by considering distributions supported by mixtures of polyhedra. As the number of polyhedra in the mixtures grows, we can approximate any distribution, but we must balance this capability with the realization that as the number of polyhedra grows, the computational time grows exponentially. To balance these effects, we choose a simple likelihood function that captures the essence of response error. We use simulations to examine whether this is a sufficient approximation.</p><p>To obtain a structure in which the prior and posterior distributions are conjugate, we assume that the noise is distributed such that the respondent chooses the profile with the highest deterministic utility with probability, , and chooses the J − 1 other profiles with probability 1 − / J − 1 . The advantages of this assumption are that it provides a feasible algorithm and nests the deterministic algorithm as the special case when = 1. While we believe this assumption is a reasonable, first-order robust assumption, it may not hold exactly in real or synthetic data. To test the robustness of this assumption, we generate data in our simulations that use a traditional logistic function and, hence, violate this assumption to some degree.</p><p>In general, is unknown and can be assumed to vary across respondents and, potentially, across choice sets within a respondent (e.g., may be higher or lower if the profiles in the choice set are closer in utility). We might include priors for and do a full Bayesian updating such that P u i D 1 q x i1 x i2</p><p>x iJ ∝ i P D i u i P i u x i1 x i2</p><p>x iJ P u . To avoid complexity, for a first test of the algorithm, we model as homogeneous and constant. Fortunately, sensitivity analyses suggest that predictive ability is not sensitive to the choice of within a wide range that is consistent with the s estimated for our simulations and empirical test. See Appendices A1 and A2 for details on estimation and sensitivity. We leave to future research the investigation of alternative ways to specify and estimate . For our empirical tests, we use pretest data to select a point estimate of . Pretest selection follows the tradition of aggregate customization <ref type="bibr" target="#b2">Huber 2001, Huber and</ref><ref type="bibr" target="#b7">Zwerina 1996)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizing the Prior Distribution</head><p>We nest THS's implicit prior distribution within a mixture of uniform distributions supported by polyhedra: M m=1 m P m u where M is any positive integer, 1 M is a set of positive weights such that M m=1 m = 1, and 1 M is a set of polyhedra. In this paper we apply and test two special cases of nonuniform priors. The first special case approximates traditional normal priors. Figure <ref type="figure" target="#fig_1">3</ref> illustrates the approximation conceptually. (In one dimension, a polyhedron is a line segment.) The uniform distributions are indicated with solid lines; the approximation with a dotted line. Appendix A3 provides a procedure for choosing weights for the polyhedra providing support for the distribution.</p><p>The second special case, denoted "population priors," selects a mixture of polyhedra such that the median of the prior importance of each feature is equal to the median (across respondents) of its importance. The polyhedra are defined by inequalities implied by the median importances of the features. If F is the number of features, this prior uses a mixture of 2 F polyhedra. 3 Details on the definitions of the polyhedra and the computation of the weights are given in Appendix 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conjugate Posterior Distributions</head><p>An important feature of our generalization is that the class of likelihood functions and the class of priors presented above are conjugate; that is, the posterior distributions remain within the set of mixtures of uniform distributions supported by polyhedra.</p><p>In order to show this result, we begin with a prior distribution, P 0 u , supported by a (single) polyhedron, 0 . Let 1 = u X 1 u ≥ 0 u ≥ 0 e u = 100 be the polyhedron defined by the answer to the first question and D 1 be the data provided by this question. Let 0 − 1 be the set in which all points in 1 are removed from 0 . Applying Bayes rule:</p><formula xml:id="formula_3">p u D 1 ∝ p D 1 u P 0 u =        0 −1 if u ∈ 1 1 − J − 1 0 −1 if u ∈ 0 − 1 (1)</formula><p>The posterior is proportional to a piecewise-constant function that takes the values of zero at all points outside 0 , 0 −1 at all points in 1 , and 1 − / J − 1 0 −1 at all points in 0 − 1 . Hence, there exists a scalar, ∈ 0 1 , such that:</p><formula xml:id="formula_4">p u D 1 = P 1 u + 1 − P 0 u =    1 −1 + 1 − 0 −1 if u ∈ 1 1 − 0 −1 if u ∈ 0 − 1 (2)</formula><p>where P 1 u is the uniform distribution with support 1 . Equation (2) demonstrates that the posterior is a mixture of two uniform distributions supported by polyhedra. The scalar is implicitly defined by equating Equations ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>):</p><formula xml:id="formula_5">1 −1 + 1 − 0 −1 1 − 0 −1 = J − 1 1 − ⇔ 0 / 1 + 1 − 1 − = J − 1 1 − Marketing Science 26(5), pp. 596-610, © 2007 INFORMS</formula><p>The computation of requires knowledge of 0 / 1 , which is the ratio of the measure of 0 to the measure of 1 . For the choice-based polyhedral algorithm, we seek choice balance such that 0 is divided into J collectively exhaustive equal-measure subpolyhedra; thus, the ratio 0 / 1 is close to its average J . Hence, J − 1 / J − 1 . Let us next consider prior distributions that are defined by any mixture of uniform distributions supported by polyhedra, M m=1 m P m u . Defining 1 as above and following the same argument, the posterior after the new choice question is proportional to:</p><formula xml:id="formula_6">P D 1 u M m=1 m P m u = M m=1 m P D 1 u P m u = M m=1 m m P m ∩ 1 u + 1 − m P m ∩ 0 u</formula><p>which is also a mixture of uniform distributions supported by polyhedra. Finally, we generalize to q questions. Let S q be the set of all subsets of 1 2 q . For a subset s of S q , let s = u X s u ≥ 0 u ≥ 0 e u = 100 be the polyhedron consistent with the choice questions contained in s (recall that X s encodes the constraints implied by the answers to the questions in s). Let w s = s 1 − q− s for all nonempty s , where s denotes the number of elements in subset s and = J − 1 / J − 1 . The posterior after q questions is a mixture of uniform distributions supported by the polyhedra s ∩ m s∈S m∈ 1 M . We approximate the weights as follows: 4</p><formula xml:id="formula_7">P q u = M m=1 s∈S q m w s P s ∩ m u (3)</formula><p>We denote question selection and estimation based on this posterior distribution as "polyhedral with error-modeling and informative priors." We also consider the following special cases in the simulations and field experiment:</p><p>• "polyhedral without error-modeling and with uniform priors" (as in THS): = 1, prior: P 0 u , posterior: P q u = P 1 q u ; • "polyhedral with error-modeling and with uniform priors:" &lt; 1, prior: P 0 u , posterior: P q u = s∈S q w s P s u ; • "polyhedral without error modeling and with informative prior:" = 1, prior: M m=1 m P m u , posterior: <ref type="bibr">4</ref> We set m w s to zero if s ∩ m = and normalize the weights to sum to one.</p><formula xml:id="formula_8">P q u = M m=1 m P 1 q ∩ m u .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting Questions and Estimating Partworths with Mixtures of Distributions</head><p>In the deterministic algorithm, THS select questions based on the analytic center and longest axes of a single polyhedron. This is a well-defined problem. For the probabilistic algorithm we must work with P q u , which is a mixture of uniform distributions supported by polyhedra. To implement choice balance and postchoice symmetry, we must compute the analytic center and longest axes of polyhedral mixtures. Fortunately, the analytic center may simply be replaced with the appropriate mixture of the analytic centers of the polyhedra in the mixture. However, computing the longest axes poses a conceptual and technical challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Longest Axes of a Mixture of Polyhedra</head><p>The longest axis of a mixture of polyhedra should summarize the directions of the longest axes of the polyhedra in the mixture and do so according to the weights of the mixture. Let v sm be the longest axis of the polyhedron s ∩ m (see Equation ( <ref type="formula">3</ref>)), and m w s be the corresponding weight. We seek the vector v * that maximizes the weighted norm of the projections of v sm on v * . Thus, the longest axis is the solution to the following mathematical program:</p><formula xml:id="formula_9">OPT1 v * q = arg max v M m=1 s∈S q m w s v T sm v 2</formula><p>Fortunately, OPT1 has a known solution. Define V as the matrix obtained by stacking the transposed longest axes, v T sm . Define as the diagonal matrix with elements equal to the weights { m w s }. We rewrite OPT1 in matrix form as follows:</p><formula xml:id="formula_10">M m=1 s∈S q m w s v T sm v 2 = v V V v</formula><p>OPT1 is now a standard optimization problem that is analogous to factor analysis: v * q is the eigenvector associated with the largest eigenvalue of V V . The matrix is symmetric and positive definite; hence, its eigenvalues are all real and nonnegative. The secondlongest axis is associated with the second eigenvalue, etc. Because the axes are eigenvectors, they are guaranteed to be orthogonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Practical Implementation</head><p>While mixtures of polyhedra can approximate almost any distribution, there are practical considerations. Not only do the population priors grow exponentially with the number of features (2 F ), but the number of subsets S q in Equation ( <ref type="formula">3</ref>) grows exponentially with the number of questions (2 q ). For small q and small F , computation can be done quickly. Choice-based questions can be selected in less than a second, such that respondents do not notice any delay. However, for large q or large F the delay can exceed a second (e.g., 2 16 = 65 536).</p><p>We take three steps to reduce computation time. First, the set of polyhedra in the posterior mixture after q questions, P s ∩ m m=1 M s∈S q , is a subset of the polyhedra in the posterior mixture after q + 1 questions, P s ∩ m m=1 M s∈S q+1 (this follows from the fact that S q ⊂ S q+1 . By saving, rather than recomputing, the longest axes and analytic center, we reduce computation time substantially. Second, one of the time-consuming steps in polyhedral methods is finding a feasible point in s ∩ m . If a point is feasible in 1 2 q ∩ m , then it is feasible for all s ∩ m , s ∈ S q . By reusing feasible points, we also reduce computational time substantially. Third, as the number of questions grows large, we sort the weights m w s in decreasing order and apply the algorithm to subsets corresponding to the largest weights, doing so until a preset time limit is reached. In our empirical work, that time limit is one second. In simulation, we use 10 seconds. Exploratory work suggests that these time limits provide excellent performance. However, all empirical and simulation results reported in this paper can be considered conservative and might improve slightly with faster computers and more efficient codes/programming/compilers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Monte Carlo Simulations</head><p>Modeling response error and informative priors promises to enhance the accuracy of choice-based polyhedral question selection. However, both extensions increase complexity and could result in overfitting the data. To evaluate performance, we turn to complementary testing tools, both synthetic and empirical data. We use Monte Carlo simulations to study the potential of the methods by investigating the range of performance in a variety of relevant domains. With synthetic data we know the "truth" and can compare estimates to that benchmark. We use the field experiments to test practical implementation in a realistic setting. We do not know the true values of the partworths, and so must use predictive ability as a surrogate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Design for the Monte Carlo Simulations</head><p>We use a 2×2×6×4 design for the Monte Carlo simulations. We simulate the respondents with a 2 × 2 subdesign that is becoming standard-allowing for two levels of response accuracy and two levels of respondent heterogeneity <ref type="bibr" target="#b2">(Arora and Huber 2001</ref><ref type="bibr" target="#b5">, Evgeniou et al. 2005</ref><ref type="bibr" target="#b15">, Toubia et al. 2004</ref>). The Arora-Huber design, as modified by THS, uses four features at four levels each to ensure complete aggregate customization and orthogonal designs. Partworths are drawn from normal distributions with means,¯ u, and variances, 2 . The four levels of each partworth have means [−¯ −¯ /3 ¯ /3 ¯ ]. Higher values of¯ imply higher response accuracy. Higher values of 2 imply greater heterogeneity. We used the standard values of¯ = 1 for the "low-accuracy" case and¯ = 3 for the "high-accuracy" case with 2 =¯ for "low heterogeneity" and 2 = 3¯ for "high heterogeneity."</p><p>For each respondent we simulate six questionselection methods:</p><p>• random • orthogonal • aggregate customization <ref type="bibr">Zwerina 1996, Arora and</ref><ref type="bibr" target="#b2">Huber 2001)</ref> • deterministic polyhedral (as in THS)</p><p>• probabilistic polyhedral with error modeling and uniform priors</p><p>• probabilistic polyhedral with error modeling and informative prior (prior approximates a normal distribution-see §3 and Appendix A3 for details).</p><p>The six question-selection methods are crossed with four estimation methods:</p><p>• hierarchical Bayes (HB) with normal priors • deterministic analytic-center estimation (AC)</p><p>• analytic-center estimation with error modeling and uniform prior (ACe)</p><p>• analytic-center estimation with error modeling and informative prior (ACe+i) (prior approximates a normal distribution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulated Environment</head><p>Aggregate customization uses relabeling and swapping to improve utility balance in choice-based questions and requires an estimate of the population partworth means. Polyhedral question selection with error modeling requires an estimate of . This estimate is derived from the same population estimates (see Appendix A1 for details). Following <ref type="bibr" target="#b2">Arora and Huber (2001)</ref>, we assume perfect pretest information. This should not affect the relative comparison of aggregate customization and probabilistic polyhedral question selection. Naturally, no such assumption is made in the empirical tests. Likewise, to investigate the impact of informative priors (relative to no priors), we use a rough approximation (four polyhedra) to the true prior distribution. <ref type="bibr">5</ref> All simulation results are interpreted in light of these assumptions.</p><p>We seek to afford the estimation benchmark methods the strongest possible performance. Evgeniou   * Best, or not significantly different from best, at p ≤ 0 05 within magnitude × heterogeneity × estimation condition.</p><p>1 HB = hierarchical Bayes estimation, AC = deterministic analytic-center estimation, ACe = analytic-center estimation with error modeling and uniform priors, ACe+i = analytic-center estimation with error modeling and informative priors. et al. <ref type="bibr">(2005)</ref> demonstrate that HB performs better if for each respondent we use rejection sampling <ref type="bibr" target="#b1">(Allenby et al. 1995)</ref> to constrain the HB estimates so that the partworth of the lowest level of each feature is also the smallest partworth for that feature. <ref type="bibr">6</ref> We adopt this procedure for HB in both the simulation and the field experiments.</p><p>Because polyhedral methods are designed for short Web-based questionnaires, we test designs of eight questions, choosing randomly for orthogonal and aggregate customization as in THS. For comparison to previously published simulations we report root mean squared error (RMSE) after normalizing the true and the estimated partworths so that their absolute values sum to the number of parameters and so that their values sum to zero for each feature. This enables us to interpret the RMSEs as a percentage of the mean (absolute) partworths. The simulations in Table <ref type="table" target="#tab_3">1</ref> are based on now-standard 10 sets of 100 respondents. This is not a computational constraint; the field tests are based on larger samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Interpretation of Synthetic Data Experiments</head><p>Question Selection. Taking response errors into account and using informative priors appears to have the potential to improve question selection. At least one of the two modifications in polyhedral question selection is best or tied for best in all 16 accuracy × heterogeneity × estimation experimental cells. Probabilistic polyhedral question selection with error modeling and uniform priors is at least as good as the deterministic algorithm in every cell and significantly better in 9 of the 16 cells. Probabilistic polyhedral question selection with error modeling and informative priors is at least as good as the deterministic algorithm in 15 of the 16 cells and significantly better in 12 of the 16 cells. The field experiment will test whether such improvements are sustained in practical implementations.</p><p>Estimation. Taking response errors into account and using informative priors also appear to have the potential to improve polyhedral estimation. At least one of the two improvements is better than deterministic analytic-center estimation in all accuracy × heterogeneity experimental cells. Informative priors appear to provide the greater improvement. However, the hierarchical Bayes estimates (HB) are still significantly better in three of the four accuracy × heterogeneity cells. The only exception is the lowaccuracy, low-heterogeneity cell in which ACe+i is statistically tied with HB. These results are consistent with the simulations of <ref type="bibr" target="#b5">Evgeniou et al. (2005)</ref>.</p><p>In summary, our simulations suggest that incorporating response error and/or informative priors into polyhedral question selection is likely to enhance accuracy in empirical applications. Analytic-center estimation is also improved, but hierarchical Bayes is likely to remain the best estimation method in most application domains for choice-based questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Empirical Application and Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Managerial Context</head><p>Traditional cork closures have dominated the wine industry for hundreds of years, but each year 5%-15% of all bottled wine is tainted due to poor-quality closures. Cork closures result in brand-name erosion and millions of dollars in lost revenue when consumers attribute the poor quality to the winery rather than the closure. As an alternative to cork closures, the wine industry developed Stelvins, a screw-cap/ twist-off closure for mid-to-high-priced wines. Stelvins eliminate cork taint and other malodorous flavors, eliminate wine oxidation that leads to rapid aging, and minimize loss of fruit flavors due to air leakage. Stelvins provide "consistent, reliable, aging characteristics, showing the wine's development as the winemaker intended <ref type="bibr" target="#b4">(Courtney 2001)</ref>."</p><p>Although Stelvins have been available for almost 50 years, and in Australia and New Zealand sales of premium wines with Stelvins now outnumber the sales of premium wines with corks, Stelvins are rarely used in the United States. To explore strategies for a U.S. introduction of Stelvins, a Napa Valley-based closure manufacturer and cooperating U.S. wineries asked us to determine preferences of leading-edge wine customers in the United States, New Zealand, and Australia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Design</head><p>In exchange for gathering these data, the sponsors agreed to set up the application as an experimental design. Each respondent completed two sequential, rotated, choice-based conjoint analysis tasks separated by a series of "memory-cleansing" questions. The advantage of this experimental design is the increased power due to methodological comparisons within respondents.</p><p>We recruited 2,255 leading-edge wine consumers from the United States, Australia, and New Zealand (late 2004). Respondents were subscribers to winerelated e-newsletters (WineX and WineBrats in the United States, Vine Cellars in Australia and New Zealand) and could be expected to be knowledgeable about fine wines. They were likely to be leadingedge consumers. As a check, 80% of the respondents scored 15 or higher on a 21-point involvement scale <ref type="bibr" target="#b11">(Lockshin et al. 2001)</ref>. We obtained 245 respondents from a first U.S. panel, 958 from a second U.S. panel, 667 from Australia, and 385 from New Zealand. As is typical in managerial applications, we did not have total control over the assignment of respondents to treatments, although there was no reason to believe that there were any systematic biases within any of the countries.</p><p>Managerially, the sponsors were interested in the trade-offs that the consumers would make between wine closures and other features of wine. The conjoint design included five features at four levels each:</p><p>• closure type: traditional cork, synthetic cork, Metacork ™ , 7 Stelvin (screw cap);</p><p>• type of wine: dry white, aromatic white, dry red, blush red;</p><p>• origin: Australia/New Zealand, France, Sonoma/ Napa, Chile/Argentina;</p><p>• vintner: small boutique, midsize regionally known winery, large nationally recognized winery, international conglomerate winery;</p><p>• price range: 8 four levels in the respondents' currency (e.g., Australian dollars).</p><p>The features in the conjoint design were introduced to respondents through self-explicated importance questions. <ref type="bibr">9</ref> Figure <ref type="figure">4a</ref>  Small boutique wineries with limited production (for example, less than 5000 cases annually)</p><p>Mid-sized regionally known wineries (for example, less than 100,000 cases annual production) We will enter you into a drawing in which we will select a winner to receive a wine club package worth $100 of wine. Please select your preferences for the following wine selection should you be chosen as the winner.</p><p>As there might be limited supplies of the different available wines, please rank your preferences for the following selections from first to sixth (where a first choice indicates your highest preference and a sixth choice is your lowest preferences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second Fourth Fifth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><formula xml:id="formula_11">Choice D Choice E Choice F Sixth Third First</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dry Red Dry Red</head><p>France and Figure <ref type="figure">4b</ref> another feature (vintners). Respondents were then asked two sets of 12 choice-based questions as illustrated by Figure <ref type="figure">4c</ref>. The first 10 questions of each set were designed by a different method (the order was rotated). The last two questions were randomly selected holdouts. Finally, after additional filler tasks, respondents were entered into a lottery with a 1 in 200 chance of winning a case of wine worth $100.</p><p>Respondents were asked to rank six cases of wine and were told that they would receive their first choice if it was available. Otherwise, they would receive their second choice, etc. All bottles within a case were the same (for example, if the wine costs $20, they received five bottles). <ref type="bibr">10</ref> The six wine profiles were randomly chosen from a 16-profile orthogonal design. This last task, designed with a different look and feel from the conjoint tasks (Figure <ref type="figure">4d</ref>), serves as a validation.</p><p>We note that this study is the first empirical test of the predictive ability of choice-based polyhedral methods. <ref type="bibr" target="#b16">Toubia et al. (2003)</ref> report on metric paired comparisons for laptop computer bags, and THS report on convergence and choice balance for an executive education study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of Question-Selection Methods: Experimental Design</head><p>We tested the following four question-selection methods: <ref type="bibr">11</ref> • orthogonal design;</p><p>• aggregate customization;</p><p>• deterministic polyhedral (THS);</p><p>• polyhedral with error modeling and uniform priors ("probabilistic polyhedral").</p><p>We chose the four methods carefully both to test the new probabilistic polyhedral method and to explore two fundamental characteristics. (1) Adaptation: Both deterministic and probabilistic polyhedral methods adapt questions within the respondent; aggregate customization and orthogonal designs do not. (2) Pretest information: aggregate customization and probabilistic polyhedral methods require pretest information to set "tuning" parameters; orthogonal designs and deterministic polyhedral methods do not. The pretest information was obtained from an HB analysis of 66 respondents who answered questions based on an orthogonal design. In order not to confound these two characteristics, we tested these methods in two pairs: orthogonal versus deterministic polyhedral, and aggregate customization versus probabilistic polyhedral. <ref type="bibr">12</ref> We did not test random question selection because prior research suggests that aggregate 10 Due to legal issues regarding alcohol as a prize, Australian respondents were not eligible to win real cases of wine. For these respondents the choice was hypothetical. Providing a reward with a fixed monetary value mitigates any wealth effect that might be present if we had endowed each respondent with money and given them the option of choosing among differently priced wines. The task remains incentive compatible as long as consumer utility is approximately linear in the number of bottles of wine over the range of the options available.</p><p>12 Due to a programming error, 227 and 204 respondents from the Australian panel were assigned, respectively, to orthogonal versus aggregate customization and to deterministic polyhedral versus probabilistic polyhedral. Orthogonal versus aggregate customization revealed no difference. Probabilistic polyhedral performed better than deterministic polyhedral, although the sample size was customization and orthogonal design are stronger benchmarks. We leave tests of informative priors in question selection to future research. We test informative priors for estimation (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of Estimation Methods</head><p>Each of the estimation methods is compatible with all of the question-selection methods enabling us to make comparisons within respondent. The methods we tested were: <ref type="bibr">13</ref> • hierarchical Bayes estimation (HB);</p><p>• deterministic analytic-center estimation (AC);</p><p>• analytic-center estimation without error modeling and with informative population priors (ACi);</p><p>• analytic-center estimation with error modeling and with uniform priors (ACe).</p><p>Because partworth values might vary by panel and treatment (and some methods shrink estimates to the mean or median), we apply all methods within panel and treatment.</p><p>We begin with estimation and then move to our primary focus: question selection. Table <ref type="table">2</ref> summarizes the comparisons of estimation methods for the validation task by reporting the correlation (averaged across respondents and question selection methods) between the predicted and observed rankings of the six wines in the validation task. <ref type="bibr">14</ref> We compared estimation methods statistically with a repeated-measures ANOVA, with performance as the dependent variable, two between-subject factors, panel (four levels) and question-selection comparison treatment (two levels); and two within-subject factors, estimation method (four levels) and a factor capturing whether the question selection is adaptive (two levels). We used contrast analysis to compare estimation methods. As predicted by the simulations, HB performs significantly better than the other estimation methods (p &lt; 0 01). <ref type="bibr">15</ref> insufficient to reach significance. Details are available from the authors. <ref type="bibr">13</ref> As is appropriate for an empirical test, we use our second form of informative priors in which the prior distributions are chosen to match the population medians. This method uses a mixture of 2 F polyhedra in the prior, where F = 5 is the number of features. Even with the computational efficiencies discussed earlier, it is not yet practical to apply both error modeling and population priors on our large data set. The former is exponential in the number of questions and the latter in the number of features. We leave development of faster heuristics to future research, noting that empirical results are thus conservative. If error modeling and population priors are separately promising, we might infer that their combination is also promising.</p><p>14 Measuring performance by the proportion of respondents for whom the first choice in the validation task was correctly predicted or using holdout hit rate yields similar qualitative implications. panel (p &lt; 0 05 -ANOVA on the Australian panel only). Probabilistic polyhedral question selection performs significantly better than aggregate customization question selection: overall (p &lt; 0 06), across the non-U.S. panels (p &lt; 0 02), and in the New Zealand panel (p &lt; 0 05). Probabilistic polyhedral question selection is better in three of the four panels and never significantly worse. While the results do not always obtain a significance level of 0.05, we can say, at minimum, that probabilistic polyhedral methods show promise.</p><p>In summary, the proposed probabilistic polyhedral question-selection methods improve correlations between predicted and actual choice in at least some situations. HB remains the best estimation method overall. Both ACe and ACi improve predictive ability relative to deterministic polyhedral methods (AC).</p><p>Substantive Results: Consumer Reactions to Stelvin Screw Caps for Fine Wine Figure <ref type="figure">5</ref> reports the estimates of the average partworths for wine closures for leading-edge wine consumers in the United States, Australia, and New Zealand. <ref type="bibr">17</ref> In Australia and New Zealand there is a slight preference for Stelvins over traditional cork closures. However, for the United States, corks are strongly preferred to Stelvins and closure type is a more important attribute. U.S. consumers even prefer MetaCorks ™ and synthetic corks to Stelvins, whereas Australians and New Zealanders prefer Stelvins to these other nontraditional closures.</p><p>We also examine the importance of wine closures relative to other features. Figure <ref type="figure">6</ref> reports the average partworths for the type of wine and the origin of the wine. Preferences for the type of wine and the country of origin are roughly the same for U.S., New Zealand, and Australian consumers, with the exception of a home-country bias. (Detailed partworth values are available from the authors.)</p><p>The data suggest that for U.S. consumers, the relative importance of Stelvins versus corks (6.91) is comparable or less than the relative importance of wine type (dry red versus blush red, 17.88), region (United States versus France, 6.68), and type of winery (regional versus international, 8.36). At least initially, bottles with Stelvin closures will have to be offered at a discount in order to capture a significant market share. For example, for higher-priced wines, market simulations based on our estimates (averaged across question-selection methods) suggest that  As an illustration, Figure <ref type="figure">7</ref> plots the estimates of the average partworths for wine closures based on the two question-selection methods in the U.S. panels. Comparing the two plots, we see subtle differences between methods. However, these plots only capture the average partworths across respondents, not the full distribution of partworths estimates. Moreover, without reference to the managerial context, it is difficult to intuit whether the estimates based on different question-selection methods imply differences in strategy. Thus, we examine the quantitative implications. We begin by comparing the predicted response to a price discount of $10 on Stelvin closures. Partworth estimates based on aggregate customization questions suggest that a price discount of $10 on Stelvin closures would capture 44.3% of this premium wine market; partworth estimates based on the probabilistic polyhedral questions suggest lower market share of 39.8% (p &lt; 0 03). Depending on the costs of marketing Stelvins, this lower reward might be the difference between a GO and a NO GO decision.</p><p>To gain further insight into whether or not partworth differences imply different managerial decisions, we draw on recent research by <ref type="bibr" target="#b3">Belloni et al. (2005)</ref>. Belloni et al. solve an optimal product design problem based on partworth data similar in structure to that collected here. Their design problem consists of selecting product features for the profiles in a product line in order to maximize profit (faced with a fixed set of competitors). Using Lagrangian relaxation with branch and bound, they identify the optimal product line for relatively large numbers of features and customers. More importantly, they compare a variety of heuristics and demonstrate that simulated annealing ( <ref type="formula">1</ref>) is feasible for reasonably sized problems and</p><p>(2) achieves 100% of the optimum in their test problems (p. 20). We adopt their structure and modify their simulated annealing code to optimize a product line of wine profiles based on cost estimates obtained from wine experts. We assumed that the competitive products available to consumers were the set of all profitable profiles containing traditional corks. We assumed that each consumer purchased exactly one bottle.</p><p>Using <ref type="bibr">Belloni et al.'s (2005)</ref> formulation, we developed optimal Stelvin-based product lines (consisting of 10 products) using (1) probabilistic polyhedral questions and (2) aggregate customization questions. Within this framework, the product line based on the partworths obtained by probabilistic polyhedral questions had three profiles in common with that designed based on the aggregate customization partworths. Another four pairs of profiles varied on one feature. Furthermore, if the partworths obtained from probabilistic polyhedral questions describe the market, the profit obtained with the polyhedral-based product line was 19.4% higher than the product line based on aggregate-customization partworths. 19 <ref type="bibr">19</ref> This calculation assumes that probabilistic polyhedral is the best estimate and is provided for illustration. Profit is guaranteed to be no worse by the principle of optimality. However, we find the magnitude of the difference-almost 20%-to be interesting, especially compared to the 2.1% difference due to optimization method found by <ref type="bibr">Belloni et al. (2005, p. 28)</ref>. For alternative methods of profitability comparisons, see <ref type="bibr" target="#b13">Rust and Verhoef (2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Research</head><p>This paper focuses on improved methods for adaptive question selection in conjoint analysis. We nest deterministic polyhedral methods using conjugate classes of likelihood functions and prior distributions. Our probabilistic Bayesian framework overcomes prior weaknesses by enabling researchers to (1) take response error into account and (2) introduce informative priors. Simulation and empirical tests suggest that these improvements are promising. The wine-closure application is the first predictive test of choice-based polyhedral methods. For this application, individual adaptation of questions shows promise.</p><p>We close by noting limitations and avenues for future research. First, computation issues forced us to use approximations and to use prior distributions described by only few polyhedra. More efficient algorithms could be developed and the structure of the problem may be exploited further to alleviate this limitation. Second, analytic-center estimation continues to improve, but does not yet perform as well as hierarchical Bayes. Using the Bayesian interpretation of polyhedral question selection, we might derive a formal Bayesian-loss-function minimization that improves analytic-center estimation <ref type="bibr" target="#b12">(Rossi and Allenby 2003)</ref>. Third, as a first approximation we used the same value of for all choice questions and all respondents. We might improve estimation if is specified as a function of the difficulty of the choice questions and/or is allowed to vary over the number of questions <ref type="bibr" target="#b9">(Liechty et al. 2005)</ref>. Using the formulae in this paper, we might also specify a prior on , conditional on the partworths and the choice set, and formulate a posterior given the observations. 20 Fourth, our simulations used a nowstandard structure, but there remain interesting tests with nondiagonal covariance matrices and specifications where the average partworths vary. Finally, other approaches to handling response error may be developed using stochastic optimization <ref type="bibr" target="#b14">(Spall 2003)</ref> or statistical learning theory <ref type="bibr" target="#b5">(Evgeniou et al. 2005)</ref>. Polyhedral methods remain a nascent technique that we hope will improve with future testing and future developments.</p><p>their wine industry expertise. Michael Yee has provided valuable comments that the authors gratefully acknowledge. They also thank Matthew Selove for suggesting the product-line optimization test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices. Derivations and Algorithms</head><p>A1. Computation of , the Tuning Parameter for Error Modeling</p><p>We compute as follows: • Estimate a population mean for the partworths,¯ u pop . In Monte Carlo simulations we use the true mean; in the empirical applications we use hierarchical Bayes estimates from the pretest subjects.</p><p>• Generate R random questions (R = 100 in the simulations and experiments) with logistic probabilities based on u pop . The probability, r , that a respondent chooses the maximum utility profile is the maximum logistic probability for that respondent on that question. Averaging over respondents gives an initial estimate, o .</p><p>• Use¯ u pop to simulate N respondents (N = 100 in the simulations and experiments) using o = J o − 1 / J − 1 for polyhedral question selection. Recompute as above (assuming logistic probabilities).</p><p>In theory, one might iterate these steps toward convergence; however, in practice we found that was not sensitive to the initial estimate, o used to generate the questions. Nonetheless, this means that our simulations are conservative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Sensitivity to</head><p>In the simulations, we purposefully base the choices of synthetic respondents on logistic probabilities. These simulated choices imply that varies by respondent and thus tests the sensitivity of our approximation that is constant across respondents. We study further the sensitivity with respect to ' using a simulation set up similar to that in §4. We simulate five sets of 100 synthetic respondents using magnitude and heterogeneity parameters equal to 2.0. Questions are selected with probabilistic polyhedral methods with error modeling and informative priors; the estimation method is ACe+i.</p><p>Figure A.1 suggests that predictive accuracy is flat for a fairly wide interval for . Fortunately, the values obtained using the above procedure always fell within this interval where m = u ˆ u − C m ≤ u ≤ˆ u + C m , u ≥ 0, e u = 100 . In the Monte Carlo simulations, we used M = 3, with C 1 = 5, C 2 = 10, and C 3 = 15. Without loss of generality, assume that C 1 &lt; C 2 &lt; • • • &lt; C M so that the "boxes" used to approximate the normal distribution are of increasing sizes. The weights 1 M are found by solving the following system of equations:</p><formula xml:id="formula_12">Prob u ∈ 1 u ∼ N u = 1 + 2 Prob u ∈ 1 u ∼ P 2 +•••+ M Prob u ∈ 1 u ∼ P M + 1 − M m=1 m P u ∈ 1 u ∼ P 0 Prob u ∈ 2 − 1 u ∼ N u = 2 Prob u ∈ 2 − 1 u ∼ P 2 + 3 Prob u ∈ 2 − 1 u ∼ P 3 + • • • + M Prob u ∈ 2 − 1 u ∼ P M + 1 − M m=1 m P u ∈ 2 − 1 u ∼ P 0 Prob u ∈ M − M−1 u ∼ N u = M Prob u ∈ M − M−1 u ∼ P M + 1 − M m=1 m P u ∈ M − M−1 u ∼ P 0</formula><p>In the Monte Carlo simulations the left-hand sides were approximated numerically by drawing 10,000 sets of parameters from N u , where N u was the distribution used to generate the true partworths. Prob( u ∈ m − m−1 u ∼ P 0 was computed numerically by drawing 10,000 sets of parameters from P 0 . We recognize that Prob( u ∈ m − m−1 u ∼ P m is equal to Prob u ∈ m − m−1 u ∼ P 0 Prob u ∈ m u ∼ P 0</p><p>The numerator and denominator were computed numerically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4. Incorporating Population Priors for</head><p>Feature Importances</p><p>In our empirical application, all features have the same number of levels, so we define importance as the sum of the partworths for that feature (setting the lowest partworth to zero). Importance can also be defined as the difference between the highest and lowest partworth for a feature. We have found that constructing priors based on constraints on the importances is more practical and intuitively appealing than using constraints on the partworths themselves. Let m f be the median of the importance of feature f based on individual-respondent estimates. Let m f be the median importance based on P 0 u and let P u be the probability that the importance of feature f is smaller than m f for P 0 u . Compute P u numerically with 10,000 draws on 0 . If m f &gt; m f ( P u &gt; 0 5 , the constraint corresponding to feature f is that its importance is greater than m f . This constraint is associated with the weight f , such that 0 5 = f + 1 − f 1 − P u . If m f ≤ m f P u ≤ 0 5 , the constraint corresponding to feature f is that its importance is less than m f , and the corresponding weight is f such that 0 5 = f + 1 − f P u .</p><p>Let F be the number of features, and S F be the set of subsets of all subsets of 1 2 F . The prior distribution is P u = s∈S F m s P s u , where</p><formula xml:id="formula_13">• m s = F f =1 f ∈s f 1 − f f s</formula><p>where f ∈ s is 1 if f is in s and 0 otherwise; f s is its complement.</p><p>• s is the polyhedron obtained from adding the constraints corresponding to the features f ∈ s to the initial constraints defining 0 . 4. Solve for the eigenvalues of V V and select the J /2 eigenvectors associated with the largest J /2 eigenvalues. (If J is odd, find the J + 1 /2 longest axes.) See the section "Longest Axes of a Mixture of Polyhedra" for details.</p><p>5. Find the intersections of the longest axes of the probability mixture with 0 ⇒ u j .</p><p>6. Find the J profiles by solving the knapsack problem, maximize x j u j subject to x j u q ≤ K, where K is a randomly drawn constant. (See THS for details.) <ref type="bibr">21</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2 Illustration of Response Error in Deterministic Polyhedral Question Selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure 3Approximating a Normal Prior with a Mixture of Polyhedra</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Notes. RMSE, lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4 Example Screenshots from the Wine-Closure Preference Study (a) Wine closures (b) Another feature (winery type)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 6Average Partworths for the Type of Wine and Wine Origin Type of wine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 7Average Partworths for the Wine Closures by Two Different MethodsAggregate customization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Figure A.1 Sensitivity of Predictive Accuracy to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>A5.</head><label></label><figDesc>Summary of Probabilistic Polyhedral Question Selection 1. Compute the weights for the probability mixture, w s = s 1 − i− s , for all s ∈ S q . 2. Compute the analytic center of the mixture, u q = M m=1 s∈S q m w s AC s ∩ m . 3. Approximate each polyhedron s ∩ m with an ellipsoid and compute the longest axis of the ellipsoid according to deterministic polyhedral methods (see THS for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Toubia, Hauser, and  Garcia: Probabilistic Polyhedral Methods for Adaptive Choice-Based Conjoint Analysis Marketing Science 26(5), pp. 596-610, © 2007 INFORMS 597</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Monte Carlo Simulation Results</figDesc><table><row><cell>RMSE</cell></row></table><note>* modeling and informative priors</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Comparing Question Selection Methods-Correlation with Choice</figDesc><table><row><cell>Australian</cell><cell>New Zealand</cell><cell>Average of</cell><cell>First U.S.</cell><cell>Second</cell><cell>Average of</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Toubia, Hauser, and Garcia: Probabilistic Polyhedral Methods for Adaptive Choice-Based Conjoint Analysis    </figDesc><table><row><cell>610</cell><cell>Marketing Science 26(5), pp. 596-610, © 2007 INFORMS</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The first-order conditions for logit-based choice-based questions indicate that the information matrix is maximized for questions that are close to, but not perfectly, choice balanced. See appendix to<ref type="bibr" target="#b6">Hauser and Toubia (2005)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The importance can be defined as the difference between the maximum and minimum partworths for a feature or the average absolute magnitudes of the partworths. Importances have both conceptual and computational advantages relative to imposing median constraints on the partworths directly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use normal priors rather than population priors on feature importances, because the latter would require that we deviate from the standard simulation design, thus reducing our ability to compare our results to previously published papers. Population priors only lead to differences when the average importances vary among the four simulated features. This does not happen in the standard simulation design, but is likely in our empirical test.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Constraints in estimation are used in other areas of marketing as well (see, for example,<ref type="bibr" target="#b0">Ailawadi et al. 2005</ref>).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">A MetaCork ™ "combines an integrated corkscrew, a drip-resistant pour feature, and a reseal cap."(www.metacork.com, viewed 2006)    8 Based on pretests, respondents felt they could best evaluate the choices among wines if price was specified as a range. This is sufficient for relative methodological comparisons and the study of the impact of consumer preferences for Stelvin closures.9  These answers allowed us to identify the lowest level of each feature. This information was used in adaptive question selection and by all the estimation methods, including HB (see previous section). The self-explicated information was used in adaptive question selection and in estimation in order to avoid endogeneity and/or violations of the likelihood principle<ref type="bibr" target="#b10">(Liu et al. 2006</ref>).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">The orthogonal and aggregate customization designs were the most D-efficient sets of 10 questions from a 16-question orthogonal design and from the corresponding aggregate customization design, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">The panel was significant at the 0.01 level and treatment was nonsignificant (p = 0 42). Adaptation is discussed below.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">Similar significance levels were obtained with an ANOVA similar to the ANOVA for the Table2data, with an additional betweensubject factor capturing the experimental cell.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">The average partworths have been normalized such that the lowest level of each attribute has a partworth of 0 and the sum of the partworths across attributes is 100.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18">Results are approximate because the sponsors defined prices with ranges. We used the midpoint of each range in our share and profit calculations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20">We would like to thank the AE for suggestions regarding the parameter .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21">In this paper, we drew K up to 30 times until all profiles were distinct. If all profiles are identical after 30 draws, it is likely that no further questions are needed and the questioning sequence stops. If after 30 draws there are only K distinct solutions (1 &lt; K &lt; K), these are presented to the respondent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the MIT Sloan School of Management, MIT's Center for Innovation in Product Development, and Northeastern University's Institute for Global Innovation Management. The authors wish to thank Kevin Stanik and Andrew Rutkiewitz for their computer support, as well as Tom Atkin and Larry Lockshin with</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although the focus of this paper is on probabilistic polyhedral question selection, probabilistic analytic-center estimation is a byproduct of probabilistic question selection, and probabilistic analyticcenter estimation improves predictions relative to deterministic analytic-center estimation. Including population priors (ACi) significantly improves performance (p &lt; 0 01) compared to deterministic analyticcenter estimation (AC). Including error modeling (ACe) improves performance as well, albeit not significantly (p = 0 30). (ACe is never significantly worse than AC, and is significantly better on the second U.S. panel.) The two improvements do not perform significantly differently (p = 0 18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Comparison of Question-Selection Methods</head><p>Table <ref type="table">3</ref> reports the average performance of the different question-selection methods (averaged across respondents and estimation methods). Due to circumstances beyond our control, all U.S. respondents were assigned to the "probabilistic polyhedral versus aggregate customization" condition. Notice that the average predictive ability varies between panels, with predictive ability significantly lower in the U.S. panels than in the Australian or New Zealand panels. While tempting, we cannot attribute these differences to across-country variation. Our panels were chosen from opt-in organizations of leading-edge wine users. These organizations might vary on other characteristics besides country of origin. Nonetheless, a future investigation of across-country differences in response quality would be interesting.</p><p>We examine significance with a repeated measures ANOVA on each experimental cell, with one between-subject factor, panel (four levels); and two within-subject factors, estimation method (four levels) and question-selection method (two levels). <ref type="bibr">16</ref> Deterministic polyhedral question selection predicts significantly better than orthogonal question selection: across panels (p &lt; 0 07) and within the Australian</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting competitive response to a major policy change: Combining gametheoretic and empirical analyses</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Ailawadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Kopalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Neslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="24" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incorporating prior knowledge into the analysis of conjoint studies</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="1995-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving parameter estimates and model prediction by aggregate customization in choice experiments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="273" to="283" />
			<date type="published" when="2001-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimizing product line designs: Efficient methods and comparisons</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci. Forthcoming</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Screwcap wine seals: Has Kiwi ingenuity gone too far? Wine of the Week</title>
		<author>
			<persName><forename type="first">S</forename><surname>Courtney</surname></persName>
		</author>
		<ptr target="http://www.wineoftheweek.com/screwcaps/history.html" />
		<imprint>
			<date type="published" when="2001-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalized robust conjoint estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boussious</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zacharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The impact of utility balance and endogeneity in conjoint analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="507" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The importance of utility balance in efficient choice designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zwerina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="307" to="317" />
			<date type="published" when="1996-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal design for multinomial choice experiments</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Kanninen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="214" to="227" />
			<date type="published" when="2002-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic models incorporating individual heterogeneity: Utility evolution in conjoint analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Liechty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K H</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Desarbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="293" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating endogeneity bias in marketing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="640" to="648" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segmentation by involvement or nationality for global retailing: A cross-national comparative study of wine shopping behavior</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lockshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Quester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Spawton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Wine Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="223" to="236" />
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian statistics and marketing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="328" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimizing the marketing interventions mix in intermediate-term CRM</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Verhoef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="489" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Introduction to Stochastic Search and Optimization Estimation Simulation and Control</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Spall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>Hoboken, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Polyhedral methods for adaptive choice-based conjoint analysis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Simester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2004-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast polyhedral adaptive conjoint estimation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="303" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
