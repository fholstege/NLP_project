<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Active Machine Learning for Consideration Heuristics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-07-15">July 15, 2011.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daria</forename><surname>Dzyabura</surname></persName>
							<email>dariasil@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Sloan School of Management</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">MIT Sloan School of Management</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
							<email>hauser@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Sloan School of Management</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">MIT Sloan School of Management</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Active Machine Learning for Consideration Heuristics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-07-15">July 15, 2011.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.1110.0660</idno>
					<note type="submission">Received: January 6, 2010; accepted: May 4, 2011; Eric Bradlow and then Preyas Desai served as the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>active learning</term>
					<term>adaptive questions</term>
					<term>belief propagation</term>
					<term>conjunctive models</term>
					<term>consideration sets</term>
					<term>consumer heuristics</term>
					<term>decision heuristics</term>
					<term>disjunctions of conjunctions</term>
					<term>lexicographic models</term>
					<term>variational Bayes estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Problem Statement: Adaptive</head><p>Questions to Identify Heuristic Decision Rules</p><p>We develop and test an active-machine-learning algorithm to identify heuristic decision rules. Specifically, we select questions adaptively based on prior beliefs and respondents' answers to previous questions. To the best of our knowledge, this is the first (nearoptimal) adaptive-question method focused on consumers' noncompensatory decision heuristics. Extant adaptive methods focus on compensatory decision rules and are unlikely to explore the space of noncompensatory decision rules efficiently (e.g., <ref type="bibr" target="#b8">Evgeniou et al. 2005;</ref><ref type="bibr" target="#b43">Toubia et al. , 2004</ref><ref type="bibr">Sawtooth 1996)</ref>. In prior noncompensatory applications, question selection was almost always based on either random profiles or profiles chosen from an orthogonal design. We focus on noncompensatory heuristics because of managerial and scientific interest. Scientific interest is well established. Experimental and revealeddecision-rule studies suggest that noncompensatory heuristics are common, if not dominant, when consumers face decisions involving many alternatives, many features, or if they are making consideration rather than purchase decisions (e.g., <ref type="bibr" target="#b12">Gigerenzer and Goldstein 1996;</ref><ref type="bibr" target="#b33">Payne et al. 1988</ref><ref type="bibr" target="#b34">Payne et al. , 1993</ref><ref type="bibr" target="#b50">Yee et al. 2007</ref>). Heuristic rules often represent a rational tradeoff among decision costs and benefits and may be more robust under typical decision environments (e.g., <ref type="bibr" target="#b14">Gigerenzer and Todd 1999)</ref>. Managerial interest is growing as more firms focus product development and marketing efforts on getting consumers to consider their products or, equivalently, preventing consumers from rejecting products without evaluation. We provide illustrative examples in this paper, but published managerial examples include Japanese banks, global positioning systems, desktop computers, smart phones, and cellular phones <ref type="bibr" target="#b6">(Ding et al. 2011</ref><ref type="bibr" target="#b27">, Liberali et al. 2011</ref>).</p><p>Our focus is on adaptive question selection, but to select questions adaptively, we need intermediate estimates after each answer and before the next question is asked. To avoid excessive delays in online questionnaires, intermediate estimates must be obtained in a second or less (e.g., <ref type="bibr" target="#b43">Toubia et al. 2004)</ref>. This is a difficult challenge when optimizing questions for noncompensatory heuristics because we must search over a discrete space of the order of 2 N decision rules, where N is the number of feature levels (called aspects, as in <ref type="bibr" target="#b44">Tversky 1972</ref>). Without special Dzyabura and Hauser: Active Machine Learning for Consideration Heuristics 802 Marketing Science 30(5), pp. 801-819, © 2011 INFORMS structure, finding a best-fitting heuristic is much more difficult than finding best-fitting parameters for an (additive) compensatory model-such estimation algorithms typically require the order of N parameters. The ability to scale to large N is important in practice because consideration heuristics are common in product categories with large numbers of aspects (e.g., <ref type="bibr" target="#b34">Payne et al. 1993)</ref>. Our empirical application searches over 9 0 × 10 15 heuristic rules.</p><p>We propose an active-machine-learning solution (hereafter, active learning) to select questions adaptively to estimate noncompensatory heuristics. The active-learning algorithm approximates the posterior with a variational distribution and uses belief propagation to update the posterior distribution. It then asks the next question to minimize expected posterior entropy by anticipating the potential responses (in this case, to consider or not consider). The algorithm runs sufficiently fast to be implemented between questions in an online questionnaire.</p><p>In the absence of error, this algorithm comes extremely close to the theoretical limit of the information that can be obtained from binary responses. With response errors modeled, the algorithm does substantially and significantly better than extant questionselection methods. We also address looking ahead S steps, generalized heuristics, and the use of population data to improve priors. Synthetic data suggest that the proposed method recovers parameters with fewer questions than extant methods. Empirically, adaptive-question selection is significantly better at predicting future consideration than benchmark question selection. Noncompensatory estimation is also significantly better than the most commonly applied compensatory method.</p><p>We begin with a brief review and taxonomy of existing methods to select questions to identify consumer decision rules. We then review noncompensatory heuristics and motivate their managerial importance. Next, we present the algorithm, test parameter recovery with synthetic data, and describe an empirical illustration in the automobile market. We close with generalizations and managerial implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Existing Methods for Question Selection to Reveal Consumer Decision Rules</head><p>Marketing has a long tradition of methods to measure consumer decision rules. Figure <ref type="figure">1</ref> attempts a taxonomy that highlights the major trends and provides examples.</p><p>The vast majority of papers focus on compensatory decision rules. The most common methods include either self-explication, which asks respondents to selfstate their decision rules, or conjoint analysis, which infers compensatory decision rules from questions in which respondents choose, rank, or rate bundles of aspects called product profiles. These methods are applied widely and have demonstrated both predictive accuracy and managerial relevance (e.g., <ref type="bibr" target="#b17">Green 1984</ref><ref type="bibr" target="#b18">, Green and Srinivasan 1990</ref><ref type="bibr" target="#b48">, Wilkie and Pessemier 1973</ref>. In early applications, profiles were chosen from either full-factorial, fractional-factorial, or orthogonal designs, but as hierarchical-Bayes estimation became popular, many researchers moved to random designs to explore interactions better. For choice-based conjoint analysis, efficient designs are a function of the parameters of compensatory decision rules, and researchers developed "aggregate customization" to preselect questions using data from prestudies (e.g., <ref type="bibr" target="#b0">Arora and Huber 2001)</ref>. More recently, faced with impatient online respondents, researchers developed algorithms for adaptive conjoint questions based on compensatory models (e.g., <ref type="bibr" target="#b43">Toubia et al. 2004</ref>). After data are collected adaptively, the likelihood principle enables the data to be reanalyzed with models using classical statistics, Bayesian statistics, or machine learning.</p><p>In some applications respondents are asked to selfstate noncompensatory heuristics. Self-explication has had mixed success because respondents often chose profiles with aspects they had previously stated as unacceptable (e.g., <ref type="bibr" target="#b19">Green et al. 1988)</ref>. Recent experiments with incentive-compatible tasks, such as having respondents write an e-mail to a friend who will act as their agent, are promising <ref type="bibr" target="#b6">(Ding et al. 2011)</ref>.</p><p>Researchers have begun to propose methods to identify heuristic decision rules from directly measured consideration of product profiles. Finding the best-fit decision rule requires solving a discrete optimization problem that is NP-hard (e.g., <ref type="bibr" target="#b32">Martignon and Hoffrage 2002)</ref>. Existing estimation uses machinelearning methods such as greedy heuristics, greedoid dynamic programs, logical analysis of data, or linear programming perturbation <ref type="bibr" target="#b5">(Dieckmann et al. 2009</ref><ref type="bibr" target="#b22">, Hauser et al. 2010</ref><ref type="bibr" target="#b24">, Kohli and Jedidi 2007</ref><ref type="bibr" target="#b50">, Yee et al. 2007</ref>. Even for approximate solutions, runtimes are exponential in the number of aspects limiting methods to moderate numbers of aspects. Bayesian methods have been used to estimate parameters for moderate numbers of aspects (e.g., <ref type="bibr">Allenby 2004, 2006;</ref><ref type="bibr" target="#b22">Hauser et al. 2010;</ref><ref type="bibr" target="#b31">Liu and Arora 2011)</ref>. To date, profiles for direct consideration measures are chosen randomly or from an orthogonal design.</p><p>Within this taxonomy Figure <ref type="figure">1</ref> illustrates the focus of this paper (thick box)-adaptive questions for noncompensatory heuristics. We also develop an estimation method for noncompensatory heuristics that scales to large numbers of aspects, even when applied to extant question-selection methods (dotted box).</p><p>We focus on questions that ask about consideration directly (consider or not). However, our methods apply to all data in which the consumer responds with a yes or no answer and might be extendable to choice-based data where more than one profile is shown at a time. We do not focus on methods where consideration is an unobserved construct inferred from choice data (e.g., <ref type="bibr" target="#b7">Erdem and</ref><ref type="bibr">Swait 2004, van Nierop et al. 2010</ref>). There is one related adaptive method-the first stage of adaptive choice-based conjoint analysis (ACBC; Sawtooth 2008) that is based on rules of thumb to select approximately 28 profiles that are variations on a "bring-your-own" profile. Profiles are not chosen optimally, and noncompensatory heuristics are not estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Noncompensatory Decision Heuristics</head><p>We classify decision heuristics as simple and more complex. The simple heuristics include conjunctive, disjunctive, lexicographic, take-the-best, and elimination by aspects. The more complex heuristics include subset conjunctive and disjunctions of conjunctions.</p><p>The vast majority of scientific experiments have examined the simple heuristics, with conjunctive the most common (e.g., <ref type="bibr">Gigerenzer and Selten 1999;</ref><ref type="bibr">Payne et al. 1988, 1993, and</ref><ref type="bibr">references therein)</ref>. The study of more complex heuristics, which nest the simple heuristics, is relatively recent, but there is evidence that some consumers use the more complex forms <ref type="bibr">Kohli 2005, Hauser et al. 2010)</ref>. Both simple and complex heuristics apply for consideration, choice, or other decisions and for a wide variety of product categories. For simplicity of exposition, we define the heuristics with respect to the consideration decision and illustrate the heuristics for automotive features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Simple Heuristics</head><p>3.1.1. Conjunctive Heuristic. For some features consumers require acceptable ("must-have") levels. For example, a consumer might only consider a sedan body type and only consider Toyota, Nissan, or Honda. Technically, for features not in the conjunction, such as engine type, all levels are acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>804</head><p>Marketing Science 30(5), pp. 801-819, © 2011 INFORMS 3.1.2. Disjunctive Heuristic. If the product has "excitement" levels of a feature, the product is considered no matter what the levels of the other features are. For example, a consumer might consider all vehicles with a hybrid engine.</p><p>3.1.3. Take-the-Best. The consumer ranks products on a single most diagnostic feature and considers only those above some cutoff. For example, the consumer may find "brand" most diagnostic, rank products on brand, and consider only those with brands that are acceptable-say, Toyota, Nissan, and Honda.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Lexicographic (by Features)</head><p>. This heuristic is similar to take-the-best except the feature need not be most diagnostic. If products are tied on a feature level, then the consumer continues examining features lower in the lexico ordering until ties are broken. For example, the consumer might rank on brand, then body style considering only Toyota, Nissan, and Honda, and, among those brands, only sedans.</p><p>3.1.5. Elimination by Aspects. The consumer selects an aspect and eliminates all products with unacceptable levels, and then he or she selects another aspect and eliminates products with unacceptable levels on that aspect, continuing until only considered products are left. For example, the consumer may eliminate all but Toyota, Nissan, and Honda and all but sedans. Researchers have also examined acceptance by aspects and lexicographic by aspects that generalize elimination by aspects in the obvious ways.</p><p>When the only data are consider versus not consider, it does not matter in which order the profiles were eliminated or accepted. Take-the-best, lexicographic (by features), elimination by aspects, acceptance by aspects, and lexicographic by aspects are indistinguishable from conjunctive heuristics. The rules predict differently when respondents are asked to rank data and differ in the underlying cognitive process, but they do not differ when predicting the observed consideration set. Disjunctive is a mirror image of conjunctive. Thus, any question-selection algorithm that optimizes questions to identify conjunctive heuristics can be applied (perhaps with a mirror image) to any of the simple heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">More Complex Heuristics</head><p>3.2.1. Subset Conjunctive. The consumer considers a product if F features have levels that are acceptable. The consumer does not require all features to have acceptable levels. For example, the consumer might have acceptable brands (Toyota, Honda, Nissan), acceptable body types (sedan), and acceptable engines (hybrid) but only require that two of the three features have levels that are acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Disjunctions of Conjunctions.</head><p>The consumer might have two or more sets of acceptable aspects. For example, the consumer might consider [Toyota and Honda sedans] or [crossover body types with hybrid engines]. Disjunctions of conjunctions nests the subset conjunctive heuristic and all of the simple heuristics (for consideration). However, its generality is also a curse. Empirical applications require cognitive simplicity to avoid overfitting data.</p><p>All of these decision heuristics are postulated as descriptions of how consumers make decisions. Heuristics are not, and need not be, tied to utility maximization. For example, it is perfectly reasonable for a consumer to screen out low-priced products because the consumer believes that he or she is unlikely to choose such a product if considered and, hence, does not believe that evaluating such a product is worth the time and effort. (Put another way, the consumer would purchase a fantastic product at a low price if he or she knew about the product but never finds out about the product because the consumer chose not to evaluate low-priced products. When search costs are considered, it may be rational for the consumer not to search the lower-priced product because the probability of finding an acceptable low-priced product is too low.)</p><p>In this paper we illustrate our question-selection algorithm with conjunctive decision rules (hence it applies to all simple heuristics). We later extend the algorithm to identify disjunctions-of-conjunctions heuristics (which nest subset conjunctive heuristics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Managerial Relevance: Stylized Motivating Example</head><p>As a stylized example, suppose that automobiles can be described by four features with two levels each: Toyota or Chevy, sedan or crossover body type, hybrid or gasoline engine, and premium or basic trim levels, for a total of eight aspects. Suppose we are managing the Chevy brand that makes only sedans with gasoline engines and basic trim, and suppose it is easy to change trim levels but not the other features. If consumers are compensatory and their partworths are heterogeneous and not "too extreme," we can get some consumers to consider our vehicle by offering sufficiently premium trim levels. It might be profitable to do so. Suppose instead that a segment of consumers is conjunctive on [Toyota ∧ crossover]. (In our notation, ∧ is the logical "and"; ∨ is the logical "or.") No amount of trim levels will attract these conjunctive consumers. They will not pay attention to Chevy advertising, visit the GM website, or travel to a Chevy dealer-they will never evaluate any Chevy sedans no matter how much we improve them. In another example, if a segment of consumers is conjunctive on [crossover ∧ hybrid], we will never get those consumers to evaluate our vehicles unless we offer a hybrid crossover vehicle no matter how good we make our gasoline-engine sedan. Even with disjunctions of conjunctions, consumers who use [(sedan ∧ hybrid)∨(crossover∧gasoline engine)] will never consider our gasoline-engine sedan. In theory we might approximate noncompensatory heuristics with compensatory partworth decision rules (especially if we include interactions), but if there are many aspects, empirical approximations may not be accurate.</p><p>Many products just never make it because they are never considered; consumers never learn that the products have outstanding aspects that could compensate for the product's lack of a conjunctive feature. Our empirical illustration is based in the automotive industry. Managers at high levels in the sponsoring organization believe that conjunctive screening was a major reason that the automotive manufacturer faced slow sales relative to other manufacturers. For example, they had evidence that more than half of the consumers in the United States would not even consider their brands. Estimates of noncompensatory heuristics are now important inputs to product-design and marketing decisions at that automotive manufacturer.</p><p>Noncompensatory heuristics can imply different managerial decisions. <ref type="bibr" target="#b22">Hauser et al. (2010)</ref> illustrate how rebranding can improve the share of a common electronic device if consumers use compensatory models but not if consumers use noncompensatory models. <ref type="bibr" target="#b6">Ding et al. (2011)</ref> illustrate that conjunctive rules and compensatory rules are correlated in the sense that feature levels with higher average partworth values also appear in more "must-have rules." However, the noncompensatory models identify combinations of aspects that would not be considered even though their combined partworth values might be reasonable.</p><p>5. Question Types, Error Structure, and Notation 5.1. Question Types and Illustrative Example Figure <ref type="figure">2</ref> illustrates the basic question formats. The example is automobiles, but these types of questions have been used in a variety of product categoriesusually durable goods, where consideration is easy to define and a salient concept to consumers <ref type="bibr" target="#b4">(Dahan and Hauser 2002</ref><ref type="bibr">, Sawtooth 2008</ref><ref type="bibr" target="#b45">, Urban and Hauser 2004</ref>. Extensive pretests suggest that respondents can accurately "configure" a profile that they would consider (Figure <ref type="figure">2</ref>(a)). If respondents use only one conjunctive rule in their heuristic, they find it difficult to accurately configure a second profile. If they use a disjunctions-of-conjunctions heuristic with sufficiently distinct conjunctions, such as [(Toyota ∧ sedan) ∨ (Chevy ∧ truck)], we believe they can configure a second profile "that is different from previous profiles that you said you will consider." In this section we focus on the first configured profile and the corresponding conjunctive heuristic. In a later section, we address more conjunctions in a disjunctions-ofconjunctions heuristic.</p><p>After configuring a considered profile, we ask respondents whether or not they will consider various Marketing Science 30(5), pp. 801-819, © 2011 INFORMS profiles (Figure <ref type="figure">2(b)</ref>). Our goal is to select the profiles that provide the most information about decision heuristics (information is defined below). With synthetic data we plot cumulative information (parameter recovery) as a function of the number of questions. In our empirical test, we ask 29 queries, half of which are adaptive and half of which are chosen randomly (proportional to market share). We compare predictions based on the two types of questions. Although the number of questions was fixed in the empirical test, we address how stopping rules can be endogenous to the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Notation, Error Structure, and</head><p>Question-Selection Goal Let M be the number of features (e.g., brand, body style, engine type, trim level; M = 4), and let N be the total numbers of aspects (e.g., Toyota, Chevy, sedan, crossover, hybrid, gasoline engine, low trim, high trim; N = 8). Let i index consumers and j index aspects. For each conjunction, consumer i's decision rule is a vector, a i , of length N , with elements a ij such that a ij = 1 if aspect j is acceptable and a ij = −1 if it is not. For example, a i = +1 −1 +1 −1 +1 −1 +1 +1 would indicate that the ith consumer finds hybrid Toyota sedans with both low and high trim to be acceptable.</p><p>Each sequential query (Figure <ref type="figure">2</ref>(b)), indexed by k, is a profile, x ik , with N elements, x ijk , such that x ijk = 1 if i's profile k has aspect j and x ijk = 0 if it does not. Each x ik has exactly M nonzero elements, one for each feature. (In our stylized example, a profile contains one brand, one body type, one engine type, and one trim level.) For example, x ik = 1 0 1 0 1 0 1 0 would be a hybrid Toyota sedan with low trim.</p><p>Let X iK be the matrix of the first K profiles given to a consumer; each row corresponds to a profile. Mathematically, profile x ik satisfies a conjunctive rule a i if whenever x ijk = 1, then a ij = 1, such that every aspect of the profile is acceptable. In our eight-aspect example, consumer i finds the hybrid Toyota sedan with low trim to be acceptable (compare a i to x ik . This condition can be expressed as min j x ijk a ij ≥ 0. It is violated only if a profile has at least one level (x ijk = 1 that is unacceptable (a ij = −1 . Following <ref type="bibr" target="#b15">Gilbride and Allenby (2004)</ref>, we define a function to indicate when a profile is acceptable: I x ik a i = 1 if min j x ijk a ij ≥ 0, and I x ik a i = 0 otherwise. We use the same coding for disjunctive rules but modify the definition of I x ik a i to use max j rather than min j .</p><p>Let y ik be consumer i's answer to the kth query, where y ik = 1 if the consumer says "consider" and y ik = 0 otherwise. Let y iK be the vector of the first K answers. If there were no response errors, we would observe y ik = 1 if and only if I x ik a i = 1. However, empirically, we expect response errors. Because the algorithm must run rapidly between queries, we choose a simple form for response error. Specifically, we assume that a consumer gives a false-positive answer with probability 1 and a false-negative answer with probability 2 . For example, the ith consumer will say "consider (y ik = 1 " with probability 1 − 2 whenever the indicator function implies "consider," but he or she will also say "consider" with probability 1 if the indicator function implies "not consider." This error structure implies the following data-generating model:</p><formula xml:id="formula_0">Pr y ik = 1 x ik a i = 1− 2 I x ik a i + 1 1−I x ik a i Pr y ik = 0 x ik a i = 2 I x ik a i + 1− 1 1−I x ik a i (1)</formula><p>Each new query x i K+1 is based on our posterior beliefs about the decision rules ( a i . After the Kth query, we compute the posterior Pr a i X iK y iK conditioned on the first K queries (X iK , the first K answers ( y iK , and the priors. (Posterior beliefs might also reflect information from other respondents; see §6.6.) We seek to select the x ik s to get as much information as feasible about a i or, equivalently, to reduce uncertainty about a i by the greatest amount. In §6.2 we define "information" and describe how we optimize it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Error Magnitudes Are Set Prior to</head><p>Data Collection We cannot know the error magnitudes until after data are collected, but we must set the s in order to collect data. Setting the s is analogous to setting "accuracy" parameters in aggregate customization. We address this conundrum in two ways: (1) We treat the s as "tuning" parameters and explore the sensitivity to these tuning parameters with synthetic data. Setting tuning parameters is common in machine-learning query selection. (2) For the empirical test, we rely on managerial judgment <ref type="bibr">(Little 2004a, b)</ref>. Because the tuning parameters are set by managerial judgment prior to data collection, our empirical test is conservative in the sense that predictions might improve if future research allows updating of the error magnitudes within or across respondents.</p><p>To aid intuition we motivate the s with an illustrative microanalysis of our stylized example. Suppose that a respondent's conjunctive heuristic is <ref type="bibr">[Toyota ∧ crossover]</ref>. This respondent should find a crossover Toyota acceptable and not care about the engine and trim. Coding each aspect as acceptable or not, and preserving the order Toyota, Chevy, sedan, crossover, hybrid, gasoline, premium trim, and basic trim, this heuristic becomes a i = +1 −1 −1 +1 +1 +1 +1 +1 . Suppose that when this respondent makes a consideration decision, he or she makes errors with probability on each aspect, where an error involves flipping that aspect's acceptability. For example, suppose he or she is shown a Toyota crossover with a hybrid engine and premium trim; that is, x ik = 1 0 0 1 1 0 1 0 . He or she matches the heuristic to the profile aspect by aspect, making errors with probability for each acceptable aspect in the profile; e.g., Toyota is acceptable per the heuristic but may be mistaken for unacceptable with probability . The respondent can make a false-negative error if any of the four aspects in the profile are mistaken for unacceptable ones. If these errors occur independently, the respondent will make a false-negative error with probability 2 = 1 − 1 − 4 . If a profile is unacceptable, say, x ik = 0 1 1 0 1 0 1 0 , we easily compute 1 = 2 1− 2 .</p><p>In this illustration, any prior belief on the distribution of the heuristics and profiles implies expected s as a function of the s. Whether one specifies the s and derives expected s or specifies the s directly depends on the researchers and managers, but in either case, the tuning parameters are specified prior to data collection. With synthetic data we found no indication that one specification is preferred to the other. Empirically, we found it easier to think about the s directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Adaptive Question Selection</head><p>To select questions adaptively, we must address the following procedure:</p><p>Step 1. Initialize beliefs by generating consumerspecific priors.</p><p>Step 2. Select the next query based on current posterior beliefs.</p><p>Step 3. Update posterior beliefs from the priors and the responses to all the previous questions.</p><p>Step 4. Continue looping Steps 2 and 3 until Q questions are asked (or until another stopping rule is reached).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Initialize Consumer-Specific Beliefs (Step 1)</head><p>Hauser and Wernerfelt (1990, p. 393) provide examples where self-stated consideration set sizes are one-tenth or less of the number of brands on the market. Our experience suggests these examples are typical. If the question-selection algorithm used noninformative priors, the initial queries would be close to random guesses, most of which would not be considered by the consumer. When a consumer considers a profile, we learn (subject to the errors) that all of its aspects are acceptable; when a consumer rejects a profile, we learn only that one or more aspects are unacceptable. Therefore, the first considered profile provides substantial information and a significant shift in beliefs. Without observing the first considered profile directly, queries are not efficient, particularly with large numbers of aspects (N . To address this issue, we ask each respondent to configure a considered profile and, hence, gain substantial information.</p><p>Prior research using compensatory rules (e.g., <ref type="bibr" target="#b43">Toubia et al. 2004</ref>) suggests that adaptive questions are most efficient relative to random or orthogonal questions when consumers' heuristic decision rules are heterogeneous. We expect similar results for noncompensatory heuristics. In the presence of heterogeneity, the initial configured profile enables us to tailor prior beliefs to each respondent.</p><p>For example, in our empirical application we tailor prior beliefs using the co-occurrence of brands in consideration sets. Such data are readily available in the automotive industry and for frequently purchased consumer goods. Alternatively, prior beliefs might be updated on the fly using a collaborative filter on prior respondents (see §6.6). Without loss of generality, let j = 1 index the brand aspect the respondent configures, and for other brand aspects, let b 1j be the prior probability that brand j is acceptable when brand 1 is acceptable. Let x i1 be the configured profile, and set y i1 = 1. When co-occurrence data are available, prior beliefs on the marginal probabilities are set such that Pr a i1 = 1 x i1 y i1 = 1 and Pr a ij = 1 x i1 y i1 priors = b ij for j = 1.</p><p>Even without co-occurrence data, we can set respondent-specific priors for every aspect on which we have strong prior beliefs. We use weakly informative priors for all other aspects. When managers have priors across features (e.g., considered hybrids are more likely to be Toyotas), we also incorporate those priors <ref type="bibr">(Little 2004a, b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Select the Next Question Based on Posterior</head><p>Beliefs from Prior Answers (Step 2) The respondent's answer to the configurator provides the first of a series of estimates of his or her decision rule, p ij1 = Pr a ij = 1 X i1 = x i1 y i1 for all aspects j. (We have suppressed the notation for "priors.") We update these probabilities by iterating through Steps 2 and 3, computing updated estimates after each question-answer pair using all data collected up to and including that the Kth question, p ijK = Pr a ij = 1 X iK y iK for K &gt; 1. (Details are in Step 3; see §6.3.) To select the K + 1st query (Step 2), assume we have computed posterior values (p ijK from prior queries (up to K and that we can compute contingent values (p ij K+1 one step ahead for any potential new query ( x i K+1 and its corresponding answer (y i K+1 . We seek those questions that tell us as much as feasible about the respondent's decision heuristic. Equivalently, we seek to reduce uncertainty about a i by the greatest amount.</p><p>Following <ref type="bibr" target="#b28">Lindley (1956)</ref> we define the most informative question as the query that minimizes a loss Marketing Science 30(5), pp. 801-819, © 2011 INFORMS function. In this paper, we use Shannon's entropy as the uncertainty measure <ref type="bibr" target="#b40">(Shannon 1948)</ref>, but other measures of uncertainty could be used without otherwise changing the algorithm. Shannon's entropy, measured in bits, quantifies the amount of information that is missing because the value of a random variable is not known for certain. Higher entropy corresponds to more uncertainty. Zero entropy corresponds to perfect knowledge. Shannon's entropy (hereafter, entropy) is used widely in machine learning, has proven robust in many situations, and is the basis of criteria used to evaluate parameter recovery and predictive ability (U 2 and Kullback-Leibler 1951 divergence). We leave to future implementations other loss functions such as <ref type="bibr" target="#b35">Rényi (1961)</ref> entropy, suprisals, and other measures of information. 1 Mathematically,</p><formula xml:id="formula_1">H a i = N j=1 − p ijK log 2 p ijK + 1 − p ijK log 2 1 − p ijK (2)</formula><p>If some aspects are more important to managerial strategy, we use a weighted sum in Equation (2).</p><p>To select the K + 1st query, x i K+1 , we enumerate candidate queries, anticipating the answer to the question, y i K+1 , and anticipating how that answer updates our posterior beliefs about the respondent's heuristic. Using the p ijK s we compute the probability the respondent will consider the profile, q i K+1 x i K+1 = Pr y i K+1 = 1 X iK y iK x i K+1 . Using the Step 3 algorithm (described in the next subsection), we update the posterior p ij K+1 s for all potential queries and answers. Let p + ij K+1 x i K+1 = Pr a ij = 1 X iK y iK x i K+1 y i K+1 = 1 be the posterior beliefs if we ask profile x i K+1 and the respondent considers it. Let p − ij K+1 x i K+1 = Pr a ij = 1 X iK y iK x i K+1 y i K+1 = −1 be the posterior beliefs if the respondent does not consider the profile. Then the expected posterior entropy is</p><formula xml:id="formula_2">E H ai x i K+1 X iK Y iK = −q i K+1 x i K+1 j p + ij K+1 x i K+1 log 2 p + ij K+1 x i K+1 + 1−p + ij K+1 x i K+1 log 2 1−p + ij K+1 x i K+1 − 1−q i K+1 x i K+1 j p − ij K+1 x i K+1 log 2 p − ij K+1 x i K+1 + 1−p − ij K+1 x i K+1 log 2 1−p − ij K+1 x i K+1<label>(3)</label></formula><p>When the number of feasible profiles is moderate, we compute Equation (3) for every profile and choose the profile that minimizes Equation (3). However, in large designs such as the 53-aspect design in our empirical example, the number of potential queries 357 210 can be quite large. Because this large number of computations cannot be completed in less than a second, we focus our search using uncertainty sampling (e.g., <ref type="bibr" target="#b26">Lewis and Gale 1994)</ref>. Specifically, we evaluate Equation (3) for the T queries about which we are most uncertain. "Most uncertain" is defined as q i K+1 x i K+1 ≈ 0 5. Profiles identified from among the T most uncertain profiles are approximately optimal and, in some cases, optimal (e.g., see Appendix A). Uncertainty sampling is similar to choice balance as used in both polyhedral methods and aggregate customization (e.g., <ref type="bibr">Huber 2001, Toubia et al. 2004)</ref>. Synthetic data tests demonstrate that with T sufficiently large, we achieve close-to-optimal expected posterior entropy. For our empirical application, setting T = 1 000 kept question selection under a second. As computing speeds improve, researchers can use a larger T .</p><p>Equation ( <ref type="formula" target="#formula_2">3</ref>) is myopic because it computes expected posterior entropy one step ahead. Extending the algorithm S steps ahead is feasible for small N . However S-step computations are exponential in S. For example, if there are 256 potential queries, a twostep ahead algorithm requires that we evaluate 256 2 = 65 536 potential queries (without further approximations). Fortunately, synthetic data experiments suggest that one-step ahead computations achieve close to the theoretical maximum information of one bit per query (when there are no response errors) and do quite well when there are response errors. For completeness we coded a two-step-ahead algorithm in the case of 256 potential queries. Even for modest problems, its running time was excessive (over 13 minutes between questions); it provided negligible improvements in parameter recovery. Our empirical application has over a thousand times as many potential queries-a two-step-ahead algorithm was not feasible computationally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Update Beliefs About Heuristic Rules Based</head><p>on Answers to the K Questions (</p><p>Step 3) In Step 3 we use Bayes theorem to update our beliefs after the Kth query:</p><formula xml:id="formula_3">Pr a i X iK y iK ∝ Pr y iK x iK a i = a Pr a = a i X i K−1 y i K−1 (4)</formula><p>The likelihood term, Pr y iK x iK a i = a , comes from the data-generating model in Equation ( <ref type="formula">1</ref>). The variable of interest, a i , is defined over all binary vectors of length N . Because the number of potential conjunctions is exponential in N , updating is not computationally feasible without further structure on the distribution of conjunctions. For example, with N = 53 in our empirical example, we would need to update the distribution for 9 0 × 10 15 potential conjunctions.</p><p>To gain insight for a feasible algorithm, we examine solutions to related problems. <ref type="bibr" target="#b15">Gilbride and Allenby (2004)</ref> use a "Griddy-Gibbs" algorithm to sample threshold levels for features. At the consumer level, the thresholds are drawn from a multinomial distribution. The Griddy-Gibbs uses a grid approximation to the (often univariate) conditional posterior. We cannot modify their solution directly, in part because most of our features are horizontal (e.g., brand) and thresholds do not apply. Even for vertical features, such as price, we want to allow non-threshold heuristics. We need algorithms that let us classify each level as acceptable or not.</p><p>For a feasible algorithm, we use a variational Bayes approach. In variational Bayes inference, a complex posterior distribution is approximated with a variational distribution chosen from a family of distributions judged similar to the true posterior distribution. Ideally, the variational family can be evaluated quickly <ref type="bibr">(Attias 1999, Ghahramani and</ref><ref type="bibr" target="#b10">Beal 2000)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Even with an uncertainty-sampling approximation in</head><p>Step 2, we must compute posterior distributions for 2T question-answer combinations and do so while the respondent waits for the next question.</p><p>As our variational distribution, we approximate the distribution of a i with N independent binomial distributions. This variational distribution has N parameters, the p ij s, rather than parameters for the 2 N potential values of a i . Because this variational approximation is within a consumer, we place no restriction on the empirical population distribution of the a ij s. Intercorrelation at the population level is likely (and allowed) among aspect probabilities. For example, we might find that those automotive consumers who screen on Toyota also screen on hybrid engines. In another application we might find that those cellular phone consumers who screen on Nokia also screen on "flip." For every respondent the posterior values of all p ijK s depend on all of the data from that respondent, not just queries that involve the jth aspect.</p><p>To calculate posteriors for the variational distribution, we use a version of belief propagation <ref type="bibr">(Yedidia et al. 2003, Ghahramani and</ref><ref type="bibr" target="#b11">Beal 2001)</ref>. The algorithm converges iteratively to an estimate of p iK . The hth iteration uses Bayes theorem to update each p h ijK based on the data and based on p h ij K for all j = j. Within the hth iteration, the algorithm loops over aspects and queries using the data-generating model (Equation (1)) to compute the likelihood of observing y k = 1 conditioned on the likelihood for k = k. It continues until the estimates of the p h ijK s stabilize. In our experience, the algorithm converges quickly: 95.6% of the estimations converge in 20 or fewer iterations, 99.2% in 40 or fewer iterations, and 99.7% in 60 or fewer iterations. Appendix B provides the pseudo-code.</p><p>Although variational distributions work well in a variety of applications, there is no guarantee for our application. Performance is an empirical question that we address in § §7 and 8. Finally, we note that the belief propagation algorithm and Equation (4) appear to be explicitly dependent only on the questions that are answered by consumer i. However, our notation has suppressed the dependence on prior beliefs. It is a simple matter to make prior beliefs dependent on the distribution of the a i s, as estimated from previous respondents (see §6.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Stopping Rules (Step 4)</head><p>Adaptive-question selection algorithms for compensatory decision rules and fixed question-selection algorithms for compensatory or noncompensatory rules rely on a target number of questions chosen by prior experience or judgment. Such a stopping rule can be used with the adaptive question-selection algorithm proposed in this paper. For example, we stopped after Q = 29 questions in our empirical illustration.</p><p>However, expected posterior entropy minimization makes it feasible to select a stopping rule endogenously. One possibility is to stop questioning when the expected reduction in entropy drops below a threshold for two or more adaptive questions. Synthetic data provide some insight. In §7 we plot the information obtained about parameters as a function of the number of questions. In theory we might also gain insight from our empirical example. However, because our empirical example used only 29 questions for 53 aspects, for 99% of the respondents the adaptive-question selection algorithm would still have gained substantial information if the respondents had been asked a 30th question. We return to this issue in §11. Because we cannot redo our empirical example, we leave this and other stopping-rule extensions to future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Extension to Disjunctions of Conjunctions</head><p>Disjunctions-of-conjunctions heuristics nest both simple and complex heuristics. The extension to disjunctions of conjunctions is conceptually simple. After we reach a stopping rule, whether it be fixed a priori or endogenous, we simply restart the algorithm by asking a second configurator question but requiring an answer that is substantially different from the profiles that the respondent has already indicated he or she will consider. If the respondent cannot configure such a profile, we stop. Empirically, cognitive simplicity suggests that respondents use relatively few conjunctions (e.g., <ref type="bibr" target="#b12">Gigerenzer and Goldstein 1996</ref><ref type="bibr" target="#b22">, Hauser et al. 2010</ref><ref type="bibr" target="#b32">, Martignon and Hoffrage 2002</ref>. Most consumers use one conjunction <ref type="bibr" target="#b22">(Hauser et al. 2010</ref> Hence the number of questions should remain within reason. We test this procedure on synthetic data and, to the extent that our data allow, empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Using Data from Previous Respondents</head><p>We can use data from other respondents to improve priors for new respondents, but in doing so, we want to retain the advantage of consumer-specific priors. Collaborative filtering provides a feasible method (e.g., <ref type="bibr" target="#b2">Breese et al. 1998)</ref>. We base our collaborative filter on the consumer-specific data available from the configurator (Figure <ref type="figure">2</ref>(a)).</p><p>Specifically, after a new respondent completes the configurator, we use collaboratively filtered data from previous respondents who configured similar profiles. For example, if an automotive consumer configures a Chevy, we search for previous respondents who configured a Chevy. For other brands we compute priors with a weighted average of the brand posteriors (p ij s) from those respondents. (We weigh previous respondents by predictive precision.) We do this for all configured features. As sample sizes increase, population data overwhelm even "bad" priors; performance will converge to performance based on accurate priors (assuming the collaborative filter is effective). We test finite-sample properties on synthetic data and, empirically, with an approximation based on the data we collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Synthetic Data Experiments</head><p>To evaluate the ability of the active-learning algorithm to recover known heuristic decision rules, we use synthetic respondents. To compare adaptivequestion selection to established methods, we choose a synthetic decision task with sufficiently many aspects to challenge the algorithm but for which existing methods are feasible. With four features at four levels (16 aspects), there are 65,536 heuristic rulesa challenging problem for extant heuristic-rule estimation methods. An orthogonal design is 32 profiles and, hence, in the range of tasks in the empirical literature. We simulate respondents who answer any number of questions K ∈ 1 256 , where 256 profiles exhaust the feasible profiles. To evaluate the questionselection methods, we randomly select 1,000 heuristic rules (synthetic respondents). For each aspect we draw a Bernoulli probability from a Beta 1 1 distribution (uniform distribution) and draw a +1 or −1 using the Bernoulli probability. This "sample size" is on the high side of what we might expect in an empirical study and provides sufficient heterogeneity in heuristic rules.</p><p>For each decision heuristic, a i , we use either the proposed algorithm or an established method to select questions. The synthetic respondent then "answers" the questions using the decision heuristic, but with response errors 1 and 2 chosen as if generated by reasonable s. To compare question-selection methods, we keep the estimation method constant. We use the variational Bayes belief-propagation method developed in this paper. The benchmark questionselection methods are orthogonal, random, and market based. Market-based questions are chosen randomly but in proportion to profile shares we might expect in the market-market shares are known for synthetic data.</p><p>With synthetic data we know the parameters a ij . For any K and for all i and j, we use the "observed" synthetic data to update the probability p ijK that a ij = 1. An appropriate information-theoretic measure of parameter recovery is U 2 , which quantifies the percentage of uncertainty explained (empirical information/initial entropy; Hauser 1978); U 2 = 100% indicates perfect parameter recovery.</p><p>We begin with synthetic data that contain no response errors. These data quantify potential maximum gains with adaptive questions, test how rapidly active-learning questions recover parameters perfectly, and bound improvements that would be possible with nonmyopic S-step-ahead algorithms. We then repeat the experiments with error-laden synthetic data and with "bad" priors. Finally, we examine whether we can recover disjunctions-of-conjunctions heuristics and whether population-based priors improve predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Tests of Upper Bounds on Parameter</head><p>Recovery (No Response Errors) Figure <ref type="figure">3</ref> presents key results. To simplify interpretation we plot random queries in Appendix D, rather than Figure <ref type="figure">3</ref>, because the results are indistinguishable from market-based queries on the scale of Figure <ref type="figure">3</ref>. Market-based queries do approximately 3% better than random queries for the first 16 queries, approximately 1% better for the first 32 queries, and approximately 0.5% better for all 256 queries. (Queries 129-256 are not shown in Figure <ref type="figure">3</ref>; the randomquery and the market-based query curves asymptote to 100%.) Orthogonal-design questions are only defined for K = 32.</p><p>Questions selected adaptively by the active-learning algorithm find respondents' decision heuristics much more rapidly than existing question-selection methods. The adaptive questions come very close to an optimal reduction in posterior entropy. With 16 aspects and equally likely priors, the prior entropy is 16 log 2 (2), which is 16 bits. The configurator reveals four acceptable aspects (four bits). Each subsequent query is a binary outcome that can reveal at most one bit. A perfect active-learning algorithm would require 12 additional queries to identify a decision rule (4 bits + 12 bits identifies the 16 elements of a i . On average, in the absence of response error, the adaptive questions identify the respondent's decision heuristic in approximately 13 questions. The variational approximation and the one-step-ahead question-selection algorithm appear to achieve closeto-optimal information (12 bits in 13 questions).</p><p>We compare the relative improvement as a result of question-selection methods by holding information constant and examining how many questions it takes to achieve that level of parameter recovery. Because an orthogonal design is fixed at 32 questions, we use it as a benchmark. As illustrated in the first line of data in Table <ref type="table" target="#tab_3">1</ref>, an orthogonal design requires 32 queries to achieve a U 2 of approximately 76%. Market-based questions require 38 queries; random questions require 40 queries, and adaptive questions only nine queries. To parse the configurator from the adaptive questions, Appendix D plots the U 2 obtained with a configurator plus market-based questions. The plot parallels the plot of purely marketbased queries requiring 30 queries to achieve a U 2 of approximately 76%. In summary, in an errorless world, the active-learning algorithm chooses adaptive questions that provide substantially more information per question than existing nonadaptive methods. The large improvements in U 2 , even for small numbers of questions, suggests that adaptive questions are chosen to provide information efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Tests of Parameter Recovery When There Are</head><p>Response Errors or "Bad" Priors We now add either response error or "bad" priors and repeat the synthetic data experiments. The plots remain quasi-concave for a variety of levels of response error and/or bad priors. <ref type="bibr">2</ref> We report representative values in Table <ref type="table" target="#tab_3">1</ref>. (Table <ref type="table" target="#tab_3">1</ref> is based on false negatives occurring 5% of the time. False positives are set by the corresponding . Bad priors perturb "good" priors with bias drawn from U 0 0 1 .) Naturally, as we add errors or bad priors, the amount of information obtained per question decreases; for example, 13 adaptive questions achieved a U 2 of 100% without response errors but only 55.5% with response errors. On average, it takes 12.4 adaptive questions to obtain a U 2 of 50% (standard deviation 8.7). The last column of Table <ref type="table" target="#tab_3">1</ref> reports the information obtained by 32 orthogonal questions. Adaptive questions obtain relatively more information per question than existing methods under all scenarios. Indeed, adaptive questions appear to be more robust to bad priors than existing question-selection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Tests of the Ability to Recovery</head><p>Disjunctions-of-Conjunctions Heuristics We now generate synthetic data for respondents who have two distinct conjunctions rather than just one conjunction. By distinct, we mean no overlap in the conjunctions. We allow both question-selection methods to allocate one-half of their questions to the first conjunction and one-half to the second conjunction. To make the comparison fair, all question-selection methods use data from the two configurators when estimating the parameters of the disjunctions-ofconjunctions heuristics. After 32 questions (plus two configurators), estimates based on adaptive questions achieve a U 2 of 80.0%, whereas random questions achieve a U 2 of only 34.5%. Adaptive questions also beat market-based and orthogonal-design questions handily.</p><p>This is an important result. With random questions false positives from the second conjunction pollute the estimation of the parameters of the first conjunction, and vice versa. The active-learning algorithm focuses questions on one or the other conjunction to provide good recovery of the parameters of both conjunctions. We expect the two-conjunction results to extend readily to more than two conjunctions. Although this initial test is promising, future tests might improve the algorithm with endogenous stopping rules that allocate questions optimally among conjunctions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Tests of Incorporating Data from</head><p>Previous Respondents To demonstrate the value of incorporating data from other respondents, we split the sample of synthetic respondents into two halves. For the first half of the sample, we use bad priors, ask questions adaptively, and estimate the a i s. We use the estimated a i s and a collaborative filter on two features to customize priors for the remaining respondents. We then ask questions of the remaining respondents using Note. Number of questions in addition to the configurator question. a U 2 (percent uncertainty explained) when heuristics estimated from 32 orthogonal questions; U 2 for other question-selection methods is approximately the same subject to integer constraints on the number of questions.</p><p>collaborative-filter-based priors. On average, U 2 is 17.8% larger on the second set of respondents (using collaborative-filter-based priors) than on the first set of respondents (not using collaborative-filter-based priors). Thus, even when we use bad priors for early respondents, the posteriors from those respondents are sufficient for the collaborative filter. The collaborative-filter-based priors improve U 2 for the remaining respondents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Summary of Synthetic Data Experiments</head><p>The synthetic data experiments suggest that • adaptive question selection via active learning is feasible and can recover the parameters of known heuristic decision rules,</p><p>• adaptive question selection provides more information per question than existing methods,</p><p>• one-step-ahead active-learning adaptive questions achieve gains in information (reduction in entropy) that are close to the theoretical maximum when there are no response errors,</p><p>• adaptive question selection provides more information per question when there are response errors,</p><p>• adaptive question selection provides more information per question when there are badly chosen priors,</p><p>• it is feasible to extend adaptive-question selection to disjunctions-of-conjunctions heuristic decision rules, and</p><p>• incorporating data from other respondents improves parameter recovery. These synthetic data experiments establish that if respondents use heuristic decision rules, then the active-learning algorithm provides a means to ask questions that provide substantially more information per question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Illustrative Empirical Application with a Large Number of Aspects</head><p>In the spring of 2009, a large American automotive manufacturer (AAM) recognized that consideration of their vehicles was well below that of non-U.S. The bulk of AAM's survey explored various marketing strategies that AAM might use to enhance consideration of their brands. The managerial test of communications strategies is tangential to the scope and focus of this paper, but we illustrate in §10 the types of insight provided by estimating consumers' noncompensatory heuristics.</p><p>Because AAM's managerial decisions depended on the accuracy with which they could evaluate their communications strategies, we were given the opportunity to test adaptive-question selection for a subset of the respondents. A subset of 872 respondents was not shown any communications inductions. Instead, after configuring a profile, evaluating 29 calibration profiles, and completing a memory-cleansing task <ref type="bibr" target="#b9">(Frederick 2005)</ref>, respondents evaluated a second set of 29 validation profiles. (A 30th profile in calibration and validation was used for other research purposes by AAM.) The profiles varied on 53 aspects: brand (21 aspects), body style (9 aspects), price (7 aspects), engine power (3 aspects), engine type (2 aspects), fuel efficiency (5 aspects), quality (3 aspects), and crashtest safety (3 aspects).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Adaptive Question Selection for</head><p>Calibration Profiles To test adaptive question selection, one-half of the calibration profiles were chosen adaptively by the active-learning algorithm. The other half were chosen randomly in proportion to market share from the top 50 best-selling vehicles in the United States. To avoid order effects and to introduce variation in the data, the question-selection methods were randomized. This probabilistic variation means that the number of queries of each type is 14.5, on average, but varies by respondent.</p><p>As a benchmark we chose market-based queries rather than random queries. The market-based queries perform slightly better on synthetic data than purely random queries and, hence, provide a stronger test. We could not test an orthogonal design because 29 queries is but a small fraction of the 13,320 profiles in a 53-aspect orthogonal design. (A full factorial would require 357,210 profiles.) Furthermore, even if we were to complete an orthogonal design of 13,320 queries, Figure <ref type="figure">2</ref> suggests that orthogonal queries do only slightly better than random or market-based queries. Following <ref type="bibr" target="#b36">Sándor and Wedel (2002)</ref> and <ref type="bibr" target="#b47">Vriens et al. (2001)</ref>, we split the marketbased profiles (randomly) over respondents.</p><p>Besides enabling methodological comparisons, this mix of adaptive and market-based queries has practical advantages with human respondents. First, the market-based queries introduce variety to engage the respondent and help disguise the choice-balance nature of the active-learning algorithm. (Respondents get variety in the profiles they evaluate.) Second, market-based queries sample "far away" from the adaptive queries chosen by the active-learning algorithm. They might prevent the algorithm from getting stuck in a local maximum (an analogy to simulated annealing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Selecting Priors for the Empirical Application</head><p>AAM had co-occurrence data available from prior research, so we set priors as described in §6.1. In addition, using AAM's data and managerial beliefs, we were able to set priors on some pairwise conjunctions such as "Porsche ∧ Kia" and "Porsche ∧ pick-up." Rather than setting these priors directly as correlations among the a ij s, AAM's managers found it more intuitive to generate "pseudo-questions" in which the respondent was assumed to "not consider" a "Porsche ∧ pick-up" with probability q, where q was set by managerial judgment. In other applications researchers might set the priors directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Validation Profiles Used to Evaluate</head><p>Predictive Ability After a memory-cleansing task, respondents were shown a second set of 29 profiles, this time chosen by the market-based question-selection method. Because there was some overlap between the marketbased validation and the market-based calibration profiles, we have an indicator of respondent reliability. Respondents consistently evaluated market-based profiles 90.5% of the time. Respondents are consistent, but not perfect, and, thus, modeling response error (via the s) appears to be appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Performance Measures</head><p>Although hit rate is an intuitive measure, it can mislead intuition for consideration data. If a respondent were to consider 20% of both calibration and validation profiles, then a null model that predicts "reject all profiles" will achieve a hit rate of 80%. Such a null model, however, provides no information, has a large number of false-negative predictions, and predicts a consideration set size of 0. On the other hand, a null model that predicts randomly proportional to the consideration set size in the calibration data would predict a larger validation consideration set size and balance false positives and false negatives, but it would achieve a lower hit rate (68%: 0 68 = 0 8 2 + 0 2 2 . Nonetheless, for interested readers, Appendix E provides hit rates.</p><p>We expand evaluative criteria by examining falsepositive and false-negative predictions. A manager might put more (or less) weight on not missing considered profiles than on predicting as considered profiles that are not considered. However, without knowing specific loss functions to weigh false positives and false negatives differently, we cannot have a single managerial criterion (e.g., . Fortunately, information theory provides a commonly used measure that balances false positives and false negatives: the Kullback-Leibler divergence (KL). KL is a nonsymmetric measure of the difference from a prediction model to a comparison model <ref type="bibr" target="#b3">(Chaloner and</ref><ref type="bibr">Verdinelli 1995, Kullback and</ref><ref type="bibr" target="#b25">Leibler 1951)</ref>. It discriminates among models even when the hit rates might otherwise be equal. Appendix C provides formulae for the KL measure appropriate to the data in this paper. We calculate divergence from perfect prediction; hence a smaller KL is better.</p><p>In synthetic data we knew the "true" decision rule and could compare the estimated parameters a ij s to known parameters. U 2 was the appropriate measure. With empirical data we do not know the true decision rule; we only observe the respondents' judgments about consider versus not consider; hence KL is an appropriate measure. However, both attempt to quantify the information explained by the estimated parameters (decision heuristics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">Key Empirical Results</head><p>Table <ref type="table" target="#tab_5">2</ref> summarizes KL divergence for the two question-selection methods that we tested: adaptive questions and market-based questions. For each question type, we use two estimation methods: (1) the variational Bayes belief-propagation algorithm computes the posterior distribution of the noncompensatory heuristics, and (2) a hierarchical Bayes logit model (HB) computes the posterior distribution for a compensatory model. HB is the most used estimation method for additive utility models <ref type="bibr">(Sawtooth 2004)</ref>, and it has proven accurate for zero-versus-one consideration decisions <ref type="bibr" target="#b6">(Ding et al. 2011</ref><ref type="bibr" target="#b22">, Hauser et al. 2010</ref>.  a Significantly better than market-based questions for noncompensatory heuristics (p &lt; 0 001).</p><p>b Significantly better than compensatory decision model (p &lt; 0 001). c Significantly better than null models (p &lt; 0 001). d Significantly better than adaptive questions for compensatory decision model (p &lt; 0 001).</p><p>The latter authors provide a full HB specification in Appendix C. Both estimation methods are based only on the calibration data. For comparison Table <ref type="table" target="#tab_5">2</ref> also reports predictions for null models that predict all profiles as considered, predict no profiles as considered, and predict profiles randomly based on the consideration set size among the calibration profiles.</p><p>When the estimation method assumes respondents use heuristic decision rules, rules estimated from adaptive questions predict significantly better than rules estimated from market-based queries. (Hit rates are also significantly better.) Furthermore, for adaptive questions, heuristic rules predict significantly better than HB-estimated additive rules. Although HB-estimated additive models nest lexicographic models (and hence conjunctive models for consideration data), the required ratio of partworths is approximately 10 15 and not realistic empirically. More likely, HB does less well because its assumed additive model with 53 parameters overfits the data, even with shrinkage to the population mean.</p><p>It is perhaps surprising that ∼14.5 adaptive questions do so well for 53 aspects. This is an empirical issue, but we speculate that the underlying reasons are (1) consumers use cognitively simple heuristics with relatively few aspects, (2) the adaptive questions search the space of decision rules efficiently to confirm the cognitively simple rules, (3) the configurator focuses this search quickly, and (4) consumer-specific priors keep the search focused.</p><p>There is an interesting, but not surprising, interaction effect in Table <ref type="table" target="#tab_5">2</ref>. If the estimation assumes an additive model, noncompensatory-focused adaptive questions do not do as well as market-based questions. Also, consistent with prior research using nonadaptive questions (e.g., <ref type="bibr" target="#b5">Dieckmann et al. 2009</ref><ref type="bibr" target="#b24">, Kohli and Jedidi 2007</ref><ref type="bibr" target="#b50">, Yee et al. 2007</ref>), noncompensatory estimation is comparable to compensatory estimation using market-based questions. Perhaps to truly identify heuristics, we need heuristic-focused adaptive questions.</p><p>But are consumers compensatory or noncompensatory?</p><p>The adaptive-question-noncompensatoryestimation combination is significantly better than all other combinations in Table <ref type="table" target="#tab_5">2</ref>. But what if we estimated both noncompensatory and compensatory models using all 29 questions (combining ∼14.5 adaptive questions and ∼14.5 market-based questions)?</p><p>The noncompensatory model predicts significantly better than the compensatory model when all 29 questions are used (KL = 0 451 versus KL = 0 560, p &lt; 0 001 using a paired t-test). Differences are also significant at p &lt; 0 001 using a related-samples Wilcoxon signed-rank test. Because we may not know a priori whether the respondent is noncompensatory or compensatory, collecting data both ways gives us flexibility for post-data-collection reestimation. (In the automotive illustration, prior theory suggested that consumers were likely to use noncompensatory heuristics.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.">Summary of Empirical Illustration</head><p>Adaptive questions to identify noncompensatory heuristics are promising. We appear able to select questions to provide significantly more information per query than market-based queries. Furthermore, it appears that questions are chosen efficiently because we can predict well with ∼14.5 questions, even in a complex product category with 53 aspects. This is an indication of cognitive simplicity. Finally, consumers appear to be noncompensatory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Initial Tests of Generalizations: Disjunctions of Conjunctions and Population Priors</head><p>Although data were collected based on the conjunctive active-learning algorithm, we undertake exploratory empirical tests of two proposed generalizations: disjunctions of conjunctions and prior-respondent-based priors. These exploratory tests complement the theory in § §6.5 and 6.6 and the synthetic data tests in § §7.3 and 7.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Disjunctions of Conjunctions</head><p>AAM managers sought to focus on consumers' primary conjunctions because, in prior studies sponsored by AAM, 93% of the respondents used only one conjunction <ref type="bibr" target="#b22">(Hauser et al. 2010</ref>). However, we might gain additional predictive power by searching for second and subsequent conjunctions using the methods of §6.5. Ideally, this requires new data, but we get an indicator by ( <ref type="formula">1</ref>) estimating the best model with the data, (2) eliminating all calibration profiles that were correctly classified with the first conjunction, and (3) using the remaining market-based profiles to search for a second conjunction. As expected, this strategy reduced false negatives because there were more conjunctions. It came at the expense of a slight increase in false positives. Overall, using all 29 questions, KL increased slightly (0.459 versus 0.452, p &lt; 0 001), suggesting that the reestimation on incorrectly classified profiles overfit the data. Because the disjunctions of conjunctions (DOC) generalization works for synthetic data, a true test awaits new empirical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Previous-Respondent-Based Priors</head><p>The priors used to initialize consumer-specific beliefs were based on judgments by AAM managers and analysts; however, we might also use the methods proposed in §6.6 to improve priors based on data from other respondents. As a test, we used the basic algorithm to estimate the p ijK s, used the collaborative filter to reset the priors for each respondent, reestimated the model (p ijK s), and compared predicted consideration to observed consideration. Previousrespondent-based priors improved predictions but not significantly (0.448 versus 0.452, p = 0 082), suggesting that AAM provided good priors for this application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Managerial Use</head><p>The validation reported in this paper was part of a much larger effort by AAM to identify communications strategies that would encourage consumers to consider AAM vehicles. At the time of the study, AAM used the data on decision heuristics for product development. AAM recognized heterogeneity in heuristics and identified clusters of consumers who share decision heuristics. There were four main clusters: high selectivity on brand and body type, selectivity on brand, selectivity on body type, and (likely) compensatory. There were two to six subclusters within each main cluster, for a total of 20 clusters. <ref type="bibr">3</ref> Each subcluster was linked to demographic and other decision variables to suggest directed communications and product development strategies. Decision rules for targeted consumer segments are proprietary, but the population averages are not. Table <ref type="table" target="#tab_6">3</ref> indicates which percentage of the population uses elimination rules for each of the measured aspects.</p><p>Although some brands were eliminated by most consumers, larger manufacturers have many targeted brands. For example, Buick was eliminated by 97% of the consumers and Lincoln by 98%, but these are not the only GM and Ford brands. For AAM, the net consideration of its brands was within the range of more-aggregate studies. Consumers are mixed on their interest in "green" technology: 44% eliminate hybrids from consideration, but 69% also eliminate large engines. Price elimination illustrates that heuristics are screening criteria, not surrogates for utility: 77% of consumers will not investigate a $12,000 vehicle. This means that consumers' knowledge of the market tells them that, net of search costs, their best strategy is to avoid investing time and effort to evaluate $12,000 vehicles. It does not mean that consumers would not buy a top-of-the-line Lexus if it were offered for $12,000. Table <ref type="table" target="#tab_6">3</ref> provides aggregate summaries across many consumer segments-AAM's product development and communications strategies were targeted within segment. For example, 84% of consumers overall eliminate sports cars indicating the sports-car segment is a relatively small market. However, the remaining 16% of consumers constitute a market that is sufficiently large for AAM to target vehicles for that market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Summary and Challenges</head><p>We found active machine learning to be an effective methodology to select questions adaptively in order to identify consideration heuristics. Both the synthetic data experiments and the proof-of-concept empirical illustration are promising, but many challenges remain.</p><p>Question selection might be improved further with experience in choosing "tuning" parameters ( s T , improved priors, an improved focus on more-complex heuristics, and better variational Bayes belief-propagation approximations. In addition, further experience will provide insight on the information gained as the algorithm learns. For example, <ref type="bibr">3</ref> AAM used standard clustering methods on the posterior p ij s. By the likelihood principle, it is possible to use latent-structure models to reanalyze the data. Post hoc clustering is likely to lead to more clusters than latent-structure modeling. Comparisons of clustering methods are beyond the scope and tangential to our current focus on methods to select questions efficiently for the estimation of heuristic decision rules. We see that, on average, adaptive questions provide substantially more information per question (5.5 times as much). Prior to the 10th question, the increasingly accurate posterior probabilities enable the algorithm to ask increasingly more accurate questions. Beyond 10 questions the expected reduction in entropy decreases and continues to decrease through the 29th question. It is likely that AAM would have been better able to identify consumers' conjunctive decision rules had they used 58 questions for estimation rather than split the questions between calibration and validation. Research might explore the mix between adaptive and market-based questions.</p><p>The likelihood principle implies that other models can be tested on AAMs and other adaptive data. The variational Bayes belief-propagation algorithm does not estimate standard errors for the p ij s. Other Bayesian methods might specify more complex distributions. Reestimation or bootstrapping, when feasible, might improve estimation.</p><p>Active machine learning might also be extended to other data-collection formats, including formats in which multiple profiles are shown on the same page or formats in which configurators are used in creative ways. The challenge for large N is that we would like to approximate decision rules in less than N queries per respondent.</p><p>Use the priors to initialize p 0 iK . Initialize all Pr y ik X iK p h−1 iK −j a ij = ±1 . While max j p h ijK − p Pr y ik = 0 X iK p h−1 iK −j a ij = 1 = 1 − 1 1 −</p><formula xml:id="formula_4">x igk =1 g =j p h−1 igK + 2 x igk =1 g =j p h−1 igK Pr y ik = 0 X iK p h−1 iK −j a ij = −1 = 1 − 1 end loop k ∈ S − j Pr y iK X iK p h−1 iK −j a ij = 1 = K k=1</formula><p>Pr y ik X iK p h−1 iK −j a ij = 1 Pr y iK X iK p h−1 iK −j a ij = −1</p><formula xml:id="formula_5">= K k=1</formula><p>Pr y ik X iK p h−1 iK −j a ij = −1 [Compute data likelihoods across all K questions as a product of marginal distributions for each k.]</p><p>Pr a ij = 1 X iK y iK p h−1 iK −j ∝ Pr y iK X iK p h−1 iK −j a ij = 1 Pr a ij = 1 prior Pr a ij = −1 X iK y iK p h−1 iK −j ∝ Pr y iK X iK p h−1 iK −j a ij = −1 1−Pr a ij = 1 prior p h ijK = Pr a ij = 1 X iK y iK p h−1 iK −j normalized The Kullback-Leibler divergence (KL) is an information theory-based measure of the divergence from one probability distribution to another. In this paper we seek the divergence from the predicted consideration probabilities to those that are observed in the validation data, recognizing the discrete nature of the data (to consider or not). For respondent i we predict that profile k is considered with probability, r ik = Pr y ik = 1 x ik model . Then the divergence from the true model (the y ik s) to the model being tested (the r ik s) is given by Equation (C1). With log-based-2, KL has the units of bits:</p><formula xml:id="formula_6">KL = k∈validation y ik log 2 y ik r ik + 1 − y ik log 2 1 − y ik 1 − r ik (C1)</formula><p>When the r ik s are themselves discrete, we must use the observations of false-positive and false-negative predictions to separate the summation into four components. Let V = the number of profiles in the validation sample, letĈ v = the number of considered validation profiles, let F p = the falsepositive predictions, and let F n = the false-negative predictions. Then KL is given by the following equation, where S c c is the set of profiles that are considered in the calibration data and considered in the validation data; the sets S c nc ,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 3 Synthetic Data Experiments (Base Comparison, No Error): Percent Uncertainty Explained U 2 for Alternative Question-Selection Methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>814</head><label></label><figDesc>Marketing Science 30(5), pp.801-819, © 2011 INFORMS    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4Average Expected Reduction in Entropy up to the 29th Question</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>until p h ijK converges.] For j = 1 to N [Loop over all aspects.] For k ∈ S + j [Use variational distribution to approximate data likelihood.]Pry ik = 1 X iK p h−1 iK −j a ij = 1 = 1 − 2 x igk =1 g =j p h−1 igK + 1 1 − x igk =1 g =j p h−1 igK Pr y ik = 1 X iK p h−1 iK −j a ij = −1 = 1 end loop k ∈ S + j For k ∈ S − j [Use variational distribution to approximate data likelihood.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 1Taxonomy of Existing Methods to Select Questions to Identify Consumer Decision Rules</figDesc><table><row><cell></cell><cell cols="5">Question selection (and example estimation) for consumer decision rules</cell><cell></cell></row><row><cell cols="3">Compensatory (additive) rules</cell><cell></cell><cell cols="2">Noncompensatory heuristics</cell><cell></cell></row><row><cell>Self-stated</cell><cell>Fixed</cell><cell>Adaptive</cell><cell>Self-stated</cell><cell>Fixed</cell><cell></cell><cell>Adaptive</cell></row><row><cell>Moderate or large number of aspects</cell><cell>Moderate or large number of aspects</cell><cell>Moderate or large number of aspects</cell><cell>Moderate or large number of aspects</cell><cell>Moderate number of aspects</cell><cell>Large number of aspects</cell><cell>Moderate or large number of aspects</cell></row><row><cell>Examples</cell><cell>Examples</cell><cell>Examples</cell><cell>Examples</cell><cell>Examples</cell><cell>Examples</cell><cell>Examples</cell></row><row><cell>Common</cell><cell>Orthogonal,</cell><cell>Polyhedral,</cell><cell>Unacceptable</cell><cell>Orthogonal,</cell><cell>No prior</cell><cell>No prior</cell></row><row><cell>since</cell><cell>random,</cell><cell>SVMs, ACA</cell><cell>aspects</cell><cell>random, or</cell><cell>applications</cell><cell>feasibility</cell></row><row><cell>1970s</cell><cell>aggregate</cell><cell></cell><cell>(but typically</cell><cell>market-based</cell><cell>are feasible</cell><cell></cell></row><row><cell></cell><cell>customization</cell><cell>(with HB, latent</cell><cell>overstate),</cell><cell></cell><cell>for all</cell><cell></cell></row><row><cell></cell><cell></cell><cell>class, analytic</cell><cell>incentive-</cell><cell>(with HB, LAD,</cell><cell>heuristics</cell><cell></cell></row><row><cell></cell><cell>(with HB, latent</cell><cell>center, etc.)</cell><cell>compatible</cell><cell>greedoid DPs,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>class, machine</cell><cell></cell><cell>e-mail task</cell><cell cols="2">integer programs,</cell><cell></cell></row><row><cell></cell><cell>learning, etc.)</cell><cell></cell><cell></cell><cell cols="2">greedy heuristics)</cell><cell></cell></row></table><note>Note. ACA, adaptive conjoint analysis; DP, dynamic program; HB, hierarchical Bayes; SVMs, support-vector machines.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Marketing Science 30(5), pp. 801-819, © 2011 INFORMS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Synthetic Data Experiments Number of Questions Necessary to Match Predictive Ability of 32 Orthogonal Questions</figDesc><table><row><cell></cell><cell>Adaptive</cell><cell>Random</cell><cell>Market-based</cell><cell>Orthogonal-</cell><cell>Percent</cell></row><row><cell></cell><cell>questions</cell><cell>questions</cell><cell>questions</cell><cell>design questions</cell><cell>uncertainty a</cell></row><row><cell>Base comparison</cell><cell>9</cell><cell>40</cell><cell>39</cell><cell>32</cell><cell>76 1</cell></row><row><cell>Error in answers</cell><cell>11</cell><cell>38</cell><cell>38</cell><cell>32</cell><cell>53 6</cell></row><row><cell>"Bad" priors</cell><cell>6</cell><cell>42</cell><cell>41</cell><cell>32</cell><cell>50 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Illustrative Empirical Application KL Divergence for</cell></row><row><cell cols="3">Question-Selection-and-Estimation Combinations (Where</cell></row><row><cell>Smaller Is Better)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Noncompensatory</cell><cell>Compensatory</cell></row><row><cell></cell><cell>heuristics</cell><cell>decision model</cell></row><row><cell>Question-selection method</cell><cell></cell><cell></cell></row><row><cell>Adaptive questions</cell><cell>0 475 abc</cell><cell>0 537 c</cell></row><row><cell>Market-based questions</cell><cell>0 512 c</cell><cell>0 512 cd</cell></row><row><cell>Null models</cell><cell></cell><cell></cell></row><row><cell>Consider all profiles</cell><cell>0.565</cell><cell></cell></row><row><cell>Consider no profiles</cell><cell>0.565</cell><cell></cell></row><row><cell>Randomly consider profiles</cell><cell>0.562</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>Percentage of Respondents Using Aspect as an Elimination Criterion two of the three American manufacturers had entered bankruptcy. AAM's top management believed that overcoming consumers' unwillingness to consider AAM vehicles was critical if AAM was to become profitable. Table2, combined with ongoing studies by AAM, was deemed sufficient evidence for managers to rely on the algorithm to identify consumers' heuristic decision rules. AAM is convinced of the relevancy Details are proprietary and beyond the scope of this paper. However, in general, the most effective communications strategies were those that surprised consumers with AAM's success in a non U.S. reference group. AAM's then-current emphasis on J.D. Power and Consumer Reports ratings did not change consumers' decision heuristics.</figDesc><table><row><cell>Brand</cell><cell>Elimination (%)</cell><cell>Body type</cell><cell>Elimination (%)</cell><cell>Engine type</cell><cell>Elimination (%)</cell></row><row><cell>BMW</cell><cell>68</cell><cell>Sports car</cell><cell>84</cell><cell>Gasoline</cell><cell>3</cell></row><row><cell>Buick</cell><cell>97</cell><cell>Hatchback</cell><cell>81</cell><cell>Hybrid</cell><cell>44</cell></row><row><cell>Cadillac</cell><cell>86</cell><cell>Compact sedan</cell><cell>62</cell><cell>Engine power</cell><cell></cell></row><row><cell>Chevrolet</cell><cell>34</cell><cell>Standard sedan</cell><cell>58</cell><cell>4 cylinders</cell><cell>9</cell></row><row><cell>Chrysler</cell><cell>66</cell><cell>Crossover</cell><cell>62</cell><cell>6 cylinders</cell><cell>11</cell></row><row><cell>Dodge</cell><cell>60</cell><cell>Small SUV</cell><cell>61</cell><cell>8 cylinders</cell><cell>69</cell></row><row><cell>Ford GMC Honda Hyundai Jeep Kia Lexus Lincoln Mazda Nissan Pontiac Saturn Subaru Toyota VW</cell><cell>23 95 14 89 96 95 86 98 90 14 97 95 99 15 86</cell><cell>Full-size SUV Pickup truck Minivan Quality Q-rating 5 Q-rating 4 Q-rating 3 Crash test C-rating 5 C-rating 4 C-rating 3</cell><cell>71 82 90 0 1 23 0 27 27</cell><cell>EPA rating 15 mpg 20 mpg 25 mpg 30 mpg 35 mpg Price ($) 12,000 17,000 22,000 27,000 32,000 37,000 45,000</cell><cell>79 42 16 5 0 77 54 46 48 61 71 87</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"> Rényi's entropy reduces to Shannon's entropy when Rényi's = 1; the only value of for which information on the a ij s is separable. To use these measures of entropy, modify Equations (2) and (3) to reflect Rényi's .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although the plots in Figure2are concave, there is no guarantee that the plots remain concave for all situations. However, we do expect all plots to be quasi-concave, and they are.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the MIT Sloan School of Management, the Center for Digital Business at MIT (http://ebusiness.mit.edu), and an unnamed American automotive manufacturer. The authors thank Rene Befurt, Sigal Cordeiro, Theodoros Evgeniou, Vivek Farias, Patricia Hawkins, Dmitriy Katz, Phillip Keenan, Andy Norton, Ele Ocholi, James Orlin, Erin MacDonald, Daniel Roesch, Joyce Salisbury, Matt Selove, Olivier Toubia, Paul Tsier, Catherine Tucker, Glen Urban, Kevin Wang, and Juanjuan Zhang for their insights, inspiration, and help on this project. Both reviewers and the area editor provided excellent comments that improved the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Example Where Uncertainty Sampling Minimizes Posterior Entropy</head><p>We choose a simple example with two aspects to demonstrate the intuition. For this formal example, we abstract away from response error by setting 1 = 2 = 0, and we choose uninformative priors such that p 0 i1 = p 0 i2 = 0 5. With two aspects there are four potential queries, x i1 = 0 0 0 1 1 0 , and 1 1 ; and four potential decision rules, a i = − 1 −1 − 1 +1 + 1 −1 , and + 1 +1 , each of which is a priori equally likely. However, the different x i1 s provide differential information about the decision rules. For example, if x i1 = 0 0 and y i1 = 1, then the decision rule must be a i = − 1 −1 . At the other extreme, if x i1 = 1 1 and y i1 = 1, then all decision rules are consistent. The other two profiles are each consistent with half of the decision rules. We compute Pr y i1 = 1 x i1 for the four potential queries as 0.25, 0.50, 0.50, and 1.00, respectively.</p><p>We use the formulae in the text for expected posterior entropy, E H x i1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential query ( x i1</head><p>Pr</p><p>Expected posterior entropy is minimized for either of the queries, 0 1 or 1 0 , both of which are consistent with uncertainty sampling (choice balance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Pseudo-Code for Belief-Propagation Algorithm</head><p>Maintain the notation of the text; let p iK be the vector of the p ijK s, and let p iK −j be the vector of all but the jth element. Define two index sets, S + j = k x ijk = 1 y ik = 1 and S − j = k x ijk = 1 y ik = 0 . Let superscript h index an iteration with h = 0 indicating a prior. The belief-propagation algorithm uses all of the data, X K and y iK , when updating for the Kth query. In application, the s are set by managerial judgment prior to data collection. Our application used 1 = 2 = 0 01 for query selection.</p><p>Marketing Science 30(5), pp. 801-819, © 2011 INFORMS S nc c , and S nc nc are defined similarly (nc → not considered):</p><p>After algebraic simplification, KL can be written as</p><p>KL is a sum over the set of profiles. Sets with more profiles are harder to fit; if V were twice as large andĈ v , F p , and F n were scaled proportionally, then KL would be twice as large. For comparability across respondents with different validation set sizes, we divide by V to scale KL. b Significantly better than the compensatory decision model (p &lt; 0 001). c Significantly better than the random null model (p &lt; 0 001). d Significantly better than the consider-all-profiles null model (p &lt; 0 001). e Significantly better than the consider-no-profiles null model (p &lt; 0 001). f Significantly better than adaptive questions for the compensatory decision model (p &lt; 0 001).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving parameter estimates and model prediction by aggregate customization in choice experiments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="283" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inferring parameters and structure of latent variable models by variational</title>
		<author>
			<persName><forename type="first">H</forename><surname>Attias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Conf. Uncertainty Artificial Intelligence (UAI-99)</title>
				<editor>
			<persName><forename type="middle">K B</forename><surname>Bayes</surname></persName>
			<persName><forename type="first">H</forename><surname>Laskey</surname></persName>
			<persName><surname>Prade</surname></persName>
		</editor>
		<meeting>15th Conf. Uncertainty Artificial Intelligence (UAI-99)<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Empirical analysis of predictive algorithms for collaborative filtering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Conf. Uncertainty Artificial Intelligence (UAI-98)</title>
				<meeting>14th Conf. Uncertainty Artificial Intelligence (UAI-98)<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian experimental design: A review</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaloner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Verdinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="304" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The virtual customer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Product Innovation Management</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="332" to="353" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compensatory versus noncompensatory models for predicting consumer preferences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dieckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dippold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dietrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Judgment Decision Making</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="213" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unstructured direct elicitation of decision rules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzyabura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gaskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="116" to="127" />
			<date type="published" when="2011-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Brand credibility, brand consideration, and choice</title>
		<author>
			<persName><forename type="first">T</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="198" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generalized robust conjoint estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boussios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zacharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cognitive reflection and decision making</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frederick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econom. Perspect</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variational inference for Bayesian mixtures of factor analyzers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Propagation algorithms for variational Bayesian learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reasoning the fast and frugal way: Models of bounded rationality</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Rev</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="650" to="669" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bounded Rationality: The Adaptive Toolbox</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Selten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The ABC Research Group</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simple Heuristics That Make Us Smart</title>
				<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A choice model with conjunctive, disjunctive, and compensatory screening rules</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Gilbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating heterogeneous EBA and economic screening rule choice models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Gilbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="494" to="509" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hybrid models for conjoint analysis: An expository review</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="169" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conjoint analysis in marketing: New developments with implications for research and practice</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Completely unacceptable levels in conjoint analysis: A cautionary note</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1988-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Testing the accuracy, usefulness, and significance of probabilistic choice models: An information-theoretic approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="406" to="421" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An evaluation cost model of consideration sets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wernerfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="393" to="408" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Disjunctions of conjunctions, cognitive simplicity, and consideration sets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Befurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzyabura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic subset-conjunctive models for heterogeneous consumers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jedidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="483" to="494" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representation and inference of lexicographic preference models and their variants</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jedidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="380" to="399" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training text classifiers by uncertainty sampling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Annual Internat</title>
				<meeting>17th Annual Internat<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Field experiments: Providing unbiased competitive information to encourage trust, consideration, and sales. Working paper</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liberali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Sloan School of Management</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On a measure of the information provided by an experiment</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Lindley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="986" to="1005" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Managers and models: The concept of a decision calculus</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D C</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1841" to="1853" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Reprinted from 1970.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Models and managers: The concept of a decision calculus&quot;: Managerial models for practice</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D C</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1854" to="1860" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Comments on</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient choice designs for a consider-thenchoose model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="338" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast, frugal, and fit: Simple heuristics for paired comparisons</title>
		<author>
			<persName><forename type="first">L</forename><surname>Martignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hoffrage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Decision</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="71" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive strategy selection in decision making</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bettman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Experiment. Psych.: Learn., Memory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="534" to="552" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note>Cognition</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The Adaptive Decision Maker</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bettman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On measures of information and entropy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Berkeley Sympos. Math., Statistics, Probability</title>
				<meeting>4th Berkeley Sympos. Math., Statistics, Probability</meeting>
		<imprint>
			<date type="published" when="1961" />
			<biblScope unit="page" from="547" to="561" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Profile construction in experimental choice designs for mixed logit models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sándor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="475" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ACA system: Adaptive conjoint analysis</title>
	</analytic>
	<monogr>
		<title level="m">Sawtooth Software</title>
				<meeting><address><addrLine>Sequim, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CBC hierarchical Bayes analysis. Technical paper</title>
	</analytic>
	<monogr>
		<title level="m">Sawtooth Software</title>
				<meeting><address><addrLine>Sequim, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Sawtooth Software</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ACBC. Technical paper, Sawtooth Software</title>
	</analytic>
	<monogr>
		<title level="m">Sawtooth Software</title>
				<meeting><address><addrLine>Sequim, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication. Bell System Tech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On managerially efficient experimental designs</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="851" to="858" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic polyhedral methods for adaptive choice-based conjoint analysis: Theory and application</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="596" to="610" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Polyhedral methods for adaptive choice-based conjoint analysis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Simester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2004-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Elimination by aspects: A theory of choice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Rev</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="281" to="299" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Listening in&quot; to find and explore new combinations of customer needs</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="87" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Retrieving unobserved consideration sets from household panel data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Nierop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bronnenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Franses</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="63" to="74" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Split-questionnaire designs: A new tool in survey design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vriens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sándor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Issues in marketing&apos;s use of multi-attribute attitude models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Wilkie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Pessemier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="428" to="441" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Understanding belief propagation and its generalizations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<editor>G. Lakemeyer, B. Nebel</editor>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page" from="239" to="269" />
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
	<note>Exploring Artificial Intelligence in the New Millennium</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Greedoid-based noncompensatory inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="549" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
