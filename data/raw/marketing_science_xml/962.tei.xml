<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Polyhedral Adaptive Conjoint Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Toubia</surname></persName>
							<email>toubia@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Duncan</forename><forename type="middle">I</forename><surname>Simester</surname></persName>
							<email>simester@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
							<email>jhauser@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ely</forename><surname>Dahan</surname></persName>
							<email>edahan@ucla.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Sloan School of Management</orgName>
								<orgName type="department" key="dep2">Anderson School</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<addrLine>38 Memorial Drive</addrLine>
									<postCode>E56-305, 02142</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California at Los Angeles</orgName>
								<address>
									<addrLine>110 Westwood Plaza</addrLine>
									<postCode>B-514, 90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Polyhedral Adaptive Conjoint Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.22.3.273.17743</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>W e propose and test new adaptive question design and estimation algorithms for partialprofile conjoint analysis. Polyhedral question design focuses questions to reduce a feasible set of parameters as rapidly as possible. Analytic center estimation uses a centrality criterion based on consistency with respondents' answers. Both algorithms run with no noticeable delay between questions.</p><p>We evaluate the proposed methods relative to established benchmarks for question design (random selection, D-efficient designs, adaptive conjoint analysis) and estimation (hierarchical Bayes). Monte Carlo simulations vary respondent heterogeneity and response errors. For low numbers of questions, polyhedral question design does best (or is tied for best) for all tested domains. For high numbers of questions, efficient fixed designs do better in some domains. Analytic center estimation shows promise for high heterogeneity and for low response errors; hierarchical Bayes for low heterogeneity and high response errors. Other simulations evaluate hybrid methods, which include self-explicated data.</p><p>A field test (330 respondents) compared methods on both internal validity (holdout tasks) and external validity (actual choice of a laptop bag worth approximately $100). The field test is consistent with the simulation results and offers strong support for polyhedral question design. In addition, marketplace sales were consistent with conjoint-analysis predictions. <ref type="bibr">(New Product Research; Measurement; Internet Marketing; Estimation and Other Statistical Techniques)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Polyhedral Methods for Conjoint Analysis</head><p>We propose and test (1) a new adaptive questiondesign method that attempts to reduce respondent burden while simultaneously improving accuracy and</p><p>(2) a new estimation procedure based on centrality concepts. For each respondent the question-design method dynamically adapts the design of the next question using that respondent's answers to previous questions. Because the methods make full use of high-speed computations and adaptive, customized local Web pages, they are ideally suited for Webbased panels. The adaptive method interprets question design as a mathematical program and estimates the solution to the program using recent developments based on the interior points of polyhedra. The estimation method also relies on interior point techniques and is designed to provide robust estimates from relatively few questions. The question design FAST POLYHEDRAL ADAPTIVE CONJOINT ESTIMATION and estimation methods are modular and can be evaluated separately and/or combined with a range of existing methods.</p><p>Adapting question design within a respondent, using that respondent's answers to previous questions, is a difficult dynamic optimization problem. Adaptation within respondents should be distinguished from techniques that adapt across respondents. Sawtooth Software's adaptive conjoint analysis (ACA) is the only published method of which we are aware that attempts to solve this problem <ref type="bibr" target="#b19">(Johnson 1987</ref><ref type="bibr" target="#b19">(Johnson , 1991</ref>. In contrast, aggregate customization methods, such as the <ref type="bibr">Huber and Zwerina (1996)</ref>, <ref type="bibr">Arora and Huber (2001)</ref>, and <ref type="bibr" target="#b35">Sandor and</ref><ref type="bibr" target="#b35">Wedel (2001, 2002)</ref> algorithms, adapt designs across respondents based on either pretests or Bayesian priors.</p><p>ACA uses a data-collection format known as metric paired-comparison questions and relies on balancing utility between the pairs subject to orthogonality and feature balance. We provide an example of a metric-paired comparison question in Figure <ref type="figure">1</ref>. To date, aggregate customization methods have focused on a stated-choice data-collection format known as choice-based conjoint (CBC; e.g. <ref type="bibr" target="#b26">Louviere et al. 2000)</ref>. Polyhedral methods can be used to design either metric-paired-comparison questions or choice-based questions. In this paper we focus on metric-paired-comparison questions because this is Figure <ref type="figure">1</ref> Metric Paired-Comparison Format for I-Zone Camera Redesign one of the most widely used and applied datacollection formats for conjoint analysis <ref type="bibr">(Green et al. 2001, p. S66;</ref><ref type="bibr">Ter Hofstede et al. 2002, p. 259)</ref>. In addition, metric paired-comparison questions are common in computer-aided interviewing, have proven reliable in previous studies <ref type="bibr">(Reibstein et al. 1988, Urban and</ref><ref type="bibr" target="#b43">Katz 1983)</ref>, provide interval-scaled data with strong transitivity properties <ref type="bibr">(Hauser and Shugan 1980)</ref>, provide valid and reliable parameter estimates <ref type="bibr" target="#b23">(Leigh et al. 1984)</ref>, and enjoy wide use in practice and in the literature <ref type="bibr">(Wittink and Cattin 1989)</ref>. We are extending polyhedral methods to CBC formats <ref type="bibr" target="#b41">(Toubia et al. 2003)</ref>.</p><p>Our goal is an initial evaluation of polyhedral methods relative to existing methods under a variety of empirically relevant conditions. We do not expect that any one method will always outperform the benchmarks, nor do we intend that our findings be interpreted as criticism of any of the benchmarks. Our findings indicate that polyhedral methods have the potential to enhance the effectiveness of existing conjoint methods by providing new capabilities that complement existing methods.</p><p>Because the methods are new and adopt a different estimation philosophy, we use Monte Carlo experiments to explore their properties. The Monte Carlo experiments explore the conditions under which polyhedral methods are likely to do better or worse than extant methods. We demonstrate practical domains where polyhedral methods show promise relative to a representative set of widely applied and studied methods. The findings also highlight opportunities for future research by illustrating domains where improvements are necessary and/or where extant methods are likely to remain superior.</p><p>We also undertake a large-scale empirical test involving a real product-a laptop computer bag worth approximately $100. Respondents first completed a series of Web-based conjoint questions chosen by one of three question-design methods (the methods were assigned randomly). After a filler task, respondents in the study were given $100 to spend on a choice set of five bags. Respondents received their chosen bag together with the difference in cash between the price of their chosen bag and the $100. We compare question-design and estimation methods TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation on both internal and external validity. Internal validity is evaluated by comparing how well the methods predict several holdout conjoint questions. External validity is evaluated by comparing how well the different conjoint methods predict which bag respondents later chose to purchase using their $100. The paper is structured as follows. We begin by describing polyhedral question design and analytic center estimation for metric paired-comparison tasks. Detailed mathematics are provided in Appendix 1 and open-source code is available from http://mktsci. pubs.informs.org. We next describe the design and results of the Monte Carlo experiments. Finally, we describe the field test and the comparative results. We close with a description of the launch of the laptop bag, a summary of the findings, and suggestions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Polyhedral Question Design and Estimation</head><p>We begin with a conceptual description that highlights the geometry of the conjoint-analysis parameter space. We illustrate the concepts with a threeparameter problem because three-dimensional spaces are easy to visualize and explain. The methods generalize easily to realistic problems that contain 10, 20, or even 100 product features. Indeed, relative to existing methods, the polyhedral methods are proposed for larger numbers of product features. By a parameter, we refer to a partworth that needs to be estimated. For example, 20 features with 2 levels each require 20 parameters because we can scale to zero the partworth of the least preferred feature. Similarly, 10 three-level features also require 20 parameters. Interactions among features require still more parameters. Suppose that we have three features of an instant camera: picture quality, picture taking (two-step versus one-step), and styling covers (changeable versus permanent). If we scale the least desirable level of each feature to zero, we have three nonnegative parameters to estimate-u 1 , u 2 , and u 3 -reflecting the additional utility (partworth) associated with the most desirable level of each feature. <ref type="bibr">1</ref> The measurement scale on which the questions are asked imposes natural boundary conditions. For example, the sum of the partworths of Camera A minus the sum of the partworths of Camera B can be at most equal to the maximum scale difference. In practice, the partworths have only relative meaning, so scaling allows us to impose a wide range of boundary conditions without loss of generality. Therefore, to better visualize the algorithm, we impose a constraint that the sum of the parameters does not exceed some large number (e.g., 100). Under this constraint, prior to any data collection, the feasible region for the parameters is the three-dimensional bounded polyhedron in Figure <ref type="figure">2a</ref>.</p><p>Suppose that we ask the respondent to evaluate a pair of profiles that vary on one or more features and the respondent says (1) that he or she prefers profile C 1 to profile C 2 , and (2) provides a rating, a, to indicate the strength of his or her preference. Assuming for the moment that the respondent answers without error, this introduces an equality constraint that the utility associated with profile C 1 exceeds the utility of C 2 by an amount equal to the rating. If we define u = u 1 u 2 u 3 T as the 3 × 1 vector of parameters, z l as the 1 × 3 vector of product features for the left profile, and z r as the 1 × 3 vector of product features for the right profile, then, for additive utility, this equality constraint can be written as z l u − z r u = a. We can use geometry to characterize what we have learned from this response.</p><p>Specifically, we define x = z l − z r such that x is a 1 × 3 vector describing the difference between the two profiles in the question. Then, x u = a defines a hyperplane through the polyhedron in Figure <ref type="figure">2a</ref>. The only feasible values of u are those that are in the intersection of this hyperplane and the polyhedron. The new feasible set is also a polyhedron, but it is reduced by one dimension (two dimensions rather than three dimensions). Because smaller polyhedra mean fewer parameter values are feasible, questions that reduce </p><formula xml:id="formula_0">u 1 + u 2 + u 3 ≤ 100 a u x = ≤ ≤ a u 1 u 2 u 3 ≤ ≤ − δ δ + ≤ ≤ − a u x a intersection of and u 1 + u 2 + u 3 ≤ 100 (a) ( b)</formula><p>Notes. (a) Metric rating without error. (b) Metric rating with error.</p><p>the size of the initial polyhedron as fast as possible lead to more precise estimates of the parameters. However, in any real problem we expect a respondent's answer to contain error. We can model this error as a probability density function over the parameter space (as in standard statistical inference). Alternatively, we can incorporate imprecision in a response by treating the equality constraint x u = a as a set of two inequality constraints: a − ≤ x u ≤ a + . In this case, the hyperplane defined by the question-answer pair has "width." The intersection of the initial polyhedron and the "fat" hyperplane is now a three-dimensional polyhedron as illustrated in Figure <ref type="figure">2b</ref>.</p><p>When we ask more questions we constrain the parameter space further. Each question, if asked carefully, will result in a hyperplane that intersects a polyhedron resulting in a smaller polyhedrona "thin" region in Figure <ref type="figure">2a</ref> or a "fat" region in Figure <ref type="figure">2b</ref>. Each new question-answer pair slices the polyhedron in Figure <ref type="figure">2a</ref> or 2b yielding more precise estimates of the parameter vector u.</p><p>We incorporate prior information about the parameters by imposing constraints on the parameter space. For example, if u m and u h are the medium and high levels, respectively, of a feature, then we impose the constraint u m ≤ u h on the polyhedron. Previous research suggests that these types of constraints enhance estimation <ref type="bibr">(Johnson 1999, Srinivasan and</ref><ref type="bibr">Shocker 1973)</ref>. We now examine question design for metric paired-comparison data by dealing first with the case in which subjects respond without error (Figure <ref type="figure">2a</ref>). We then describe how to modify the algorithm to handle error (e.g., Figure <ref type="figure">2b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting Questions to Shrink the Feasible Set Rapidly</head><p>The question-design task describes the design of the profiles that respondents are asked to compare. Questions are more informative if the answers allow us to estimate partworths more quickly. For this reason, we select the respondent's next question in a manner that is likely to reduce the size of the feasible set (for that respondent) as fast as possible.</p><p>Consider for a moment a 20-dimensional problem (without errors in the answers). As in Figure <ref type="figure">2a</ref>, a question-based constraint reduces the dimensionality by one. That is, the first question reduces a 20-dimensional set to a 19-dimensional set; the next question reduces this set to an 18-dimensional set, and so on. After the twelfth question, for example, we reach an eight-dimensional set: 8 dimensions = 20 parameters − 12 questions. Without further restriction, the feasible parameters are generally not unique-any point in the eight-dimensional set (polyhedron) is still feasible. However, the eight-dimensional set might be quite small, and we might have a very good idea of the partworths. For example, the first 12 questions TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation might be enough to tell us that some features-say picture quality, styling covers, and battery life-have large partworths, while other features-say folding capability, light selection, and film ejection methodhave very small partworths. If this holds across respondents, then during an early phase of a product development process, the product development team might feel they have enough information to focus on the key features.</p><p>Although the polyhedral algorithm is designed for high-dimensional spaces, it is hard to visualize 20-dimensional polyhedra. Instead, we illustrate the polyhedral question-design method in a situation where the remaining feasible set is easy to visualize. Specifically, by generalizing our notation slightly to q questions and p parameters, we define a as the q × 1 vector of answers and X as the q × p matrix with rows equal to x for each question (recall that x is a 1 × p vector). Then the respondent's answers to the first q questions define a (p − q)-dimensional hyperplane given by the equation X u = a. This hyperplane intersects the initial p-dimensional polyhedron to give us a (p − q)-dimensional polyhedron. In the example of p = 20 parameters and q = 18 questions, the result is a two-dimensional polyhedron that is easy to visualize. x′ a′ ( , )</p><formula xml:id="formula_1">a x ( , ) a ′ ′ ( , ) x a x ( , )</formula><p>x′ a′ <ref type="bibr">( , )</ref> x′ a′ ( , ) One such two-dimensional polyhedron is illustrated in Figure <ref type="figure">3</ref>. Our task is to select questions that reduce the twodimensional polyhedron as fast as possible. Mathematically, we select a new question vector, x, and the respondent answers this question with a new rating, a. We add the new question vector as the last row of the question matrix and we add the new answer as the last row of the answer vector. While everything is really happening in p-dimensional space, the net result is that the new hyperplane will intersect the two-dimensional polyhedron in a line segment (i.e., a one-dimensional polyhedron). The slope of the line will be determined by x and the intercept by a. We illustrate two potential question-answer pairs in Figure <ref type="figure">3a</ref>. The slope of the line is determined by the question, the specific line by the answer, and the remaining feasible set by the line segment within the polyhedron. In Figure <ref type="figure">3a</ref> one of the question-answer pairs ( x a) reduces the feasible set more rapidly than the other question-answer pair ( x a ). Figure <ref type="figure">3b</ref> repeats a question-answer pair ( x a) and illustrates an alternative answer to the same question ( x a ).</p><formula xml:id="formula_2">a x ( , ) x ( , ) a ′ ′ ( , ) x a ( ) x a x ( , ) x ( , )<label>(a</label></formula><p>If the polyhedron is elongated, as in Figure <ref type="figure">3</ref>, then in most cases, questions that imply line segments TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation perpendicular to the longest "axis" of the polyhedron are questions that result in the smallest remaining feasible sets. Also, because the longest "axis" is in some sense a bigger target, it is more likely that the respondent's answer will select a hyperplane that intersects the polyhedron. From analytic geometry we know that hyperplanes (line segments in Figure <ref type="figure">3</ref>) are perpendicular to their defining vectors ( x). Thus, we can reduce the feasible set as fast as possible (and make it more likely that answers are feasible) if we choose question vectors that are parallel to the longest "axis." For example, both line segments based on x in Figure <ref type="figure">3b</ref> are shorter than the line segment based on x in Figure <ref type="figure">3a</ref>.</p><p>If we can develop an algorithm that works in any p-dimensional space, then we can generalize this intuition to any question, q, such that q ≤ p. After receiving answers to the first q questions, we could find the longest vector of the (p-q)-dimensional polyhedron of feasible parameter values. We could then ask the question based on a vector that is parallel to this "axis." The respondent's answer creates a hyperplane that intersects the polyhedron to produce a new polyhedron. We address later the cases where respondents' answers contain error and where q &gt; p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Centrality Estimation</head><p>Polyhedral geometry also gives us a means to estimate the parameter vector, u, when q ≤ p. Recall that, after question q, any point in the remaining polyhedron is consistent with the answers the respondent has provided. If we impose a diffuse prior that any feasible point is equally likely, then we would like to select the point that minimizes the expected error. This point is the center of the feasible polyhedron, or more precisely, the polyhedron's center of gravity. The smaller the feasible set, either due to better question design or more questions (higher q), the more precise the estimate. If there were no respondent errors, then the estimate would converge to its true value when q = p (the feasible set becomes a single point, with zero dimensionality). For q &gt; p the same point would remain feasible. As we discuss below, this changes when responses contain error.</p><p>This technique of estimating partworths from the center of a feasible polyhedron is related to that proposed by <ref type="bibr">Srinivasan and Shocker (1973, p. 350)</ref> who suggest using a linear program to find the "innermost" point that maximizes the minimum distance from the hyperplanes that bound the feasible set. Philosophically, the proposed polyhedral method makes use of the information in the constraints and then takes a central estimate based on what is still feasible. Carefully chosen questions shrink the feasible set rapidly. We then use a centrality estimate that has proven to be a surprisingly good approximation in a variety of engineering problems. More generally, the centrality estimate is similar in some respects to the proven robustness of linear models, and in some cases, to the robustness of equally weighted models <ref type="bibr" target="#b6">(Dawes and Corrigan 1974</ref><ref type="bibr" target="#b7">, Einhorn 1971</ref><ref type="bibr" target="#b16">, Huber 1975</ref><ref type="bibr">, Moore and Semenik 1988</ref><ref type="bibr">, Srinivasan and Park 1997</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interior-Point Algorithms and the Analytic Center of a Polyhedron</head><p>To select questions and obtain intermediate estimates, the proposed heuristics require that we solve two nontrivial mathematical programs. First, we must find the longest "axis" of a polyhedron (to select the next question) and second, we must find the polyhedron's center of gravity (to provide a centrality estimate). If we were to define the longest "axis" of a polyhedron as the longest line segment in the polyhedron, then one method to find the longest "axis" would be to enumerate the vertices of the polyhedron and compute the distances between the vertices. However, solving this problem requires checking every extreme point, which is computationally intractable <ref type="bibr" target="#b12">(Gritzmann and Klee 1993)</ref>. In practice, solving the problem would impose noticeable delays between questions. Also, the longest line segment in a polyhedron may not capture the concept of a longest "axis." Finding the center of gravity of the polyhedron is even more difficult and computationally demanding.</p><p>Fortunately, recent work in the mathematical programming literature has led to extremely fast algorithms based on projections within the interior of polyhedrons (much of this work started with <ref type="bibr" target="#b21">Karmarkar 1984)</ref>. Interior-point algorithms are now used routinely to solve large problems and have TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation spawned many theoretical and applied generalizations. One such generalization uses bounding ellipsoids. In 1985, Sonnevend demonstrated that the shape of a bounded polyhedron can be approximated by proportional ellipsoids, centered at the "analytic center" of the polyhedron. The analytic center is the point in the polyhedron that maximizes the geometric mean of the distances to the boundaries of the polyhedron. It is a central point that approximates the center of gravity of the polyhedron, and finds practical use in engineering and optimization. Furthermore, the axes of the ellipsoids are well-defined and intuitively capture the concept of an "axis" of a polyhedron. For more details see <ref type="bibr" target="#b10">Freund (1993)</ref>, <ref type="bibr" target="#b29">Nesterov and Nemirovskii (1994)</ref>, <ref type="bibr">Sonnevend (1985a, b), and</ref><ref type="bibr" target="#b44">Vaidja (1989)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polyhedral Question Design and Analytic Center Estimation</head><p>We illustrate the proposed process in Figure <ref type="figure" target="#fig_3">4</ref>, using the same two-dimensional polyhedron depicted in Figure <ref type="figure">3</ref>. The algorithm proceeds in four steps. We first find a point in the interior of the polyhedron. This is a simple linear programming (LP) problem and runs quickly. Then, following <ref type="bibr" target="#b10">Freund (1993)</ref> we use Newton's method to make the point more central. This is a well-formed problem and converges quickly to yield the analytic center as illustrated by the black dot in Figure <ref type="figure" target="#fig_3">4</ref>. We next find a bounding ellipsoid based on a formula that depends on the analytic center and the question-matrix, X. We then find the longest axis of the ellipsoid (diagonal line in Figure <ref type="figure" target="#fig_3">4</ref>) with a quadratic program that has a closed-form solution. The next question, x, is based on the vector most nearly parallel to this axis. A formal (mathematical) description of each step is provided in Appendix 1. Analytically, this algorithm works well in higher dimensional spaces. For example, Figure <ref type="figure">5</ref> illustrates the algorithm when p − q = 3, where we reduce a three-dimensional feasible set to a two-dimensional feasible set. Figure <ref type="figure">5a</ref> illustrates a polyhedron based on the first q questions. Figure <ref type="figure">5b</ref> illustrates a bounding three-dimensional ellipsoid, the longest axis of that ellipsoid, and the analytic center. The longest axis defines the question that is asked next, which in turn defines the slope of the hyperplane that intersects the polyhedron. One such hyperplane is shown in Figure <ref type="figure">5c</ref>. The respondent's answer locates the specific hyperplane. The intersection of the selected hyperplane and the three-dimensional polyhedron is a new two-dimensional polyhedron, such as that in Figure <ref type="figure" target="#fig_3">4</ref>. This process applies (in higher dimensions) from the first question to the pth question. For example, the first question implies a hyperplane that cuts the first p-dimensional polyhedron such that the intersection yields a (p − 1)-dimensional polyhedron.</p><p>The polyhedral algorithm runs extremely fast. We have implemented the algorithm for the Web-based empirical test described later in this paper. Based on this example, with 10 two-level features, respondents noticed no delay in question design nor any difference in speed versus a fixed design. For a demonstration see the Website referenced in the Acknowledgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inconsistent Responses and Error-Modeling</head><p>Figures 2 through 5 illustrate the geometry when respondents answer without error. However, real respondents are unlikely to be perfectly consistent. It is more likely that, for some q &lt; p, the respondent's answers will be inconsistent and the polyhedron will become empty. That is, we will no longer be able to find any parameters, u, that satisfy the equations that define the polyhedron, X u = a. Thus, for real applications, we extend the polyhedral algorithm to address response errors. Specifically, we adjust the polyhedron in a minimal way to ensure that some parameter values are still feasible. We do this by modeling errors, , in the respondent's answers such that a − ≤ X u ≤ a + (recall Figure <ref type="figure">2b</ref>). We then choose the minimum errors such that these constraints are satisfied. This same modification covers estimation for the case of q &gt; p. Appendix 1 provides the mathematical program (OPT4) that we use to estimate u and . The algorithm is easily modified to incorporate alternative error formulations, such as least-squares or minimum sum of absolute deviations, rather than this "minimax" criterion. <ref type="bibr">2</ref> Exploratory simulations suggest that the algorithm is robust to the choice of error criterion.</p><p>To implement this policy for analytic center estimation, we use a two-stage algorithm. In the first stage we treat the responses as if they occurred without error-the feasible polyhedron shrinks rapidly and the analytic center is a working estimate of the true parameters. However, as soon as the feasible set becomes empty, we adjust the constraints by adding or subtracting "errors," where we choose the minimum errors, , for which the feasible set is nonempty. The analytic center of the new polyhedron becomes the working estimate and becomes an index of response error. <ref type="bibr">3</ref> As with all our heuristics, the accuracy of our error-modeling method is tested with simulation. While estimates based on this heuristic seem to converge for the domains that we test (reduce mean errors, no measured bias), we recognize the need for further exploration of this heuristic, together with the development of a formal error theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Addressing Other Practical Implementation Issues</head><p>Implementation raises several additional issues. Alternative solutions to these issues may yield more or less accurate parameter estimates, and so the performance of the polyhedral methods in the validation tasks are lower bounds on the performance of this class of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation</head><p>Product Profiles with Discrete Features. In most conjoint analysis problems the features are specified at discrete levels, as in Figure <ref type="figure">1</ref>. This constrains the elements of the x vector to be 1 −1 0, or 0, depending on whether the left profile, the right profile, neither profile, or both profiles have the "high" feature, respectively. In this case we choose the vector that is most nearly parallel to the longest axis of the ellipsoid. Because we can always recode multilevel features or interacting features as binary features, the geometric insights still hold even if we otherwise simplify the algorithm.</p><p>Restrictions on Question Design. For a p-dimensional problem we may wish to vary fewer than p features in any paired-comparison question. For example, Sawtooth Software (1996, p. 7) suggests that: "Most respondents can handle three attributes after they've become familiar with the task. Experience tells us that there does not seem to be much benefit from using more than three attributes." We incorporate this constraint by restricting the set of questions over which we search when finding a question-vector that is parallel to the longest axis of the ellipse.</p><p>First Question. Unless we have prior information before any question is asked, the initial polyhedron of feasible utilities is defined by the boundary constraints. If the boundary constraints are symmetric, the polyhedron is also symmetric and the polyhedral method offers little guidance for the choice of the first question. In these situations we choose the first question for each respondent so that it helps improve estimates of the population means by balancing how often each feature level appears in the set of questions answered by all respondents. In particular, for the first question presented to each respondent we choose feature levels that appeared infrequently in the questions answered by previous respondents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Design When the Parameter Set Becomes</head><p>Infeasible. Analytic center estimation is well-defined when the parameter set becomes infeasible, but question design is not. Thus, in the simulations we use a random question design heuristic when the parame-ter set is infeasible. <ref type="bibr">4</ref> This provides a lower bound on what might be achieved.</p><p>Programming. The optimization algorithms used for the simulations are written in Matlab and are available at http://mitsloan.mit.edu/vc. This Web site contains (1) open source code to implement the methods described in this paper, (2) open source code for the simulations described in this paper, (3) detailed instructions for implementing the method, ( <ref type="formula">4</ref>) worked examples, (5) demonstrations of Web-based questionnaires, (6) raw empirical data, and ( <ref type="formula">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Monte Carlo Simulations</head><p>Polyhedral methods for conjoint analysis are new and untested. Although interior-point algorithms and the centrality criterion have been successful in many engineering problems, we are unaware of any prior application to marketing problems. Thus, we turn first to Monte Carlo experiments to identify circumstances in which polyhedral methods may contribute to the effectiveness of current methods. Monte Carlo simulations offer at least three advantages for the initial test of a new method. First, they facilitate comparison of different techniques in a range of domains such as varying levels of respondent heterogeneity and response accuracy. We can also evaluate combinations of the techniques, for example, mixing polyhedral question design with extant estimation methods. Second, simulations resolve the issue of identifying the correct answer. In studies involving actual customers, the true partial utilities are unobserved. In simulations the true partial utilities are constructed so that we can compare how well alternative methods identify the true utilities from noisy TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation responses. Finally, other researchers can readily replicate the findings. However, simulations do not guarantee that real respondents behave as simulated nor do they reveal which domain is likely to best summarize field experience. Thus, following the simulations, we examine a field test that matches one of the simulated domains.</p><p>Many papers have used the relative strengths of Monte Carlo experiments to study conjoint techniques, providing insights on interactions, robustness, continuity, feature correlation, segmentation, new estimation methods, new data-collection methods, post-analysis with hierarchical Bayes methods, and comparisons of ACA, CBC, and other conjoint methods. Although we focus on specific benchmarks, there are many comparisons in the literature of these benchmarks to other methods (see reviews and citations in <ref type="bibr">Green 1984;</ref><ref type="bibr">Green et al. 2001</ref><ref type="bibr">Green et al. , 2002</ref><ref type="bibr" target="#b24">Green and</ref><ref type="bibr">Srinivasan 1978, 1990;</ref><ref type="bibr">Hauser and Rao 2003;</ref><ref type="bibr" target="#b28">Moore 2003)</ref>.</p><p>We test polyhedral question design versus three question design benchmarks and analytic center estimation versus hierarchical Bayes estimation. The initial simulations vary respondent heterogeneity, accuracy of respondent answers, and the number of questions. In a second set of simulations we also consider the role of self-explicated responses and vary the accuracy of self-explicated responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Respondent Heterogeneity, Response Errors, and Number of Questions</head><p>We focus on a design problem involving 10 features, where a product development team is interested in learning the incremental utility contributed by each feature. We follow convention and scale to zero the partworth of the low level of a feature and, without loss of generality, bound it by 100. This results in a total of 10 parameters to estimate (p = 10). We anticipate that the polyhedral methods are particularly well-suited to solving problems in which there are a large number of parameters relative to the number of responses from each individual (q &lt; p). Thus, we vary the number of questions from slightly less than the number of parameters (q = 8) to comfortably more than the number of parameters (q = 16).</p><p>We simulate each respondent's partworths by drawing independently and randomly from a normal distribution with mean 50 and variance 2 u , truncated to the range. We explored the sensitivity of the findings to this specification by testing different methods of drawing partworths, including beta distributions that tend to yield more similar partworths (inverted-U shape distributions), more diverse partworths (U-shaped distributions), or moderately diverse partworths (uniform distributions). Sensitivity analyses for key findings did not suggest much variation. Nonetheless, this is an important area for more systematic future research. By manipulating the standard deviation of the normal distribution we explore a relatively homogeneous population ( u = 10) and a relatively heterogeneous population ( u = 30). These values were chosen because they are comparable to those used elsewhere in the literature, because they result in moderate-to-low truncation, and because their range illustrates how the accuracy of the methods varies with heterogeneity.</p><p>To simulate the response to each metric pairedcomparison (PC) question, we calculate the true utility difference between each pair of product profiles by multiplying the design vector by the vector of true partworths: x u. We assume that the respondents' answers to the questions equal the true utility difference plus a zero-mean normal response error with variance 2 pc . The assumption of normally distributed error is common in the literature and appears to be a reasonable assumption about PC response errors <ref type="bibr">(Wittink and Cattin 1981</ref> report no systematic effects due to the type of error distribution assumed). We select response errors, comparable to those used in the literature. Specifically, to illustrate the range of response errors we use both a low response error ( pc = 20) and a high response error ( pc = 40). <ref type="bibr">5</ref> For each comparison, we simulate 500 respondents (in five sets of 100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Design Benchmarks</head><p>We compare the polyhedral question-design method against three benchmarks: random question design, a fixed design, and the question design used by adaptive conjoint analysis (ACA). For levels within random benchmark, the feature levels are chosen randomly and equally-likely. The fixed design provides another nonadaptive benchmark. For q &gt; p, we select the (q = 16) design with an algorithm that seeks the highest obtainable D-efficiency <ref type="bibr">(Kuhfield et al. 1994)</ref>. Efficiency is not defined for q &lt; p, thus, for q = 8, we follow the procedure established by <ref type="bibr" target="#b24">Lenk et al. (1996)</ref> and choose questions randomly from an efficient design for q = 16. We choose ACA question design as our third benchmark because it is the industry and academic standard for within-respondent adaptive question design. For example, <ref type="bibr">Green et al. (1991, p. 215</ref>) stated that "in the short span of five years, Sawtooth Software's Adaptive Conjoint Analysis has become one of the industry's most popular software packages for collecting and analyzing conjoint data," and go on to cite a number of academic papers on ACA. Although accuracy claims vary, ACA appears to predict reasonably well in many situations <ref type="bibr" target="#b19">(Johnson 1991</ref><ref type="bibr" target="#b31">, Orme 1999</ref>.</p><p>The ACA method includes five sections: an unacceptability task (that is often skipped), a ranking of levels within features, a series of self-explicated (SE) questions, the metric paired-comparison (PC) questions, and purchase intentions for calibration concepts. The question design procedure has not changed since it was "originally programmed for the Apple II computer in the late 70s" <ref type="bibr" target="#b31">(Orme and King 2002)</ref>. It adapts the PC questions based on intermediate estimates (after each question) of the partworths. These intermediate estimates are based on an OLS regression using the SE and PC responses and ensure that the pairs of profiles are nearly equal in estimated utility (utility balance). Additional constraints restrict the overall design to be nearly orthogonal (features and levels are presented independently) and balanced (features and levels appear with near equal frequency).</p><p>To avoid handicapping the ACA question design in the initial simulations, we simulate the SE responses without adding error. In particular, Sawtooth Software asks for SE responses using a four-point scale, in which the respondent states the relative importance of improving the product from one feature level to another (e.g., adding automatic film ejection to an instant camera). We set the SE responses equal to the true partworths but discretize the answer to match the ACA scale.</p><p>Our code was written using Sawtooth Software's documentation together with e-mail interactions with the company's representatives. We then confirmed the accuracy of the code by asking Sawtooth Software to re-estimate partworths for a small sample of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation Benchmark</head><p>The two estimation methods are the analytic center (AC) method described earlier and hierarchical Bayes (HB) estimation. Hierarchical Bayes estimation uses data from the population to inform the distribution of partworths across respondents and, in doing so, estimates the posterior mean of respondent-level partworths with an algorithm based on Gibbs sampling and the Metropolis Hastings algorithm <ref type="bibr" target="#b0">(Allenby and Rossi 1999</ref><ref type="bibr" target="#b2">, Arora et al. 1998</ref><ref type="bibr" target="#b19">, Johnson 1999</ref><ref type="bibr" target="#b24">, Lenk et al. 1996</ref><ref type="bibr" target="#b25">, Liechty et al. 2001</ref><ref type="bibr">, Sawtooth Software 2001</ref><ref type="bibr" target="#b49">, Yang et al. 2002</ref>. For ACA question design, Sawtooth Software recommends HB as their most accurate sestimation method <ref type="bibr">(Sawtooth Software 2002, p. 11)</ref>. For this initial comparison, for all question-design methods, we use data from the SEs as starting values and we use the SEs to constrain the rank order of the levels for each feature (Sawtooth Software 2001, p. 13). 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Criterion</head><p>To compare the performance of each benchmark we calculate the mean absolute accuracy of the parameter estimates (true versus estimated values averaged across parameters and respondents). We chose TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation to report mean absolute error (MAE) rather than root mean squared error (RMSE) because the former is less sensitive to outliers and is more robust over a variety of induced error distributions <ref type="bibr" target="#b15">(Hoaglin et al. 1983</ref><ref type="bibr" target="#b42">, Tukey 1960</ref>). However, as a practical matter, the qualitative implications of our simulations are the same for both error measures. Indeed, except for a scale change, the results are almost identical for both MAE and RMSE. This is not surprising; for normal distributions the two measures differ only by a factor of (2/ 1/2 . The results are based on the average of five simulations, each with 100 respondents. To reduce unnecessary variance among question-design methods, we first draw the partworths and then use the same partworths to evaluate each question-design method. The use of multiple draws makes the results less sensitive to spurious effects from a single draw.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results of the Initial Monte Carlo Experiments</head><p>We begin with the results obtained from using eight (q = 8) paired comparison questions. This is the type of domain for which polyhedral question design and analytic center estimation were developed (more parameters to estimate than there are questions). Moreover, within this domain there are generally a range of partworths that are feasible, so the polyhedron is not empty. In our simulations the polyhedron contains feasible answers for an average of 7.97 questions when response errors are low. When response errors are high, this average drops to 6.64.</p><p>Table <ref type="table" target="#tab_0">1</ref> reports the MAE in the estimated partworths for a complete crossing of question-design methods, estimation methods, response error, and heterogeneity. The best results (lowest error) in each column are indicated by bold text. In Table <ref type="table" target="#tab_1">2</ref> we reorganize the data to indicate the directional impact of either heterogeneity or response errors on the performance of the question-design and estimation methods. In particular, we average the performance of each question-design method across estimation methods (and vice versa). To indicate the directional effect of heterogeneity we average across response errors (and vice versa).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question-Design Methods</head><p>The findings indicate that when there are only a small number of PC questions, the polyhedral questiondesign method performs well compared to the other three benchmarks. This conclusion holds across the different levels of response error and heterogeneity. The improvement over the random question-design method is reassuring, but perhaps not surprising. The improvement over the fixed method is also not surprising when there are a small number of questions, because it is not possible to achieve the balance and orthogonality goals that the fixed method seeks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOUBIA, SIMESTER, HAUSER, AND DAHAN</head><p>Fast Polyhedral Adaptive Conjoint Estimation The comparison with ACA question design is more interesting. Further investigation reveals that the relatively poor performance of the ACA method can be attributed, in part, to endogeneity bias, resulting from utility balance-the method that ACA uses to adapt questions. To understand this result we first recognize that any adaptive question-design method is potentially subject to endogeneity bias. Specifically, the qth question depends upon the answers to the first q − 1 questions. This means that the qth question depends, in part, on any response errors in the first q − 1 questions. This is a classical problem, which often leads to bias (see, for example, <ref type="bibr">Judge et al. 1985, p. 571)</ref>. Thus, adaptivity represents a tradeoff: We get better estimates more quickly, but with the risk of endogeneity bias. In our simulations, the absolute bias with ACA questions and AC estimation is approximately 6.6% of the mean when averaged across domains. This is statistically significant, in part because of the large sample size in the simulations. Polyhedral question design is also adaptive and it, too, could lead to biases. However, in all four domains, the bias for ACA questions is significantly larger than the bias for polyhedral questions (1.0% on average for AC). The endogeneity bias in ACA questions appears to be from utility-balanced question design; it is not removed with HB estimation.</p><p>Detailed results are available from the authors. <ref type="bibr">7</ref> While further analyses of endogeneity bias are beyond the scope of this paper, they represent an interesting topic for future research. In particular, it might be possible to derive estimation methods that correct for these endogeneity biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation Methods</head><p>For homogeneous populations, hierarchical Bayes consistently performed better than analytic center estimation, irrespective of the question-design method. The performance differences were generally large. Hierarchical Bayes estimation uses populationlevel data to moderate individual estimates. If the population is homogenous, then at the individual level, the ratio of noise to true variation is higher, so moderating this variance through population-level data improves accuracy. However, if the population is heterogeneous, then reliance on population data makes it more difficult to identify the true individuallevel variation, and analytic center estimation does better. For a heterogeneous population, the combination of polyhedral question design and analytic center estimation was significantly more accurate than any other combination of question-design or estimation method (Table <ref type="table" target="#tab_0">1</ref>).</p><p>The findings also suggest that hierarchical Bayes is relatively more accurate when response errors are high, while analytic center estimation is more likely to be favored when response errors are low. The reliance of hierarchical Bayes on population-level data may also explain the role of response errors. If response errors are large, much of the individual-level variance is due to noise. Population-level data are less sensitive to response errors (due to aggregation), so reliance on this data helps to improve accuracy. On the other hand, when response errors are low, the polyhedron stays feasible longer and the analytic center method appears to do a better job of identifying individuallevel variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation</head><p>Additional Paired-Comparison Questions Although polyhedral methods were developed primarily for situations with only a relatively small number of questions, there remain important applications in which a larger number of questions can be asked of each respondent. To examine whether the potential accuracy advantages of polyhedral methods for low q leads to a loss of accuracy at high q, we re-examined the performance of each method after 16 paired-comparison questions (q = 16).</p><p>Recall that the polyhedral method is used to design questions only when the polyhedron contains feasible responses. For low response errors the polyhedron is typically empty after eight questions, while for high response errors this generally occurs at around six or seven questions. Once the polyhedron is empty, we choose questions randomly. Because the polyhedral question-design method is only responsible for around half of the questions we use the label "poly/random."</p><p>The findings are reported in Table <ref type="table" target="#tab_2">3</ref>, which is analogous to Table <ref type="table" target="#tab_1">2</ref>. They reveal the emergence of fixed question-design methods in some domains. Asking a larger number of questions results in more complete coverage of the parameter space. This increases the importance of orthogonality and balance-the criteria used in efficient fixed question design. With more complete coverage, the ability to customize questions to focus on specific regions of the question space becomes less important, mitigating the advantage offered by adaptive techniques.</p><p>However, even after 16 questions there remain domains in which polyhedral question design can improve performance. The poly/random method appears to be at least as accurate as the fixed design when the population is homogenous and/or response errors are high. Its advantage for low q does not seem to be particularly harmful for high q, especially for high response errors. Table <ref type="table" target="#tab_2">3</ref> also suggests that analytic center estimation remains a useful estimation procedure when populations are heterogeneous and/or response errors are low. We note that this result is consistent with <ref type="bibr">Andrews et al. (2002, p. 87)</ref>, who conclude "individual-level models overfit the data." They test OLS rather than the analytic center method and do not test adaptive methods.</p><p>In summary, our Monte Carlo experiments suggest that there are domains in which polyhedral question design and/or analytic center estimation improve the accuracy of conjoint analysis, but there are also domains better served by extant methods. Specifically,</p><p>• Polyhedral question design shows promise for low number of questions, such as the fuzzy frontend of product development and/or Web-based interviewing.</p><p>• For larger numbers of questions, efficient fixed designs appear to be best, but poly/random question design does well, especially when response errors are high and populations are homogenous.</p><p>• Analytic center estimation shows promise for heterogeneous populations and/or low response errors where the advantage of an individual-respondent focus is strongest.</p><p>• Hierarchical Bayes estimation is preferred when populations are more homogeneous and response errors are large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The Role of Self-Explicated Questions</head><p>Hybrid conjoint models refer to methods that combine both compositional methods, such as self-explicated (SE) questions, and decompositional methods, such as metric paired-comparison (PC) questions, to produce new estimates. Although there are instances in TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation which methods that use just one of these data sources outperform or provide equivalent accuracy to hybrid methods, there are many situations and product categories in which hybrid methods improve accuracy (e.g., <ref type="bibr">Green 1984</ref>). An important hybrid from the perspective of evaluating polyhedral methods is ACA-the most widely used method for adaptive metric pairedcomparison questions. While ACA's question design algorithm has remained constant since the late 1970s, its estimation procedures have evolved to address the incommensurability of the SE and PC scales. Its default estimation procedure relies on an ordinary least-squares (OLS) regression that weighs the SE and the PC data in proportion to the number of questions asked (Sawtooth Software 2002, Version 5). <ref type="bibr">8</ref> We label the current version "weighted hybrid" estimation and denote it by the acronym WHSE (the SE suffix indicates reliance on SE data). Sawtooth Software also incorporates the SE responses in their hierarchical Bayes estimation procedure by using the SEs to constrain the estimates of partworths both within a feature and between features to satisfy the ordinal conditions imposed by the SE data. For example, if a respondent's responses to the SE questions indicate that picture quality is more important than battery life, then the hierarchical Bayes parameters are restricted to satisfying this condition. We denote this algorithm with the acronym HBSE to indicate that the SE responses play a larger role in the estimation.</p><p>We also create a polyhedral hybrid by extending AC estimation to incorporate SE responses. To do so, we introduce constraints on the feasible polyhedron similar to those used by HBSE. For example, we impose a condition that picture quality is more important than battery life by using an inequality constraint on the polyhedron to exclude points in the partworth space that breach this condition. When the polyhedron becomes empty, we extend OPT4 to incorporate both the PC and SE constraints. We distinguish this method from the analytic center method by adding a suffix to the acronym: ACSE.</p><p>To compare WHSE, HBSE, and ACSE to their purebred progenitors, we must consider the accuracy of the SE data. If the SE data are perfectly accurate, then a model based on SEs alone will predict perfectly, and the hybrids would be almost as accurate. On the other hand, if the SEs are extremely noisy, then the hybrids may actually predict worse than methods that do not use SE data. To examine these questions, we undertook a second set of simulation experiments.</p><p>To simulate SE responses we assume that respondents' answers to SE questions are unbiased but imprecise. In particular, we simulate response error in the SE answers by adding to the vector of true partworths, u, a vector of independent identically distributed normal error terms with variance 2 se . We simulate two levels of SE response error-low error relative to PC responses ( se = 10) and high error relative to PC responses ( se = 70), truncating the SEs to a 0-to-100 scale. <ref type="bibr">9</ref> We expect that these benchmarks should bound empirical situations. Recall that in the first set of simulations we assumed no SE errors ( se = 0), but discretized the scale. For consistency, we also use a discrete scale when there are nonzero SE errors. Based on these SE errors, we redo the simulations for each level of PC response errors and heterogeneity.</p><p>We summarize the results with Table <ref type="table" target="#tab_3">4</ref> for lower numbers of questions (q = 8), where we report the most accurate question-design/estimation methods for each level of response error and heterogeneity. Detailed results are available from the authors. For ease of comparison, the earlier results (from Table <ref type="table" target="#tab_0">1</ref>) are summarized in the column labeled "Initial Simulations."</p><p>There are three results of interest. First, when the SEs are more accurate than the PCs, then the hybrids do well. In this situation, the PC question-design method matters less: polyhedral, fixed, and random hybrids are not significantly different in accuracy. Second, the insights obtained from Tables <ref type="table" target="#tab_0">1 through 3</ref> TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation for population-level versus individual-level estimation continue to hold: HB or HBSE do well in homogeneous domains while AC or WHSE do well in heterogeneous domains. Third, when the SEs are noisy relative to the PCs, then the hybrid methods do not do as well as the purebred methods. Indeed, we expect a crossover point at some intermediate level of relative accuracy.</p><p>Table <ref type="table" target="#tab_3">4</ref> also highlights the emergence of WHSE in some domains. <ref type="bibr">10</ref> Of the hybrids tested, WHSE is the only method that makes use of the interval-scale properties of the SEs. These metric properties appear to help when the "signal-to-noise ratio" is high (more variation in true partworths, less error in the SEs). This result suggests that other methods which use interval-scaled properties of the SEs should do well in these domains-a topic for further hybrid development (e.g., Ter <ref type="bibr" target="#b39">Hofstede et al. 2002)</ref>.</p><p>In summary, as in biology, where genetically diverse offspring often have traits superior to their purebred parents, heterosis in conjoint analysis improves predictive accuracy in some domains. Furthermore, polyhedral question design remains promising in these domains, and many of the insights from our earlier simulations still hold for hybrid methods. Finally, we expect and obtain analogous 10 If WHSE were not available, polyhedral ACSE is best for low response errors and polyhedral HBSE is best for high response errors. These results suggest that there is room for future research on how best to incorporate SE data with AC estimation. results for larger numbers of questions (q = 16). We could identify no additional insight beyond Tables <ref type="table" target="#tab_0">1  through 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Empirical Application and Test of Polyhedral Methods</head><p>While tests of internal validity are common in the conjoint-analysis literature, tests of external validity at the individual level are rare. <ref type="bibr">11</ref> A search of the literature revealed four studies that predict choices in the context of natural experiments and one study based on a lottery choice. <ref type="bibr" target="#b47">Wittink and Montgomery (1979)</ref>, <ref type="bibr" target="#b38">Srinivasan (1988)</ref>, and <ref type="bibr">Srinivasan and Park (1997)</ref> all use conjoint analysis to predict MBA job choice. Samples of 48, 45, and 96 student subjects, respectively, completed a conjoint questionnaire prior to accepting job offers. The methods were compared on their ability to predict actual job choices. First-preference predictions ranged from 64% to 76% versus randomchoice percentages of 26 to 36%. In another natural experiment, <ref type="bibr" target="#b48">Wright and Kriewall (1980)</ref> used conjoint analysis (Linmap) to predict college applications by 120 families. They were able to correctly predict 20% of the applications when families were prompted to 11 Some researchers report aggregate predictions relative to observed market share. See <ref type="bibr" target="#b3">Bucklin and Srinivasan (1991)</ref>, <ref type="bibr" target="#b4">Currim (1981)</ref>, <ref type="bibr">Green and Srinivasan (1978)</ref>, <ref type="bibr">Griffin and Hauser (1993)</ref>, <ref type="bibr" target="#b14">Hauser and Gaskin (1984)</ref>, <ref type="bibr" target="#b27">McFadden (2000)</ref>, <ref type="bibr" target="#b32">Page and Rosenbaum (1989)</ref>, and <ref type="bibr" target="#b34">Robinson (1980)</ref>.</p><p>TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation think seriously about the features measured in conjoint analysis; 15% when they were not. This converts to a 16% improvement relative to their null model. <ref type="bibr" target="#b23">Leigh et al. (1984)</ref> allocated 122 undergraduate business majors randomly to 12 different conjoint tasks designed to measure partworths for 5 features. Respondents indicated their preferences for 10 calculators offered in a lottery. There were no significant differences among methods with first-preference predictions in the range of 26%-41% and percentage improvements of 28%. The authors also compared the performance of estimates based solely on SE responses and observed similar performance to the conjoint methods.</p><p>In this section, we test polyhedral methods with an empirical test involving an innovative laptopcomputer carrying bag. Our test differs from the natural experiment studies because it is based on a controlled experiment in which we chose pareto sets of product features. At the time of our study, the product was not yet on the market, so respondents had no prior experience with it. The bag includes a range of separable product features, such as the inclusion of a mobile-phone holder, side pockets, or a logo. We focused on nine product features, each with two levels, and included price as a tenth feature. Price is restricted to two levels ($70 and $100)-the extreme prices for the bags in both the internal and external validity tests. We estimated the partworths associated with prices between $70 and $100 by linearly interpolating. A more detailed description of the product features can be found on the Web sites cited earlier in the paper (and in the Acknowledgments).</p><p>Because ACA is the dominant industry method for adaptive question design, we chose a product category where we expected ACA to perform well-a category where separable product features would lead to moderately accurate SE responses. We anticipate that SE responses are more accurate in categories where customers make purchasing decisions about features separately by choosing from a menu of features. In contrast, we expect SE responses to be less accurate for products where the features are typically bundled together, so that customers have little experience in evaluating the importance of the individual features. If polyhedral question design and/or estimation does well in this category, then, based on Table <ref type="table" target="#tab_3">4</ref>, we expect it to do well in categories where SE responses are less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Design</head><p>Subjects were randomly assigned to one of the three conjoint question-design methods: polyhedral (two cells), fixed, or ACA. We omitted random question design because the fixed question-design method dominates random design in Tables <ref type="table" target="#tab_1">2 and 3</ref>. After completing the respective conjoint tasks, all the respondents were presented with the same validation exercises. The internal validation exercise involved four holdout metric paired-comparison (PC) questions, which occurred immediately after the 16 PC questions designed by the respective conjoint methods. The external validation exercise was the selection of a laptop-computer bag from a choice set of five bags. This exercise occurred in the same session as the conjoint tasks and holdout questions, but was separated from these activities by a filler task designed to cleanse memory (see Table <ref type="table" target="#tab_4">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conjoint Tasks</head><p>Recall that ACA requires five sets of questions. Pretests confirmed that all of the features were acceptable to the target market, allowing us to skip the unacceptability task. This left four remaining tasks: ranking of levels within features, self-explicated (SE) questions, metric paired-comparison (PC) questions, and purchase intention (PI) questions. ACA uses the SE questions to select the PC questions, thus the SE questions in ACA must come first, followed by the PC questions and then the PI questions. To test ACA fairly, we adopted this question order for the ACA condition.</p><p>The fixed and polyhedral question design techniques do not require SE or PI questions. Because asking the SE questions first could create a questionorder effect, we asked only PC questions (not the SE or PI questions) prior to the validation task in the fixed condition. To investigate the question-order effect we included two polyhedral data collection procedures: one that matched the fixed design (polyhedral 1) and one that matched ACA (polyhedral 2). In polyhedral 1, only PC questions preceded the validation task; while in polyhedral 2, all the questions preceded the validation task. This enables us to (a) explore whether the SE questions affect the responses to the PC questions and (b) evaluate the hybrid estimation methods that combine data from PC and SE questions. <ref type="bibr">12</ref> The complete research design, including the question order, is summarized in Table <ref type="table" target="#tab_4">5</ref>. Questions associated with the conjoint tasks are highlighted in green (rows 1, 2, and 4), while the validation tasks are highlighted in yellow (rows 3 and 6). The filler task is highlighted in blue (row 5). In this design, polyhedral 1 can be matched with fixed; polyhedral 2 can be matched with ACA.</p><p>Internal Validity Task: Holdout PC Questions Each of the question-design methods designed 16 metric paired-comparison (PC) questions. Following these questions, respondents answered four holdout PC questions-a test used extensively in the literature. The holdout profiles were randomly selected from an independent efficient design of 16 profiles and did not depend on prior answers by that respondent. There was no separation between the 16 initial questions and the four holdout questions, so that respondents were not aware that the questions were serving a different role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filler Task</head><p>The filler task was designed to separate the conjoint tasks and the external validity task. It was hoped that this separation would mitigate any memory effects that might influence how accurately the information from the conjoint tasks predicted which bags respondents chose in the external validity tasks. The filler task was the same in all four experimental conditions and comprised a series of questions asking respondents about their satisfaction with the survey questions. There was no significant difference in the responses to the filler task across the four conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>External Validity Task: Final Bag Selection</head><p>Respondents were told that they had $100 to spend and were asked to choose among five bags. The five bags shown to each respondent were drawn randomly from an orthogonal fractional factorial design of 16 bags. This design was the same across all four experimental conditions, so that there was no difference, on average, in the bags shown to respondents in each condition. The five bags were also independent of responses to the earlier conjoint questions. The price of the bags varied between $70 and $100 reflecting the difference in the anticipated market price of the features included with each bag. By pricing the bags in this manner we ensured that the choice set represented a Pareto frontier, as recommended by <ref type="bibr" target="#b8">Elrod et al. (1992</ref><ref type="bibr">), Green et al. (1988</ref><ref type="bibr" target="#b18">), and Johnson et al. (1989</ref>.</p><p>Respondents were instructed that they would receive the bag they chose. If the bag was priced at less than $100, they were promised cash for the difference. To obtain a complete ranking, we told respondents that if one or more alternatives were unavailable, they might receive a lower ranked bag. The page used to solicit these rankings is presented in Figure <ref type="figure">6</ref>. <ref type="bibr">13</ref> At the end of the study the chosen bags were distributed to respondents together with TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6</head><p>Respondents Choose and Keep a Laptop Computer Bag the cash difference (if any) between the price of the selected bag and $100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Explicated and Purchase Intention Questions</head><p>The self-explicated questions asked respondents to rate the importance of each of the 10 product features. For a fair comparison to ACA, we used the wording for the questions, the (four-point) response scale, and the algorithm for profile selection proposed by Sawtooth Software <ref type="bibr">(1996)</ref>. For the purchase intention questions, respondents were shown six bags, and we asked how likely they were to purchase each bag. We adopted the wording, response scale, and algorithms for profile selection suggested by Sawtooth Software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjects</head><p>The subjects (respondents) were first-year MBA students. They were not informed about the objectives choice design and PC question design and leave such investigations to future research. However, the forced choice design might add noise to the most accurate methods relative to less accurate methods. This would make it more difficult to achieve significant differences and is thus conservative. Pragmatically, we designed the task to maximize the power of the statistical comparisons of the four treatments. The forced choice also helped to reduce the (substantial) cost of this research.</p><p>of the study, nor had they taken a course in which conjoint analysis was taught in detail. We received 330 complete responses (there was one incomplete response) from an e-mail invitation to 360 studentsa response rate of over 91%. Pure random assignment (without quotas) yielded 80 subjects for the ACA condition, 88 for the fixed condition, and 162 for the polyhedral conditions, broken out as 88 for the standard question order (polyhedral 1) and 74 for the alternative question order (polyhedral 2). The questionnaires were pretested on a total of 69 subjects drawn from professional market research and consulting firms, former students, graduate students in operations research, and second-year students in an advanced marketing course that studied conjoint analysis. The pretests were valuable for finetuning the question wording and the Web-based interfaces. By the end of the pretest, respondents found the questions unambiguous and easy to answer. Following standard scientific procedures, the pretest data were not merged with the experimental data. However, analysis of this small sample suggests that the findings agree directionally with those reported here, albeit not at the same level of significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Details</head><p>Figure <ref type="figure">7</ref> illustrates some of the key screens in the conjoint analysis questionnaires. In Figure <ref type="figure">7a</ref> respondents are introduced to the price feature. Figure <ref type="figure">7b</ref> illustrates one of the dichotomous features-the closure on the sleeve. This is an animated screen that provides more detail as respondents move their pointing devices past the picture. Figure <ref type="figure">7c</ref> illustrates one of the PC tasks. Respondents were asked to rate their relative preference for two profiles that varied on three features. Both text and pictures were used to describe the profiles. In the pictures, features that did not vary between the products were chosen to coincide with the respondent's preferences for feature levels obtained in the tasks such as Figure 7b. The format was identical for all four experimental treatments. Finally, Figure <ref type="figure">7d</ref> illustrates the first three self-explicated questions. The full questionnaires for each treatment are available on a Web site cited earlier in this paper. We note that some of these Web site improvements (e.g., dynamically changing pictures) are not standard in Sawtooth Software's implementation; thus, our tests should be considered a test of ACA question design (and estimation) rather than a test of Sawtooth Software's commercial implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results of the Field Test</head><p>To evaluate the conjoint methods we calculated the Spearman rank-order correlation between the actual and observed rankings for the five bags shown to each respondent. <ref type="bibr">14</ref> We report the results in Table <ref type="table" target="#tab_5">6</ref> using the same benchmark methods that we used for the Monte Carlo simulations.</p><p>In the simulation analysis we had the luxury of large sample sizes (500 respondents) and were able to completely control for respondent heterogeneity. Although the sample sizes in Table <ref type="table" target="#tab_5">6</ref> are large compared to previous tests of this type, they are small compared to the simulation analysis. As a result, none of the differences across methods are significant at the 0.05 level in independent-sample t-tests. However, we focus on the correlation measure. There are additional reasons to focus on correlations. First-choice prediction is a dichotomous variable highly dependent upon the number of items in the choice set. In addition, it provides less power because it has higher variance than the Spearman correlation, which is based on a rank order of five items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOUBIA, SIMESTER, HAUSER, AND DAHAN</head><p>Fast Polyhedral Adaptive Conjoint Estimation these independent-sample t-tests do not use all the information available in the data. We also evaluate significance by an alternative method that pools the correlation measures calculated after each additional PC question. This results in a total of sixteen observations for each respondent.</p><p>To control for heteroscedasticity we estimate a separate intercept for each question number. We also controlled for respondent heterogeneity in the randomly assigned conditions with a null model that assumes that the 10 laptop bag features are equally important. If, despite the random assignment of respondents to conditions, the responses in one condition are more consistent with the null model, then the comparisons would be biased in favor of this condition. We control for such potential heterogeneity by including a measure describing how accurately the equal-weights (null) model performs on the predictive correlations. The complete specification for this model is described in Equation ( <ref type="formula">1</ref>), where r indexes the respondents and q indexes the number of PC questions used in the partworth estimates. The s and s are coefficients in the regression and rq is an error term.</p><formula xml:id="formula_3">Correlation rq = 16 q=1 q Question q + M−1 m=1 m Method m + EqualWeights r + rq (1)</formula><p>The Question and Method terms refer to dummy variables identifying the question and method effects.</p><p>The EqualWeight variable measures the correlation obtained for respondent r between the actual rankings and the rankings obtained from an equal weights model. Under this specification, the coefficients represent the expected increase or decrease in this correlation across questions due to method m relative to an arbitrarily chosen base method. Positive (negative) values for the coefficients indicate that the correlations between the actual and predicted rankings are higher (lower) for method m than the base method.</p><p>We further control for potential heteroscedasticity introduced by the panel nature of the data by calculating robust standard errors <ref type="bibr" target="#b46">(White 1980)</ref>. We also estimated a random effects model, but there were almost no differences in the coefficients of interest. Moreover, the Hausman specification test favored the fixed-effects specification. The findings are summarized in Table <ref type="table" target="#tab_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Estimation Methods</head><p>We compare the accuracy of the different estimation methods by comparing the findings in Table <ref type="table" target="#tab_5">6</ref> within a column (for a specific set of questions) and looking to Table <ref type="table" target="#tab_6">7</ref> for significance tests. This comparison holds the question design constant and varies the estimation method. For those experimental cells that were designed to obtain estimates without the SE questions, hierarchical Bayes and analytic center estimation offer similar predictive accuracy for fixed TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation questions, but analytic center estimation performs better for polyhedral questions.</p><p>If SE responses are available, the preferred estimation method appears to depend upon both the question-design method and the number of PC responses used in the estimation. For polyhedral questions HBSE performs extremely well for low numbers of PC questions, perhaps due to its use of population level data. However, increasing the number of PC responses yields less improvement in the accuracy of HBSE relative to WHSE. After 16 questions all three estimation methods converge to comparable accuracy levels, suggesting that there are sufficient data at the individual level to provide estimates that need not depend on population distributions. When using PC questions designed by ACA, WHSE out-performs HBSE, albeit not significantly so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Question-Design Methods</head><p>The findings in Table <ref type="table" target="#tab_5">6</ref> also facilitate comparison of the question-design methods. Comparing across columns (within rows) in Table <ref type="table" target="#tab_5">6</ref> holds the estimation method constant and varies the question design. The findings favor the two conditions in which the polyhedral question design was used. When the SE measures were not collected, the polyhedral question design yielded significantly p &lt; 0 01 more accurate predictions than the fixed design. This holds true irrespective of the estimation method.</p><p>When SE responses were collected, the polyhedral question design was more accurate than ACA across every estimation method, although the difference was not significant for WHSE. Detailed investigation reveals that for every estimation method we tested, the estimates derived using the polyhedral questions outperform the corresponding estimates derived using ACA questions after each and every question number.</p><p>The Incremental Predictive Value of the SE Questions and of PC Questions In this category, Table <ref type="table" target="#tab_5">6</ref> suggests that hybrid methods that use both SE and PC questions consistently outperform methods that rely on PC questions alone. Thus, in this category, the SE questions provide incremental predictive ability. We caution that the product category was chosen at least in part because the SE responses were expected to be accurate. The simulations suggest that this improvement in accuracy may not be true in all domains.</p><p>We also evaluate whether the PC responses contributed incremental accuracy. Predictions that use the SE responses alone (without the PC responses) yield an average correlation with actual choice of 0.64. This is lower than the performance of the best methods that use both SE and PC responses and comparable at q = 16 to those methods that do not use SE responses (see Table <ref type="table" target="#tab_5">6</ref>). We conclude (in this category) that the TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation 16 PC questions provide roughly the same amount of information as the 10 SE questions and that, for methods that use both, the PC data add incremental predictive ability. This conclusion is consistent with previous evidence in the literature <ref type="bibr">(Green et al. 1981</ref><ref type="bibr" target="#b16">, Huber et al. 1993</ref><ref type="bibr" target="#b19">, Johnson 1999</ref><ref type="bibr" target="#b23">, Leigh et al. 1984</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Internal Validity Task</head><p>We repeated the analysis of question design and estimation methods using the correlation measures from the internal validity (holdout questions) task. Details are in Appendix 2. The results for internal validity are similar to the results for external validity. However, there are two differences worth noting. First, while HBSE predicted better than WHSE for polyhedral question design in the choice task, there was no significant difference in the holdout task. Second, while polyhedral question design was significantly better than fixed design for the choice task, there were no significant differences for the holdout task.  <ref type="bibr">(Green et al. 1991</ref><ref type="bibr" target="#b16">, Huber et al. 1993</ref><ref type="bibr" target="#b19">, Johnson 1991</ref>. By comparing the two experimental cells we investigate whether the prior SE questions affected the accuracy of the respondents' PC responses. The predictive accuracy of the two conditions are not statistically different (t = −0 05 for AC estimation, the preferred estimation method from Tables <ref type="table" target="#tab_0">1 and 7</ref>). This suggests that by the sixteenth question any wear out or warm-up/learning had disappeared. However, there might still be an effect for the early questions. When we estimate performance of AC estimation using a version of Equation ( <ref type="formula">1</ref>), the effect is not significant for external validity task (t = 0 68) but is significant for the internal validity task (t = 2 60). In summary, the evidence is mixed. There is no evidence that the SE questions improve or degrade the accuracy of the PC questions for the choice task, but they might improve accuracy for the holdout task. Further testing is warranted. For example, if the first cut of the polyhedron is critical, then it is important that the first PC question be answered as accurately as feasible. The use of SE questions might sensitize the respondent and enhance the accuracy of the first PC question. Alternatively, researchers might investigate other warm-up questions, such as those in which the respondent configures an ideal product <ref type="bibr" target="#b5">(Dahan and Hauser 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of the Field Test</head><p>In the field test, polyhedral question-design appears to be the most accurate of the tested question-design methods. When SE data are available, the most accurate estimation methods were the HBSE and WHSE hybrids. If SE data were unavailable, the most accurate estimation method was AC for polyhedral questions and HB for fixed questions.</p><p>To compare the field test to the simulations, we must identify the relevant domain. Fortunately, estimates of heterogeneity and PC response errors are a by-product of the hierarchical Bayes estimation, and we can use HB to estimate SE errors. These estimates suggest high levels of heterogeneity ( 2 u ≈ 29) and PC response errors ( 2 pc ≈ 43) but moderately low SE response errors ( 2 pc ≈ 18). When SEs are unavailable, the simulations predict that in this domain:</p><p>(1) Polyhedral question design should be better than fixed for both estimation methods, (2) AC should be much better than HB for polyhedral questions, (3) AC should remain better than HB for fixed questions, but the difference is not as large. The significant findings in Table <ref type="table" target="#tab_6">7</ref> are consistent with (1) and (2). Contrary to (3), HB is better for fixed questions, but not significantly so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation</head><p>For accurate SEs, the simulations predict that in this domain: (4) Polyhedral questions will remain strong for hybrid estimation methods, but the differences among question-design methods will be less for hybrid methods than for purebred methods;</p><p>(5) hybrid estimation methods will outperform the purebred methods; and (6) WHSE will outperform ACSE (in the detailed simulation data for this domain HBSE also outperforms ACSE). Predictions (4), ( <ref type="formula">5</ref>), and ( <ref type="formula">6</ref>) hold true in the field test.</p><p>It is always difficult to compare field data to simulations because, despite experimental controls, there may be unobserved phenomena in the field test that are not captured in the simulations. However, the two types of data are remarkably consistent, albeit not perfectly so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Product Launch</head><p>Subsequent to our research, Timbuk2 launched the laptop bags with features similar to those tested including multiple sizes, custom colors, logo options, accessory holders (PDA and cellular phone), mesh pockets, and laptop sleeves. Timbuk2 considers the product a success-It is selling well and is profitable. We now compare the laboratory experiment and the national launch. However, we do so with caution because the goal of the field test was to compare methods rather than to forecast the national launch. By design we used a student sample rather than a national sample, offered only two color combinations, and did not offer the large size bag. Furthermore, one tested feature, the "boot," was not included in the national launch because production cost (and feasibility) exceeded the price that could be justified. One feature, a bicycle strap, was added based on managerial judgment.</p><p>There were five comparable features that appeared in both the field test and the national launch. With the above caveats in mind, the correlation of the predicted feature shares from the conjoint analyses with those observed in the marketplace was 0.9, which was significant. (By feature share we mean percent of customers who chose each of the five features.) Predictions with various null models were not significant. Unfortunately, these data do not provide sufficient power to compare the relative accuracies of the methods nor report correlations to more than one significant figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions and Future Research</head><p>Recent developments in math programming provide new methods for designing metric paired-comparison questions and estimating partworths. The questiondesign method uses a multidimensional polyhedron to characterize feasible parameters and focus questions to reduce the size of the polyhedron as fast as possible. The estimation method uses the analytic center to approximate the center of the polyhedron. This centrality estimate summarizes what is known about the feasible set of parameters. Our goals in this paper were to (1) propose practical algorithms using polyhedral methods, (2) demonstrate their feasibility,</p><p>(3) test their potential in a variety of domains, (4) compare their theoretical accuracy relative to existing methods, and (5) compare their predictive accuracy relative to existing methods in a realistic empirical situation. The field test was designed to match one of the theoretical domains that was favorable to existing methods. The overall conclusion is that polyhedral methods are worth further development, experimentation, and study. Detailed findings are summarized in Table <ref type="table" target="#tab_8">8</ref>. We are encouraged by the performance of polyhedral methods in the Monte Carlo and external-validity experiments. We feel that with further research new algorithms based on polyhedral methods have the potential to become easier to use, more accurate, and applicable in a broader set of domains. We close by highlighting some of the opportunities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Improvements</head><p>Error Theory. Our proposed heuristic (OPT4) provides a practical means by which to use the analytic center to obtain partworth estimates from noisy data. Although the maximum error, , is likely to increase with the number of questions, the mean error, /q, is likely to decrease. Furthermore, the analytic center estimates appear to be unbiased, except when there TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feasibility</head><p>• The polyhedral question-design method can design questions in real time for a realistic number of parameters.</p><p>• The analytic center estimation heuristic yields real time partworth estimates that provide reasonable accuracy with little or no bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monte Carlo Experiments</head><p>• Polyhedral question design shows the most promise for lower numbers of questions where it does well in all tested heterogeneity and response-error domains. • Fixed question design remains best for larger numbers of questions, but poly/random does well for homogeneous populations and high response errors. • AC estimation shows promise for heterogeneous populations and low response errors; HB performs well when the population is homogeneous and response errors are high. • When SE data are available and relatively noisy:</p><p>Polyhedral question design continues to perform well. Purebred estimation methods may be preferred. • When SE data are available and relatively accurate:</p><p>Question design is relatively less important. For homogeneous populations, HBSE is the best estimation method. For heterogeneous populations, WHSE is the best estimation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>External-Validity Experiment</head><p>• The field test domain had high heterogeneity and PC response errors, and moderately low SE response errors. The findings are consistent with the Monte Carlo results in this domain. • Polyhedral question design shows promise irrespective of the availability of SE data. This is true for all tested estimation methods.</p><p>• When SE data are unavailable, analytic center estimation is a viable alternative to hierarchical Bayes estimation, especially when paired with polyhedral question design. • When SE data are available, existing hybrid estimation methods (HBSE and WHSE) appear to outperform the ACSE hybrid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product Launch</head><p>• The laptop bags were launched to the market, but we must be cautious when evaluating predictive accuracy because there were many differences between the empirical experiment and the market launch. In addition, there were insufficient data for relative comparisons. • With these caveats, the conjoint analyses correlate well with the marketplace outcome.</p><p>is endogeneity in question design. However, we have not yet developed a formal proof of either hypothesis. More generally, it might be possible to derive the error-handling heuristics from more-fundamental distributional assumptions.</p><p>Algorithmic Improvements for Question Design. There are a number of ways in which polyhedral question design might be improved. For example, the proposed algorithm reverts to random selection when the polyhedron becomes empty (around q = 8 when p = 10). We might explore algorithms that use relaxed constraints (OPT4) after the initial polyhedron becomes empty. Multistep look-ahead algorithms might yield further improvements.</p><p>Improved Hybrid Estimation. Analytic center estimation appears to do quite well when SE data are not available or when SE data are relatively noisy. However, when the SE data are relatively accurate, existing methods are better. In particular, for some domains a method that directly exploits the metric properties of the SEs seems to be best. We proposed ACSE as a natural way to mix AC estimation with SE constraints, but there might be other methods that make better use of the metric properties of the SEs. Table <ref type="table" target="#tab_3">4</ref> also suggests that SE metric properties might be useful with hierarchical Bayes hybrids, such as the Ter <ref type="bibr" target="#b39">Hofstede et al. (2002)</ref> algorithm.</p><p>Complexity Constraints. <ref type="bibr" target="#b9">Evgeniou et al. (2002)</ref> demonstrate that "support vector machines" (SVM) can improve estimation by automatically balancing complexity of the partworth specification with fit. These researchers are now exploring a hybrid between analytic center and SVM estimation for stated-choice data-an exciting development that can TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation deal with nonlinearities in polyhedral specifications. We might also extend the algorithm in Appendix 1 to incorporate complexity considerations directly either by using relaxed constraints (thick hyperplanes as in Figure <ref type="figure">2b</ref>) or the direct use of OPT4 in the definition of the hyperplanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monte Carlo Experiments</head><p>Addressing the "Why." Our heuristics are designed to obtain better questions by reducing the feasible polyhedron as rapidly as possible. In an analogy to the theory of efficient questions, e.g., D-efficiency, this heuristic focuses the next question on that portion of the parameter space where we have the least information. Future Monte Carlo experiments might test this hypothesis or identify an alternative explanation. Such theory might lead to further improvements in the algorithm.</p><p>Learning and Wear-Out. The comparison of question order (polyhedral 1 versus polyhedral 2) suggests that the SE questions might increase the accuracy of the paired-comparison questions by acting as warmup questions. This is an example of the more general issue that response errors might depend upon q. For example, exploratory simulations, using pc = 1 + 2 q, suggest that learning (positive 2 ) causes mean absolute error (MAE) to decline more rapidly as the number of questions, q, increases. Wear-out (negative 2 ) yields a U-shaped function. Other simulations might explore further many behavioral issues affecting response accuracy <ref type="bibr" target="#b40">(Tourangeau et al. 2000)</ref>.</p><p>Interactions. Analytic center estimation is a byproduct of polyhedral question design. Tables <ref type="table" target="#tab_0">1, 6</ref>, and 7 suggest that there might be interactions among question-design and estimation methods in terms of MAE or predictive ability. Although none of these interactions was significant in our data, interactions are worth further exploration. For example, while AC does well when the SE data are unavailable or relatively noisy, ACSE does not do as well for relatively accurate SE data.</p><p>Other Domains. Our simulations explore heterogeneity, response error, and the number of questions. We made a number of decisions such as the manner in which we specified heterogeneity (normally distributed with mean at the center of the range) and response error (normally distributed with no bias). Initial simulations suggested that the results were not sensitive to these assumptions, but more systematic exploration might identify interesting phenomena that we were not able to explore. Simulations for extremely large p or q might also yield new insight.</p><p>Other Criteria. We chose MAE as our evaluative criteria. Exploratory simulation suggested that root mean squared error (RMSE) appeared to be proportional to MAE. We might also explore other criteria such as predictive accuracy (analogous to Table <ref type="table" target="#tab_5">6</ref>), the dollar value of product features <ref type="bibr">(Jedidi et al. 2003, Ofek and</ref><ref type="bibr" target="#b30">Srinivasan 2002)</ref>, or the incremental explanatory power of each question and question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Tests in Other Categories</head><p>Known Applications. Polyhedral methods are beginning to diffuse. Sawtooth Software, Inc. now offers a polyhedral option to its ACA software, Harris Interactive, Inc. has begun initial testing, and National Family Opinion, Inc. is exploring feasibility. Sawtooth Software has completed an empirical test of internal validity using a poly/ACA question design algorithm <ref type="bibr" target="#b31">(Orme and King 2002)</ref>. In their data, on average, the ACA portion chose 63% of the paired-comparison questions. They observed no significant differences between the methods after q = 30. We have not been able to obtain for their application estimates of heterogeneity, PC response error, SE response error, or performance for low q.</p><p>Other Product Categories. We choose a category with separable features. In this category, the SE data were relatively accurate, and thus the ACA benchmark was not handicapped. This domain matched one of the 3×2×2 classes of categories in Table <ref type="table" target="#tab_3">4</ref>, and the empirical data were consistent with the Monte Carlo data. We hypothesize that the Monte Carlo experiments are consistent with the 11 other classes of categories, but further empirical tests might explore this hypothesis further. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2Respondent's Answers Affect the Feasible Region</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3Choice of Question (2-Dimensional Slice)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) ( b) Notes. (a) Two question-answer pairs. (b) One question, two potential answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4Bounding Ellipsoid and the Analytic Center (2-Dimensions)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5Question Design with a 3-Dimensional Polyhedron</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) related papers on Web-based interviewing methods. The open source code, detailed instructions data and worked examples are also available on the Marketing Science Web site.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 7Example Screens from Questionnaires</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Comparison</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Homogeneous Population</cell><cell cols="2">Heterogeneous Population</cell></row><row><cell></cell><cell></cell><cell>Low Response</cell><cell>High Response</cell><cell>Low Response</cell><cell>High Response</cell></row><row><cell>Question Design</cell><cell>Estimation</cell><cell>Error</cell><cell>Error</cell><cell>Error</cell><cell>Error</cell></row><row><cell>Random</cell><cell>AC</cell><cell>16 5</cell><cell>2 4 1</cell><cell>1 5 9</cell><cell>2 1 7</cell></row><row><cell></cell><cell>HB</cell><cell>8 1</cell><cell>1 0 2</cell><cell>1 9 8</cell><cell>2 2 2</cell></row><row><cell>Efficient fixed</cell><cell>AC</cell><cell>13 7</cell><cell>2 2 9</cell><cell>1 4 3</cell><cell>2 1 0</cell></row><row><cell></cell><cell>HB</cell><cell>7 8  *</cell><cell>10 3</cell><cell>2 0 4</cell><cell>2 2 5</cell></row><row><cell>ACA</cell><cell>AC</cell><cell>14 9</cell><cell>2 4 2</cell><cell>1 6 1</cell><cell>2 2 1</cell></row><row><cell></cell><cell>HB</cell><cell>8 3</cell><cell>9 8  *</cell><cell>23 9</cell><cell>2 2 9</cell></row><row><cell>Polyhedral</cell><cell>AC</cell><cell>10 7</cell><cell>2 0 9</cell><cell>12 5  *</cell><cell>19 7  *</cell></row><row><cell></cell><cell>HB</cell><cell>7 8  *</cell><cell>9 9  *</cell><cell>20 6</cell><cell>2 2 2</cell></row></table><note>of Question-Design and Estimation Methods for q = 8, Mean Absolute Errors Notes. Smaller numbers indicate better performance. * For each column, lowest error or not significantly different from lowest (p &lt; 0 05). All others are significantly different from lowest.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Directional Implications of Response Errors and Heterogeneity for q = 8, Mean Absolute Errors</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Low</cell><cell>Low</cell></row><row><cell></cell><cell cols="4">Homogeneous Heterogeneous Response Response</cell></row><row><cell></cell><cell>Population</cell><cell>Population</cell><cell>Error</cell><cell>Error</cell></row><row><cell>Question design</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell>14 7</cell><cell>1 9 9</cell><cell>1 5 1</cell><cell>1 9 5</cell></row><row><cell>Efficient fixed</cell><cell>13 6</cell><cell>1 9 5</cell><cell>1 4 0</cell><cell>1 9 1</cell></row><row><cell>ACA</cell><cell>14 3</cell><cell>2 1 2</cell><cell>1 5 8</cell><cell>1 9 8</cell></row><row><cell>Polyhedral</cell><cell>12 3  *</cell><cell>18 4  *</cell><cell>12 9  *</cell><cell>18 2  *</cell></row><row><cell>Estimation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AC</cell><cell>18 5</cell><cell>17 9  *</cell><cell>14 3  *</cell><cell>22 1</cell></row><row><cell>HB</cell><cell>9 0  *</cell><cell>21 8</cell><cell>14 6  *</cell><cell>16 2</cell></row></table><note>*Notes. Smaller numbers indicate better performance.* For each column within question design or estimation, lowest error or not significantly different from lowest (p &lt; 0 05). All others are significantly different from lowest.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 Directional</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Low</cell><cell>High</cell></row><row><cell cols="5">Homogeneous Heterogeneous Response Response</cell></row><row><cell></cell><cell>Population</cell><cell>Population</cell><cell>Error</cell><cell>Error</cell></row><row><cell>Question design</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell>12 1</cell><cell>1 4 3</cell><cell>1 0 4</cell><cell>1 5 9</cell></row><row><cell>Efficient fixed</cell><cell>10 3</cell><cell>13 1  *</cell><cell>8 8  *</cell><cell>14 6</cell></row><row><cell>ACA</cell><cell>12 5</cell><cell>1 8 1</cell><cell>1 3 5</cell><cell>1 7 2</cell></row><row><cell>Poly/random</cell><cell>9 2  *</cell><cell>15 2</cell><cell>1 0 4</cell><cell>14 0  *</cell></row><row><cell>Estimation method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AC HB</cell><cell>13 9 8 2  *</cell><cell>12 5  *  17 9</cell><cell>9 9  *  1 1 6</cell><cell>16 4 14 4</cell></row></table><note>Implications of Response Errors and Heterogeneity for q = 16, Mean Absolute Errors * Notes. Smaller numbers indicate better performance.* For each column within question design or estimation, lowest error or not significantly different from lowest (p &lt; 0 05). All others are significantly different from the lowest.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">The Impact of Self-Explicated (SE) Questions (q = 8)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Initial Simulations</cell><cell>Relatively</cell><cell>Relatively</cell></row><row><cell>Heterogeneity</cell><cell>Response Errors</cell><cell>(No SEs)</cell><cell>Accurate SEs</cell><cell>Noisy SEs</cell></row><row><cell>Homogeneous</cell><cell>Low error</cell><cell>Polyhedral HB</cell><cell>Polyhedral HBSE</cell><cell>Polyhedral HB</cell></row><row><cell></cell><cell></cell><cell>Fixed HB</cell><cell>Fixed HBSE</cell><cell>Fixed HB</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Random HBSE</cell><cell></cell></row><row><cell></cell><cell>High error</cell><cell>Polyhedral HB</cell><cell>Polyhedral HBSE</cell><cell>Polyhedral HBSE</cell></row><row><cell></cell><cell></cell><cell>ACA HB</cell><cell>Fixed HBSE</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Random HBSE</cell><cell></cell></row><row><cell>Heterogeneous</cell><cell>Low error</cell><cell>Polyhedral AC</cell><cell>Polyhedral WHSE</cell><cell>Polyhedral AC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fixed WHSE</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Random WHSE</cell><cell></cell></row><row><cell></cell><cell>High error</cell><cell>Polyhedral AC</cell><cell>Polyhedral WHSE</cell><cell>Polyhedral AC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fixed WHSE</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Random WHSE</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Detailed Research Design</cell><cell></cell><cell></cell></row><row><cell>Row</cell><cell>Polyhedral 1</cell><cell>Fixed</cell><cell>Polyhedral 2</cell><cell>ACA</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>Self-explicated</cell><cell>Self-explicated</cell></row><row><cell>2</cell><cell>Polyhedral paired comparison</cell><cell>Fixed paired comparison</cell><cell>Polyhedral paired comparison</cell><cell>ACA paired comparison</cell></row><row><cell>3</cell><cell cols="4">Internal validity task Internal validity task Internal validity task Internal validity task</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell cols="2">Purchase intentions Purchase intentions</cell></row><row><cell>5</cell><cell>Filler task</cell><cell>Filler task</cell><cell>Filler task</cell><cell>Filler task</cell></row><row><cell>6</cell><cell>External validity task</cell><cell>External validity task</cell><cell>External validity task</cell><cell>External validity task</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>External Validity Tests: Correlation with Actual Choice</figDesc><table><row><cell></cell><cell cols="2">After 8 Questions</cell><cell cols="2">After 16 Questions</cell></row><row><cell></cell><cell>Fixed</cell><cell>Polyhedral 1</cell><cell>Fixed</cell><cell>Polyhedral 1</cell></row><row><cell>Methods Without SE Data</cell><cell>Questions</cell><cell>Questions</cell><cell>Questions</cell><cell>Questions</cell></row><row><cell>Analytic center (AC)</cell><cell>0 51</cell><cell>0 59</cell><cell>0 62</cell><cell>0 68</cell></row><row><cell>Hierarchical Bayes (HB)</cell><cell>0 53</cell><cell>0 57</cell><cell>0 61</cell><cell>0 64</cell></row><row><cell>Sample size</cell><cell>88</cell><cell>88</cell><cell>88</cell><cell>88</cell></row><row><cell></cell><cell>ACA</cell><cell>Polyhedral 2</cell><cell>ACA</cell><cell>Polyhedral 2</cell></row><row><cell>Methods That Use SE Data</cell><cell>Questions</cell><cell>Questions</cell><cell>Questions</cell><cell>Questions</cell></row><row><cell>WHSE estimation</cell><cell>0 66</cell><cell>0 68</cell><cell>0 68</cell><cell>0 72</cell></row><row><cell>ACSE estimation</cell><cell>0 63</cell><cell>0 70</cell><cell>0 65</cell><cell>0 71</cell></row><row><cell>HBSE estimation</cell><cell>0 64</cell><cell>0 73</cell><cell>0 65</cell><cell>0 74</cell></row><row><cell>Sample size</cell><cell>80</cell><cell>74</cell><cell>80</cell><cell>74</cell></row><row><cell cols="2">Note. Larger numbers indicate better performance.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>External Validity Tests: Conclusions from the Multivariate Analysis Method m is more accurate than method n but the difference is not significant.</figDesc><table><row><cell></cell><cell>Without SE Questions</cell><cell>With SE Questions</cell></row><row><cell>Comparison of Estimation Methods</cell><cell></cell></row><row><cell>Fixed questions</cell><cell>HB &gt; AC</cell></row><row><cell>Polyhedral 1 questions</cell><cell>AC &gt;&gt; HB</cell></row><row><cell>ACA questions</cell><cell></cell><cell>WHSE &gt; HBSE &gt; ACSE</cell></row><row><cell>Polyhedral 2 questions</cell><cell></cell><cell>HBSE &gt;&gt;&gt; WHSE &gt; ACSE</cell></row><row><cell>Comparison of Question-Design Methods</cell><cell></cell></row><row><cell>AC estimation</cell><cell>Polyhedral 1 &gt;&gt;&gt; fixed</cell></row><row><cell>HB estimation</cell><cell>Polyhedral 1 &gt;&gt;&gt; fixed</cell></row><row><cell>WHSE estimation</cell><cell></cell><cell>Polyhedral 2 &gt; ACA</cell></row><row><cell>ACSE estimation</cell><cell></cell><cell>Polyhedral 2 &gt;&gt; ACA</cell></row><row><cell>HBSE estimation</cell><cell></cell><cell>Polyhedral 2 &gt;&gt;&gt; ACA</cell></row><row><cell>Notes. Method m &gt; Method n:</cell><cell></cell></row></table><note>Method m &gt;&gt; Method n: Method m is significantly more accurate than method n p &lt; 0 05 . Method m &gt;&gt;&gt; Method n: Method m is significantly more accurate than method n p &lt; 0 01 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>Detailed Summary of Findings</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>TOUBIA, SIMESTER, HAUSER, AND DAHANFast Polyhedral Adaptive Conjoint Estimation</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this example, we assume preferential independence which implies an additive utility function. We can handle interactions by relabeling features. For example, a 2 × 2 interaction between two features is equivalent to one four-level feature. We hold to this convention throughout the paper.Marketing Science/Vol. 22, No. 3, Summer 2003</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Marketing Science/Vol. 22, No. 3, Summer 2003</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Technically, the minimax criterion is called the " -norm." To handle least-squares errors we use the "2-norm" and to handle average absolute errors we use the "1-norm." Either is a simple modification to OPT4 in Appendix 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3"> By construction, grows (weakly)  with the number of questions, q, thus a better measure of fit might be mean error, the error divided by the number of questions-an analogy to mean-squared error in regression.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Earlier implementations, including the field test, used ACA question design when the parameter set became infeasible. Further analysis revealed that it is better to switch to random question design than ACA question design when the parameter set becomes infeasible. This makes the performance of polyhedral question design in the field test conservative.Marketing Science/Vol. 22, No. 3, Summer 2003</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Response errors in the literature, often reported as a ratio of error variance to true variance (heterogeneity), vary considerably. In our case, the "respondent's" answer, a, is the difference between the sum of the u f s. Thus the variance of a is a multiple of 2 u . For our situation, the percent errors vary from 8% to 57%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Another version of Sawtooth Software's HB algorithm also uses the SEs to constraint the relative partworths across features. We test this version in our next set of simulations. This enables us to isolate the impact of the paired-comparison question design algorithm.Marketing Science/Vol. 22, No. 3, Summer 2003</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The endogeneity biases persist in most domains for q = 16. In most cases the endogeneity biases for ACA questions are larger than those for polyhedral questions. We have been able to show formally that utility balance leads to bias for OLS, but we have not yet been able to construct proofs for AC or HB. Proofs available from the authors.Marketing Science/Vol. 22, No. 3, Summer 2003</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Earlier versions of ACA either weighed the scales equally (Version 3) or selected weights to fit purchase-intention questions (Version 4).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">For high SE errors truncation is approximately 50%; for low SE errors truncation is approximately 0% and 11%, respectively, for low and high heterogeneity. This is consistent with our manipulation of high versus low information content of the SEs.Marketing Science/Vol. 22, No. 3, Summer 2003</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Although the SE responses are collected in the polyhedral 2 condition, they are not used in analytic center estimation or polyhedral question design. However, they do provide the opportunity to test hybrid estimation methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">We acknowledge two trade-offs in this design. The first is an endowment effect because we endow each respondent with $100. The second is the lack of a "no bag" option. While both are interesting research opportunities and quite relevant to market forecasting, a priori neither should favor one of the three methods relative to the other; we expect no interaction between the endowment/forced-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">As an alternative metric, we compared how well the methods predicted which product the respondents favored. The two metrics provide a similar pattern of results so, for ease of exposition,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge the contribution of Robert M. Freund, who proposed the use of the analytic center and approximating ellipsoids and gave us detailed advice on the application of these methods. This research was supported by the Sloan School of Management and the Center for Innovation in Product Development at MIT. The authors thank Limor Weisberg for creating the graphics used on the Web-based questionnaire, Rob Hardy, Adlar Kim, and Bryant Lin, who assisted in the development of the Web-based questionnaire, and Evan Davidson for documenting and improving the code. Bryan Orme and Richard Johnson at Sawtooth Software provided many useful comments on the empirical test and generously cooperated in providing information about Sawtooth Software's ACA algorithm and verification of our code. The study benefited from comments by numerous pretests at the MIT Operations Research Center, Analysis Group Economics, Applied Marketing Science, the Center for Innovation in Product Development, and Dražen Prelec's market measurement class. Our thanks to Brennan Mulligan of Timbuk2 Designs, Inc. This paper has benefited from presentations at the CIPD Spring Research Review, the Epoch Foundation Workshop, the Marketing Science Conference in Wiesbaden Germany, the MIT ILP Symposium on Managing Corporate Innovation, the MIT Marketing Workshop, the MIT Operations Research Seminar Series, the MSI Young Scholars Conference, the New England Marketing Conference, and the Stanford Marketing Workshop. Open source code and data are available at mitsloan.mit.edu/vc and at mktsci.pubs.informs.org. Demos are available at mitsloan.mit.edu.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 1: Mathematics of Fast Polyhedral Adaptive Conjoint Estimation</head><p>Consider the case of p parameters and q questions where q ≤ p. Let u j be the jth parameter of the respondent's partworth function and let u be the p × 1 vector of parameters. Without loss of generality we assume binary features such that u j is the high level of the jth feature and constrain their values between 0 and 100. For more levels we simply recode the u vector and impose constraints such as u m ≤ u h . We handle such inequality constraints by adding slack variables, v hm ≥ 0, such that u h = u m + v hm . Let r be the number of externally imposed constraints, of which r ≤ r are inequality constraints.</p><p>Let z il be the 1 × p vector describing the left-hand profile in the ith paired-comparison question and let z ir be the 1 × p vector describing the right-hand profile. The elements of these vectors are binary indicators taking on the values 0 or 1. Let X be the q × p matrix of x i = z il − z ir for i = 1 to q. Let a i be the respondent's answer to the ith question and let a be the q × 1 vector of answers for i = 1 to q. Then, if there were no errors, the respondent's answers imply X u = a. To handle additional constraints, we augment these equations such that X becomes a q + r × p + r matrix, a becomes a q + r × 1 vector, and u becomes a p + r × 1 vector. These augmented relationships form a polyhedron, P = u ∈ p+r X u = a u ≥ 0 . We begin by assuming that P is nonempty, that X is full-rank, and that no j exists such that u j = 0 for all u in P. We later indicate how to handle these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finding an Interior Point of the Polyhedron</head><p>To begin the algorithm we first find a feasible interior point of P by solving a linear program, (LP1) <ref type="bibr" target="#b10">(Freund et al. 1985)</ref>. Let e be a p + r × 1 vector of 1s and let 0 be a p + r × 1 vector of 0s; the y j s and are parameters of (LP1) and y is the p + r × 1 vector of the y j s. (When clear in context, inequalities applied to vectors apply for each element.) (LP1) is given by</p><p>If u * y * * solves (LP1), then * −1 u * is an interior point of P whenever y * &gt; 0. If there are some y j s equal to 0, then there are some js for which u j = 0 for all u ∈ P. If LP1 is infeasible, then P is empty. We address these cases later in this appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finding the Analytic Center</head><p>The analytic center is the point in P that maximizes the geometric mean of the distances from the point to the faces of P. We find the analytic center by solving (OPT1). max p+r j=1 ln u j subject to X u = a u &gt; 0 (OPT1) <ref type="bibr" target="#b10">Freund (1993)</ref> proves with projective methods that a form of Newton's method will converge rapidly for (OPT1). To implement Newton's method, we begin with the feasible point from (LP1) and improve it with a scalar, , and a direction, d, such that u + d is close to the optimal solution of (OPT1). ( d is a p + r × 1 vector of d j s.) We then iterate subject to a stopping rule.</p><p>We first approximate the objective function with a quadratic expansion in the neighborhood of u.</p><p>If we define U as a p + r × p + r diagonal matrix of the u j s, then the optimal direction solves (OPT2):</p><p>Newton's method solves (OPT1) quickly by exploiting an analytic solution to (OPT2). To see this, consider first the Karush-Kuhn-Tucker (KKT) conditions for (OPT2). If z is a p + r × 1 vector parameter of the KKT conditions that is unconstrained in sign, then the KKT conditions are written as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOUBIA, SIMESTER, HAUSER, AND DAHAN Fast Polyhedral Adaptive Conjoint Estimation</head><p>Multiplying (A2) on the left by XU 2 gives X d − XU e = XU 2 X T z. Applying (A3) to this equation gives: −XU e = XU 2 X T z. Because U e = u and since X u = a, we have − a = XU 2 X T z. Because X is full rank and U is positive, we invert XU 2 X T to obtain z = − XU 2 X T −1 a. Now replace z in (A2) by this expression and multiply by U 2 to obtain d = u − U 2 X T XU 2 X T −1 a. According to Newton's method, the new estimate of the analytic center, u , is given by u = u + d = U e + U −1 d . There are two cases for . If U −1 d &lt; 1/4, then we use = 1 because u is already close to optimal and e + U −1 d &gt; 0. Otherwise, we compute with a line search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Special Cases</head><p>If X is not full rank, XU 2 X T might not invert. We can either select questions such that X is full rank or we can make it so by removing redundant rows. Suppose that x k is a row of X such that</p><p>q+r i=1 i =k i a i , then P is empty and we employ (OPT4) described later in this appendix.</p><p>If in (LP1) we detect cases where some y j s = 0, then there are some js for which u j = 0 for all u ∈ P. In the later case, we can still find the analytic center of the remaining polyhedron by removing those js and setting u j = 0 for those indices. If P is empty we employ (OPT4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finding the Ellipsoid and Its Longest Axis</head><p>If ū is the analytic center and U is the corresponding diagonal matrix, then <ref type="bibr">Sonnevend (1985a, b)</ref> demonstrates that E ⊆ P ⊆ E p+r , where E = u X u = a u − ū T U −2 u − ū ≤ 1 and E p+r is constructed proportional to E by replacing 1 with p + r . Because we are interested only in the direction of the longest axis of the ellipsoids we can work with the simpler of the proportional ellipsoids, E. Let g = u − ū, then the longest axis will be a solution to (OPT3).</p><p>) has an easy-to-compute solution based on the eigenstructure of a matrix. To see this we begin with the KKT conditions (where and are parameters of the conditions)</p><p>It is clear that g T U −2 g = 1 at optimal, else we could multiply g by a scalar greater than 1 and still have g feasible. It is likewise clear that is strictly positive, else we obtain a contradiction by leftmultiplying (A4) by g T and using X g = 0 to obtain g T g = 0, which contradicts g T U −2 g = 1. Thus, the solution to (OPT3) must satisfy g = U −2 g + X T g T U −2 g = 1 X g = 0, and &gt; 0. We rewrite (A4)-(A6) by letting I be the identify matrix and defining = 1/ and = − / .</p><p>We left-multiply (A7) by X and use (A9) to obtain X U −2 g = XX T . Because X is full rank, XX T is invertible, and we obtain = XX T −1 X U −2 g, which we substitute into (A7) to obtain U −2 − X T XX T −1 X U −2 g = g. Thus, the solution to (OPT3) must be an eigenvector of the matrix, M ≡ U −2 − X T XX T −1 X U −2 . To find out which eigenvector, we left-multiply (A7) by g T and use (A8) and (A9) to obtain g T g = 1, or g T g = 1/ where &gt; 0. Thus, to solve (OPT3) we maximize 1/ by selecting the smallest positive eigenvalue of M. The direction of the longest axis is then given by the associated eigenvector of M. We then choose the next question such that x q+1 is most nearly collinear to this eigenvector subject any constraints imposed by the questionnaire design. (For example, in our simulation we require that the elements of x q+1 be −1, 0, or 1.) The answer to x q+1 defines a hyperplane orthogonal to x q+1 .</p><p>We need only establish that the eigenvalues of M are real. To do this we recognize that M = P U −2 where P = I − X T XX T −1 X is symmetric, i.e., P = P T Then if is an eigenvalue of M det P U −2 − I = 0, which implies that det U U −1 P U −1 − I U −1 = 0. This implies that is an eigenvalue of U −1 P U −1 , which is symmetric. Thus, is real <ref type="bibr">(Hadley 1961, p. 240)</ref>.</p><p>Adjusting the Polyhedron so That It Is Nonempty P will remain nonempty as long as respondents' answers are consistent. However, in any real situation there is likely to be q &lt; p such that P is empty. To continue the polyhedral algorithm, we adjust P so that it is nonempty. We do this by replacing the equality constraint, X u = a, with two inequality constraints, X u ≤ a + and X u ≥ a− , where is a q ×1 vector of errors, i , defined only for the question-answer imposed constraints. We solve the following optimization problem. Our current implementation uses the -norm, where we minimize the maximum i , but other norms are possible. The advantage of using the -norm is that (OPT4) is solvable as a linear program.</p><p>At some point such that q &gt; p, extant algorithms will outperform (OPT4) and we can switch to those algorithms. Alternatively, a researcher might choose to switch to constrained regression (norm-2) or mean-absolute error (norm-1) when q &gt; p. Other options include replacing some, but not all, of the equality constraints with inequality constraints. We leave these extensions to future research.   <ref type="bibr">Hauser. 1993</ref>. The voice of the customer.</p><p>Marketing Sci. 12(1) 1-27.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marketing models of consumer heterogeneity</title>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econometrics</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="57" to="78" />
			<date type="published" when="1999-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An empirical comparison of logit choice models with discrete versus continuous representations of heterogeneity</title>
		<author>
			<persName><forename type="first">Rick</forename><forename type="middle">L</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><surname>Currim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="479" to="487" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving parameter estimates and model prediction by aggregate customization in choice experiments</title>
		<author>
			<persName><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="283" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Marketing Sci.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Determining interbrand substitutability through survey measurement of consumer preference structures</title>
		<author>
			<persName><forename type="first">Randolph</forename><forename type="middle">E</forename><surname>Bucklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="58" to="71" />
			<date type="published" when="1991-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using segmentation approaches for better prediction and understanding from consumer mode choice models</title>
		<author>
			<persName><forename type="first">Imran</forename><forename type="middle">S</forename><surname>Currim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="301" to="309" />
			<date type="published" when="1981-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The virtual customer</title>
		<author>
			<persName><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Ely</surname></persName>
		</author>
		<author>
			<persName><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Product Innovation Management</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="332" to="354" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Linear models in decision making</title>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">M</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Corrigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="95" to="106" />
			<date type="published" when="1974-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Use of nonlinear, noncompensatory models as a function of task and amount of information</title>
		<author>
			<persName><forename type="first">Hillel</forename><forename type="middle">J</forename><surname>Einhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organ. Behavior Human Performance</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical comparison of ratings-based and choice-based conjoint models</title>
		<author>
			<persName><forename type="first">Terry</forename><surname>Elrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnakumar</forename><forename type="middle">S</forename><surname>Davey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="368" to="377" />
			<date type="published" when="1992-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generalized robust conjoint estimation. Working paper, INSEAD</title>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Boussios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Zacharia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Fontainebleau, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Projective transformations for interior-point algorithms, and a superlinearly convergent algorithm for the W-center problem</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Roundy</surname></persName>
		</author>
		<author>
			<persName><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="385" to="414" />
			<date type="published" when="1985" />
		</imprint>
		<respStmt>
			<orgName>MIT Sloan School of Management</orgName>
		</respStmt>
	</monogr>
	<note>Identifying the set of alwaysactive constraints in a system of linear inequalities by a single linear program. WP 1674-85</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Simester</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><surname>Dahan</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Fast Polyhedral Adaptive Conjoint Estimation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computational complexity of inner and outer J-radii of polytopes in finite-dimensional normed spaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gritzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Klee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="213" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Linear Algebra</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hadley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961" />
			<publisher>Addison-Wesley Publishing Company, Inc</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Application of the &quot;Defender&quot; consumer model</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><surname>Gaskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Marketing Research: Progress and Prospects. Forthcoming. , Steven M. Shugan</title>
				<imprint>
			<date type="published" when="1980" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="278" to="320" />
		</imprint>
	</monogr>
	<note>Oper. Res.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Understanding Robust and Exploratory Data Analysis</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Hoaglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Mosteller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The effectiveness of alternative preference elicitation procedures in predicting choice</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Wittink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="114" />
			<date type="published" when="1975-08" />
		</imprint>
	</monogr>
	<note>J. Marketing Res.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measuring heterogeneous reservation prices for product bundles</title>
		<author>
			<persName><forename type="first">Kamel</forename><surname>Jedidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Manchanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Jagpal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Forthcoming</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When choice models fail: Compensatory models in negatively correlated environments</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="270" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The joys and sorrows of implementing HB methods for conjoint analysis</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Sequim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sawtooth Software</title>
				<meeting><address><addrLine>Sequim, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="223" to="225" />
		</imprint>
	</monogr>
	<note>Adaptive conjoint analysis: Some caveats and suggestions. Working paper, Sawtooth Software</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Theory and Practice of Econometrics</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lutkepohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>John Wiley and Sons</publisher>
			<biblScope unit="page">571</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new polynomial time algorithm for linear programming</title>
		<author>
			<persName><forename type="first">N</forename><surname>Karmarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="373" to="395" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient experimental design with marketing research applications</title>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">F</forename><surname>Kuhfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName><surname>Garratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reliability and validity of conjoint analysis and self-explicated weights: A comparison</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">O</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="456" to="462" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical Bayes conjoint analysis: Recovery of partworth heterogeneity from reduced experimental designs</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Desarbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="191" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Choice-Menus for mass customization: An experimental approach for analyzing customer demand with an application to a Webbased information service</title>
		<author>
			<persName><forename type="first">John</forename><surname>Liechty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatram</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="196" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Stated Choice Methods: Analysis and Application</title>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">J</forename><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Hensher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joffre</surname></persName>
		</author>
		<author>
			<persName><surname>Swait</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Disaggregate behavioral travel demand&apos;s RUM side: A thirty-year retrospective. Working paper</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcfadden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Measuring preferences with hybrid conjoint analysis: The impact of a different number of attributes in the master design</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bus. Res</title>
		<imprint>
			<biblScope unit="page" from="261" to="274" />
			<date type="published" when="1988" />
		</imprint>
		<respStmt>
			<orgName>University of Utah</orgName>
		</respStmt>
	</monogr>
	<note>A cross-validity comparison of conjoint analysis and choice models</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Interior-point polynomial algorithms in convex programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovskii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>SIAM, Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How much does the market value an improvement in a product attribute</title>
		<author>
			<persName><forename type="first">Elie</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="98" to="411" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving ACA algorithms: Challenging a twenty-year-old approach</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Orme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cbc</forename><surname>Aca</surname></persName>
		</author>
		<author>
			<persName><surname>Sequim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Christopher</forename><surname>Wa</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Effective strategies for conjoint research. Working paper, Sawtooth Software</title>
				<meeting><address><addrLine>Vail, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Redesigning product lines with conjoint analysis: How Sunbeam does it</title>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><surname>Rosenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Product Innovation Management</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="120" to="137" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conjoint analysis reliability: Empirical findings</title>
		<author>
			<persName><surname>Reibstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">E G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bateson</surname></persName>
		</author>
		<author>
			<persName><surname>Boulding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="286" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Applications of conjoint analysis to pricing problems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Market Measurement and Analysis: Proc. 1979 ORSA/TIMS Conf. Marketing. Marketing Science Institute</title>
				<editor>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Montgomery</surname></persName>
			<persName><forename type="first">Dick</forename><surname>Wittink</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="page" from="183" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Profile construction in experimental choice designs for mixed logit models</title>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Sandor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Wedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="398" to="411" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Marketing Sci.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The ACA/hierarchical Bayes technical paper</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Sequim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Sequim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACA system: Adaptive conjoint analysis. ACA Manual</title>
				<meeting><address><addrLine>Sequim, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Sawtooth Software, Inc</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>ACA 5.0 Technical paper</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new method for solving a set of linear (convex) inequalities and its applications for identification and optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sonnevend</surname></persName>
		</author>
		<author>
			<persName><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><surname>Simester</surname></persName>
		</author>
		<author>
			<persName><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><surname>Dahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th IFIP Conf. System Modeling and Optim</title>
				<meeting>12th IFIP Conf. System Modeling and Optim<address><addrLine>Budapest, Hungary; Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
		<respStmt>
			<orgName>Department of Numerical Analysis, Institute of Mathematics, Eötvös University ; Fast Polyhedral Adaptive Conjoint Estimation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>An &quot;analytic&quot; center for polyhedrons and new classes of global algorithms for linear (smooth, convex) programming</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Surprising robustness of the selfexplicated approach to customer preference structure measurement</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="369" />
			<date type="published" when="1973" />
			<publisher>Allan D. Shocker</publisher>
		</imprint>
	</monogr>
	<note>Psychometrika</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bayesian prediction in hybrid conjoint analysis</title>
		<author>
			<persName><forename type="first">Ter</forename><surname>Hofstede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngchan</forename><surname>Frenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Wedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="253" to="261" />
			<date type="published" when="2002-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The Psychology of Survey Response</title>
		<author>
			<persName><surname>Tourangeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">J</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Rips</surname></persName>
		</author>
		<author>
			<persName><surname>Rasinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="197" to="229" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Polyhedral methods for adaptive choice-based conjoint analysis</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res. Forthcoming</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey of sampling from contaminated distributions</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to Probability and Statistics</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Olkin</surname></persName>
			<persName><forename type="first">S</forename><surname>Ghurye</surname></persName>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
			<persName><forename type="first">W</forename><surname>Madow</surname></persName>
			<persName><forename type="first">H</forename><surname>Mann</surname></persName>
		</editor>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Stanford University Press</publisher>
			<date type="published" when="1960" />
			<biblScope unit="page" from="448" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pre-test market models: Validation and managerial implications</title>
		<author>
			<persName><forename type="first">Glen</forename><forename type="middle">L</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerald</surname></persName>
		</author>
		<author>
			<persName><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="221" to="234" />
			<date type="published" when="1983-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A locally well-behaved potential function and a simple Newton-type method for finding the center of a polytope</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vaidja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Progress in Mathematical Programming: Interior Points and Related Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="79" to="90" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity</title>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="817" to="838" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Philippe Cattin. 1981. Alternative estimation methods for conjoint analysis: A Monte Carlo study</title>
		<author>
			<persName><forename type="first">Dick</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Montgomery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Proc. 1979 AMA. American Marketing Association</title>
				<editor>
			<persName><forename type="first">Neil</forename><surname>Beckwith</surname></persName>
		</editor>
		<meeting><address><addrLine>Chicago, IL.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1979-02" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="101" to="106" />
		</imprint>
	</monogr>
	<note>Predictive validity of trade-off analysis for alternative segmentation schemes</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">State-of-mind effects on accuracy with which utility functions predict marketplace utility</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Kriewall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="277" to="293" />
			<date type="published" when="1980-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Modeling variation in brand preference: The roles of objective environment and motivating conditions</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geraldine</forename><surname>Fennell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="31" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
