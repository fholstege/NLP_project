<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Critical Condition: People Don&apos;t Dislike a Corporate Experiment More Than They Dislike Its Worst Condition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-23">December 23, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><surname>Mislavsky</surname></persName>
							<email>mislavsky@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Carey Business School</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21202</postCode>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Berkeley</forename><surname>Dietvorst</surname></persName>
							<email>berkeley.dietvorst@chicagobooth.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Booth School of Business</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Uri</forename><surname>Simonsohn</surname></persName>
							<email>uri.simonsohn@esade.edu</email>
							<affiliation key="aff2">
								<orgName type="department">ESADE</orgName>
								<orgName type="institution">Ramon Llull University</orgName>
								<address>
									<postCode>08022</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Wharton Behavioral Laboratory, and the University of Chicago Booth School of Business for fi-nancial support</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Critical Condition: People Don&apos;t Dislike a Corporate Experiment More Than They Dislike Its Worst Condition</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2019-12-23">December 23, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2019.1166</idno>
					<note type="submission">Received: October 25, 2016 Revised: October 13, 2017; August 23, 2018; January 21, 2019 Accepted: February 11, 2019 History: This paper has been accepted for the Marketing Science Special Issue on Field Experiments.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>field experiments</term>
					<term>public opinion</term>
					<term>market research</term>
					<term>business ethics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In June 2014, the Proceedings of the National Academy of Sciences published an article describing the results of a field experiment in which academic authors <ref type="bibr" target="#b14">(Kramer et al. 2014</ref>) partnered with Facebook to manipulate content users saw (i.e., "News Feeds"), showing either more positive or more negative emotional content to measure potential emotional contagion. A month later, the online dating site OkCupid published a blog post titled "We Experiment on Human Beings," which described three experiments it had run on users <ref type="bibr" target="#b24">(Rudder 2014)</ref>. Reaction to the revelation of these experiments was swift and highly negative.</p><p>The backlash the Facebook and OkCupid experiments received, described by a Forbes contributor as "one epic freak out" <ref type="bibr" target="#b21">(Muse 2014)</ref>, dominated several news cycles despite competing for attention with the 2014 World Cup and major U.S. Supreme Court rulings. Articles describing the negative reaction to the Facebook experiment reached the front page of the Wall Street Journal and were the number one most popular/shared articles on several news outlets, including The Atlantic, the Wall Street Journal, and the BBC. <ref type="bibr">1</ref> Articles on CNN.com and in the New York Times proclaimed that Facebook treated users like "lab rats" <ref type="bibr" target="#b9">(Goel 2014</ref><ref type="bibr" target="#b10">, Goldman 2014</ref>). When the OkCupid experiments were revealed, an article in FastCompany declared that the experiment was "way creepier" than Facebook's <ref type="bibr" target="#b11">(Greenfield 2014)</ref>. Even legislators got involved, calling for investigations into datacollection practices <ref type="bibr" target="#b17">(Meyer 2014</ref><ref type="bibr" target="#b28">, Stampler 2014</ref>. A few months later, Facebook's chief technology officer formally acknowledged that the company was "unprepared" for the reaction elicited by the experiments and admitted that it "should have considered nonexperimental ways" to conduct research on the topic <ref type="bibr" target="#b25">(Schroepfer 2014)</ref>.</p><p>In this paper, we present evidence suggesting that the backlash to these experiments was likely driven by the specific policies that these experiments contained (i.e., the individual treatment arms) rather than the fact that the policies were implemented as part of an experiment. As a result, we posit that reactions would have been at least as negative if these treatments were implemented as stand-alone policy changes outside of an experimental context. We conclude that marketing researchers and organizational decision makers face similar scrutiny for running experiments as they do for implementing policy changes. Thus, we propose that organizations do not face more backlash when they implement objectionable policies as part of an experiment. Similarly, we propose that implementing objectionable policies outside of an experiment does not make them more palatable to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Field Experiments and Marketing Science</head><p>Experimentation provides an unrivalled source of actionable intelligence for businesses, governments, and nonprofit organizations <ref type="bibr" target="#b32">(Zoumpoulis et al. 2015)</ref>, allowing researchers to identify the causal effects that alternative policies have on behavior. 2 Field experiments overcome the lower external validity of stylized laboratory experiments by taking place in the precise environment in which specific policy changes will occur <ref type="bibr" target="#b5">(DellaVigna 2009)</ref>. In part because of these advantages, field experimentation has become a popular tool for marketing scholars that is used to test and complement existing theory as well as to develop new insights into buyer behavior on wide-ranging topics. In this journal alone, field experiments have been used to explore charitable giving behavior <ref type="bibr" target="#b29">(Sudhir et al. 2016)</ref>, the effect of social influence on the adoption of new technologies <ref type="bibr" target="#b18">(Miller and Mobarak 2014)</ref>, strategies for inducing multichannel buying <ref type="bibr" target="#b20">(Montaguti et al. 2015)</ref>, and consumer purchasing habits after the end of a promotion <ref type="bibr" target="#b30">(Wang et al. 2016)</ref>.</p><p>Given the value of field experimentation, concerns about its acceptability must be taken seriously. Many pundits and scholars have interpreted the backlash to well-known field experiments as evidence that people have a broad and substantial aversion to experimentation. <ref type="bibr" target="#b8">Gino (2015)</ref>, for instance, proposed that managers are hesitant to run experiments within their own organizations in part because they believe that customers and employees do not want to be experimented on. <ref type="bibr" target="#b13">Hill (2014)</ref> found that companies that do run experiments often resort to using terms such as "diagnostic test" or "A/B test" to avoid presumed negative associations with experimentation (see also <ref type="bibr" target="#b15">Luca 2014)</ref>. <ref type="bibr">Meyer (2015, p. 278</ref>) stated that people view field experiments as "more morally suspicious than an immediate, universal implementation of an untested practice" and titled this preference the "A/B illusion."</p><p>If consumers are indeed averse to experimentation, it would constitute an important barrier to evidencebased marketing and to future collaborations between academics and organizations. Organizational decision makers may hesitate to run or publicize the results of experiments for fear of negative publicity, and customers may fear engaging with companies that they believe will experiment on them. In this article, we investigate whether such an aversion to experimentation exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Three Forms of Experiment Evaluation</head><p>We define three different forms that experiment evaluation could take and then preview our ability to empirically distinguish among them in this article:</p><p>1. Absolute experiment aversion: All experiments are deemed unacceptable, independent of the policies they include.</p><p>2. Relative experiment aversion: An experiment is less acceptable than the policies it contains because either experimentation is a negative attribute (i.e., a negative main effect of experimentation) or the underlying policies are deemed less acceptable when they are part of an experiment (i.e., an interaction between experimentation and policy acceptability). This means that experiments with acceptable policies could still be considered acceptable in absolute terms but less acceptable than their underlying policies. Similarly, an experiment with an unacceptable policy could be viewed more negatively than the unacceptable policy on its own.</p><p>3. Critical condition: There is no experiment aversion. The acceptability of an experiment is instead a weighted average of the acceptability of its policies. Most importantly, this implies an experiment is no less acceptable than its least acceptable policy. Therefore, if an experiment is viewed negatively, it is only because one (or more) of its conditions (i.e., its "critical" condition) is viewed negatively and not because experimentation is a negative attribute per se. People find an experiment that contains only acceptable conditions to be acceptable.</p><p>In Studies 1 and 2, we test for absolute experiment aversion and find several instances in which experiments are, in fact, rated positively. Thus, we reject absolute experiment aversion. In Studies 3 and 4, we directly pit the acceptability of experiments against the acceptability of their underlying policies. Consistent with the critical condition account of experiment evaluation, we find that experiments are rated as no less acceptable than their least acceptable policies. Experiments, however, were also rated as less acceptable than the simple average acceptability of their underlying policies. This may reflect either moderate relative experiment aversion or negativity bias, with which people give more weight to negative attributes than to positive ones (e.g., <ref type="bibr" target="#b26">Skowronski and Carlston 1989</ref><ref type="bibr" target="#b6">, Folkes and Kamins 1999</ref><ref type="bibr" target="#b23">, Rozin and Royzman 2001</ref>. In Study 5, we tease these two apart by asking participants to evaluate experiments with two positive policies that are similarly acceptable (and with which negativity bias should be absent). We find no evidence of even modest experiment aversion. In Study 6, we also include two similarly negative arms, again finding no evidence of relative experiment aversion. In sum, our evidence is inconsistent with both absolute and relative experiment aversion and consistent with the critical condition account of experiment aversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparent Reporting</head><p>In all six studies, participants read scenarios describing an action that a company could take (either <ref type="bibr">Mislavsky, Dietvorst, and Simonsohn: Critical Condition Marketing Science, 2020</ref><ref type="bibr">, vol. 39, no. 6, pp. 1092</ref><ref type="bibr">-1104</ref> an experiment or a universal policy change) and indicated how acceptable each action is. We ran all studies, except for Studies 3b and 6, on Amazon's Mechanical Turk (MTurk) using Qualtrics. Study 3b was a pen-and-paper survey of nonacademic university staff. Study 6 was run in collaboration with Lucid (http://luc.id), a market research firm.</p><p>Study materials, data, analysis code, and supplements for all studies as well as preregistrations for Studies 3b-6 are available at http://osf.io/z39aq. We report studies in the order they were conducted (except for Study 3b, which was added at the request of reviewers and conducted after Study 4) and discuss all additional studies conducted but not reported in the paper in Supplements 6 and 7. For all studies, we determined sample size before beginning data collection. <ref type="bibr">3</ref> We report all data exclusions, all manipulations, and all measures.</p><p>Study 1: People Do Find (Some) Experiments Acceptable</p><p>Our first study tests for absolute experiment aversion: people always object to experiments even if all conditions are unambiguously beneficial. We presented participants with descriptions of corporate experiments that contained unambiguously positive conditions (e.g., giving $5 to employees for visiting the gym) or unambiguously negative conditions (e.g., taking $5 from employees for not visiting the gym). If absolute experiment aversion exists, participants should find all experiments objectionable. If experiments are instead evaluated based on their conditions, participants should only object to experiments that contain unambiguously negative conditions. Throughout these scenarios, we also added various aspects of experimentation that may contribute to experiment aversion, such as deception and lack of consent. If these specific features cause experiment aversion, participants should view these experiments negatively even if they have only unambiguously positive conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sample. We recruited 577 participants on MTurk, of which 505 successfully passed the attention check (37.5% female, M age = 34.1 years). Participants were paid $0.75 for completing the study.</p><p>Design. Participants were assigned to one of 10 experimental conditions. Fifty-three participants were assigned to the policy change condition. The remaining participants (n = 452) were assigned to one of nine experiment conditions.</p><p>Participants in the policy change condition read descriptions of nine possible policy changes. These involved bad, good, or very good outcomes in three different contexts. See Table <ref type="table" target="#tab_0">1</ref>. Participants evaluated all nine policies in random order, answering three questions about their acceptability. We average them (Cronbach's α = 0.96) to construct the "policy acceptability index." These ratings served as a manipulation check for our stimuli in the experiment conditions.</p><p>Participants in the nine experiment conditions read one scenario about a company running an experiment that randomly assigned employees/customers to one of two policy changes from one of the three contexts in Table <ref type="table" target="#tab_0">1</ref>. The condition pairs were bad/good, control/ good, or good/very good. For example, the shipping control/good scenario read as follows:</p><p>A shopping company runs an experiment on their shipping system where one group of customers is randomly picked and the company starts upgrading all "Standard 5-day" shipped packages to "Priority 3day" shipping (without changing the cost to the customer). Another group of customers is randomly picked and gets no change in their shipping. The company will then compare customer satisfaction across the two groups.</p><p>Participants then answered the same three questions from the policy change condition (measures 1-3 in Table <ref type="table" target="#tab_0">1</ref>) but now focusing on the experiment as a whole rather than the underlying policies. They also answered three additional questions designed to more unambiguously evaluate the acceptability of the experiment (rather than willingness to participate in it). We average only these additional three questions (α = 0.86) to construct the "experiment acceptability index." <ref type="bibr">4</ref> Participants also answered five comprehension checks to ensure they noticed potentially controversial attributes of the experiments (e.g., "People will be included in this study without agreeing to be included"). No other measures were collected in this condition. Results for measures not reported here are reported in Supplement 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Acceptability of Policy Changes. Validating our choice of stimuli, the overall policy acceptability index for bad policy changes (M = 1.91) was below the midpoint (4) and below both the good (M = 6.18) and very good policy changes (M = 6.11), which were both above the midpoint. All t-tests versus midpoint are ts &gt; 20.9, ps &lt; 0.001. The good and very good policies were rated as similarly acceptable, t(312) = 0.48, p = 0.63, and were close to the highest possible rating (medians of 6.7 and 7, respectively, on a sevenpoint scale).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acceptability of Experiments.</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the average experiment acceptability index for the nine Mislavsky, Dietvorst, and Simonsohn: Critical Condition experiment conditions. The results are inconsistent with absolute experiment aversion. In particular, when experiments did not include an objectionable condition (control/good, M = 5.11; good/very good, M = 5.17), they were rated above the midpoint and as more acceptable than when experiments did include an objectionable condition (bad/good, M = 3.25). The experiments with objectionable conditions were, in turn, rated below the midpoint. All t-tests versus midpoint are ts &gt; 5.9, ps &lt; 0.001. People found experiments to be acceptable when all conditions in the experiment were acceptable and found experiments to be unacceptable when a condition in the experiment was unacceptable. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results from Study 1 are inconsistent with absolute experiment aversion and consistent with a critical condition account of experiment evaluation. Additionally, participants found experiments with deception (e.g., one shipping speed was promised; another was actually delivered), unequal outcomes (e.g., some participants get $5 for attending the gym; others get $10), and lack of consent to be acceptable as long as all conditions were themselves acceptable.</p><p>However, Study 1 has some important limitations. First, the experiments evaluated as acceptable had unambiguously beneficial outcomes (e.g., free shipping upgrade) and may not generalize to more routine corporate experiments in which benefits to participants, if any, are less obvious. Second, we measured agreement with statements rather than absolute measures of acceptability, making it difficult to know whether the experiments are sufficiently acceptable. For example, the good/very good experiments were rated M = 5.17 on a seven-point scale on which seven implies strong agreement with the experiment being acceptable. Although this is significantly above the midpoint, is it high enough to suggest people would not object to the experiment? Third, participants' ratings in the policy change and experiment conditions are not directly comparable because (i) the sets of dependent variables and their interpretation are different in the policy and experiment conditions (see endnotes 4 and 5) and (ii) participants saw all nine policies in the policy change condition and only two in the experiment conditions. Fourth, participants in this experiment may have higher-than-average tolerance for experiments because they routinely volunteer for experiments on Amazon Mechanical Turk.  Notes. Participants in the policy change condition rated all nine policy changes. Participants in the experiment conditions rated one of nine experiments created by pairing two policy changes within a context. The pairs consisted of bad/good, control/good, and good/very good. Control consists of keeping the status quo (e.g., shipping item as promised). The average of questions 1-3 is the policy acceptability index, the average of questions 4-6 the experiment acceptability index. In Studies 2-6, we address all of these issues. We use a wider variety of stimuli (Studies 3a and 3b) and have participants evaluate experiments similar to (controversial) experiments that companies have actually run (Studies 2 and 4). We use questions with less ambiguous end points (Studies 2-6) and with neutral and labeled midpoints (Studies 3-6). We have participants in the policy change condition rate only the two policy changes that are included in the corresponding experiment condition (Studies 3-6) and use the same measures of acceptability across conditions (Studies 2-6). Finally, in Studies 3b and 6, we recruited participants who do not routinely volunteer for experiments.</p><p>Study 2: Predicting Experiment Ratings from Condition Ratings <ref type="bibr" target="#b14">Kramer et al. (2014)</ref> ran an experiment studying emotional contagion through social networks. They manipulated mood by modifying the emotional content of Facebook users' status updates and measured its effect on users' subsequent emotion expression, which upset many users and spurred public outrage <ref type="bibr" target="#b0">(Albergotti 2014)</ref>. If, as we have conjectured, people objected to the study because of its policies and not just because it was an experiment, then they should not object to a similar experiment with only acceptable conditions. In Study 2a, we conduct an exploratory search for acceptable and unacceptable mood inductions Facebook could have employed.</p><p>In Study 2b, we test if the acceptability of the experiment hinges on the acceptability of the mood inductions used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 2a: Finding (Un)Acceptable Mood Inductions</head><p>Method Sample. We recruited 382 participants on MTurk, of which 303 passed the attention check (40.7% female, M age = 30.3 years). Participants were paid $0.30 for completing the study.</p><p>Design. We generated six interventions, involving positive and negative versions of three possible changes to the site: showing only sad ads, showing only happy ads, showing sad status updates first, showing happy status updates first, showing the least liked status updates first, and showing the most liked status updates first. Each participant evaluated three alternative policies, one for each possible change to the site, randomizing whether participants saw the positive or negative change. We counterbalanced the order of the stimuli.</p><p>Measures. Participants answered two questions for each policy change: "Is it okay for a company to do this?" and "Would you object to a company doing this?" These questions were answered on seven-point scales with end points labeled "1. It's definitely not okay"/"7. It's definitely okay" and "1. I would definitely not object"/"7. I definitely would object," respectively. We average the two items (r = 0.69, second question reverse-coded) to construct the policy acceptability index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Participants found negative changes less acceptable than positive ones and manipulating status updates less acceptable than manipulating ads. From most to least acceptable, they ranked happy ads (M = 5.67), most liked status updates (M = 4.63), happy status updates (M = 4.58), sad ads (M = 3.90), least liked status updates (M = 3.62), and sad status updates (M = 3.08). For Study 2b, we used the highest rated (happy ads) and lowest rated (sad status updates) changes to test our prediction that experiments are only objectionable if they contain objectionable conditions.  Design. Participants were randomly assigned to one of two conditions in a between-subjects design. In both conditions, participants read descriptions of a social networking company that ran an experiment, assigning half of its customers to a control condition and the other half to a treatment condition. The treatment condition in those experiments was either the happy ads or sad status updates policy described in Study 2a. Participants answered the same two acceptability questions from Study 2a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results were consistent with the critical condition account of experiment evaluation and inconsistent with absolute experiment aversion; only the experiment with an objectionable condition was considered objectionable. Participants rated the happy ads experiment significantly above the midpoint (M = 4.72), t(98) = 3.47, p &lt; 0.001, and the sad status updates experiment below it (M = 2.59), t(99) = 9.30, p &lt; 0.001.</p><p>Although Study 2 shows that experiments with acceptable conditions are acceptable in an absolute sense, relative experiment aversion may still exist if experiments are rated as being less acceptable than their underlying conditions. In Study 3, we examine this possibility by directly comparing ratings of individual policies to experiments that use these policies as conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3a: Testing for Relative Experiment Aversion</head><p>Method Sample. We recruited 533 participants on MTurk, of which 423 passed the attention check (43.5% female, M age = 36.0 years). Participants were paid $0.50 for completing the study.</p><p>Design. Participants were randomly assigned to one of six conditions in a 2 (action: policy change versus experiment) × 3 (policy combination: negative/positive versus no change/positive versus negative/no change) fully between-subjects design.</p><p>Participants in the policy change conditions were told that a company was deciding between two policies. They were then told to imagine the company chose one of the policies and answered three questions about the acceptability of this action. They then answered the same questions but imagining that the other policy had been chosen.</p><p>Participants in the experiment conditions were told that a company was running an experiment that randomly assigned customers to one of two policies (from the same pool of policy pairs as the policy change conditions) and answered the same questions as the policy change conditions.</p><p>Stimulus Selection and Sampling. To reduce the probability that the results would be driven by idiosyncratic features of the selected stimuli <ref type="bibr" target="#b31">(Wells and Windschitl 1999)</ref>, we presented policy changes for seven different contexts (e.g., showing emotionally charged ads, changing a product recommendation system, and changing frequency of issuing coupons). See Supplement 2 for a full list of stimuli.</p><p>Measures. Participants in all conditions answered the following three questions containing labeled neutral midpoints:</p><p>1. How okay is it for the company to do this? (1 = It's really bad; 4 = It's okay; 7 = It's really good)</p><p>2. If you were a customer of this company and learned about the company's plans, how would this influence your opinion of the company? (1 = I would view the company much more negatively; 4 = [. . .] not view the company any differently; 7 = [. . .] much more positively)</p><p>3. If you were a customer of this company and learned about the company's plans, how likely would you be to switch to a different company? ( <ref type="formula">1</ref> Participants in the policy change condition answered these questions twice, once for each policy (in counterbalanced order). Participants in the experiment condition answered these questions once, evaluating only the experiment. We average these items (α = 0.86) to construct an acceptability index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Evaluating Policy Changes. Validating our choice of stimuli, the negative policies were rated as the least acceptable (M = 2.65), followed by the no change (M = 4.61) and positive (M = 5.46) policies. The negative policies were rated below the midpoint (4), and the no change and positive policies were rated above the midpoint, all ts &gt; 6.4, ps &lt; 0.001.</p><p>Evaluating Experiments. Replicating the results from Studies 1 and 2 and again inconsistent with absolute experiment aversion, experiments that only included acceptable policy changes (no change/positive) were rated as acceptable (M = 4.38), significantly above midpoint, t(79) = 3.54, p &lt; 0.001. Conversely, experiments with an unacceptable policy (negative/positive, M = 3.22; negative/no change, M = 3.31) were rated below the midpoint, ts &gt; 4.3, ps &lt; 0.001. Because, in this study, we used a labeled neutral midpoint <ref type="bibr">Mislavsky, Dietvorst, and Simonsohn: Critical Condition Marketing Science, 2020</ref><ref type="bibr">, vol. 39, no. 6, pp. 1092</ref><ref type="bibr">-1104</ref> (see "Measures"), evaluations above/below the midpoint are unambiguously positive/negative.</p><p>Because participants may not all have the same opinion of which policy is "worst," we compare participants' average ratings of each experiment in the experiment conditions to the average rating of each participant's less preferred policy in the corresponding policy change conditions. When comparing average experiment ratings to the average of the lowest rated corresponding policies, participants found experiments to be significantly more acceptable in the no change/positive, t(139) = 2.53, p = 0.013, and negative/positive, t(139) = 4.23, p &lt; 0.001, conditions, and marginally more acceptable in the negative/no change conditions, t(137) = 1.77, p = 0.079. Collapsing across all policy combinations, experiments were rated as significantly more acceptable than the policy that represented their least acceptable condition, t(419) = 5.16, p &lt; 0.001. <ref type="bibr">6</ref> Most importantly, experiments were not rated as less acceptable than their worst conditions (see Figure <ref type="figure" target="#fig_2">2</ref>). This suggests that participants rate experiments as some weighted average of its policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 3b: Replication with Field Survey</head><p>One concern about the generalizability of our findings may be that our results to this point have relied on a sample (MTurkers) that regularly opts in to taking experiments and may, therefore, be less experiment averse than the general public. In this study, following suggestions of the review team, we replicated our findings using a sample of participants from outside an established participant pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sample. Three research assistants walked around a university campus, approached nonacademic staff members, and asked them if they were willing to take a short, one-page, pen-and-paper survey. We specifically instructed the research assistants to approach staff in and around nonacademic buildings (e.g., the student union and library) to reduce the likelihood that our participants themselves would be involved in conducting research. It is also important to note that our respondents did not initiate participation in the study (reducing potential selection effects), nor were they compensated for completing the survey (which may have caused them to view academic research and experimentation more favorably). In total, we obtained 247 responses (68.4% female, M age = 33.4 years).</p><p>Design. Participants were assigned to one of two conditions (policy change versus experiment) in a between-subjects design.</p><p>The design of the study was nearly identical to that of Study 3a with two changes. First, participants only evaluated the negative/positive stimuli (i.e., the leftmost panel from Figure <ref type="figure" target="#fig_2">2</ref>). Second, to make the survey fit on one page, we only included one of the three dependent variables ("How okay is it for the company to do this?") from Study 3a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Replicating our results from Study 3a, participants rated the experiments (M = 3.54) more favorably than their worst conditions (M = 2.41), t(239) = 7.06, p &lt; 0.001. <ref type="bibr">7</ref> These ratings are similar to MTurker ratings of identical stimuli in Study 3 (experiments: M = 3.35; worst conditions: M = 2.26). 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results from Studies 3a and 3b are inconsistent with absolute experiment aversion, with which people find all experimentation objectionable. Additionally, these results are inconsistent with a version of relative experiment aversion that is large enough to make an experiment less acceptable than its worst condition. In our next study, we apply the paradigm from Study 3 to directly examine the potential role of experiment aversion in the backlash to the Facebook experiment by <ref type="bibr" target="#b14">Kramer et al. (2014)</ref>. Specifically, we assess whether the backlash may actually be attributed to the policies to which people were assigned rather than experimentation per se. Notes. Each participant (n = 423) rated the acceptability of a company choosing one of two policies or running an experiment using those two policies as conditions. The policies involved a negative change, a positive change, or no change. Circular markers depict means evaluation of each policy, squared markers the evaluations of the experiment that combines them. Error bars represent 95% confidence intervals.</p><p>Mislavsky, Dietvorst, and Simonsohn: Critical Condition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 4: Was Facebook Backlash Really Experiment Aversion?</head><p>As in Study 2, we investigated perceptions of an experiment based on <ref type="bibr" target="#b14">Kramer et al. (2014)</ref>. Unlike in Study 2, we used only stimuli that represented the specific conditions used in that experiment rather than modifying certain aspects to find an "acceptable" version. We also used the same bipolar scales as Study 3, with labeled neutral midpoints, to evaluate policy changes and experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sample. We recruited 748 participants on MTurk, of which 608 passed the attention check (41.3% female, M age = 32.2 years). Participants were paid $0.30 for completing the study.</p><p>Design. The overall design of Study 4 was nearly identical to that of Study 3 but used different stimuli. Participants were randomly assigned to one of six conditions in a 2 (action: policy change versus experiment) × 3 (policy combination: sad/happy versus no change/happy versus sad/no change) fully between-subjects design.</p><p>Participants in the policy change condition read that Facebook was considering making two policy changes (randomly selected from sorting status updates to prioritize happy ones, to prioritize sad ones, or making no change). They then read that Facebook chose to implement one of the two policies. Participants in the experiment condition read that Facebook was considering running an experiment in which they would randomly assign customers to two of the policy changes described.</p><p>Measures. Participants answered the same acceptability questions from Study 3. However, because Facebook does not have an obvious competitor, we did not ask if participants would switch to a different company. <ref type="bibr">9</ref> We average these two variables (r = 0.80) to construct the acceptability index. Participants then indicated whether they had previously heard of Facebook taking similar actions in the past. This was collected to account for participants that may have been influenced by media coverage of the Facebook study. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Figure <ref type="figure" target="#fig_3">3</ref> shows the main results from Study 4. All three experiments (gray squares), even the experiment with ostensibly good conditions (i.e., happy/no change), were rated significantly below the acceptability midpoint (ts &gt; 4.8, ps &lt; 0.001). At first glance, this could be consistent with absolute or relative experiment aversion. However, this conclusion is not supported once we take into account the fact that the underlying policies are unacceptable even outside of an experimental context. The lowest rated condition in each experiment was rated no higher than a 2.93 on a seven-point scale; significantly below the midpoint, ts &gt; 9.4, ps &lt; 0.001. <ref type="bibr">11</ref> As was the case in Study 3, when we directly compare the acceptability of experiments to the acceptability of their treatments in the corresponding policy change conditions, we see that experimentation does not decrease the acceptability of the company's actions relative to some weighted average of its policy ratings. Indeed, experiments were again rated as at least marginally more acceptable than their worst conditions when considering each experiment individually, ts &gt; 1.88, ps &lt; 0.061, and significantly more acceptable when collapsing across all three experiments, t(599) = 3.94, p &lt; 0.001. 12 Discussion Again, if there is relative experiment aversion, it is not large enough to push the experiment's ratings below the ratings of its policies. Thus, it is probable that participants were not reacting negatively to experimentation per se but to each experiment's underlying policies. Although the reaction to the <ref type="bibr" target="#b14">Kramer et al. (2014)</ref> Facebook experiment is held up as evidence of a public distaste for corporate experiments, in Study 4, we find that Facebook probably did not face backlash because it ran an experiment, but because it Notes. Each participant (n = 601) rated the acceptability of Facebook changing how status updates are sorted or of running an experiment randomly assigning users to one of those changes. Circular markers depict mean evaluations of the least and most acceptable change in the pair; squared markers depict mean evaluations of an experiment randomly assigning users to them. For example, the first panel shows that people evaluating sorting status updates by sad/happy rated the worst of these with M = 2.70, the highest with M = 4.22, and an experiment with M = 2.92. Error bars represent 95% confidence intervals. <ref type="bibr">Mislavsky, Dietvorst, and Simonsohn: Critical Condition Marketing Science, 2020</ref><ref type="bibr">, vol. 39, no. 6, pp. 1092</ref><ref type="bibr">-1104</ref> implemented unacceptable policies. This suggests the public's reaction would have been even worse had Facebook modified how status updates are sorted for all (rather than for a random subset) of its users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 5: Relative Experiment Aversion vs. Critical Condition</head><p>Studies 3 and 4 demonstrate that relative experiment aversion, if it exists, may not be strong enough to drive ratings of an experiment below some weighted average of its policies. However, we cannot conclusively reject the existence of some relative experiment aversion. Even though the experiments were not rated worse than the least preferred policy, they were still rated below the equally weighted average of its policies. This could be consistent with the critical condition account of experiment aversion if participants are taking a weighted average of their ratings of the two policies and giving more weight to the worse rated policy as they might if they exhibit negativity bias <ref type="bibr" target="#b26">(Skowronski and Carlston 1989)</ref>. However, this finding could also be consistent with the existence of moderate relative experiment aversion. For example, participants may be averaging their opinions of the policies and then applying some fixed "experiment penalty." Alternatively, participants' ratings of policies could be lower when those policies are part of an experiment. We ran Study 5 to more directly tease apart these two explanations by creating an experiment in which both policies would be deemed equally acceptable. If there is relative experiment aversion, an experiment over both policies would be rated as lower than either, which would not happen if people evaluate experiments based on their critical conditions. We view this design as one that maximizes the ability to detect relative experiment aversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sample. We recruited 502 participants on MTurk, of which 406 passed the attention check (46.4% female, M age = 35.0 years). Participants were paid $0.40 for completing the study.</p><p>Design. Participants were randomly assigned to one of two between-subjects conditions (policy change versus experiment). We pretested the acceptability of 30 policies (see Supplement 4) and chose two that had nearly identical means (Ms = 5.54 and 5.59 out of 7) and distributions of responses (SDs = 1.40 and 1.32). The general design of Study 5 was similar to that of Studies 3 and 4. Participants read that a ridesharing company (e.g., Uber, Lyft) was considering implementing two discounts (either a flat 10% discount or a $1 credit for every $10 spent) and either chose one of the two (policy change condition) or ran an experiment in which they randomly assigned customers to receive one of the two discounts (experiment condition).</p><p>In both conditions, participants answered the following question: "How okay is it for the company to do this?" (1 = It's really bad; 4 = It's okay; 7 = It's really good).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Participants rated both discounts (10% discount: M = 5.84; $1 credit for every $10 spent: M = 4.85) significantly above the midpoint, ts &gt; 9.14, ps &lt; 0.001, indicating that they viewed both discounts positively. <ref type="bibr">13</ref> Participants rated the experiment that assigned participants to one of two discounts (M = 5.32) nearly identically to the average discount (M = 5.34), t(399) = 0.21, p = 0.83, and well above the least preferred discount (M = 4.61), t(399) = 5.24, p &lt; 0.001. Participants in this study do not show even small levels of experiment aversion. 14</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study 6: Attitudes from Actual Customers</head><p>Our previous studies did not distinguish between reactions of customers and noncustomers of the company running the experiment. Following suggestions of the review team, this study replicates our general design, asking participants to evaluate experiments run and policies implemented by a company from which they regularly purchase: Amazon. We also include a condition in which the company is running an experiment with two negative policies to show that our results are robust when implementing objectionable policies may be unavoidable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sample. We partnered with Lucid, a market research firm, to identify a nationally representative sample of regular Amazon customers (in our preregistration, we defined regular users as those self-reporting making at least one purchase per month). Of the 3,681 people who started our survey, 2,185 successfully completed an attention check. Our final sample consists of the 1,304 regular Amazon customers among them (50.5% female; M age = 44.2 years; 71.1% Amazon Prime members).</p><p>Design. Similar to Studies 3-5, participants were randomly assigned to one of six conditions, in a 2 (action: policy change versus experiment) × 3 (policy combination: negative/negative versus negative/positive versus positive/positive) between-subjects design.</p><p>Participants in the policy change condition read that Amazon was considering two changes to its product recommendation system (presented in counterbalanced order). They evaluated how acceptable it would be if Amazon picked the first policy; then they were asked to evaluate how acceptable it would be if Amazon picked the second policy. Participants in the experiment condition read that Amazon was conducting an experiment in which it randomly assigned customers to one of the two changes to its product recommendation system and evaluated the acceptability of such an experiment.</p><p>In the negative/negative condition, the changes were (a) recommending the most profitable items or (b) recommending items that weren't selling well. In the negative/positive condition, the changes were (a) recommending the most profitable items or (b) recommending the most highly rated items across the entire site. In the positive/positive condition, the changes were (a) recommending the most highly rated items across the entire site or (b) recommending items that similar users have rated highly.</p><p>All participants were asked, "How okay is it for the company to do this?" (1 = It's really bad; 4 = It's okay; 7 = It's really good).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Figure <ref type="figure" target="#fig_4">4</ref> shows that in this non-MTurk sample of actual (self-identified) Amazon customers evaluating experiments that would directly affect them, we replicate the critical condition finding; experiments are at least as acceptable as their worst condition is (all ts &gt; 1.93, ps &lt; 0.06). Again, there is no experiment aversion. <ref type="bibr">15</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Discussion</head><p>Taken together, the results of our studies are inconsistent with absolute experiment aversion; experiments are considered acceptable if all policies tested in the experiment are themselves acceptable. The results are also inconsistent with relative experiment aversion; experiments are considered to be at least as acceptable as their least acceptable policy. Experiments are not only acceptable under some circumstances; they are at least as acceptable as the worst policies they contain. We have called this the critical condition account of experiment evaluation.</p><p>These results are good news for companies that want to learn from experiments. Companies should not be more hesitant to run an experiment that includes a certain policy than they would be to implement that policy outright. A practical takeaway for organizations interested in running experiments is to first determine if their planned policy changes are objectionable (e.g., through a survey) and then run an experiment to determine which acceptable policy best achieves their desired objective. In these cases, companies are unlikely to face backlash for their experiments. Unfortunately, objectionable policies are sometimes unavoidable. Still, we find that experimentation with objectionable policies is preferred to implementing the worst policy by itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>We have identified three key limitations with our studies. The first limitation is that our samples consist primarily of people who volunteered to complete our studies, possibly excluding individuals who most strongly oppose data collection in general or experiments in particular. We are optimistic this is not a consequential limitation for two main reasons. First, our respondents did negatively evaluate experiments that included negative policies, indicating that they do not have universally positive opinions of experiments and that they do discriminate between acceptable and unacceptable practices. Second, Study 3b surveyed a sample of nonacademic university staff who do not regularly participate in experiments, and Study 6 used a non-MTurk sample provided by a market research firm. Their responses were indistinguishable from those of our MTurk samples. It is nevertheless impossible to obtain data on the attitudes of people who are unwilling to participate in an experiment.</p><p>The second limitation is that it is difficult to specify the threshold of acceptability that an action must reach to prevent a backlash. For example, a small group of motivated people (e.g., activists or media personalities) could be vocal enough to cause backlash against an experiment that most people find acceptable. At the same time, this concern applies to any action an organization can take and not solely experiments. Comparing the most extreme ratings across policy and experiment evaluations in our studies Notes. Each participant (n = 1,304) rated the acceptability of Amazon changing how they recommend products to customers or running an experiment randomly assigning customers to one of those changes. Circular markers depict mean evaluations of the least and most acceptable change in the pair; squared markers depict mean evaluations of an experiment randomly assigning users to them. Error bars represent 95% confidence intervals. <ref type="bibr">Mislavsky, Dietvorst, and Simonsohn: Critical Condition Marketing Science, 2020</ref><ref type="bibr">, vol. 39, no. 6, pp. 1092</ref><ref type="bibr">-1104</ref> suggests experiments are not more polarizing than are policies. In Study 3a, for example, 12.5% of participants gave the negative policy the lowest possible rating, and 7.6% of participants gave the experiment the lowest possible rating, a pattern that holds in all studies we run for which this comparison is possible. <ref type="bibr">16</ref> This also speaks to a larger issue of how different people may view different policy changes; what some may consider fine, others may find completely unacceptable. For this reason, we compared experiments to each participant's least preferred policy rather than the average of each specific policy. Additionally, it is important to examine distributions of responses (beyond just means) to determine if a certain policy, although it may have a high mean, may be especially divisive (i.e., having a high variance). We encourage researchers and practitioners to pretest the acceptability of policies using surveys and measures such as those we used in Studies 3-6.</p><p>Finally, and perhaps most substantially, all of our scenarios are hypothetical. We simply cannot rule out the possibility that people will react differently to experiments that have actually occurred or in which they were participants than they would to a hypothetical study. For example, in some real-world contexts, people could find a specific policy to be more objectionable when it is implemented as part of an experiment. We have not found any evidence that experimentation makes actions more objectionable, and we propose that experimentation generally does not make actions more objectionable. However, of course, we cannot unequivocally claim that an experiment will never make any policy more objectionable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Aversion Is an Interaction</head><p>Finally, there are many factors that could influence how acceptable experiments are. For example, much research has examined how people view the ethics of corporate practices that can be included in experiments, such as collecting sensitive data (e.g., <ref type="bibr" target="#b4">Culnan and Armstrong 1999</ref><ref type="bibr" target="#b1">, Awad and Krishnan 2006</ref><ref type="bibr" target="#b19">, Miyazaki 2008</ref>, changing pricing practices (e.g., <ref type="bibr" target="#b3">Campbell 1999</ref><ref type="bibr" target="#b2">, Bolton et al. 2003</ref><ref type="bibr" target="#b12">, Haws and Bearden 2006</ref>, or introducing new marketing strategies (e.g., <ref type="bibr" target="#b27">Smith and Cooper-Martin 1997)</ref>.</p><p>Using the more specific context of our motivating example, it may be that Facebook's experiment was more objectionable because it involved emotions (or specifically negative emotions). <ref type="bibr">17</ref> In addition, our review team proposed that perhaps people view an experiment as less acceptable when they participated in it or when they are told about it after it has already been run. We report three studies that test these two hypotheses in the supplement (Studies S3-S5). We find that people prefer to hear about experiments before (rather than after) they are run (Study S3); that people's stated acceptability of an experiment is not affected by considering having been a participant in it (Study S4); and that, even when people consider having been assigned to the worst arm within an experiment, they rate the experiment overall as more acceptable than that worst arm (Study S5).</p><p>However, asking, "Do these factors impact the acceptability of experiments?" does not teach us about experiment aversion because these factors can be present in corporate actions both within and without an experiment. For example, a company can, outside of an experiment, take an action that affects consumers and inform them only after the fact. The critical question for the purposes of this paper, then, is "Do these factors impact the acceptability of experiments more than they impact the acceptability of underlying policies?" That is, is there an interaction between these factors and whether they are part of an experiment? In Studies S3 and S4, we find none of these hypothesized interactions (Study S3: t(794) = 0.99, p = 0.32; Study S4: t(793) = 0.56, p = 0.58). For example, in Study S3, we find that the negative effect of learning about an experiment after it is conducted (versus before it is conducted) is not larger than the negative effect of learning about a policy change after it is implemented (versus before it is implemented).</p><p>Of course, we did not test a completely exhaustive list of potential interactions. Similarly, we did not test any three-way interactions between these factors, so we cannot rule out those possibilities. For example, it is possible that people who are customers of a company show experiment aversion when they find out about an experiment after it is run or that people who are not customers of a company do not show experiment aversion regardless of when the experiment is disclosed. We propose that these interactions do not exist or, if they do exist, that they would not be large enough to be practically relevant. That said, we cannot rule out the possibility that any interaction does exist, and we encourage researchers to test for interactions. We expect all factors that influence opinion about experiments to also influence opinion about the acceptability of policy changes. We propose that, if something makes a policy unpopular, it will make an experiment that contains that policy unpopular, but not more so. Experiments are not unpopular; unpopular policies are unpopular. Table <ref type="table" target="#tab_4">3</ref> summarizes the contents of the online supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Endnotes</head><p>1 Screenshots from the cited media coverage available from http://osf .io/z39aq. <ref type="bibr">2</ref> We define an experiment as an instance in which an organization implements, at random, different policies for different groups with the intention of learning how they differently influence a specific outcome.</p><p>3 In our online studies, we typically obtained sample sizes that slightly exceeded our goals because some participants did not submit a completion code, allowing additional participants to take the survey. Participants, identified by their MTurk ID number, were not able to participate in more than one study. We included an attention check <ref type="bibr" target="#b22">(Oppenheimer et al. 2009</ref>) in the first question, and only those who answered correctly were able to participate in the studies. All participant responses are included in analyses regardless of whether they completed the entire survey. <ref type="bibr">4</ref> In hindsight, we found questions 1-3 to be ambiguous for interpreting the evaluation of experiments. Therefore, the experiment acceptability index in the main text is based only on questions 4-6. We report results aggregating over all six questions in endnote 5. <ref type="bibr">5</ref> These results are based on questions 4-6 in Table <ref type="table" target="#tab_0">1</ref> (see endnote 4). Including all six questions, the results are very similar. Experiments with a bad condition (bad/good, M = 3.01) were rated below the midpoint and below experiments without a bad condition (control/good, M = 5.17; good/very good, M = 5.30), which were both above the midpoint. All t-tests versus midpoint are ts &gt; 8.4, ps &lt; 0.001. <ref type="bibr">6</ref> These results are consistent when comparing each experiment to the policy change with the lowest average rating (as opposed to the average of each participant's lowest rated policy). Experiments were rated directionally more acceptable than their worst policies in all three cases (significantly so for the negative/positive experiment; t(139) = 3.94, p &lt; 0.001; negative/no change experiment, t(132) =2.06, p = 0.04; and when collapsing across all policy pairs, t(419) = 4.27, p &lt; 0.001). 7 This analysis was done using a regression with fixed effects for each stimulus. We preregistered that we would also conduct a simple t-test collapsing across stimuli. The results are consistent, t(245) = 6.24, p &lt; 0.001. 8 These numbers are not the same as those in Study 3a (and in the left panel of Figure <ref type="figure" target="#fig_2">2</ref>) because in Study 3a we used a composite of three measures. Here, we compare only results for the question ("Is it okay for the company to do this?") that we used in both studies. <ref type="bibr">9</ref> We exploratorily asked if participants would be inclined to cancel their Facebook membership; see preregistration file.</p><p>10 Most participants said that they had not heard of Facebook doing something similar (70.3% in the experiment condition and 82.2% in the policy change condition). Those with prior knowledge in the experiment condition rated Facebook's actions slightly more negatively (M = 2.77) than those with no prior knowledge (M = 3.06), t(301) = 1.75, p = 0.08. There was no difference between ratings in the policy change condition (p = 0.81). Therefore, we report results from all participants in our analysis. <ref type="bibr">11</ref> The only specific policy that was rated above the midpoint was making no change (M = 5.10). Both sad status updates (M = 2.48) and happy status updates (M = 3.67) are viewed as unacceptable (all pairwise ts &gt; 8.0, all ts versus midpoint &gt; 3.0). 12 As indicated in our preregistration, we ran a regression estimating ratings using fixed effects for each policy pair and an indicator for whether the participant rated a policy or an experiment. The coefficient for experiments was positive (b = 0.39; p &lt; 0.001), indicating that experiments were rated more highly than policies when controlling for which policies participants saw. <ref type="bibr">13</ref> We should point out that the mean ratings of the individual discounts diverged more in Study 5 (Ms = 4.85 and 5.85) than they did in the pilot (Ms = 5.54 and 5.59). We believe that this is because evaluating only two discounts (compared with 10 in the pilot) made those discounts seem less similar. 14 The 95% confidence interval for the difference between the acceptability of the experiment and the average policy is (−0.21, +0.26); thus, we reject experiment aversion that is larger than 0.26 on our seven-point scale. With a pooled standard deviation of 1.19, we can reject experiment aversion having a Cohen's d &gt; 0.22. <ref type="bibr">15</ref> As an exploratory analysis, we preregistered that we would compare results for customers with and without an Amazon Prime membership. Across the nine evaluations (three experiments and six policy changes), there were no statistically significant differences between self-reported Prime (N = 927) versus non-Prime (N = 377) customers. Among the nine comparisons, pvalues range from 0.09 to 0.50. See Supplement 5 for the full set of results. <ref type="bibr">16</ref> In Study 3b, 35.5% gave the lowest possible rating to the worst policy compared with 9.8% for the experiment. In Study 4, these values are 20.7% and 12.9%; in Study 5, they are 2.5% and 1.0%; and in Study 6, they are 16.6% and 8.7%, respectively. 17 See Supplements 6 and 7 for descriptions of studies that test these questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Experiments Without Bad Policies (Gray and White Circles) Are Rated Positively (Study 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>= [. . .] definitely not switch [. . .]; 4 = [. . .] not change how likely I am to switch [. . .]; 7 = [. . .] would definitely switch [. . .]; reverse-coded)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Experiments (Gray Squares) Are No Less Acceptable Than Their Least Acceptable Condition (White Circles) (Study 3a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Facebook Experiments (Gray Squares) Are No Less Acceptable Than Their Least Acceptable Condition (White Circles) (Study 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Amazon Customers Rate Potential Amazon Experiments (Gray Squares) More Positively Than Their Worst Policies (White Circles) (Study 6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Stimuli and Measures for Study 1</figDesc><table><row><cell>Policy changes</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Context</cell><cell>Bad</cell><cell>Good</cell><cell>Very good</cell></row><row><cell>1. Shipping</cell><cell>Slower delivery</cell><cell>Faster delivery</cell><cell>Much faster delivery</cell></row><row><cell>2. Company gym</cell><cell>$5 penalty for not going</cell><cell>$5 bonus for going</cell><cell>$10 bonus for going</cell></row><row><cell>3. Product recommendations</cell><cell>Poorly rated products</cell><cell>Highly rated products</cell><cell>Highest rated overall</cell></row><row><cell>Measures of acceptability</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Participants indicated agreement (1 = strongly disagree; 7 = strongly agree), with these statements</cell><cell></cell></row><row><cell>Acceptability of policy changes</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1. It is okay for the company to do this.</cell><cell></cell><cell></cell></row><row><cell cols="3">2. If I were [an employee/a customer], I would object to this. (reverse-coded)</cell><cell></cell></row><row><cell cols="3">3. If I were [an employee/a customer] and was asked, I would agree to this.</cell><cell></cell></row><row><cell>Acceptability of experiment</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4. It is immoral to run this experiment. (reverse-coded)</cell><cell></cell><cell></cell></row><row><cell cols="3">5. People in this experiment are being treated like guinea pigs. (reverse-coded)</cell><cell></cell></row><row><cell cols="3">6. The company should be not allowed to run this experiment. (reverse-coded)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We recruited 255 participants on MTurk, of which 201 passed the attention check (43.9% female,</figDesc><table><row><cell>Study 2b: Experiments with</cell></row><row><cell>(Un)Acceptable Mood Inductions</cell></row><row><cell>Method</cell></row><row><cell>Sample.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Overview of Study Design and Contributions Study 1 • Test of absolute experiment aversion • People find experiments with unambiguous benefits acceptable Studies 2, a and b • Extend Study 1 with more realistic stimuli • Acceptability of conditions predicts acceptability of experiments Adds condition with experiments using two negative policies Mislavsky, Dietvorst, and Simonsohn: Critical Condition M age = 34.2 years). Participants were paid $0.30 for completing the study.</figDesc><table><row><cell>Study 3a</cell><cell>• Direct comparison of experiments with underlying conditions</cell></row><row><cell></cell><cell>• Experiments rated at least as acceptable as worst condition is</cell></row><row><cell></cell><cell>• Results hold for variety of stimuli</cell></row><row><cell>Study 3b</cell><cell>• Replicates Study 3a results using a sample that does not regularly volunteer for experiments</cell></row><row><cell>Study 4</cell><cell>• Best-known example of experiment aversion is not an instance of experiment aversion</cell></row><row><cell>Study 5</cell><cell>• Experiments with similar and positively viewed policies are rated identically to the average policy</cell></row><row><cell>Study 6</cell><cell>• Replicates Study 3 using company's own customers</cell></row><row><cell></cell><cell>•</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Index of Supplementary Materials</figDesc><table><row><cell>Section</cell><cell>Pages</cell></row><row><cell>Supplement 1. Additional Study 1 analysis</cell><cell>2-3</cell></row><row><cell>Supplement 2. Full list of Study 3 stimuli</cell><cell>4</cell></row><row><cell>Supplement 3. Additional analyses for Study 4 included</cell><cell>5-7</cell></row><row><cell>in preregistration</cell><cell></cell></row><row><cell>Supplement 4. Study 5 pilot results</cell><cell>8-9</cell></row><row><cell>Supplement 5. Prime versus non-Prime customers in</cell><cell>10</cell></row><row><cell>Study 6</cell><cell></cell></row><row><cell>Supplement 6. Overview of studies not included in</cell><cell>11-13</cell></row><row><cell>main manuscript</cell><cell></cell></row><row><cell>Supplement 7. Details and results for studies not</cell><cell>14-25</cell></row><row><cell>included in main manuscript</cell><cell></cell></row><row><cell cols="2">Notes. These supplementary materials are available from http://osf</cell></row><row><cell>.io/z39aq.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">MarketingScience, 2020, vol. 39, no. 6, pp. 1092-1104 </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Mislavsky, Dietvorst, and Simonsohn: Critical Condition   </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the review team for their constructive comments. They also thank Laura Kuder, Johanna Matt-Navarro, and Catherine O'Donnell for valuable research assistance.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Furor erupts over Facebook&apos;s experiment on users</title>
		<author>
			<persName><forename type="first">R</forename><surname>Albergotti</surname></persName>
		</author>
		<ptr target="https://www.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840" />
	</analytic>
	<monogr>
		<title level="j">Wall Street Journal</title>
		<imprint>
			<date type="published" when="2014-07-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The personalization privacy paradox: An empirical evaluation of information transparency and the willingness to be profiled online for personalization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Inform. Systems Quart</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="28" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Consumer perceptions of price (un)fairness</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Warlop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Alba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="474" to="491" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perceptions of price unfairness: Antecedents and consequences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information privacy concerns, procedural fairness, and impersonal trust: An empirical investigation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Culnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organ. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="115" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Psychology and economics: Evidence from the field</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dellavigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econom. Literature</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="372" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effects of information about firms&apos; ethical and unethical actions on consumers&apos; attitudes</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Folkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Psych</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="259" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Dietvorst</forename><surname>Mislavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonsohn</forename></persName>
		</author>
		<title level="m">Critical Condition Marketing Science</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1092" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Companies like Amazon need to run more tests on workplace practices</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gino</surname></persName>
		</author>
		<ptr target="https://hbr.org/2015/08/companies-like-amazon-need-to-run-more-tests-on-workplace-practices" />
	</analytic>
	<monogr>
		<title level="j">Harvard Bus. Rev</title>
		<imprint>
			<date type="published" when="2015-08-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facebook tinkers with users&apos; emotions in news feed experiment, stirring outcry</title>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2014/06/30/technology/facebook-tinkers-with-users-emotions-in-news-feed-experiment-stirring-outcry.html" />
	</analytic>
	<monogr>
		<title level="j">New York Times</title>
		<imprint>
			<date type="published" when="2014-06-29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Facebook and Silicon Valley treat you like a laboratory rat</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
		<ptr target="https://money.cnn.com/2014/06/30/technology/social/facebook-experiment/index.html" />
		<imprint>
			<date type="published" when="2014-02-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">OkCupid&apos;s human experiments are way creepier than Facebook&apos;s</title>
		<author>
			<persName><forename type="first">R</forename><surname>Greenfield</surname></persName>
		</author>
		<ptr target="https://www.fastcompany.com/3033645/okcupids-human-experiments-are-way-creepier-than-facebooks" />
		<imprint>
			<date type="published" when="2014-02-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic pricing and consumer fairness perceptions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Haws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">O</forename><surname>Bearden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="311" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">OkCupid lied to users about their compatibility as an experiment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hill</surname></persName>
		</author>
		<ptr target="https://www.forbes.com/sites/kashmirhill/2014/07/28/okcupid-experiment-compatibility-deception/" />
	</analytic>
	<monogr>
		<title level="j">Forbes</title>
		<imprint>
			<date type="published" when="2014-07-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Experimental evidence of massive-scale emotional contagion through social networks</title>
		<author>
			<persName><forename type="first">Adi</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="8788" to="8790" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Were OkCupid&apos;s and Facebook&apos;s experiments unethical?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<ptr target="https://hbr.org/2014/07/were-okcupids-and-facebooks-experiments-unethical" />
	</analytic>
	<monogr>
		<title level="j">Harvard Bus. Rev</title>
		<imprint>
			<date type="published" when="2014-07-29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two cheers for corporate experimentation: The A/B illusion and the virtues of data-driven innovation. Colorado Tech</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Law J</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="331" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Everything we know about Facebook&apos;s secret mood manipulation experiment. The Atlantic</title>
		<author>
			<persName><forename type="first">R</forename><surname>Meyer</surname></persName>
		</author>
		<ptr target="https://www.theatlantic.com/technology/archive/2014/06/everything-we-know-about-facebooks-secret-mood-manipulation-experiment/373648/" />
		<imprint>
			<date type="published" when="2014-06-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning about new technologies through social networks: Experimental evidence on nontraditional stoves in Bangladesh</title>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mobarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="480" to="499" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online privacy and the disclosure of cookie use: Effects on consumer trust and anticipated patronage</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Miyazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Public Policy Marketing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="33" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Can marketing campaigns induce multichannel buying and more profitable customers? A field experiment</title>
		<author>
			<persName><forename type="first">E</forename><surname>Montaguti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Neslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="217" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Facebook experiment: What it means for you</title>
		<author>
			<persName><forename type="first">T</forename><surname>Muse</surname></persName>
		</author>
		<ptr target="https://www.forbes.com/sites/dailymuse/2014/08/04/the-facebook-experiment-what-it-means-for-you/" />
		<imprint>
			<date type="published" when="2014-02-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Instructional manipulation checks: Detecting satisficing to increase statistical power</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meyvis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davidenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Experiment. Soc. Psych</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="867" to="872" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Negativity bias, negativity dominance, and contagion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Royzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Rev</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="296" to="320" />
			<date type="published" when="2001" />
			<publisher>Personality Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">We experiment on human beings! Accessed</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudder</surname></persName>
		</author>
		<ptr target="https://web.archive.org/web/20170316051630/https://theblog.okcupid.com/we-experiment-on-human-beings-5dd9fe280cd5" />
		<imprint>
			<date type="published" when="2014-01-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Research at Facebook | Facebook newsroom</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schroepfer</surname></persName>
		</author>
		<ptr target="https://newsroom.fb.com/news/2014/10/research-at-facebook/" />
		<imprint>
			<date type="published" when="2014-02-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Negativity and extremity biases in impression formation: A review of explanations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Skowronski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ethics and target marketing: The role of product harm and consumer vulnerability</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cooper-Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Facebook isn&apos;t the only website running experiments on human beings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Stampler</surname></persName>
		</author>
		<ptr target="http://time.com/3047603/okcupid-oktrends-experiments/" />
		<imprint>
			<date type="published" when="2014-02-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Do sympathy biases induce charitable giving? The effects of advertising content</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sudhir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="849" to="869" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enduring effects of goal achievement and failure within customer loyalty programs: A large-scale field experiment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sprigg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="565" to="575" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stimulus sampling and social psychological experimentation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Windschitl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality Soc. Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1115" to="1125" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Run field experiments to make sense of your big data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zoumpoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<ptr target="https://hbr.org/2015/11/run-field-experiments-to-make-sense-of-your-big-data" />
	</analytic>
	<monogr>
		<title level="j">Harvard Bus. Rev</title>
		<imprint>
			<date type="published" when="2015-11-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dietvorst</forename><surname>Mislavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonsohn</forename></persName>
		</author>
		<imprint>
			<publisher>Critical Condition</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
