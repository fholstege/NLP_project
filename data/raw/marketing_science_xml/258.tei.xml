<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Scalable Rejection Sampling for Bayesian Hierarchical Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-03-24">March 24, 2015.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Braun</surname></persName>
							<email>braunm@smu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Damien</surname></persName>
							<email>paul.damien@mccombs.utexas.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Edwin L</orgName>
								<orgName type="institution" key="instit1">Cox School of Business</orgName>
								<orgName type="institution" key="instit2">Southern Methodist University</orgName>
								<address>
									<postCode>75275</postCode>
									<settlement>Dallas</settlement>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">McCombs School of Business</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Scalable Rejection Sampling for Bayesian Hierarchical Models</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2015-03-24">March 24, 2015.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2014.0901</idno>
					<note type="submission">Received: November 22, 2013; accepted: October 28, 2014; Pradeep Chintagunta, Dominique Hanssens, and John Hauser served as special issue editors and Robert McCulloch served as associate editor for this article.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>parallel Bayesian computation</term>
					<term>rejection sampling</term>
					<term>big data</term>
					<term>multilevel models</term>
					<term>marginal likelihood</term>
					<term>customer heterogeneity</term>
					<term>MCMC</term>
					<term>sparse optimization</term>
					<term>exploiting sparsity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In 1970, John D. C. Little famously wrote, "The big problem with management science models is that managers practically never use them. There have been a few applications, of course, but the practice is a pallid picture of the promise" <ref type="bibr">(Little 1970, p. B-466)</ref>. The same may be true today about Bayesian estimation of hierarchical probability models. The impact Bayesian methods have had on academic research across multiple disciplines in the managerial, social, and natural sciences is undeniable. Marketing, in particular, has benefited from Bayesian methods because of their natural suitability for capturing heterogeneity in customer types and tastes <ref type="bibr" target="#b38">(Rossi and Allenby 2003)</ref>. But further diffusion of Bayesian methods is constrained by a scalability problem. As the size and complexity of data sources for both research and commercial purposes grow, the impracticality of simulation-based Bayesian methods for estimating parameters of a general class of hierarchical models becomes increasingly salient <ref type="bibr" target="#b0">(Allenby et al. 2014</ref>).</p><p>The problem is not with the Bayesian approach itself, but with the most familiar methods of simulating from the posterior distributions of the parameters. Without question, the resurgence of Bayesian ideas is due to the popularity of Markov chain Monte Carlo (MCMC), which was introduced to statistical researchers by <ref type="bibr" target="#b21">Gelfand and Smith (1990)</ref> via the Gibbs sampler. MCMC estimation involves iteratively sampling from the marginal posterior distributions of blocks of parameters. Only after some unknown (and theoretically infinite) number of iterations will the algorithm generate samples from the correct distributions; earlier samples are discarded. The Bayesian computational literature has exploded with numerous methods for generating valid and efficient MCMC algorithms. It would be difficult to list all of them here, so we refer the reader to <ref type="bibr" target="#b22">Gelman et al. (2003)</ref>, <ref type="bibr" target="#b10">Chen et al. (2000)</ref>, <ref type="bibr" target="#b39">Rossi et al. (2005)</ref>, and <ref type="bibr" target="#b8">Brooks et al. (2010)</ref>, and the hundreds of references therein.</p><p>Despite the justifiable success MCMC has enjoyed, there remains the question of whether a particular chain has run long enough that we can start collecting samples for estimation (or, colloquially, whether the chain has "converged" to the target distribution). This is a particular problem for hierarchical models in which each heterogeneous unit is characterized by its own set of parameters. For example, each household in a customer data set might have its own preferences for product attributes. Both the number of parameters and the cycle time for each MCMC iteration grow with the size of the data set. Also, if the data represent outcomes of multiple interdependent processes (such as the timing and magnitude of purchases), both the posterior parameters and successive MCMC samples tend to be correlated, requiring a Marketing Science 35(3), pp. <ref type="bibr">427-444, © 2016 INFORMS</ref> larger, yet unknown, number of iterations. We believe that the most important reason Bayesian methods have not been embraced "in the field" nearly as much as classical approaches is that they are difficult and expensive to implement routinely using MCMC, even with semiautomated software procedures. Practitioners simply do not have an academician's luxury of letting an MCMC chain run for days or weeks with no guarantee that the chain has converged to produce "correct" answers at the end of the process.</p><p>With recent developments in multiple core processing and distributed computing systems, it is reasonable to look to parallel computing technology as a solution to the convergence problem. However, each MCMC cycle depends on the outcome of the previous one, so we cannot collect posterior samples in parallel by allocating the work across distributed processing cores. Using parallel processors to generate one draw from a target distribution, or running several MCMC chains in parallel, is not the same as generating all of the required independent samples in parallel. Hence, extant parallel MCMC methods are also subject to the same pesky question of convergence; indeed, now one has to ensure that all of the parallel chains have converged. On the other hand, non-MCMC methods like rejection sampling have the advantage of being able to generate samples from the correct target posterior in parallel, but these methods are beset with their own set of implementation issues. For instance, the inability to find efficient "envelope" distributions renders standard rejection sampling almost impractical for all but the smallest problems.</p><p>In this paper, we propose a solution to sample from Bayesian parametric, hierarchical models that is inspired by two pre-MCMC approaches: rejection sampling and sampling from a multivariate normal (MVN) approximation around the posterior mode. Our contribution is an algorithm that recasts traditional rejection sampling in a way that circumvents the difficulties associated with these two approaches. The algorithm requires that one be able to compute the unnormalized log posterior of the parameters (or a good approximation of it), that the posterior distribution is bounded from above over the parameter space, and that available computing resources can locate any local maxima of the log posterior. There is no need to derive conditional posterior distributions (as with blockwise Gibbs sampling), and there are no conjugacy requirements.</p><p>We present the details of our method in §2, and in §3 we share some examples that demonstrate the method's effectiveness. In broad strokes, the method involves scaling an MVN distribution around the mode and using that distribution as the source of proposal draws for the modified rejection algorithm. At first glance, one might think that finding the posterior mode and sampling from an MVN distribution are themselves intractable tasks in large dimensions. After all, the Hessian of the log posterior density, which grows quadratically with the number of parameters, is an important determinant of the efficiency of both MVN sampling and nonlinear optimization. Fortunately, several independent software development projects have spawned novel, freely available numerical computation tools that, when used together, allow our method to scale. In §4, we explain how to manage this scalability issue and show that the complexity of our method scales approximately linearly with the number of heterogeneous units.</p><p>Another complication of Bayesian methods is the estimation of the marginal likelihood of the data. The marginal likelihood is the probability of observing the data under the proposed model, which can be used as a metric for model comparison. Except in rare special cases, computing the marginal likelihood involves numerically integrating over all of the prior parameters; note that we consider hyperpriors to be part of the data in this case. In §5, we explain how to estimate the marginal likelihood as a by-product of our method.</p><p>In §6, we discuss key implementation issues and identify some relevant software tools. We also discuss limitations of our approach. We are not claiming that our method should replace MCMC in all cases. It may not be practical for models with discrete parameters, with a very large number of modes, or for which computing the log posterior density itself is difficult. The method does not require that the model be hierarchical or that the conditional independence assumption hold, but without those assumptions, it will not be as scalable. Nevertheless, many models of the kind researchers encounter could be properly estimated using our method, at least relative to the effort involved in using MCMC. Like MCMC and other non-MCMC methods, our method is another useful algorithm in the researcher's and practitioner's toolkits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Theoretical Basis</head><p>The goal is to sample a parameter vector from a posterior density y , where is the prior on , f y is the data likelihood conditional on , and y is the marginal likelihood of the data. Therefore,</p><formula xml:id="formula_0">y = f y y = y y (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where y is the joint density of the data and the parameters (of the unnormalized posterior density). In a marketing research context, under the conditional 429 independence assumption, the likelihood can be factored as</p><formula xml:id="formula_2">f y = N i=1 f i y i i (2)</formula><p>where i indexes households. 1 Each y i is a vector of observed data, each i is a vector of heterogeneous parameters, and is a vector of homogeneous population-level parameters. The i are distributed across the population of households according to a mixing distribution i , which also serves as the prior on each i . The elements of may influence either the household-level data likelihoods or the mixing distribution (or both). In this example, includes all 1 N and all elements of . The prior itself can be factored as</p><formula xml:id="formula_3">= N i=1 i i × (3)</formula><p>Let * be the mode of y , which is also the mode of y , since y is a constant that does not depend on . One will probably use some kind of iterative numerical optimizer to find * , such as a quasi-Newton line search or trust region algorithm. Define c 1 = * y , and choose a proposal distribution g that also has its mode at * . Define c 2 = g * , and define the function</p><formula xml:id="formula_4">y = f y • c 2 g • c 1 (4)</formula><p>Through substitution and rearranging terms, we can write the target posterior density as</p><formula xml:id="formula_5">y = y • g • c 1 c 2 • y (5)</formula><p>An important restriction on the choice of g is that the inequality 0 ≤ y ≤ 1 must hold, at least for any with a nonnegligible posterior density. We discuss this restriction, along with the choice of g , in more detail in §2.3.</p><p>Next, let u y be an auxiliary variable that is distributed uniformly on 0 y / y , so that</p><formula xml:id="formula_6">p u y = y / y = c 1 / c 2 y g</formula><p>Then construct a joint density of y and u y, where</p><formula xml:id="formula_7">p u y = y y u &lt; y (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>By integrating Equation ( <ref type="formula" target="#formula_7">6</ref>) over u, the marginal density of y is</p><formula xml:id="formula_9">p y = y y y 0 du = y (7)</formula><p>Simulating from p y is now equivalent to simulating from the target posterior y . Using Equations ( <ref type="formula">5</ref>) and ( <ref type="formula" target="#formula_7">6</ref>), the marginal density of u y is</p><formula xml:id="formula_10">p u y = y y u &lt; y d (8) = c 1 c 2 y u &lt; y g d (9) = c 1 c 2 y q u (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where q u = u &lt; y g d . This q u function is the probability that any candidate draw from g will satisfy y &gt; u. The sampler comes from recognizing that p u y can be written differently from, but equivalently to, Equation ( <ref type="formula" target="#formula_7">6</ref>)</p><formula xml:id="formula_12">p u y = p u y p u y (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>The method involves sampling a u from an approximation to p u y and then sampling from p u y . Using the definitions in Equations ( <ref type="formula">4</ref>)-(6), we get</p><formula xml:id="formula_14">p u y = p u y p u y (12) = c 1 c 2 y p u y u &lt; y g (13)</formula><p>To sample directly from p u y , one needs only to sample from p u y and then sample repeatedly from g until y &gt; u. The samples of form the marginal distribution p y , and since sampling from p y is equivalent to sampling from y , they form an empirical estimate of the target posterior density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Implementation</head><p>But how does one simulate from p u y ? In Equation (8), we see that p u y is proportional to the function q u . <ref type="bibr" target="#b43">Walker et al. (2011)</ref> sample from a similar kind of density by first taking M proposal draws from the prior to construct an empirical approximation to q u and then approximating that continuous density using Bernstein polynomials. However, in highdimensional models, this approximation tends to be a poor one at the end points, even with an extremely large number of Bernstein polynomial components.</p><p>Our approach is similar in that we effectively trace out an empirical approximation to q u by repeatedly sampling from g and computing y for each </p><formula xml:id="formula_15">q v v = q u exp −v .</formula><p>With q v v denoting the "true" cumulative distribution function (CDF) of v, letq v v be the empirical CDF of v after taking M proposal draws from g and ordering the proposals 0 &lt; v 1 &lt; v 2 &lt; • • • &lt; v M &lt; . To be clear,q v v is the proportion of samples that are strictly less than v. As M becomes large, the empirical approximation becomes more accurate.</p><p>Becauseq v v is discrete, we can sample from a density proportional to q u exp −v by partitioning the domain into M + 1 segments with the break point of each partition at each v i . The probability of sampling a new v that falls between v i and v i+1 is now</p><formula xml:id="formula_16">i =q v v exp −v i − exp −v i+1<label>(14)</label></formula><p>so we can sample an interval bounded by v i and v i+1 from a multinomial density with weights proportional to i . Once we have the i that corresponds to that interval, we can sample the continuous v by sampling from a standard exponential density, truncated on the right at v i+1 − v i , and setting v = v i + . Thus, we can sample v by first sampling i with weight i , then sampling a standard uniform random variable , and finally setting</p><formula xml:id="formula_17">v = v i − log 1 − 1 − exp v i − v i+1<label>(15)</label></formula><p>To sample R independent draws from the target posterior, we need R "threshold" draws of v. Then, for each v, we repeatedly sample from g until − log y &lt; v. Once we have a that meets this criterion, we save it as a valid sample from y . The complete algorithm is summarized as Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The Proposal Distribution</head><p>The only restriction on g is that the inequality 0 ≤ y ≤ 1 must hold, at least for any with a nonnegligible posterior density. Because v &gt; 0, we must have u &lt; 1. Thus, any for which y &gt; 1 would always be accepted, no matter how small Y might be. By construction, * y = 1, meaning that no candidate will have a higher acceptance probability than the with the highest posterior density. This is an intuitively appealing property.</p><p>In principle, it is up to the researcher to choose g , and some choices may be more efficient than others. We have found that a MVN proposal distribution, with mean * , works well for the kinds of continuous posterior densities that marketing researchers typically encounter. The MVN density, with a covariance matrix equal to the negative inverse Hessian of the log posterior at * , is an asymptotic approximation (specifically, a second-order Taylor series) to the posterior density itself <ref type="bibr">(Carlin and</ref>  <ref type="table" target="#tab_6">Louis 2000,  §5.2)</ref>. if log m y &gt; 0 then 15:</p><p>FLAG ← TRUE 16: break 17:</p><p>end if 18: end for 19: end while 20: Reorder elements of v, so 0</p><formula xml:id="formula_18">&lt; v 1 &lt; v 2 &lt; • • • &lt; v M &lt; . Define v M+1 = 21: for i = 1 to M do 22:q v v i ← M j=1 v j &lt; v i . 23: i ←q v v i exp −v i − exp −v i+1 . 24: end for 25: for r = 1 to R do 26: Sample j ∼ Multinomial 1 M . 27: Sample ∼ Uniform 0 1 . 28: v * ← v j − log 1 − 1 − exp v j − v j+1 . 29: p ← 0 30: n r ← 0.</formula><p>{Counter for number of proposals} 31: while p &gt; v * do 32:</p><p>Sample r ∼ g . 33:</p><p>p ← − log r y . 34:</p><p>n r ← n r + 1. 35: end while 36: end for 37: return 1 R (plus n 1 n R and v 1 v M if computing a marginal likelihood).</p><p>By multiplying that covariance matrix by a scaling constant s, we can derive a proposal distribution that has the general shape of the target posterior near its mode. That proposal distribution will be valid as long as s is large enough so that y is between 0 and 1 for any plausible value of and the mode of g is at * . We illustrate the idea of scaling the proposal density in Figure <ref type="figure" target="#fig_0">1</ref>. The solid line (the same in all three panels) is a "target" posterior density. The dotted lines plot potential normal proposal densities, multiplied by the corresponding c 2 /c 1 ratio. The covariance of the proposal density in the left panel is the negative inverse Hessian of the log posterior density. Samples from the left tail of the posterior distribution will have y &gt; 1. In the middle and right panels, the covariance is the same as in the left panel, but multiplied by 1.4 and 1.8, respectively. As the covariance increases, more of the posterior support will have y ≤ 1. It is possible that g could undersample values of in the tails of the posterior. However, if M is sufficiently large, and y ≤ 1 for all M proposals, then it is unlikely that we would see y &gt; 1 in the rejection sampling phase of the algorithm. If that were to happen, we can stop, rescale, and try again. Any values of that we might miss would have such low posterior density that there would be little meaningful effect on inferences we might make from the output.</p><p>We believe that the potential cost from undersampling the tails is dwarfed by our method's relative computational advantage. We recognize that there may be some applications for which sampling extreme values of may be important, and this may not be the best estimation method for those applications. Otherwise, there is nothing special about using the MVN distribution for g . It is straightforward to implement with a manual adaptive selection of s. This is similar, in spirit, to the concept of tuning a Metropolis-Hastings algorithm. Heavier-tailed proposals, such as the multivariate-t (MVT) distribution, can fail because of high kurtosis at the mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Comparison to Other Methods</head><p>2.4.1. Rejection Sampling. At first glance, our method looks quite similar to standard rejection sampling. With rejection sampling, one repeatedly samples both a threshold value from a standard uniform distribution (p u = 1) and a proposal from g until y /g ≥ Ku, where K is a positive constant. This is different from our method, for which the threshold values are sampled from a posterior p u y , and K is specifically defined as the ratio of modal densities of the posterior to the proposal. The advantages that rejection sampling has over our approach are that the distribution of u is exact, and the proposal density does not have to dominate the target density for all values of . However, for any model with more than a few dimensions, the critical ratio can be extremely small for even small deviations of away from the mode. Thus, the acceptance probabilities are virtually nil. In contrast, we accept a discrete approximation of p u y in exchange for higher acceptance probabilities.</p><p>2.4.2. Direct Sampling. <ref type="bibr" target="#b43">Walker et al. (2011)</ref> introduced and demonstrated the merits of a non-MCMC approach called direct sampling for conducting Bayesian inference. Like our method, direct sampling removes the need to concern oneself with issues like chain convergence and autocorrelation, and generates independent samples from a target posterior distribution in parallel. <ref type="bibr" target="#b43">Walker et al. (2011)</ref> also proved that the sample acceptance probabilities using direct sampling are better than those from standard rejection algorithms. Put simply, for many common Bayesian models, they demonstrate an improvement over MCMC in terms of efficiency, resource demands, and ease of implementation.</p><p>However, direct sampling suffers from some important shortcomings that limit its broad applicability. One is the failure to separate the specification of the prior from the specifics of the estimation algorithm. Another is an inability to generate accepted draws for even moderately sized problems; the largest number of parameters that <ref type="bibr" target="#b43">Walker et al. (2011)</ref> consider is 10. Our method allows us to conduct full Bayesian inference on hierarchical models in high dimensions, with or without conjugacy, without MCMC.</p><p>Although our method shares some important features with direct sampling, it differs in several respects. While direct sampling focuses on the shape of the data likelihood alone, we are concerned with the characteristics of the entire posterior density. Our method bypasses the need for Bernstein polynomial approximations, which are integral to the direct sampling algorithm. Finally, whereas direct sampling takes proposal draws from the prior (which may conflict with the data), our method samples proposals from a separate density that is ideally a good approximation to the target posterior density itself.</p><p>Marketing Science 35(3), pp. 427-444, © 2016 INFORMS 2.4.3. Markov Chain Monte Carlo. We have already mentioned the key advantage of our method over traditional MCMC: generating independent samples that can be collected in parallel. We do not need to be concerned with issues like autocorrelation, convergence of estimation chains, and so forth. Without delving into a discussion of all possible variations and improvements to MCMC that have been proposed since <ref type="bibr" target="#b21">Gelfand and Smith (1990)</ref>, there have been some attempts to parallelize MCMC that deserve some mention. For a deeper analysis, see <ref type="bibr" target="#b41">Suchard et al. (2010)</ref>.</p><p>It is possible to run multiple independent MCMC chains that start from different starting points. Once all of those chains have converged to the posterior distribution, we can estimate the posterior by combining samples from all of the chains. The numerical efficiency of that approximation should be higher than if we used only one chain, because there should be no correlation between samples collected in different chains. However, each chain still needs to converge to the posterior independently, and only after that convergence can we start collecting samples. If it takes a long time for one chain to converge, it will take at least that long for all chains to converge. Thus, the potential for parallelization is much greater for our method than for MCMC.</p><p>Another approach to parallelization is to exploit parallel processing power for individual steps in an algorithm. One example is a parallel implementation of a multivariate slice sampler (MSS), as in <ref type="bibr" target="#b42">Tibbits et al. (2010)</ref>. The benefits of parallelizing the MSS come from parallel evaluation of the target density at each of the vertices of the slice, and from more efficient use of resources to execute linear algebra operations (e.g., Cholesky decompositions). But the MSS itself remains a Markovian algorithm, and thus will still generate dependent draws. Using parallel technology to generate a single draw from a distribution is not the same as generating all of the required draws themselves in parallel. The sampling steps of our method can be run in their entirety in parallel.</p><p>Another attractive feature of our method is that the model is fully specified by the log posterior, and possibly its gradient and Hessian. The tools that we discuss in §4 are components of a reusable infrastructure. Only the function that returns the log posterior changes from model to model. This feature is unlike a blockwise Gibbs sampler, for which we need to derive and implement a set of conditional densities for each model. A small change in the model specification can require a substantial change in the sampling strategy. For example, a change to a prior might mean that the sampler is no longer conditionally conjugate. So although it might be possible to construct a highly efficient Gibbs sampler for a particular model (e.g., through blocking, data augmentation, or parameter transformation), there can be considerable upfront investment in doing so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Examples</head><p>We now provide three examples of our method in action. In the first example, we simulate data from a basic, conditionally conjugate model and compare the marginal posterior distributions that were generated by our method with those from a Gibbs sampler. The second example is a nonconjugate hierarchical model of store-level retail sales. In that example, we compare estimates from our method with those from the Hamiltonian Monte Carlo (HMC) method, using the Stan software package <ref type="bibr" target="#b40">(Stan Development Team 2014)</ref>. For these first two examples, the MCMC methods are efficient, so it is likely that they generate good estimates of the true posterior densities. Thus, we can use those estimates as benchmarks against which we can assess the accuracy of our method. The third example is a more complicated model for which MCMC performs poorly. We use this example not only to assess the accuracy of our method (for those parameters for which we think the MCMC estimates are reasonable), but also to illustrate some of the computational problems that are inherent in MCMC methods. In all of our examples, we implemented our method using the bayesGDS package (Braun 2015a) for R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditionally Conjugate Model with</head><p>Simulated Data In our first example, we simulated T = 10 observations for each of N = 1 500 heterogeneous units. Each observation for unit i = 1 N is a sample from a normal distribution, with mean i and standard deviation = 2. The i are normally distributed across the population, with mean = −1 and standard deviation = 3. We place uniform priors on , log , and . There are 1,503 parameters in this model. This model allows for a conditionally conjugate Gibbs sampler; the steps are described in <ref type="bibr">Gelman et al. (2003, §11.7)</ref>. In this case, the Gibbs sampler is sufficiently fast and efficient, so we have confidence that it does indeed sample from the correct posterior distributions. The Gibbs sampler was run for 2,000 iterations, including a 1,000-iteration burn-in. The process took about five minutes to complete. We then collected 360 independent samples using our method, after estimating q v v with M = 70 000 proposals and applying a scaling factor on the inverse Hessian of 1.02. To get those 360 proposals, we needed 381,507 proposals. Using a single core of a 2014-vintage Apple Mac Pro, this process took about 23 minutes. However, each draw can be collected in parallel. It took about five minutes to collect all 360 samples when Ten of the 360 samples required more than 10,000 proposals.</p><p>Figure <ref type="figure">2</ref> shows the quantiles of samples in the estimated marginal posterior densities of the populationlevel parameters , 2 , and 2 , as well as the log of the unnormalized posterior density. We can see that both methods generate effectively the same estimated posterior distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Model Without Conditional Conjugacy</head><p>In the second example, we model weekly sales of sliced cheese in 88 retail stores. The data are available in the bayesm package for R (Rossi 2012), and were used by <ref type="bibr" target="#b3">Boatwright et al. (1999)</ref>. Under this model, mean sales for store i in week t has a gamma distribution with mean it and shape r i . The mean is a log-linear function of price and the percentage of "all category volume" on display in that week</p><formula xml:id="formula_19">log it = i1 + i2 log PRICE it + i3 DISP it (16)</formula><p>The prior on each r i is a half-Cauchy distribution with a scale parameter of 5, and the prior on each i is M = 1,000 M = 10,000 M = 50,000 Stan  <ref type="bibr" target="#b18">(Duane et al. 1987</ref><ref type="bibr" target="#b30">, Neal 2011</ref>, which uses the gradient of the log posterior to simulate a dynamic physical system. We selected HMC mainly because it is known to generate successive samples that are less serially correlated than draws that one might sample using other MCMC methods.</p><formula xml:id="formula_20">• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • 3.</formula><p>We implemented HMC using the Stan software package (Stan Development Team 2014). We ran five parallel chains for 800 iterations each, discarding the first half of the draws. The chains appeared to display little autocorrelation, so we have confidence that the HMC samples form a good estimate of the true posterior distributions. We then estimated the model using our method with different numbers of proposal draws to estimate q v v , and different scale factors on the covariance of the proposal density. In Figure <ref type="figure" target="#fig_2">3</ref>, we compare the estimated densities for elements of , the baseline and marginal effects on sales. We see that even with a relatively coarse estimate of q v v and an overly diffuse proposal density, our method generates estimates of the posterior densities that are not Marketing Science 35(3), pp. 427-444, © 2016 INFORMS only comparable with each other but also comparable with those generated by Stan. The mean acceptance rate across the different runs using our method was 0.0003. It took about 1.4 seconds to sample and evaluate a block of 1,000 proposals on a single processing core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model with Weakly Identified Parameters</head><p>Our third example concerns a nonconjugate heterogeneous hierarchical model in which some parameters are only weakly identified. The data structure is described by <ref type="bibr" target="#b29">Manchanda et al. (2004)</ref>; we use the simulated data that are available in the bayesm package for R <ref type="bibr" target="#b37">(Rossi 2012)</ref>. In this data set, for each of the 1,000 physicians, we observe weekly prescription counts for a single drug (y it ), weekly counts of sales visits from representatives of the drug manufacturer (x it ), and some time-invariant demographic information (z i ). Although one could model the purchase data as conditional on the sales visits, <ref type="bibr" target="#b29">Manchanda et al. (2004)</ref> argue that the rate of these contacts is determined endogenously, so that physicians who are expected to write more prescriptions, or who are more responsive to the sales visits, will receive visits more often.</p><p>In this model, y it is a random variable that follows a negative binomial distribution with shape r and mean it , and x it is a Poisson-distributed random variable with mean i . The expected number of prescriptions per week depends on the number of sales visits, so we let log it = i0 + i1 x it , where i is a vector of heterogeneous coefficients. We then model the contact rate so it depends on the physician-specific propensities to purchase, so log i = 0 + 1 i0 + 2 i1 . Define z i as a vector of four physician-specific demographics (including an intercept), and define as a 2 × 4 matrix of population-level coefficients. The mixing distribution for (i.e., the prior on each i ), is MVN with mean z i and covariance V . We place weakly informative MVN priors on and the rows of , an inverse Wishart prior on V and a gamma prior on r. There are 2,015 distinct parameters to be estimated. This model differs slightly from the one in the paper by <ref type="bibr" target="#b29">Manchanda et al. (2004)</ref> only in that i depends only on expected sales in the current period, and not the long-term trend. We made this change to make it easier to run the baseline MCMC algorithm.</p><p>As before, our baseline estimation algorithm is HMC, but instead of using Stan, we use the "double averaging" method to adaptively scale the step size <ref type="bibr" target="#b26">(Hoffman and Gelman 2014)</ref>, and we set the expected path length to 16. 2 By implementing HMC ourselves, 2 Shorter path lengths were less efficient, and longer ones frequently jumped so far from the regions of high posterior mass that the computation of the log posterior would underflow. We had the same Iteration (× 1,000) log posterior density Notes. One chain was started at the mode, and the others were started at random values. Every 500th iteration is plotted.</p><p>we can use the same computer code to compute the log posterior, and its gradient, that we use with our method. This allows the two methods to compete on a level playing field.</p><p>We ran four independent HMC chains for 700,000 iterations each, during a period of more than three weeks. Searching for the posterior mode is considered, in general, to be "good practice" for Bayesian inference, and especially with MCMC; see Step 1 of the "Recommended Strategy for Posterior Simulation" in §11.10 of <ref type="bibr" target="#b22">Gelman et al. (2003)</ref>. Finding the mode of the log posterior is the first step of our method anyway, so we initialized one chain there, and the other three at randomly selected starting values. Figure <ref type="figure">4</ref> is a trace plot of the log posterior density. The chains begin to approach each other only after about 500,000 iterations. The panels in Figure <ref type="figure" target="#fig_4">5</ref> are trace plots of the population-level parameters. Some parameter chains appear to have converged to each other, with little autocorrelation, but others seem to have made no progress at all. Table <ref type="table" target="#tab_3">1</ref> summarizes the effective sample sizes for estimates of the marginal posterior distributions of population-level parameters, using the final 100,000 samples of the HMC chains. Many of these parameters may require more than a million additional iterations to achieve an effective sample size large enough to make reasonable inferences.</p><p>The convergence problems are even worse when we consider that each of the 16 steps in the path length for iteration requires one evaluation for both the log posterior and its gradient. Using "reverse problem with the No U-Turn Sampler (Hoffman and Gelman 2014), whether using Stan, or coding the algorithm ourselves. The HMC extensions in <ref type="bibr" target="#b23">Girolami and Calderhead (2011)</ref> are inappropriate for this problem because the Hessian is not guaranteed to be positive definite for all parameter values.  Iteration (× 1,000)</p><formula xml:id="formula_21">γ 2 γ 3 chol(V 1, 1 ) chol(V 2, 2 ) mean( 1 ) mean<label>(</label></formula><formula xml:id="formula_22">Value ∆ 2, 1 ∆ 1, 2 ∆ 2, 2 -0.01 0.00 0 0.02 0.03 ∆ 1, 3 ∆ 2, 3 ∆ 1, 4 ∆ 2, 4</formula><p>Note. Every 500th iteration is plotted.</p><p>mode" automatic differentiation (AD), which we discuss more in §4.1, the time to evaluate the gradient is about five times the time it takes to evaluate the log posterior, regardless of the number of variables <ref type="bibr">(Griewank and Walther 2008, p. xii)</ref>. Therefore, each HMC iteration requires resources that equate to 96 evaluations of the posterior. In other words, the computational cost of 700,000 HMC iterations is equivalent to more than 67 million evaluations of the log posterior. And this assumes that 700,000 iterations were sufficient to collect enough samples from the true posterior. So how much more efficient is our sampling method? We estimated the marginal density q v v by taking M = 100 000 proposal draws from an MVN distribution with the mean at the posterior mode * and the covariance matrix equal to the inverse of the Hessian at the mode, multiplied by a scaling factor of s = 1 3. This value of s is the smallest value for which 0 ≤ y ≤ 1 for all 100,000 samples from g . We then collected 300 independent samples in parallel from the target posterior y . The median number of proposals required for each posterior sample was just under 38,000, the total number of likelihood evaluations was about 16.5 million, and the average acceptance rate was 1 8 × 10 −5 .</p><p>In absolute terms, the low acceptance rate appears to be unfavorable. However, the total run time is much lower than for MCMC. In our implementation (using a single core on a 2014-vintage Apple Mac Pro), the total time to compute the log posterior density of 1,000 proposal samples is about 8.7 seconds. The time to sample 1,000 proposals from an MVN distribution and compute the MVN densities for each is about 0.89 seconds. Therefore, to collect and evaluate 16.5 million proposal samples (to get 300 posterior draws) would take about 44 hours. But this is for a single processing core. Using all 12 cores on our Apple Mac Pro, the sampling time is reduced to 220 minutes. We discuss the scalability of the component steps of the algorithm in §4. Access to more processing nodes could reduce this time even further. It is the ability to collect posterior samples in parallel that gives our method its greatest advantage over MCMC methods. One can run multiple MCMC chains in parallel, but this involves waiting until all of the chains, individually, converge to the target posterior before one can begin collecting samples for inference. Even then, there is no way to confirm that the chain has, in fact, converged.</p><p>We can draw inferences about the accuracy of our method by comparing the estimated marginal densities to those that we get from HMC. Note that the HMC estimates are accurate only if all of the chains converge to the target density, and we have a largeenough effective sample size. This condition clearly does not hold, but it is sufficiently close for the majority of the population-level parameters for us to use HMC samples as a baseline standard. Figure <ref type="figure">6</ref> is a comparison of the quantiles. For the elements of and the Cholesky decomposition of V , our method's estimated distributions are close to the HMC estimates. For other parameters, the convergence of the  </p><formula xml:id="formula_23">∆ 1, 1 γ 1 ∆ 2, 1 ∆ 1, 2 ∆ 2, 2 ∆ 1, 3 ∆ 2, 3 ∆ 1, 4 ∆ 2, 4 chol(V 2, 1 ) chol(V 2, 2 ) mean( 1 ) mean( 2 )</formula><p>estimates is less clear. However, the parameters for which the densities are not aligned are the same parameters for which there is high autocorrelation, and little movement, in the HMC chains. Thus, we infer that our method compares with HMC well in terms of the marginal densities that it generates, with substantial computational effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Scalability and Sparsity</head><p>The ability for our method to generate independent samples in parallel already makes it an attractive alternative to MCMC. In this section, we present an argument in favor of our method's scalability. Our criterion for scalability is that the cost of running the algorithm grows close to linearly in the number of households. Our analysis considers the fundamental computational tasks involved: computing the log posterior, its gradient, and its Hessian; computing the Cholesky decomposition of the Hessian; and sampling from an MVN proposal distribution. We will show that scalability can be achieved because, under the conditional independence assumption, the Hessian of the log posterior is sparse, with a predictable sparsity pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Computing Log Posteriors, Gradients, and Hessians</head><p>Under the conditional independence assumption, the log posterior density is the sum of the logs of Equations ( <ref type="formula">2</ref>) and (3), with a heterogeneous component</p><formula xml:id="formula_24">N i log f i y i i + log i i (17)</formula><p>and a homogeneous component log . This homogeneous component is the hyperprior on the population-level parameters, so its computation does not depend on N , while each additional household adds another element to the summation in Equation (17). Therefore, computation of the log posterior grows linearly in N . In the subsequent text, let k be the number of elements in each i , and let p be the number of elements in . Using the notation from §2, is a vector that concatenates all of the i together, along with .</p><p>There are two reasons why we might need to compute the gradient and Hessian of the log posterior, namely, for use in a derivative-based optimization algorithm to find the posterior mode and for estimating the precision matrix of an MVN proposal distribution. <ref type="bibr">3</ref> Ideally, we would derive the gradient and Hessian analytically, and write code to estimate them efficiently. For complicated models, the required effort for coding analytic gradients may not be worthwhile. An alternative would be a numerical approximation through finite differencing. The fastest, yet least accurate, method for finite differencing for gradients, using either forward or backward differences, requires Nk + p + 1 evaluations of the log posterior. Since the computational cost of the log posterior also grows linearly with N , computing the gradient this way will grow quadratically in N . The cost of estimating a Hessian using finite differencing grows even faster in N . Also, if the Hessian is estimated by taking finite differences of gradients, and those gradients themselves are finite differences, the accumulated numerical error can be so large that the Hessian estimates are useless.</p><p>Instead, we can turn to AD (also sometimes known as algorithmic differentiation). A detailed explanation of AD is beyond the scope of this paper, so we refer the reader to <ref type="bibr" target="#b25">Griewank and Walther (2008)</ref>, or §8.2 in <ref type="bibr" target="#b33">Nocedal and Wright (2006)</ref>. Put simply, AD treats a function as a composite of subfunctions, and computes derivatives by repeatedly applying the chain rule. In practical terms, AD involves coding the log posterior using a specialized numerical library that keeps track of the derivatives of these subfunctions. When we compile the function that computes the log posterior, the AD library will "automatically" generate additional functions that return the gradient, the Hessian, and even higher-order derivatives. <ref type="bibr">4</ref> The remarkable feature of AD is that computing the gradient of a scalar-valued function takes no more than five times as long as computing the log posterior, regardless of the number of parameters <ref type="bibr">(Griewank and Walther 2008, p. xii)</ref>. If the cost of the log posterior grows linearly in N , so will the cost of the gradient.</p><p>In most statistical software packages, like R, the default storage mode for any matrix is in a "dense" format; each element in the matrix is stored explicitly, regardless of the value. For a model with n variables, this matrix consists of n 2 numbers, each consuming eight bytes of memory at double precision. If we have a data set in which N = 10 000, k = 5, and p is relatively small, the Hessian for this model with 50 000 + p variables will consume more than 20 GB of RAM. Furthermore, the computational effort for matrix-vector multiplication is quadratic in the number of columns, and matrix-matrix multiplication is cubic. To the extent that either of these operations is used in the mode-finding or sampling steps, the computational effort will grow much faster than the size of the data set. Since multiplying a triangular matrix is roughly one-sixth as expensive as multiplying a full matrix, we could gain some efficiency by working with the Cholesky decomposition of the Hessian instead. However, the complexity of the Cholesky decomposition algorithm itself will still be cubic in N (Golub and Van Loan 1996, Chapter 1).</p><p>For our purposes, the source of scalability is in the sparsity of the Hessian. If the vast majority of elements in a matrix are zero, we do not need to store them explicitly. Instead, we need to store only the nonzero values, the row numbers of those values, and the index of the values that begin each column. <ref type="bibr">5</ref> Under the conditional independence assumption, the cross-partial derivatives between heterogeneous parameters across households are all zero. Thus, the Hessian becomes sparser as the size of the data set increases.</p><p>To illustrate, suppose we have a hierarchical model with six households, two heterogeneous parameters <ref type="bibr">4</ref> There are a number of established AD tools available for researchers to use for many different programming environments. For C++, we use CppAD <ref type="bibr" target="#b2">(Bell 2013)</ref>, although ADOL-C <ref type="bibr" target="#b44">(Walther and Griewank 2012</ref>) is also popular. We call our C++ functions from R (R Development Core Team 2014) using the Rcpp package <ref type="bibr">François 2011, Bates and</ref><ref type="bibr" target="#b1">Eddelbuettel 2013)</ref>. CppAD is also available for Python. Matlab users have access to ADMAT <ref type="bibr" target="#b13">(Coleman and Verma 2000)</ref>, among other options. <ref type="bibr">5</ref> This storage scheme is known as the compressed sparse column format. This common format is used by the Matrix package in R and the Eigen numerical library, but it is not the only way to store a sparse matrix. per household, and two population-level parameters, for a total of 14 parameters. Figure <ref type="figure">7</ref> is a schematic of the sparsity structure of the Hessian; the vertical lines are the nonzero elements, and the dots are the zeros.</p><p>There are 196 elements in this matrix, but only 76 are nonzero, and only 45 values are unique. Although the savings in RAM is modest in this illustration, the efficiencies are much greater when we add more households. If we had 1,000 households, with k = 3 and p = 9, there would be 3,009 parameters, and more than nine million elements in the Hessian, yet no more than 63,000 would be nonzero, of which about 27,600 would be unique. As we add households, the number of nonzero elements of the Hessian grows only linearly in N . The cost of estimating a dense Hessian using AD grows linearly with the number of variables <ref type="bibr" target="#b25">(Griewank and Walther 2008)</ref>. When the Hessian is sparse, with a pattern similar to that in Figure <ref type="figure">7</ref>, we can estimate the Hessian so that the cost is only a multiple of the cost of computing the log posterior. We achieve this by using a graph coloring algorithm to partition the variables into a small number of groups (or "colors" in the graph theory literature), such that a small change in the variable in one group does not affect the partial derivative of any other variable in the same group. This means we could perturb all of the variables in the same group at the same time, recompute the gradient, and, after doing that for all groups, still be able to recover an estimate of the Hessian. Thus, the computational cost for computing the Hessian grows with the number of groups, not the number of parameters. Because the householdlevel parameters are conditionally independent, we do not need to add groups as we add households. For the Hessian sparsity pattern in Figure <ref type="figure">7</ref>, we need only four groups: one for each of the heterogeneous parameters across all of the households, and one for each of the two population-level parameters. In the upcoming binary choice example in §4.4, for which k = 3, there are 1 2 k 2 + 5k = 12 groups, no matter how many households we have in the data set. <ref type="bibr" target="#b16">Curtis et al. (1974)</ref> introduce the idea of reducing the number of evaluations to estimate sparse Jacobians. <ref type="bibr" target="#b34">Powell and Toint (1979)</ref> describe how to partition variables into appropriate groups and how to recover Hessian information through backsubstitution. <ref type="bibr" target="#b12">Coleman and Moré (1983)</ref> show that the task of grouping the variables amounts to a classic graph-coloring problem. Most AD software applies this general principle to computing sparse Hessians. Alternatively, R users can use the sparseHessianFD package <ref type="bibr" target="#b6">(Braun 2015b</ref>) to efficiently estimate sparse Hessians through finite differences of the gradient, as long as the sparsity pattern is known in advance, and as long as the gradient was not itself estimated through finite differencing. This package is an interface to the algorithms in <ref type="bibr">Coleman et al. (1985a, b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Finding the Posterior Mode</head><p>For simple models and small data sets, standard default algorithms (like the optim function in R) are sufficient for finding posterior modes and estimating Hessians. For larger problems, one should choose optimization tools more thoughtfully. For example, many of the R optimization algorithms default to finite differencing of gradients when a gradient function is not provided explicitly. Even if the user can provide the gradient, many algorithms will store a Hessian, or an approximation to it, densely. Neither feature is attractive when the number of households is large.</p><p>For this section, let us assume that the log posterior is twice differentiable and unimodal. <ref type="bibr">6</ref> There are two approaches that one can take. The first is to use a "limited memory" optimization algorithm that approximates the curvature of the log posterior over successive iterations. Several algorithms of this kind are described in <ref type="bibr" target="#b33">Nocedal and Wright (2006)</ref>, and are available for many technical computing platforms. Once the algorithm finds the posterior mode, there remains the need to compute the Hessian exactly.</p><p>The second approach is to run a quasi-Newton algorithm and compute the Hessian at each iteration explicitly, but store the Hessian in a sparse format. The trustOptim package for R (Braun 2014) implements a trust region algorithm that exploits the sparsity of the Hessian. The user can supply a Hessian that is derived analytically, computed using AD, or estimated numerically using sparseHessianFD. Since memory requirements and matrix computation costs will grow only linearly in N , finding the posterior mode becomes feasible for large problems, compared to similar algorithms that ignore sparsity.</p><p>We should note that we cannot predict the time to convergence for general problems. Log posteriors with ridges or plateaus, or that require extensive computation themselves, may still take a long time to find the local optimum. Whether any mode-finding algorithm is "fast enough" depends on the specific application. However, if one optimization algorithm has difficulty finding a mode, another algorithm may do better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sampling from an MVN Distribution</head><p>Once we find the posterior mode, and the Hessian at the mode, generating proposal samples from an MVN( * sH −1 ) distribution is straightforward. Let 1/s H = represent the Cholesky decomposition of the precision of the proposal, and let z be a vector of Nk + p samples from a standard normal distribution. To sample from an MVN distribution, solve the triangular linear system = x, and then add * . Since E z = 0, E = * , and since</p><formula xml:id="formula_25">E zz = I, cov = −1 −1 = −1 = sH −1 . Because</formula><p>is sparse, the costs of both solving the triangular system and the premultiplication grow linearly with the number of nonzero elements, which itself grows linearly in N <ref type="bibr" target="#b17">(Davis 2006</ref>). If were dense, then the cost of solving the triangular system would grow quadratically in N . Furthermore, computing the MVN density would involve premultiplying z by a triangular matrix, whose cost is cubic in N <ref type="bibr" target="#b24">(Golub and Van Loan 1996)</ref>.</p><p>Computation of the Cholesky decomposition can also benefit from the sparsity of the Hessian. If H were dense Nk + p square, symmetric matrix, then, holding k and p constant, the complexity order of the Cholesky decomposition would be N 3 <ref type="bibr" target="#b24">(Golub and Van Loan 1996)</ref>. There are a number of different algorithms that one can use for decomposing sparse Hessians <ref type="bibr" target="#b17">(Davis 2006)</ref>. The typical strategy is to first permute the rows and columns of H to minimize the number of nonzero elements in , and then compute the sparsity pattern. This part can be done just once. With the sparsity pattern in hand, the next step is to compute those nonzero elements in . The time for this step grows with the sum of the squares of the number of nonzero elements in each column of <ref type="bibr" target="#b17">(Davis 2006)</ref>. Because each additional household adds k columns to , with an average of p + 1 2 k + 1 nonzero elements per column, we can compute the sparse Cholesky decomposition in time that is linear in N . Software for sparse Cholesky decompositions is widely available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Scalability Test</head><p>Next, we provide some empirical evidence of scalability through a simulation study. For a hypothetical data set with N households, let y i be the number of times household i visits a store during a T week period. The probability of a visit in a single week is p i , where logit p i = i x i , and x i is a vector of k covariates. The distribution of i across the population is MVN with mean¯ and covariance . In all conditions of the test, we set k = 3 and T = 52, and vary the number of households by setting N to one of 10 discrete values from 500 to 50,000. The "true" values of¯ are −10, 0, and 10, and the "true" is 0 1I. We place weakly informative priors on both¯ and .</p><p>In Figure <ref type="figure">8</ref>, we plot the average time, across 100 replications, to compute the log posterior, the gradient, and the Hessian. As expected, each of these computations grows linearly in N . In Figure <ref type="figure">9</ref>, we plot average times for the steps involved in sampling from an MVN distribution: adding a vector to columns of a dense matrix, computing a sparse Cholesky decomposition, multiplying a sparse triangular matrix by a dense matrix, sampling standard normal random variates, and solving a sparse triangular linear system. Again, we see that the time for all of these steps is linear in N .</p><p>Table <ref type="table" target="#tab_6">2</ref> summarizes the acceptance rates and scale factors when generating 50 samples from the posterior for different values of N . Although there is a weak trend of increasing acceptance rates with N , we cannot say with any certainty that acceptance rates will always be either larger or smaller for larger data sets. The acceptance rate could be influenced by using different scale factors on the Hessian for the MVN proposal density. However, we expect higher acceptance rates as the target posterior density approaches an MVN distribution asymptotically. Since none of the steps in the algorithm grows faster than linearly in N , we are confident in the scalability of the overall algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Estimating Marginal Likelihoods</head><p>Now we turn to another advantage of our method: the ability to generate accurate estimates of the marginal likelihood of the data with little additional computation. A number of researchers have proposed methods for approximating the marginal likelihood, y , from MCMC-generated samples <ref type="bibr" target="#b20">(Gelfand and Dey 1994</ref><ref type="bibr" target="#b32">, Newton and Raftery 1994</ref><ref type="bibr" target="#b11">, Chib 1995</ref><ref type="bibr" target="#b36">, Raftery et al. 2007</ref>), but no method has achieved universal acceptance as being consistent, stable, and easy to compute. In fact, <ref type="bibr" target="#b27">Lenk (2009)</ref> demonstrated that methods that depend solely on samples from the posterior density could suffer from a "pseudo-bias," and he proposed an importance-sampling method to correct for it. This pseudo-bias arises because the convex hull of MCMC samples defines only a subset of the posterior support, whereas y is defined as an integral of the   <ref type="bibr" target="#b27">Lenk (2009)</ref> demonstrated that his method dominates other popular methods, although with substantial computational effort. Thus, the estimation of the marginal likelihood remains a difficult problem in MCMC-based Bayesian statistics.</p><p>We estimate the marginal likelihood using quantities that we already collected during the course of the estimation procedure. Recall that q u is the probability that, given a threshold value u, a proposal from g is accepted as a sample from y . Therefore, after substituting in Equation ( <ref type="formula">8</ref>), we can express the expected marginal acceptance probability for any one posterior sample as</p><formula xml:id="formula_26">= 1 0 q u p u y du = c 1 c 2 y 1 0 q 2 u du (18)</formula><p>Applying a change of variables so v = − log u and then rearranging terms,</p><formula xml:id="formula_27">y = − c 1 c 2 0 q 2 v exp −v dv (<label>19</label></formula><formula xml:id="formula_28">)</formula><p>The values for c 1 and c 2 are immediately available from the algorithm. A reasonable estimator of isˆ ,  <ref type="formula" target="#formula_27">19</ref>), for which we use the same proposal draws that we already collected for estimatingq v v . The empirical CDF of these draws is discrete, so we can partition the support of q v v at v 1 v M . Also, sinceq v v is the proportion of proposal draws less than v, we have</p><formula xml:id="formula_29">q v v i = i/M. Therefore, 0 q 2 v exp −v dv ≈ M i=1 v i+1 v i i M 2 exp −v i dv (20) = 1 M 2 M i=1 i 2 exp −v i − exp −v i+1 (21) = 1 M 2 M i=1 2i − 1 exp −v i (22)</formula><p>Putting all of this together, we can estimate the marginal likelihood as</p><formula xml:id="formula_30">y ≈ c 1 M 2 c 2ˆ M i=1 2i − 1 exp −v i (23)</formula><p>As a demonstration of the accuracy of this estimator, we use the same linear regression example that <ref type="bibr" target="#b27">Lenk (2009)</ref> used.</p><formula xml:id="formula_31">y it ∼ N x i 2 i = 1 n t = 1 T (<label>24</label></formula><formula xml:id="formula_32">)</formula><formula xml:id="formula_33">∼ N 0 2 V 0 2 ∼ IG r (25)</formula><p>For this model, y is an MVT density, which we can compute analytically. This allows us to compare the estimates of y with the "truth." To do this, we conducted a simulation study for simulated data sets of different numbers of observations n ∈ 200 2000 and numbers of covariates k ∈ 5 25 100 . For each n k pair, we simulated 25 data sets. For each data set, 2 1 1 5 2 6 0 7 2 9 2 6 3 6 3 9 2 7 3 3</p><p>Note. For each condition, k = 3, so there are six population-level parameters and 3N heterogeneous parameters.</p><p>each vector x i included an intercept and k i.i.d. samples from a standard normal density. Thus, there were k + 2 parameters, corresponding to the elements of , plus . The true intercept term was 5, and the remaining true parameters were linearly spaced from −5 to 5. In all cases, there were T = 25 observations per unit. Hyperpriors were set as r = 2, = 1, 0 as a zero vector, and</p><formula xml:id="formula_34">V 0 = 0 2 • I k .</formula><p>For each data set, we collected 250 samples from the posterior density, with different numbers of proposal draws (M = 1 000 or 10,000) and different scale factors (s = 0 5, 0.6, 0.7, or 0.8) on the Hessian (−sH is the precision matrix of the MVN proposal density, and lower scale factors generate more diffuse proposals). We excluded the s = 0 8, n = 200 case because the proposal density was not sufficiently diffuse to ensure that y was between 0 and 1 across the M proposal draws.</p><p>Table <ref type="table" target="#tab_8">3</ref> presents the true log marginal likelihood (MVT), along with estimates using our method, the importance sampling method in <ref type="bibr" target="#b27">Lenk (2009)</ref>, and the harmonic mean estimator (HME) <ref type="bibr" target="#b32">(Newton and Raftery 1994)</ref>. We also included the mean acceptance (acc) probabilities and the standard deviations of the various estimates across the simulated data sets. Our estimates for the log marginal likelihood are remarkably close to the MVT densities and are robust when we use different scale factors. Accuracy appears to be better for larger data sets than smaller ones. Improving the approximation of p u y by increasing the number of proposal draws offers negligible improvement. The performance of our method is comparable to that of <ref type="bibr" target="#b27">Lenk's (2009)</ref> method, but is much better than that of the harmonic mean estimator. Our method is similar to <ref type="bibr" target="#b27">Lenk's (2009)</ref> in that it computes the probability that a proposal draw falls within the support of the posterior density. However, the inputs to the estimator of the marginal likelihood are intrinsically generated as the algorithm progresses. In contrast, the <ref type="bibr" target="#b27">Lenk (2009)</ref> estimator requires an additional importance sampling run after the MCMC draws are collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion of Practical Considerations and Limitations</head><p>To those who have spent long work hours dealing with MCMC convergence and efficiency issues, the utility of an alternative algorithm is appealing.</p><p>Ours allows for sampling from a posterior density in parallel, without having to worry about whether an MCMC estimation chain has converged. If heterogeneous units (like households) are conditionally independent, then the sparsity of the Hessian of the log posterior lets us construct a sampling algorithm whose complexity grows only linearly in the number of units. This method makes Bayesian inference more attractive to practitioners who might otherwise be put off by the inefficiencies of MCMC. This is not to say that our method is guaranteed to generate perfect samples from the target posterior distribution. One area of potential concern is that the empirical distributionq v v is only a discrete approximation to q v v . That discretization could introduce some error into the estimate of the posterior density. This error can be reduced by increasing M (the number of proposal draws that we use to computeq v v ), at the expense of costlier computation ofq v v and, possibly, lower acceptance rates. In our experience, and consistent with Figure <ref type="figure" target="#fig_2">3</ref>, we have not found this to be a problem, but some applications for which this may be an issue might exist.</p><p>Like many other methods that collect random samples from posterior distributions, its efficiency depends in part on a prudent selection of the proposal density g . For the examples in this paper, we use an MVN density that is centered at the posterior mode with a covariance matrix that is proportional to the inverse of the Hessian at the mode. One might then wonder if there is an optimal way to determine just how "scaled out" the proposal covariance needs to be. At this time, we think that manual search is the best alternative. If we start with a small M (say, 100 draws) and find that y &gt; 1 for any of the M proposals, we have learned that the proposal density is not valid, with little computational or real-time cost. We can then rescale the proposal until y &lt; 1, and then gradually increase M until we get a good approximation to p u . This is no different, in principle, than the tuning step in a Metropolis-Hastings algorithm. However, our method has the advantage that we can make these adjustments before the posterior sampling phase begins. In contrast, with adaptive MCMC methods, an improperly tuned sampler might not be apparent until the chain has run for a substantial period of time. Also, even if an acceptance rate appears to be low, we can still collect draws in parallel, so the "clock time" remains much less than  the time we spend trying to optimize selection of the proposal.</p><p>There are many popular models, such as multinomial probit, for which the likelihood of the observed data is not available in closed form. When direct numerical approximations to these likelihoods (e.g., Monte Carlo integration) are not tractable, MCMC with data augmentation is a possible alternative. Recent advances in parallelization using graphical processing units might make numerical estimation of integrals more practical than it was even 10 years ago <ref type="bibr" target="#b41">(Suchard et al. 2010</ref>). If so, then our method could be a viable, efficient alternative to data augmentation in these kinds of models. Multiple imputation of missing data could suffer from the same kinds of problems, since a latent parameter, introduced for the data augmentation step, is only weakly identified on its own. If the number of missing data points is small, then one could represent them as if they were parameters, but the implications of this require additional research.</p><p>Another opportunity for further research involves the case of multimodal posteriors. Our method does require finding the global posterior mode, and all of the models discussed in this paper have unimodal posterior distributions. When the posterior is multimodal, one might instead use a mixture of normals as the proposal distribution. The idea is to not only find the global mode, but any local ones as well, and center each mixture component at each of those local modes. The algorithm itself will remain unchanged as long as the global posterior mode matches the global proposal mode.</p><p>We recognize that finding all of the local modes could be a hard problem, and there is no guarantee that any optimization algorithm will find all local extrema in a reasonable amount of time. In practical terms, MCMC offers no such guarantees either. Even if the log posterior density is unimodal, one should take care that the mode-finding optimizer does not stop until it reaches the optimum. For R, trustOptim <ref type="bibr" target="#b4">(Braun 2014</ref>) is one such package, in that its stopping rule depends on the norm of the gradient being sufficiently close to zero.</p><p>There are a number of packages for the R statistical programming language that can help with implementation of our method. The bayesGDS package <ref type="bibr" target="#b5">(Braun 2015a</ref>) includes functions to run the rejection sampling phase (lines 20-36 in Algorithm 1). This package also includes a function that estimates the log marginal likelihood from the output of the algorithm. If the proposal distribution is MVN, and either the covariance or precision matrix is sparse, then one can use the sparseMVN <ref type="bibr" target="#b7">(Braun 2015c</ref>) package to sample from the MVN distribution by taking advantage of that sparsity. The sparseHessianFD <ref type="bibr" target="#b6">(Braun 2015b)</ref> package estimates a sparse Hessian by taking finite differences of gradients of the function, as long as the user can supply the sparsity pattern (which should be the case under conditional independence). Finally, the trustOptim package <ref type="bibr" target="#b4">(Braun 2014</ref>) is a nonlinear optimization package that uses a sparse Hessian to include curvature information in the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>Supplemental material to this paper is available at http://dx .doi.org/10.1287/mksc.2014.0901.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 (</head><label>1</label><figDesc>Algorithm to collect R samples from y ) 1: R ← number of required samples from y 2: M ← number of proposal draws for estimatinĝ q v v . 3: * ← mode of y 4: c 1 ← * y 5: FLAG ← TRUE 6: while FLAG do 7: Choose new proposal distribution g 8: FLAG ← FALSE 9: c 2 ← g * . 10: for m = 1 to M do 11: Sample m ∼ g . 12: log m y ← log m y − log g m − log c 1 + log c 2 . 13: v m = − log m y 14:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1 (Color online) A "Target" Posterior Density (Solid Line, Same in All Panels) and Three Scaled Normal Densities (Dotted Lines, Increasing in Covariance from Left to Right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Estimated Posterior Distributions of for the Example in §3.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4Trace Plots of Four HMC Chains for the Log Posterior Distribution of the Example in §3.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5Trace Plots of Four HMC Chains for Population-Level Parameters in the Example in §3.3, Starting at Iteration 500,000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 6Comparative Estimates of Posterior Distributions for the Example in §3.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 7 Example of Sparsity Pattern Under Conditional Independence [1,] | | . . . . . . . . . . | | [ 2,] | | . . . . . . . . . . | | [ 3,] . . | | . . . . . . . . | | [4,] . . | | . . . . . . . . | | [5,] . . . . | | . . . . . . | | [6,] . . . . | | . . . . . . | | [7,] . . . . . . | | . . . . | | [8,] . . . . . . | | . . . . | | [9,] . . . . . . . . | | . . | | [10, ] . . . . . . . . | | . . | | [11, ] . . . . . . . . . . | | | | [12, ] . . . . . . . . . . | | | | [13, ] | | | | | | | | | | | | | | [14, ] | | | | | | | | | | | | | |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 8Average Computation Time for 100 Evaluations of the Log Posterior, Gradient, and Hessian</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 9Computation Time, AveragedOver 100 Replications, for Adding a Vector to Matrix Columns (Add), a Sparse Cholesky Decomposition (Chol), Multiplying a Sparse Triangular Matrix by a Dense Matrix (Mult), Sampling Standard Normal Random Variates (Rnorm), and Solving a Sparse Triangular Linear System (Solve)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>To avoid the end point problem in the Walker et al. method, we instead sample a transformed variable v = − log u. Applying a change of variables,</figDesc><table><row><cell>Marketing Science 35(3), pp. 427-444, © 2016 INFORMS</cell></row><row><cell>of those proposal draws.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 2Comparative Estimates of Posterior Distributions for the Example in §3.1</figDesc><table><row><cell>-0.90</cell><cell>•</cell><cell></cell><cell>1.05</cell><cell>• • •</cell><cell>τ 2</cell><cell>• •</cell><cell>4.06</cell><cell>• • • • • •</cell><cell>2</cell><cell>-319,100</cell><cell>Log posterior • • •</cell></row><row><cell>-1.00 -0.95</cell><cell></cell><cell></cell><cell>0.95 1.00</cell><cell></cell><cell></cell><cell></cell><cell>4.00 4.04 4.02</cell><cell></cell><cell></cell><cell>-319,200 -319,150</cell></row><row><cell>-1.05</cell><cell>• •</cell><cell>• • • •</cell><cell>0.85 0.90</cell><cell>• • •</cell><cell></cell><cell></cell><cell>3.96 3.98</cell><cell>• •</cell><cell>•</cell><cell>-319,250</cell><cell>• • • • • •</cell></row><row><cell></cell><cell cols="2">Gibbs Ours</cell><cell></cell><cell cols="3">Gibbs Ours</cell><cell></cell><cell cols="2">Gibbs Ours</cell><cell></cell><cell>Gibbs Ours</cell></row><row><cell cols="7">using 12 processing cores. The mean number of pro-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">posals for each posterior sample was 1,060 (an accep-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">tance rate of 0.0009), but the median was only 29.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Effective Sample Sizes for Estimates of Posterior</cell><cell></cell></row><row><cell></cell><cell cols="3">Distributions for the Example in  §3.3, Using the Final</cell><cell></cell></row><row><cell></cell><cell cols="2">100,000 Samples in the HMC Chains</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Chain</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>1 1</cell><cell>295</cell><cell>250</cell><cell>266</cell><cell>267</cell></row><row><cell>1 2</cell><cell>16</cell><cell>24</cell><cell>27</cell><cell>29</cell></row><row><cell>1 3</cell><cell>6</cell><cell>12</cell><cell>26</cell><cell>14</cell></row><row><cell>1 4</cell><cell>51</cell><cell>43</cell><cell>43</cell><cell>60</cell></row><row><cell>2 1</cell><cell>4 703</cell><cell>5 933</cell><cell>5 981</cell><cell>1 444</cell></row><row><cell>2 2</cell><cell>507</cell><cell>573</cell><cell>526</cell><cell>538</cell></row><row><cell>2 3</cell><cell>407</cell><cell>445</cell><cell>400</cell><cell>407</cell></row><row><cell>2 4</cell><cell>3 708</cell><cell>4 797</cell><cell>3 680</cell><cell>4 926</cell></row><row><cell>1</cell><cell>3</cell><cell>4</cell><cell>6</cell><cell>2</cell></row><row><cell>2</cell><cell>10</cell><cell>33</cell><cell>22</cell><cell>22</cell></row><row><cell>3</cell><cell>3</cell><cell>15</cell><cell>3</cell><cell>3</cell></row><row><cell>Chol V 1 1</cell><cell>553</cell><cell>513</cell><cell>500</cell><cell>511</cell></row><row><cell>Chol V 2 1</cell><cell>7 844</cell><cell>18 761</cell><cell>13 796</cell><cell>9 852</cell></row><row><cell>Chol V 2 2</cell><cell>530</cell><cell>523</cell><cell>517</cell><cell>477</cell></row><row><cell>Mean( 1i )</cell><cell>6</cell><cell>5</cell><cell>12</cell><cell>12</cell></row><row><cell>Mean( 2i )</cell><cell>21</cell><cell>22</cell><cell>28</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc>Acceptance Rates for Scalability Test in §4.4</figDesc><table><row><cell>N</cell><cell>500</cell><cell>1,000</cell><cell>2,000</cell><cell>5,000</cell><cell>10,000</cell><cell>15,000</cell><cell>20,000</cell><cell>30,000</cell><cell>40,000</cell><cell>50,000</cell></row><row><cell>Scale factor</cell><cell>1 22</cell><cell>1 16</cell><cell>1 10</cell><cell>1 08</cell><cell>1 04</cell><cell>1 03</cell><cell>1 03</cell><cell>1 03</cell><cell>1 03</cell><cell>1 02</cell></row><row><cell>Acc. rate (×10 −5 )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc>Results of Simulation Study for Effectiveness of Estimator for Log Marginal Likelihood</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MVT</cell><cell></cell><cell>Ours</cell><cell></cell><cell cols="2">Lenk (2009)</cell><cell>HME</cell><cell></cell><cell></cell></row><row><cell>k</cell><cell>n</cell><cell>M</cell><cell>Scale</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean acc %</cell></row><row><cell>5</cell><cell>200</cell><cell>1 000</cell><cell>0 5</cell><cell>−309</cell><cell>6 6</cell><cell>−309</cell><cell>6 6</cell><cell>−311</cell><cell>6 8</cell><cell>−287</cell><cell>7 1</cell><cell>22 1</cell></row><row><cell>5</cell><cell>200</cell><cell>1 000</cell><cell>0 6</cell><cell>−309</cell><cell>6 6</cell><cell>−309</cell><cell>6 7</cell><cell>−310</cell><cell>6 9</cell><cell>−287</cell><cell>6 9</cell><cell>40 5</cell></row><row><cell>5</cell><cell>200</cell><cell>1 000</cell><cell>0 7</cell><cell>−309</cell><cell>6 6</cell><cell>−309</cell><cell>6 7</cell><cell>−310</cell><cell>6 5</cell><cell>−287</cell><cell>6 3</cell><cell>57 1</cell></row><row><cell>5</cell><cell>200</cell><cell>10 000</cell><cell>0 5</cell><cell>−309</cell><cell>6 6</cell><cell>−309</cell><cell>6 6</cell><cell>−311</cell><cell>6 7</cell><cell>−287</cell><cell>6 7</cell><cell>24 0</cell></row><row><cell>5</cell><cell>200</cell><cell>10 000</cell><cell>0 6</cell><cell>−309</cell><cell>6 6</cell><cell>−309</cell><cell>6 6</cell><cell>−310</cell><cell>7 5</cell><cell>−287</cell><cell>6 8</cell><cell>40 9</cell></row><row><cell>5</cell><cell>200</cell><cell>10 000</cell><cell>0 7</cell><cell>−309</cell><cell>6 6</cell><cell>−309</cell><cell>6 7</cell><cell>−310</cell><cell>7 0</cell><cell>−287</cell><cell>7 1</cell><cell>55 2</cell></row><row><cell>5</cell><cell>2 000</cell><cell>1 000</cell><cell>0 5</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 865</cell><cell>46 3</cell><cell>−2 868</cell><cell>46 2</cell><cell>−2 836</cell><cell>46 2</cell><cell>22 1</cell></row><row><cell>5</cell><cell>2 000</cell><cell>1 000</cell><cell>0 6</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 868</cell><cell>45 7</cell><cell>−2 836</cell><cell>45 5</cell><cell>37 8</cell></row><row><cell>5</cell><cell>2 000</cell><cell>1 000</cell><cell>0 7</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 866</cell><cell>46 3</cell><cell>−2 867</cell><cell>45 9</cell><cell>−2 836</cell><cell>45 9</cell><cell>49 6</cell></row><row><cell>5</cell><cell>2 000</cell><cell>1 000</cell><cell>0 8</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 867</cell><cell>46 3</cell><cell>−2 835</cell><cell>46 3</cell><cell>64 6</cell></row><row><cell>5</cell><cell>2 000</cell><cell>10 000</cell><cell>0 5</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 866</cell><cell>46 4</cell><cell>−2 867</cell><cell>46 7</cell><cell>−2 836</cell><cell>46 9</cell><cell>25 3</cell></row><row><cell>5</cell><cell>2 000</cell><cell>10 000</cell><cell>0 6</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 867</cell><cell>45 8</cell><cell>−2 836</cell><cell>46 3</cell><cell>36 3</cell></row><row><cell>5</cell><cell>2 000</cell><cell>10 000</cell><cell>0 7</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 866</cell><cell>46 4</cell><cell>−2 867</cell><cell>46 0</cell><cell>−2 836</cell><cell>46 3</cell><cell>51 4</cell></row><row><cell>5</cell><cell>2 000</cell><cell>10 000</cell><cell>0 8</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 866</cell><cell>46 2</cell><cell>−2 867</cell><cell>46 5</cell><cell>−2 835</cell><cell>46 3</cell><cell>72 0</cell></row><row><cell>25</cell><cell>200</cell><cell>1 000</cell><cell>0 5</cell><cell>−387</cell><cell>8 1</cell><cell>−385</cell><cell>8 2</cell><cell>−391</cell><cell>7 6</cell><cell>−292</cell><cell>8 5</cell><cell>2 8</cell></row><row><cell>25</cell><cell>200</cell><cell>1 000</cell><cell>0 6</cell><cell>−387</cell><cell>8 1</cell><cell>−386</cell><cell>8 1</cell><cell>−390</cell><cell>9 5</cell><cell>−292</cell><cell>8 8</cell><cell>8 1</cell></row><row><cell>25</cell><cell>200</cell><cell>1 000</cell><cell>0 7</cell><cell>−387</cell><cell>8 1</cell><cell>−386</cell><cell>8 3</cell><cell>−390</cell><cell>8 0</cell><cell>−292</cell><cell>8 8</cell><cell>16 2</cell></row><row><cell>25</cell><cell>200</cell><cell>10 000</cell><cell>0 5</cell><cell>−387</cell><cell>8 1</cell><cell>−385</cell><cell>8 5</cell><cell>−390</cell><cell>8 2</cell><cell>−292</cell><cell>8 4</cell><cell>1 7</cell></row><row><cell>25</cell><cell>200</cell><cell>10 000</cell><cell>0 6</cell><cell>−387</cell><cell>8 1</cell><cell>−385</cell><cell>8 2</cell><cell>−390</cell><cell>8 9</cell><cell>−292</cell><cell>8 8</cell><cell>6 2</cell></row><row><cell>25</cell><cell>200</cell><cell>10 000</cell><cell>0 7</cell><cell>−387</cell><cell>8 1</cell><cell>−386</cell><cell>8 2</cell><cell>−390</cell><cell>8 7</cell><cell>−292</cell><cell>9 1</cell><cell>20 0</cell></row><row><cell>25</cell><cell>2 000</cell><cell>1 000</cell><cell>0 5</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 989</cell><cell>28 8</cell><cell>−2 994</cell><cell>28 3</cell><cell>−2 865</cell><cell>28 8</cell><cell>2 7</cell></row><row><cell>25</cell><cell>2 000</cell><cell>1 000</cell><cell>0 6</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 989</cell><cell>28 7</cell><cell>−2 993</cell><cell>28 4</cell><cell>−2 864</cell><cell>29 0</cell><cell>4 6</cell></row><row><cell>25</cell><cell>2 000</cell><cell>1 000</cell><cell>0 7</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 989</cell><cell>28 9</cell><cell>−2 991</cell><cell>30 0</cell><cell>−2 864</cell><cell>29 5</cell><cell>15 4</cell></row><row><cell>25</cell><cell>2 000</cell><cell>1 000</cell><cell>0 8</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 992</cell><cell>29 6</cell><cell>−2 864</cell><cell>29 4</cell><cell>43 1</cell></row><row><cell>25</cell><cell>2 000</cell><cell>10 000</cell><cell>0 5</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 988</cell><cell>29 2</cell><cell>−2 992</cell><cell>28 5</cell><cell>−2 864</cell><cell>28 9</cell><cell>0 8</cell></row><row><cell>25</cell><cell>2 000</cell><cell>10 000</cell><cell>0 6</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 989</cell><cell>29 1</cell><cell>−2 993</cell><cell>29 4</cell><cell>−2 864</cell><cell>28 9</cell><cell>3 7</cell></row><row><cell>25</cell><cell>2 000</cell><cell>10 000</cell><cell>0 7</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 990</cell><cell>29 0</cell><cell>−2 993</cell><cell>28 9</cell><cell>−2 864</cell><cell>28 9</cell><cell>17 1</cell></row><row><cell>25</cell><cell>2 000</cell><cell>10 000</cell><cell>0 8</cell><cell>−2 990</cell><cell>28 7</cell><cell>−2 990</cell><cell>28 6</cell><cell>−2 993</cell><cell>28 2</cell><cell>−2 865</cell><cell>28 2</cell><cell>43 3</cell></row><row><cell>100</cell><cell>200</cell><cell>1 000</cell><cell>0 5</cell><cell>−660</cell><cell>6 7</cell><cell>−661</cell><cell>6 5</cell><cell>−683</cell><cell>8 8</cell><cell>−292</cell><cell>9 2</cell><cell>0 3</cell></row><row><cell>100</cell><cell>200</cell><cell>1 000</cell><cell>0 6</cell><cell>−660</cell><cell>6 7</cell><cell>−660</cell><cell>6 6</cell><cell>−678</cell><cell>8 5</cell><cell>−286</cell><cell>9 0</cell><cell>0 3</cell></row><row><cell>100</cell><cell>200</cell><cell>1 000</cell><cell>0 7</cell><cell>−660</cell><cell>6 7</cell><cell>−659</cell><cell>7 1</cell><cell>−673</cell><cell>7 8</cell><cell>−282</cell><cell>8 0</cell><cell>0 4</cell></row><row><cell>100</cell><cell>200</cell><cell>10 000</cell><cell>0 5</cell><cell>−660</cell><cell>6 7</cell><cell>−659</cell><cell>6 9</cell><cell>−682</cell><cell>9 1</cell><cell>−288</cell><cell>10 4</cell><cell>0 1</cell></row><row><cell>100</cell><cell>200</cell><cell>10 000</cell><cell>0 6</cell><cell>−660</cell><cell>6 7</cell><cell>−660</cell><cell>5 7</cell><cell>−678</cell><cell>8 8</cell><cell>−286</cell><cell>8 9</cell><cell>0 1</cell></row><row><cell>100</cell><cell>200</cell><cell>10 000</cell><cell>0 7</cell><cell>−660</cell><cell>6 7</cell><cell>−658</cell><cell>6 7</cell><cell>−674</cell><cell>7 3</cell><cell>−282</cell><cell>8 4</cell><cell>0 1</cell></row><row><cell>100</cell><cell>2 000</cell><cell>1 000</cell><cell>0 5</cell><cell>−3 364</cell><cell>24 4</cell><cell>−3 364</cell><cell>24 8</cell><cell>−3 370</cell><cell>27 5</cell><cell>−2 871</cell><cell>27 1</cell><cell>0 3</cell></row><row><cell>100</cell><cell>2 000</cell><cell>1 000</cell><cell>0 6</cell><cell>−3 364</cell><cell>24 4</cell><cell>−3 362</cell><cell>24 6</cell><cell>−3 369</cell><cell>24 3</cell><cell>−2 868</cell><cell>25 3</cell><cell>0 6</cell></row><row><cell>100</cell><cell>2 000</cell><cell>1 000</cell><cell>0 7</cell><cell>−3 364</cell><cell>24 4</cell><cell>−3 361</cell><cell>23 9</cell><cell>−3 371</cell><cell>25 6</cell><cell>−2 870</cell><cell>25 4</cell><cell>1 1</cell></row><row><cell>100</cell><cell>2 000</cell><cell>1 000</cell><cell>0 8</cell><cell>−3 364</cell><cell>24 4</cell><cell>−3 362</cell><cell>23 9</cell><cell>−3 370</cell><cell>26 0</cell><cell>−2 868</cell><cell>26 1</cell><cell>3 2</cell></row><row><cell>100</cell><cell>2 000</cell><cell>10 000</cell><cell>0 5</cell><cell>−3 364</cell><cell>24 4</cell><cell>−3 362</cell><cell>24 0</cell><cell>−3 372</cell><cell>25 3</cell><cell>−2 870</cell><cell>25 2</cell><cell>0 1</cell></row><row><cell>100</cell><cell>2 000</cell><cell>10 000</cell><cell>0 6</cell><cell>−3 364</cell><cell>24 4</cell><cell>−3 360</cell><cell>24 9</cell><cell>−3 368</cell><cell>25 3</cell><cell>−2 867</cell><cell>25 4</cell><cell>0 1</cell></row><row><cell>100</cell><cell>2 000</cell><cell>10 000</cell><cell>0 7</cell><cell>−3 364</cell><cell>24 4</cell><cell>−3 360</cell><cell>24 6</cell><cell>−3 370</cell><cell>25 5</cell><cell>−2 869</cell><cell>25 5</cell><cell>0 4</cell></row><row><cell>100</cell><cell>2 000</cell><cell>10 000</cell><cell>0 8</cell><cell>−3 364</cell><cell>24 4</cell><cell>−3 362</cell><cell>24 5</cell><cell>−3 367</cell><cell>24 3</cell><cell>−2 867</cell><cell>24 4</cell><cell>3 0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For brevity, we use the term "household" to describe any heterogeneous unit.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We do not require either derivative-based optimization or using MVN proposals, but these are most likely reasonable choices for differentiable, unimodal posteriors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Neither assumption is required, but most marketing models satisfy them, and maintaining them simplifies our exposition.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors acknowledge research assistance from Jonathan Smith and are grateful for helpful suggestions and comments from Eric Bradlow, Peter Fader, Fred Feinberg, John Liechty, Blake McShane, Steven Novick, John Peterson, Marc Suchard, Stephen Walker, and Daniel Zantedeschi.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Perspectives on Bayesian methods and big data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liechty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Customer Needs Solutions</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="175" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and elegant numerical linear algebra using the RcppEigen package</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eddelbuettel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Statist. Software</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">CppAD: A package for C++ algorithmic differentiation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<ptr target="http://www.coin-or.org/CppAD" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Account-level modeling for trade promotion: An application of a constrained parameter hierarchical model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boatwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">448</biblScope>
			<biblScope unit="page" from="1063" to="1073" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">trustOptim: An R package for trust region optimization with sparse Hessians</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Statist. Software</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">bayesGDS: An R package for generalized direct sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<ptr target="http://cran.r-project.org/web/packages/bayesGDS" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>R package version 0.6.0.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">sparseHessianFD: An R package for estimating sparse Hessians</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<ptr target="http://cran.r-project.org/web/packages/sparseHessianFD" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>R package version 0.2.0.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">sparseMVN: An R package for MVN sampling with sparse covariance and precision matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<ptr target="http://cran.r-project.org/web/packages/sparseMVN" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>R package version 0.2.0.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Handbook of Markov Chain Monte Carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Chapman and Hall/CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bayes and Empirical Bayes Methods for Data Analysis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Chapman and Hall/CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Monte Carlo Methods in Bayesian Computation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Ibrahim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Marginal likelihood from the Gibbs output</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">432</biblScope>
			<biblScope unit="page" from="1313" to="1321" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimation of sparse Jacobian matrices and graph coloring problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numerical Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="187" to="209" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ADMIT-1: Automatic differentiation and MATLAB interface toolbox</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="175" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm 636: Fortran subroutines for estimating sparse Hessian matrices</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Garbow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">378</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Software for estimating sparse Hessian matrices</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Garbow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="363" to="377" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the estimation of sparse Jacobian matrices</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Institute Math. Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="117" to="119" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<title level="m">Direct Methods for Sparse Linear Systems</title>
				<meeting><address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hybrid Monte Carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roweth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. B</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rcpp: Seamless R and C++ integration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eddelbuettel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>François</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Statist. Software</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian model choice: Asymptotics and exact calculations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc., Ser. B</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="514" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sampling-based approaches to calculating marginal densities</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">410</biblScope>
			<biblScope unit="page" from="398" to="409" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Chapman and Hall/CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Riemann manifold Langevin and Hamiltonian Monte Carlo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calderhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc., Ser. B</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
				<meeting><address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Griewank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walther</surname></persName>
		</author>
		<title level="m">Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation</title>
				<meeting><address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The no-U -turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1593" to="1623" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simulation pseudo-bias correction to the harmonic mean estimator of integrated likelihoods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computational Graphical Statist</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="941" to="960" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Models and managers: The concept of a decision calculus</title>
		<author>
			<persName><forename type="first">Jdc</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="B466" to="B485" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Response modeling with nonrandom marketing-mix variables</title>
		<author>
			<persName><forename type="first">P</forename><surname>Manchanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chintagunta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="478" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Handbook of Markov Chain Monte Carlo</title>
		<author>
			<persName><forename type="first">Rm ;</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Chapman and Hall/CRC Press</publisher>
			<biblScope unit="page" from="113" to="162" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>MCMC using Hamiltonian dynamics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="427" to="444" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Approximate Bayesian inference with the weighted likelihood bootstrap</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc., Ser. B</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="48" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<title level="m">Numerical Optimization</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the estimation of sparse Hessian matrices</title>
		<author>
			<persName><forename type="first">Mjd</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numerical Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1060" to="1074" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">R: A language and environment for statistical computing. R Foundation for Statistical Computing</title>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Vienna</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating the integrated likelihood via posterior simulation using the harmonic mean identity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Satagopan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pn ;</forename><surname>Krivitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Bayarri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Statistics Proceedings</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">bayesm: Bayesian inference for marketing/microeconometrics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rossi</surname></persName>
		</author>
		<ptr target="http://cran.r-project.org/web/packages/bayesm" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>R package version 2.2.5.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian statistics and marketing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="328" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<title level="m">Bayesian Statistics and Marketing</title>
				<meeting><address><addrLine>Chichester, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Stan: A C++ library for probability and sampling, Version 2.2</title>
		<author>
			<persName><forename type="first">Stan</forename><surname>Development Team</surname></persName>
		</author>
		<ptr target="http://www.mc-stan.org" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding GPU programming for statistical computation: Studies in massively parallel massive mixtures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frelinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graphical Statist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="419" to="438" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Parallel multivariate slice sampling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Tibbits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Liechty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="430" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Direct sampling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Laud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Damien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graphical Statist</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="692" to="713" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Getting started with</title>
		<author>
			<persName><forename type="first">A</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Griewank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combinatorial Scientific Computing</title>
				<editor>
			<persName><forename type="first">U</forename><surname>Adol-C. Naumann</surname></persName>
			<persName><forename type="first">O</forename><surname>Schenk</surname></persName>
		</editor>
		<meeting><address><addrLine>Boca Raton, FL</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="181" to="202" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
