<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Prediction in Marketing Using the Support Vector Machine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2003-01-08">January 8, 2003</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dapeng</forename><surname>Cui</surname></persName>
							<email>dapeng.cui@ipsos-na.com</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Curry</surname></persName>
							<email>david.curry@uc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Ipsos Insight</orgName>
								<orgName type="institution" key="instit2">North America</orgName>
								<address>
									<addrLine>111 North Canal, Suite 405</addrLine>
									<postCode>60606</postCode>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Business Administration</orgName>
								<orgName type="institution">University of Cincinnati</orgName>
								<address>
									<postCode>45221-0145</postCode>
									<settlement>Cincinnati</settlement>
									<region>Ohio</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Prediction in Marketing Using the Support Vector Machine</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 e 1526-548X 05 2404 0595</idno>
						<imprint>
							<date type="published" when="2003-01-08">January 8, 2003</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.1050.0123</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automated modeling</term>
					<term>choice models</term>
					<term>kernel transformations</term>
					<term>multinomial logit model</term>
					<term>predictive models</term>
					<term>support vector machine History: This paper was received</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please scroll down for article-it is on subsequent pages</head><p>With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many marketing problems require accurately predicting the outcome of a process or the future state of a system. In the past two decades, prediction of consumer choice has attracted the most attention, but prediction of other marketing phenomena plays a fundamental role in modern marketing practice and is essential to accomplish the deeper goals of marketing science. Examples include predicting segment membership, most "switchable" customers, most effective ad, and website navigational choices. This paper presents a systematic study of the strengths and weaknesses of a methodology well suited for a variety of prediction environments in marketing, the support vector machine (SVM). Accurate prediction, though essential, is often hindered by complex relationships between predictor and target variables and an absence of theory to guide model identification. For reasons explained in this research, the support vector machine predicts accurately in such environments.</p><p>The support vector machine is a semiparametric technique with origins in the machine-learning literature of engineering and computer science. There are currently no applications of the support vector machine reported in the marketing literature and only one application reported in any major business journal; i.e., <ref type="bibr" target="#b88">Viaene et al. (2002)</ref>. The SVM is novel, but as <ref type="bibr">Bucklin et al. (2002)</ref> emphasize, with today's diverse data sets, " it may be counterproductive to rely primarily on standard statistical methods. Emphasizing scalable methods and predictive results may enable us to observe a richer set of behavioral phenomena in marketing data." 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Objectives</head><p>Though relatively unstudied in marketing, the support vector machine has demonstrated its utility in a variety of other disciplines, including statistics, computer science, agriculture, and engineering. The present research clarifies the strengths and weaknesses of the SVM for the kinds of applications, traditional and future, most relevant to marketing scientists. This goal is accomplished through a combination of analytic discourse and direct empirical comparisons with the multinomial logit model (MNL). <ref type="bibr">2</ref> In marketing, the logit is the gold standard;</p><note type="other">596</note><p>Marketing Science 24(4), pp. <ref type="bibr">595-615, © 2005 INFORMS</ref> it is widely applied and is known to perform well. It provides a well-understood benchmark for the SVM. <ref type="bibr">3</ref> However, in later sections of the paper, we emphasize the complementary, not competitive, relationship between the SVM, MNL, and other existing models in marketing.</p><p>Although marketing scientists have traditionally emphasized structural understanding over predictive accuracy, in many emerging marketing contexts there is a notable absence of theory, and predictionnot structural understanding-is the primary goal. These contexts include automated modeling, intelligent agents, and data mining, to name just a few. In the present study, we engineer environments with properties like those expected in these and other areas relevant to marketing's expanding scope. These environments require individual-level predictions, but data are not collected using a controlled experimental design. More likely, the data are pure one-shot field data that mix information about the prediction target, the individual, and the prediction context.</p><p>Such contexts represent a proving ground for the support vector machine. The SVM behaves mathematically in a way that avoids overreliance on particular structural assumptions. It implicitly automates the model identification process, and by so doing, enters the parameter estimation phase with a family of structural possibilities rather than a single possibility. The SVM uses a kernel-induced transformation from the original attribute space to a higherdimensional space to capture relevant features of the data. Through clever use of kernel transforms, the SVM solves a nonlinear problem with a linear model. <ref type="bibr">4</ref> The approach has certain stability and robustness characteristics lacking in the maximumlikelihood procedure, whether based on actual or simulated likelihood. <ref type="bibr">5</ref> These characteristics also control overfitting. Thus, in this paper, the term "prediction" always means out-of-sample prediction, not goodness-of-fit. class of models takes, as well as the sophisticated estimation methods being used and under development. For simplicity, we use the terms "logit model," "logit," or "MNL" to refer to this class of models. In subsequent sections, the precise nature of the models used in this research is clarified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Omitted Topics</head><p>This research is defined as much by what it omits as by what it includes. Readers interested in a technical exposition of the support vector machine are directed to <ref type="bibr" target="#b19">Burges (1998)</ref>. For an exposition in a businessoriented setting, see <ref type="bibr" target="#b25">Curry and Cui (2003)</ref>. This paper only superficially considers statistical learning theory, the theory of statistical inference underpinning the SVM <ref type="bibr" target="#b85">(Vapnik 1998)</ref>. We highlight the fundamental result linking machine (model) capacity, sample size, and empirical risk. The bibliography contains abundant references for readers interested in further study of this important topic. Finally, we focus exclusively on one-dimensional, discrete prediction, not predictors with continuous outcomes or 2-d or higher classification. The SVM has been successfully applied to 2-d-type problems, including face detection <ref type="bibr" target="#b60">(Osuna et al. 1997</ref>) and image classification <ref type="bibr" target="#b20">(Chapelle et al. 1999)</ref>. It has also been adapted to problems with continuous outcomes, such as regression problems <ref type="bibr" target="#b56">(Mattern and Haykin 1999</ref>, principle components analysis , and density estimation <ref type="bibr" target="#b92">(Weston et al. 1999</ref>). However, these areas are beyond the scope of the present research.</p><p>The remaining sections of this paper are organized as follows. Section 2 reviews the critical role of prediction in marketing and provides a framework for organizing predictive models. The SVM's position in this framework helps identify the types of problems for which it is well suited. Section 3 outlines major areas where the SVM differs from conventional models. The section addresses topics vitally important to practitioners and modelers alike-including model capacity, boundary bias, and the curse of dimensionality-to build a case for the SVM's superior predictive capability. Section 4 details how and why an SVM works and indicates how we implemented the SVMs used in this research. Section 5 presents the methodology used for the major experimental comparisons presented in this paper, testing the SVM in a variety of environments where it is baselined against the multinomial logit. Section 6 discusses weaknesses of the SVM and shortcomings of our empirical comparisons. Section 7 presents two main directions for future research, both stressing the complementary nature of the SVM and random utility theory (RUT) models. Section 8 offers concluding comments. Appendices A, B, and C in the main text explain technical elements of the SVM, while two online appendices offer additional detail for interested readers (http://mktsci.pubs.informs.org).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prediction in Marketing</head><p>More than a half-century ago, <ref type="bibr">Politz and Deming (1953, p. 51</ref>) noted the vital role of predictive models in marketing: "Every (marketing) decision entails the expectation of a specific result. Therefore, every decision, if it is rational, depends on prediction." Because so much is riding on prediction in marketing, taking care to draw the problem in the most refined way possible increases the likelihood of obtaining useful results. To this end, we distinguish four major contexts in which accurate prediction is paramount for marketing success: (a) pure prediction, (b) robust prediction, (c) analytic prediction, and (d) structural gap analysis. These categories position the support vector machine against current methods dominant in marketing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pure Prediction</head><p>Pure prediction includes all cases in which structural understanding of a phenomenon is neither feasible nor necessary. Management's mission is entirely practical, not intellectual, because a course of action must be determined or a value computed for a large number of cases. In these contexts, the firm can realize higher revenues, lower costs, or both, by using predictive models. In addition to the burden of too many cases to model "by hand," relevant predictors may change from one case to the next, so that human intervention is not only costly, but not useful. These properties define several important frameworks discussed by marketing scholars and implemented to varying degrees by many firms today, including massproduced models, automated modeling, data mining, intelligent agents, and certain subdomains of other marketing areas, such as direct marketing.</p><p>Mass-produced models have already made important contributions in the consumer packaged-goods (CPG) industry, and their importance is likely to expand during the next decade <ref type="bibr" target="#b11">(Blattberg et al. 1994)</ref>. Similarly, automated models, inspired by IRI's Cover Story ™ <ref type="bibr" target="#b69">(Schmitz et al. 1990</ref>) and PROMOTER ™ <ref type="bibr">Lodish 1987, 1993)</ref> are being improved and upgraded <ref type="bibr" target="#b16">(Bucklin et al. 1998</ref><ref type="bibr" target="#b53">, Little 2001</ref>. With both mass-produced models and automated modeling, the emphasis is on pure prediction precisely because there are too many cases to consider individually. Although predictions are often based on fairly simple models, they are consistent and "accurate enough," relative to the alternative of human intervention.</p><p>Data-mining applications are prominent in direct marketing <ref type="bibr" target="#b10">(Berry and</ref><ref type="bibr">Linoff 1997, Cooper and</ref><ref type="bibr" target="#b21">Giuffrida 2000)</ref> and have rapidly expanded in the CPG environment. In CPG applications, data mining emphasizes predictions about the future coincidence of items in a shopping cart, rather than structural understanding or inference <ref type="bibr">(Bucklin et al. 2002)</ref>. In direct marketing, data-mining algorithms maximize expected profit from solicitations <ref type="bibr" target="#b65">(Ratner 2003)</ref>, including cross-selling opportunities <ref type="bibr" target="#b48">(Kamakura et al. 2003)</ref>.</p><p>Although the agent framework is relatively new in marketing, a number of marketing scientists are actively developing agents and exploring their consequences <ref type="bibr" target="#b5">(Ariely et al. 2004</ref><ref type="bibr" target="#b6">, Avery et al. 1999</ref><ref type="bibr" target="#b27">, Diehl et al. 2003</ref><ref type="bibr" target="#b35">, Gershoff and West 1998</ref><ref type="bibr" target="#b41">, Häubl and Trifts 2000</ref><ref type="bibr" target="#b46">, Iacobucci et al. 2000</ref><ref type="bibr" target="#b90">, West et al. 1999</ref>. <ref type="bibr">West and colleagues (1999)</ref> foresee a wide range of applications for consumers, many of which require accurate prediction of utility, satisfaction level, and price sensitivity on a client-by-client basis with information sets that are unique to each client and change over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Robust Prediction</head><p>The second major marketing context where prediction is a key factor falls under the general rubric of marketing engineering, e.g., the "systematic process of putting marketing data and knowledge to practical use" to enhance decision-makers' mental models <ref type="bibr" target="#b52">(Lilien et al. 2002)</ref>. In marketing engineering, prediction is only part of the equation, though a crucial part. We call this robust prediction because, paradoxically, the structural accuracy of the model is not as important as its ability to provide the decision maker with an accurate sense of trends, outliers, boundary conditions, and other elements of the "big picture." In fact, robust means structurally naïve but predictively accurate. The models in a marketing management support system (MMSS) <ref type="bibr" target="#b94">(Wierenga et al. 1999, Wierenga and</ref><ref type="bibr" target="#b93">van Bruggen 2000)</ref> are valued precisely because they provide intuitive understanding, not scientific explanation <ref type="bibr" target="#b45">(Hunt 1983)</ref>.</p><p>Conjoint analysis is a good example of robust modeling. In hundreds of commercial applications and nearly all published research, no structure deeper than additive is sought for an individual's utility function. The additive model is robust <ref type="bibr" target="#b26">(Dawes and Corrigan 1974)</ref>; its predictions are accurate enough, despite the fact that many studies of human information integration reveal a wide assortment of noncompensatory behavior <ref type="bibr" target="#b31">(Einhorn 1970</ref><ref type="bibr" target="#b47">, Kahneman 2002</ref><ref type="bibr" target="#b49">, Kardes 1999</ref><ref type="bibr" target="#b14">, Brazerman 1994</ref><ref type="bibr" target="#b67">, Russo and Shoemaker 1989</ref><ref type="bibr" target="#b82">, Tversky 1972</ref>. The success of additive conjoint analysis emphasizes that management clearly accepts the trade-off between deep structural understanding and ease of interpretation in light of respondent effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Analytic Prediction</head><p>Analytic prediction describes contexts in which a model is developed to solve a particular problem or class of problems or where an existing model is adapted for this purpose. In many cases the model is just beyond inclusion in the robust prediction category, but could end up there as its structure becomes better understood and its robustness a proven commodity. Recent examples include the model of Moe Marketing Science 24(4), pp. <ref type="bibr">595-615, © 2005 INFORMS and</ref><ref type="bibr">Fader (2001)</ref> to predict the conversion rate from website visit to online purchase and Sismeiro's and <ref type="bibr" target="#b77">Bucklin's (2004)</ref> model to predict how long a visitor will view each webpage on a given website.</p><p>Discrete-choice models that operationalize RUT are included in this category. Although these models yield important structural insights, it is worth recalling that in both the transportation and marketing literatures, discrete-choice models were originally designed to support "if-then" analysis for policy implications, a purpose for which accurate prediction is a necessity <ref type="bibr" target="#b28">(Domencich and McFadden 1975)</ref>. In this spirit, <ref type="bibr" target="#b38">Guadagni and Little (1983)</ref> thoroughly tested the out-of-sample predictive ability of the multinomial logit they fit to household scanner data. <ref type="bibr">6</ref> They emphasize the model's application as a market response tool to assist managers in anticipating market reactions to changes in marketing-mix variables.</p><p>Discrete-choice models offer both structural insight and accurate prediction. Regarding structure, however, the main evolutionary path has been to refine structural understanding of the stochastic component of the model, not to seek additional structure in the model's deterministic component. Substantial progress has been made to unpool variance due to consumer heterogeneity and other specialized components, such as correlated attributes. This purifies estimates of the parameters in the deterministic component of the model, adding power to significance tests and enhancing analyst confidence in the magnitude and sign of estimates. However, with rare exceptions, V ij = X ij remains linear in attributes even though modern applications of discrete-choice modeling venture far beyond the simple choice contexts envisioned by <ref type="bibr" target="#b54">Luce (1959)</ref> and <ref type="bibr" target="#b80">Thurstone (1927)</ref>. Today's models mix attributes of the choice options with attributes of the customer and the choice environment. It is reasonable to expect interactions and higher-order effects between attributes from different domains, e.g., product attributes and decision-maker attributes. Thus, we argue that there are potential gains from including additional structural complexity in V ij , not just in error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Prediction and Structural Gap Analysis</head><p>Marketing science demands highly accurate predictive models precisely because such models push analysts to increase their structural understanding of a process. Although unambiguous profit motives drive the previous three cases, here financial gain is subordinated to the search for scientific understanding.</p><p>In his thesis of structural identity, <ref type="bibr">Hempel (1965, p. 367</ref>) addresses the idea squarely: "(1) every adequate explanation is potentially a prediction, and (2) every adequate prediction is potentially an explanation." Social scientists sometimes defend a model by claiming that it is designed to explain rather than predict a phenomenon. Hunt bluntly calls this excuse vacuous, since "all adequate explanations must have predictive capacity" <ref type="bibr">(1983, p. 117)</ref>.</p><p>We shall see that the support vector machine, because it predicts so well, has the potential to play a central role in a deeper understanding of marketing phenomena. This role takes the form of presenting the marketing scientist with a prediction gap that leads unequivocally to a structural identification gap. Foreshadowing results, the support vector machine's predictive superiority in nonlinear contexts signals omitted structure in the baseline model. Of course, when this structure is included, the baseline model can predict up to the inherent limits of uncertainty. This is big news, not because the increased predictive power was unexpected, given perfect structural insight, but rather because perfect structural insight is impossible. Finding a reliable technique like the SVM that provides a reasonable target for predictive accuracy supports positivist goals.</p><p>Not all techniques used for prediction in marketing fit neatly into the foregoing framework. Examples include scenario analysis, capabilities analysis, and game theory. The SVM fits easily into the class of pure predictors, but has properties, reviewed next, that suggest potential contributions in all four categories of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Properties and Predictive Ability</head><p>The relative strengths and weaknesses of the SVM can be established by probing four fundamental properties that influence the predictive ability of any model. These properties-decision boundary, structural capacity, boundary bias, and empirical risk-are tightly interconnected. They all contribute to a fifth area, recognition of complexity that involves scientific attitudes and positivist tenets that can stymie methodological progress. We discuss each of these areas in turn to build an understanding of differences between the support vector machine and parametric models in marketing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implicit Decision Boundaries</head><p>All parametric models for discrete prediction, including those based directly on the general linear model (GLM), such as discriminant analysis; and those based indirectly on GLM, such as discrete-choice models; involve two basic stages. Stage 1, function estimation, yields continuous decision boundaries that are then triggered in Stage 2 when an observation is assigned to a particular discrete class. In the simplest two-outcome, logit model, for example, the boundaries are contours of constant probability of belonging to Class 1, "choose option A," versus Class 2, "choose something else." The statistical prediction rule is based on one of these contours, typically the contour corresponding to 50/50 posterior odds. The logit approximates this contour by using hyperplanes in the space spanned by the predictor variables. The linear combination of predictors is transformed to lie between 0 and 1 in order to approximate probabilities. Discrete-choice models with richer structure, such as conditional, nested, and mixed; or hierarchical logit models, place additional constraints on these hyperplanes, but do not alter the most fundamental property of the resulting decision boundary; it is functional, not relational, in nature. Thus parametric models face a structural inadequacy when dealing with a relational boundary between classes. In consumer choice, such a boundary arises even with very simple information integration rules. For example, if a consumer uses the Latitude of Acceptance rule shown in Figure <ref type="figure">1</ref>, acceptable options (squares) are nested within a ring of unacceptable options (circles) defined by cutoffs on two attributes <ref type="bibr" target="#b89">(West et al. 1997)</ref>.</p><p>Recognizing a nested relation mathematically is difficult for a parametric model based on function approximation. To illustrate, consider the prediction hit-rates for a simple binary logit model versus an SVM for the data in Figure <ref type="figure">1</ref>. We estimated parameters of both models using a sample of size l = 400, then used the model to predict the outcome for N = 50 000 observations drawn from the same joint There are three basic reasons for the disparity, and these reasons exert their influence interactively. First, the structural capacity of the two models differs substantially. Second, the SVM skips the function estimation step and calculates the optimal decision boundary directly, as described subsequently. Finally, the SVM establishes this boundary while preserving degrees of freedom through the application of a unique data transformation that, contrary to common practice, increases rather than decreases the dimensionality of the solution space. How this works and why it should work well are clarified in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Machine Capacity</head><p>Let us elaborate the model as machine analogy through the idea of a machine's capacity. Capacity is akin to the Fisherian notion of structural error caused by model misspecification <ref type="bibr" target="#b33">(Fisher 1950)</ref>, but is indexed quantitatively in the theory of the support vector machine. Consider the following four functional forms for a regression model.</p><formula xml:id="formula_0">1: y = 0 + 1 X 1 2: y = 0 + 1 X 1 + 2 X 2 1 3: y = 0 + 1 X 1 + 2 X 2 4: y = 0 + 1 X 1 + 2 X 2 + 12 X 1 X 2</formula><p>These forms differ in certain explicit properties such as number of inputs and number of parameters. Several of these models are nested; e.g., 1 ⊂ 2 and 1 ⊂ 3 ⊂ 4. Among nested models, we naturally think of the super-model as having greater capacity than a submodel in the sense that it must exhibit superior in-sample fit. For nonnested models with the same number of parameters, their relative capacity is not as clear. For example, 2 and 3 have the same number of parameters, but 2 involves only one input while 3 involves two. In such cases, our intuitive notion of capacity is confounded by econometric questions such as multicollinearity, e.g., X 1 versus X 2 1 . These questions in turn engender others about the nature of the data collection process (field data or experimental data; i.e., are X 1 and X 2 1 orthogonal?), and whether the likelihood or simulated likelihood surface is unimodal, a property that can strongly influence the quality of estimation.</p><p>Rather than use nesting, degrees of freedom, or more generally, an object-oriented assessment of observable model properties, <ref type="bibr">Chervonenkis (1964, 1971)</ref> focused on an unobservable but more fundamental property from which these others follow. This property, known as the VC-dimension of A function is said to shatter a set of p points if it can be instantiated so as to subdivide the points into all 2 p possible subsets. The function's capacity is the maximum number of points it can shatter. For example, three points can be partitioned eight ways into two groups.</p><p>Figure <ref type="figure">2</ref>(a) shows that the linear boundary can shatter three points because it can identify all eight partitions. However, the linear boundary cannot shatter four points, because in panel (b) it cannot achieve the partition ac bd . This partition can be achieved using the quadratic boundary, 2, which has capacity 5. ( 2 can shatter four and five points, but not six.) Put another way, no matter how well we estimate the parameters of the linear machine 1, as a decision boundary it lacks the (structural) capacity to solve problem (b). Capacity measured in this way would be, at most, an interesting addendum to the area of structural analysis in modeling if the story ended here. However, Vapnik and colleagues were able to derive a unifying relationship between capacity h , sample size l , and empirical risk R emp <ref type="bibr" target="#b86">(Vapnik and Chervonenkis 1971)</ref>. To define empirical risk, consider the following characterization of the discrete prediction problem. Suppose we are given l observations drawn independently from an unknown distribution P x y . (P symbolizes the cumulative probability function; vectors and matrices appear in boldface type.) Each observation consists of an n-dimensional vector x i ∈ R n , i = 1 l and an associated output y i . For binary choice situations, y i is either 0 or 1. For multiclass problems, y i is an element of an index set.</p><p>Suppose that a real-valued function underlies the data-generating process; g x i → y i . The researcher wants to "build" a machine to approximate this function and use it to predict outcomes. The machine is defined by a family of possible functions f x , where is a set of adjustable (hyper)parameters of the family. For a fixed , the machine is deterministic. It will generate the same output f x if the same x is input. However, f x may or may not equal the true value of y. Under these conditions, the actual (expected) empirical risk is the expectation of prediction error for a perfectly trained machine; i.e., R = 1 2 y − f x dP x y . However, because we do not know the function P , R can only be approximated. This approximation produces empirical risk, defined as the observed mean error rate on a training set for a fixed number of observations</p><formula xml:id="formula_1">R emp = 1 2l l i=1 y i −f x i</formula><p>Note that R emp is based on an approximation to the true f . For a fixed sample size, there are infinitely many such realizations. Thus, the approximationf is a realization of the random variable f . The realization varies as a function of the observations in the training set. Note further that f x may not belong to the same class of functions as the actual data-generating process g. In the best case, f ⊆ g, but it may not be so, because the class of mathematical functions is not ordered. For example, a quadratic can be used to approximate y = 0 1 − e − 1 x , but neither function is a special case of the other.</p><p>Vapnik (1998) refers to the use of R emp to approximate the function R as the empirical risk minimization (ERM) principle. R emp is computed without reference to a probability distribution. It is a fixed number for a particular choice of and a given set of training data x i y i i = 1 l . Various models and associated computational methods have been proposed to approximate R based on R emp . However, minimizing empirical risk (in-sample fit conditional on f ) can be, but need not necessarily be, equivalent to minimizing expected total risk. The difference R − R emp is bounded and is the subject of the second contributor to overall risk, structural risk, caused by the choice of f in relation to the true g. At the heart of the SVM approach is the goal of optimally trading-off empirical and structural risk. Unlike parametric approaches that address this task sequentially-i.e., fix structure first, then minimize empirical risk-the SVM solves the problem simultaneously.</p><p>The unifying relationship between capacity h , sample size l , and empirical risk was originally developed by <ref type="bibr" target="#b86">Vapnik and Chervonenkis (1971)</ref> for losses taking either 1 or 0. Using 0 ≤ ≤ 1, they show that the following bounds hold with probability 1 − .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head><formula xml:id="formula_2">≤ R emp + h log 2l/h + 1 − log /4 l (1)</formula><p>The right-hand side (rhs) of ( <ref type="formula">1</ref>) is the risk bound and the term under the radical is called VC confidence. VC confidence, for a fixed sample size l, monotonically increases in capacity h . For a fixed capacity, VC confidence shrinks as l → . As this value shrinks, the rhs of (1) more tightly bounds expected risk; i.e., the approximation R ≈ R emp becomes better. Thus, with large samples, fixing structure and focusing on empirical risk minimization can give good results. More precisely, for a given sample, if the ratio l/h is "large" (sample size is much larger than capacity), expected risk is dominated by empirical risk. In this case, minimizing empirical risk is roughly equivalent to minimizing expected risk. This is the path followed when using classical criteria such as the likelihood ratio statistic, AIC, or BIC. However, each of these criteria depends on the asymptotic properties of the probability distribution presumed to generate the data, whereas (1) does not. Furthermore, if the ratio l/h is "small," then empirical risk and expected (actual) risk are quite different. What one means by "small" is, therefore, of paramount importance for assessing the quality of a model.</p><p>The trade-offs in (1) serve as defining logic for the support vector machine. We illustrate using models from the classes, 1 and 2, mentioned earlier, and their application to a two-group prediction problem. Let x = x 1 x 2 T be drawn with probability 0.5 from either N 1 1 1 or N 2 2 2 , where 1 = 0 0 T , 2 = 2 0 T , 1 = I 2×2 , and 2 = 1 0 5 0 5 1 . Applying Bayes' theorem to classify x based on the maximum posterior probability of group membership leads to a quadratic boundary Q between the two groups in the x 1 , x 2 attribute space, as shown in Figure <ref type="figure">3</ref>. Pooling the variance-covariance (vcv) matrices leads to the structurally incorrect, linear (L) boundary. Q is the Bayesian (optimal) rule, while L is computed using Bayes' theorem, but based on less information.</p><p>These rules are virtually identical in the areas where probability mass is more highly concentrated. Thus, intuition suggests that their empirical counterparts should have very similar prediction hit-rates. More important, nothing from Figure <ref type="figure">3</ref> suggests that the structurally misspecified model L should outperform the structurally correct model Q. The relation (1) implies, however, that with small sample sizes, empirical risk and true risk will differ significantly. This means that with smaller sample sizes, empirical realizations of the structurally misspecified model will outperform those from the structurally correct model. We offer additional explanation of why this should be so after viewing results.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows results when estimation sample size l is varied over the values 10 50 100 200 400 . Within each sample size, 100 trials were run. In each trial, both the linear and quadratic discriminant models were estimated using the same l observations, balanced to reflect equal priors. Each instantiated x 2 Q L model was then used to predict the group membership of N = 1 000 fresh draws from the population, again balanced 500/500. For each trial, Table <ref type="table" target="#tab_0">1</ref> (column (a)) shows the frequency of times each model wins; column (b) shows each model's mean error rate misses = 1 − hit rate ; and columns (c) and (d) show the standard deviation and range, respectively, of these error rates over the 100 trials for the sample size in question. Column (a) shows that for smaller estimation samples, the linear model resoundingly outpredicts the structurally correct model, despite the apparent similarity in their decision boundaries. When l = 10, Q wins in only 15 cases. Even though Q generates the data, its prediction error rate is 26% higher on average than the error rate of the linear machine. When l = 200, L still wins by a 3:2 margin, though hit-rates are nearly equal. This pattern a Ties were split evenly between L and Q. Ties occurred when l = 100 (4 ties), l = 200 (7), and l = 400 (6).</p><p>Marketing Science 24(4), pp. 595-615, © 2005 INFORMS persists up to l = 400, where Q finally wins in slightly more than half the trials. Even with this "large" sample size-relative to the number of parameters estimated-the linear machine's mean empirical error rate is virtually identical to that of Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Positivist Habits, Structural Diagnosticity, and</head><p>Prediction This simple example shows that a marketing scientist who focuses on uncovering the "real" latent structure generating data will be misled. In all cases above, it must be true that Q fits better in sample than L, because L ⊂ Q. However, neither this fact nor the fact that L outpredicts Q can be trusted to find the actual model generating the data. As engineers of the datagenerating process, we have the benefit of knowing the real process. The marketing scientist trying to reverse engineer the data to judge whether L is structurally right or wrong would be misled (especially for smaller sample sizes) by L's superior in-sample fit (adjusted for df ), its superior out-of-sample prediction rate, and its relative simplicity, which satisfies the scientific standard for parsimony.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Decision Boundary Bias</head><p>In discrete-class prediction, decision boundary bias interacts with the ERM principle to further complicate the scientist's ability to recognize true structure. Recall that empirical risk is calculated usingf x i , an approximation of the function used by a perfectly trained machine. This approximation is one realization of the random variable, f . Figure <ref type="figure">3</ref> shows the true boundaries L and Q for the engineered process. However, in each sample summarized in Table <ref type="table" target="#tab_0">1</ref>, the estimated boundariesL and Q differ from their population counterparts. (Q is the real process, thus using the analogy g ≡ Q and f ≡ L, we see thatf is a double approximation to g. In the present case, f ⊂ g, but this fact is specific to our example.)</p><p>The discrete predictions generating the hit-rates for Table <ref type="table" target="#tab_0">1</ref> are obtained using the classic two-phase process, function estimation followed by discrete classification. It is well known that the function estimation phase improves monotonically with increasing sample size. However, using a clever decomposition of variance, <ref type="bibr" target="#b34">Friedman (1997)</ref> shows that, surprisingly, this is not the case for improvements in the discrete classification phase. In particular, given a training sample (of size l), the error rate, averaged over all future predictions at a given point x, depends on whether the classification rule is Bayes' rule (the optimal rule) or not. If it is, then the error rate is the irreducible error associated with Bayes' rule (i.e., class overlap in 2-d). <ref type="bibr">7</ref> If not, there is an added component of variance that depends on the variance of the estimated decision boundary; e.g., V f = E f − E f 2 . The classification error rate and the function estimation error rate are influenced completely differently by this variance. Decreasing V f decreases classification error when decision boundary bias is negative, but increases it when bias is positive. Thus, focusing on improved function accuracy; i.e., minimizing V f can actually make the discrete-choice prediction phase less accurate. In such cases, a simpler (but incorrect) model will outpredict the structurally correct model if it is biased on the "correct side" of x more often than not. This is precisely the case for L in our example. Table <ref type="table" target="#tab_0">1</ref> shows that the paradox persists to the point where the sample size is large enough to reduce empirical risk and boundary bias (of Q in this case) below that ofL. This usually requires very large samples, where large is indexed by the ratio of sample size to capacity, i.e., the VC-dimension of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Dimensionality, Parametric Estimation, and</head><p>Prediction As Ben-Akiva et al. <ref type="bibr">(1997, p. 279</ref>) noted, "When the utility function has many covariates, the data points are sparse over the high dimensional space and the estimation becomes unstable As the number of dimensions increase (sic), an exponential increase in sample size is needed to maintain reliable estimation." This trade-off between model complexity, available data, and empirical risk <ref type="bibr" target="#b7">(Bellman 1961</ref>) interacts with decision boundary bias in parametric models as suggested in Figure <ref type="figure">4</ref>. Ultimately, the selected decision boundary is subject to a three-link chain of instability. These links involve (a) the hypothesized parametric form(s) selected to model various stochastic elements of the process, (b) the relationship between the dimensionality of the problem space and the number of data points (and their sampling properties) when empirically approximating the identified form(s), and (c) properties of the estimation technique (analytic or numeric), i.e., estimation in RUT entails finding the global optimum of the likelihood or simulated likelihood surface.</p><p>Figure <ref type="figure">4</ref> suggests very generally how the SVM avoids these potential pitfalls. Foremost, the decision boundary is found directly from available data, not indirectly through function approximation. (This derivation is shown in the next section.) In fact, <ref type="bibr" target="#b83">Vapnik's (1979)</ref> original work focused on finding the optimal decision boundary for linearly separable outcome classes. The result, known as the maximum margin optimal classifier, does not depend on any the overlap may disappear. This, of course, is a restatement of the deeper question of whether or not nature contains inherent randomness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head><p>Dimensionality and Prediction parametric assumptions about the process generating the data, but requires only that samples be drawn independently from this process. <ref type="bibr" target="#b12">Boser et al. (1992)</ref> extended this result to nonlinear problems using a kernel function to map attribute space into a higherdimensional feature space. In feature space, a linear boundary is still sought. However, classes that are linearly separable in feature space may be nested or otherwise highly intertwined in attribute space. The resulting boundary, when translated back to attribute space may, therefore, be nonlinear. <ref type="bibr" target="#b22">Cortes and Vapnik (1995)</ref> then extended the solution to soft-margin classifiers to deal with data that are not perfectly separable, even in feature space.</p><p>The parameters of the optimal linear classifier in feature space are solutions to a constrained quadratic programming problem. As such, these estimates are provably globally optimal and are not conditioned on a particular parametric form or chain of forms. <ref type="bibr">8</ref> They are mathematical in nature (like OLS or minimum chi-square), not statistical in nature (like maximum likelihood). We return to this point because it clearly forces additional reflection about the-not necessarily mutually exclusive-goals of prediction and structural diagnostics. The resulting decision boundary is not subject to decision boundary bias because it is not the discretized version of a function approximation exercise, but rather a direct attack on the optimal boundary problem. As Figure <ref type="figure">4</ref> implies, structural <ref type="bibr">8</ref> For example, <ref type="bibr" target="#b84">Vapnik (1995)</ref> documents the failure of analytic maximum likelihood to solve a mixture of normal distributions. Train and <ref type="bibr">Sándor (2002)</ref>, while exploring various draw techniques for mixed logit models (e.g., pseudorandom draws, Halton sequences, orthogonal arrays, and Latin hypercube draws) identify cases where the maximum of the simulated likelihood function is never found.</p><p>interpretations are possible with the support vector machine, but they are made post hoc once the model's predictive capacity has been fully utilized. Figure <ref type="figure">4</ref> links the SVM and classic parametric paradigms to emphasize their complementary rather than competitive nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementing an SVM</head><p>The support vector machine combines concepts from abstract Hilbert spaces with modern optimization techniques. We concentrate in this section on the main steps required to implement an SVM rather than on technical detail. Technical detail is provided in Appendix A for the optimization components of the SVM, including extensions to soft-margin classifiers and multiclass problems. Appendix B illustrates how kernel transformations work. Kernel transformations are a fundamental ingredient of a support vector machine because they allow a linear machine to solve a nonlinear problem. <ref type="bibr" target="#b83">Vapnik (1979)</ref> approached the discrete classification problem from the perspectives of function capacity and shattering. He reasoned that although many hyperplanes can fit between linearly separable groups, the optimal separating hyperplane should lie midway between the convex hulls of the two groups and be orthogonal to the shortest line connecting these hulls. The solution has both a primal and a dual form, as shown in (2). (The observations x and x i are vectors in n .) Each form can be solved as a quadratic programming problem (see Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Maximum Margin Classifiers</head><formula xml:id="formula_3">f x = w * • x + w * 0 = n i=1 * i y i x • x i + w * 0 (2)</formula><p>Note that the dual representation contains the inner product x • x i . The dual's dependence on these inner products is key because it facilitates solutions to nonlinear variations of the problem. In particular, <ref type="bibr" target="#b84">Vapnik (1995)</ref> noted that a problem in attribute space could be transformed to a problem in a higher-dimensional feature space without altering the solution form. This technique-mapping a problem to a new domain, solving it there (where the solution technique is easier), then mapping the solution back to the original domain-is frequently used in mathematics. A simple example is solving multiplication problems by addition using the mutually inverse log/exp transforms.</p><p>A second example is solving differential equations using Laplace (and inverse Laplace) transforms to migrate between the time and frequency domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Kernel Transformations</head><p>The SVM machine uses kernel transformations that take the general form x • z = x • z . That is, the transform of the inner product of the n-vectors x and z is the vector inner product of their transformed images. This means that the form of the dual problem is the same whether it is solved in attribute space using x • z or in feature space using x • z . Note further that the inherent size of the problem is the same in both spaces. The degrees of freedom used in estimating the decision boundary depends only on the number of data points, l, not on the product l • n as with maximum likelihood estimators. In fact the boundary depends on a subset of l, the vectors that form the inner edges of the convex hulls of the two classes: the problem's support vectors. Hence, the curse of dimensionality is neutralized.</p><p>The optimal boundary is calculated as the solution to the dual quadratic programs (QPs) problem in feature space.</p><p>Max</p><formula xml:id="formula_4">n i=1 i − 1 2 n i j=1 i j y i y j x i • x j (3a) s.t. n i=1 y i i = 0 i ≥ 0 i = 1 l (3b)</formula><p>We implement the solution using <ref type="bibr" target="#b61">Platt's (1999)</ref> highly efficient Sequential Minimization Optimization (SMO) algorithm. SMO breaks the problem into a series of smallest possible QPs that are solved analytically. The algorithm uses less memory and is easier to implement than "standard" algorithms, and can be more than 1,000 times faster <ref type="bibr" target="#b61">(Platt 1999)</ref>. Two types of kernels are most often used in practice, the Gaussian (or radial basis) kernel</p><p>x z = exp − x − z 2 / 2 with hyperparameter and the polynomial kernel</p><p>x z = x • z + 1 d with hyperparameter d. If the data have dimensionality n, then the feature space mapped by a polynomial kernel has dimension n+d d . The dimensionality of a feature space mapped by a Gaussian kernel can be infinite <ref type="bibr" target="#b19">(Burges 1998</ref>).</p><p>Appendix B contains numerical details of a complete example using a polynomial kernel. We selected a problem known to be unsolvable with standard GLM methods and with multinomial logit. Neither can the problem be solved by the artificial intelligence technique known as perceptrons <ref type="bibr" target="#b57">(Minsky and Papert 1969)</ref>. It can be solved using an artificial neural net with hidden layers, but the solution is very slow to converge <ref type="bibr" target="#b51">(Langley and Burgess 2000)</ref>. The SVM solution using a second-degree polynomial kernel is intuitive and numerically efficient, solving virtually instantly even with more than the two predictors illustrated in Appendix B.</p><p>The support vector machine implemented by a kernel family imposes a structure on the set of functions that comprise the learning machine. Currently, there is no metatheory regarding the choice of kernel family. Normally, this choice is based on domain knowledge or researcher preference supplemented by numerical results <ref type="bibr" target="#b12">(Boser et al. 1992</ref>). If both standard kernels (Gaussian and polynomial) prove ineffective, more elaborate kernels can be constructed from "building blocks" according to mathematical principles from the theory of integral operators <ref type="bibr" target="#b24">(Cristianini and Shawe-Taylor 2000)</ref>. Empirical evidence suggests that the choice of kernel family has little influence on the generalization performance of the machine <ref type="bibr" target="#b84">(Vapnik 1995</ref><ref type="bibr" target="#b72">, Schölkopf et al. 1998</ref>. In other words, the best kernel in the polynomial family and the best kernel in the Gaussian family typically perform equally well in a given problem context. However, training times to achieve this performance may differ by several orders of magnitude as a function of the selected kernel. The procedure we used to estimate the hyperparameters of the kernels used in the empirical tests reported next is based on well-known resampling techniques <ref type="bibr" target="#b29">(Edgington 1995</ref><ref type="bibr" target="#b30">, Efron and Tibshirani 1993</ref><ref type="bibr" target="#b37">, Good 2005</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Empirical Tests of the Support Vector Machine</head><p>We turn now to comprehensive tests of the predictive capacity of the support vector machine baselined by direct comparisons to those from multinomial logit. <ref type="bibr">9</ref> For this purpose, Monte Carlo simulation is appropriate because it allows us to control key aspects of the data-generating process over a wide range of experimental conditions. 10 Ben-Akiva and Lerman (1985) discuss seven variables relevant to the performance of a discrete-choice model: (1) the number of product attributes, (2) the estimation sample size, (3) the magnitude of error in the stochastic component of the random utility model, (4) the number of individual characteristics, (5) the number of choice alternatives, (6) the type of error distribution, and (7) whether or not a correlated error structure is present. We manipulate these seven variables in a 2 5 × 3 2 factorial design using the levels shown in Table <ref type="table" target="#tab_3">2</ref>. Levels were chosen to cover a broad range of situations encountered in practice, particularly in consumer choice experiments using the logit model. Given our focus on predictive accuracy, the dependent variable is the first-choice hit-rate produced by an estimated model in a validation sample (of size 3,000) from the same data-generating process that created the estimation/training sample. A 16-treatment fraction of the full factorial is used. <ref type="bibr">11</ref> The plan permits clear estimation of all main effects along with six selected two-way interactions. We repeat the experiment 10 times within each treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data-Generating Functions</head><p>For the deterministic component of utility, we simulate a variety of data-generating functions that mimic known consumer decision structures. We illustrated in §3.1 that for pure noncompensatory structures, such as latitude of acceptance, logit will not perform well. In the tests reported here, we confine attention to cases where logit has no inherent limitations. Thus, we constrain decision rules to be compensatory, but not necessarily linear. Further, we include no effect higher than quadratic. Quadratic effects in utility are common for many product attributes. We include effects in pure form, for an attribute alone, and in moderated form where an effect interacts with an individual characteristic. For example, many consumers exhibit a quadratic response to variations in price. This response is likely to be moderated by an individual's income level. We also include bilinear interactions between two product attributes and allow these to be moderated by individual characteristics.</p><p>The systematic components of utility are shown in Table <ref type="table" target="#tab_4">3</ref>. These functions share the following properties. Each component includes two linear terms of product attributes and one linear term of individual characteristic. Each component includes one interaction term between two product attributes. Each includes two interaction terms between a product attribute and an individual characteristic. Each contains a quadratic effect for a product attribute. Finally, each component contains a quadratic by linear interaction between a product attribute (quadratic) and an 11 See Hahn and Shapiro (1966; Plan Code 65A, Master Plan 5). </p><formula xml:id="formula_5">1 x ik = x ik1 x ik2 x ik1 x ik2 x ik1 z i x ik2 z i x 2 ik1 x 2 ik2 z i z i x ik1 , x ik2 are Person i Choice k's attribute data; z i is Person i's single individual characteristic.</formula><p>Utility 2 x ik = x ik1 x ik2 x ik1 x ik2 x ik1 z i1 x ik2 z i2 x 2 ik1 x 2 ik2 z i2 z i3 x ik1 , x ik2 are Person i Choice k's attribute data; z i1 , z i2 , and z i3 are Person i's three individual characteristics.</p><p>Utility 3 Utility 5 Utility 6 individual characteristic (linear). This latter type of effect was recovered by <ref type="bibr" target="#b78">Brynjolfsson and Smith (2001)</ref> in their study of online consumer decision making.</p><formula xml:id="formula_6">x ik = x ik1 x ik2 x ik1 x ik2 x ik2 z i x ik3 z i x 2 ik3 x 2 ik4 z i z i x ik1 , x ik2 , x</formula><formula xml:id="formula_7">x ik = x ik1 x ik2 x ik1 x ik2 x ik3 z i x ik4 z i x 2 ik5 x 2 ik6 z i z i x ik1 , x ik2 , x ik3 , x ik4 , x</formula><formula xml:id="formula_8">x ik = x ik1 x ik2 x ik1 x ik2 x ik3 z i1 x ik4 z i2 x 2 ik5 x 2 ik6 z i2 z i3 x ik1 , x ik2 , x ik3 , x ik4 , x</formula><p>All six deterministic components have the same number of terms. These six components of the overall data-generating process get progressively more complex as more product attributes and individual characteristics are incorporated. Because data are represented in an "exploded" form when estimating a logit model, the size of the design matrix increases in number of choices, number of product characteristics, and number of individual characteristics. Thus, even though the number of individuals in the validation sample is held constant in each case, the effective estimation/ prediction effort varies by treatment.</p><p>We implemented the SVM using a polynomial kernel with hyperparameter training using resampling. For logit modeling, we used SAS's multinomial discrete-choice procedure (MDC). MDC supports a wide variety of structural forms, including simple, conditional, and nested logit. We use logit models with alternative specific constants and alternative specific individual characteristics. Whenever error terms are correlated, we fit a nested logit model and assume that the nested structure is known a priori. These assumptions provide an advantage to the logit class of models, giving maximum flexibility to capture the data-generating process within the confines of what we refer to as "standard practice;" e.g., linear-inparameters forms as described in § §2.3 and 3.3.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>Table <ref type="table" target="#tab_6">4</ref> shows the mean first-choice hit-rates for both estimation (training) and validation (prediction) in each of the 16 treatment conditions. Each mean is the average of the 10 values in its corresponding cell. The standard deviation among these 10 values is shown in parentheses. Table <ref type="table" target="#tab_6">4</ref> indicates that in every treatment condition the SVM outpredicts the corresponding logit model. This is true not only on average, but for every one of the 160 total cases run across all cells. The overall mean prediction rate of the logit is 72.7% while this hit-rate is 85.9% for the support vector machine. Computing the percent dominance within treatment, then averaging these 16 values, the SVM predicts 29.9% better on average than the logit, ranging from a virtual tie in cell 5 to 169.6% and 169.3% better in cells 13 and 15. (For reference, Cell 13 has four product attributes, estimation sample size of 100, high error, one individual characteristic, six choice alternatives, a nonnormal error density, and no correlated error. Cell 15 has six choice alternatives, uses the large sample size, and has low, normally distributed correlated error.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Subexperiments on Main Effects</head><p>To develop a better understanding of the direct impact of certain factors, we conducted three subexperiments. We focused on three of the four largest sources of variation from the main experiment: (a) the number of choice alternatives, (b) the number of individual characteristics, and (c) the estimation sample size. 12 (Table <ref type="table" target="#tab_7">5</ref> shows proportion of variance explained by each factor and estimable interactions in the main experiment. Results are significant at p &lt; 0 05 or better unless otherwise indicated.)</p><p>In the subexperiments, nonmanipulated factors were held fixed while the manipulated factor was varied. Manipulating just one factor removes confounding in the raw marginal means from the main experiment. <ref type="bibr">13</ref> In the subexperiment for number of choice alternatives, we expanded the number of levels to five (2-6 alternatives) to get a complete picture of the marginal effect of this important variable.</p><p>Figure <ref type="figure">5</ref> shows results. Because each subexperiment was replicated 10 times, every difference between hit-rate proportions is statistically significant at a minimum of p &lt; 0 01, and usually at much smaller p-values. Panel (a) indicates that as the number of choice alternatives increases, the prediction hitrate of each model falls. This is expected because the "first-choice" prediction task increases in difficulty with more alternatives. However, the decline is much steeper for the MNL, dropping from 85.6% at two alternatives to 74.9% with six, a 12.4% decline. The falloff for the SVM, from 88.0% to 84.2%, is a in cell 14 of the main experiment, where the SVM and MNL perform nearly the same; i.e., cell 14 is a reasonable starting point for comparisons of marginal effects. When not varied, the estimation sample size was l = 400, the number of choice alternatives was three, and the number of individual characteristics was three. Note that manipulating the number of attributes factor would alter the "common size" of the utility functions shown in Table <ref type="table" target="#tab_6">4</ref>. Hence, this factor was not included in the subexperiments. 4.3% decline over the same range. Panel (b) shows that adding individual characteristics-and therefore, complicating the structural form of the relationship between predictors and target-has no effect on the performance of the SVM. The SVM's hit-rate actually improves slightly in the case shown. The MNL, because it performs density estimation prior to the prediction step, is more sensitive to the ratio of parameters to sample size. Since sample size is fixed (at l = 400 in this experiment), cases with three individual characteristics require additional parameters be estimated. This use of degrees of freedom negatively impacts prediction ability. Finally, Panel (c) shows that all else being equal, increased sample size helps both models as expected. The MNL is more sensitive to the change, improving 3.62% over the range shown versus 1.84% for the SVM. Of course, the SVM starts from a significantly higher base prediction rate, and hence has less room to improve. These subexperiments allow us to anticipate the marginal effect on prediction hit-rate in tasks with varying numbers of alternatives, individual characteristics, and estimation sample sizes. However, from the main experiment, we found significant interactions between certain factors. Because the fractional experimental design limited our search for interactions, we suspect higher-order interactions are also at work, particularly given the very low prediction rates for the MNL in cells 13 and 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion</head><p>Results suggest that the SVM has considerable promise for accurately predicting consumer choice in the "pure prediction" environments found in automated modeling, mass-produced models, intelligent agents, and data mining. In our main experiment, a single SVM significantly outpredicts the best model from a set of appropriate MNL models even though perfect a priori knowledge is used to correctly select the nesting structure in cases with correlated error. In practice, analyst insight is desirable in controlled experimental settings or with surveyed field data, but in areas where automated modeling is useful, the "one size fits all" aspect of the SVM is a definite advantage.</p><p>Results from our subexperiments suggest that the prediction hit-rates of MNL are more severely affected by increases in the choice set size and the number of individual characteristics than are those from the SVM. Although further experimentation is warranted, the addition of more individual characteristics seems not to negatively impact the predictive accuracy of the SVM and may even increase it. Increased Marketing Science 24(4), pp. 595-615, © 2005 INFORMS accuracy would follow from the fact that individual differences mediate the main effects of choice dimensions. This effect is likely to be much stronger in real choice tasks when covariates are judiciously selected. In other words, with the SVM, more covariates will drive predictive accuracy up, not down.</p><p>6. Limitations of the Present Research 6.1. Weaknesses of the Support Vector Machine Although the support vector machine shows promise, there are obstacles to overcome before the approach gains acceptance in marketing. The SVM's novel inferential philosophy requires additional theoretical development before it can be routinely used in practical situations. Because there are no probability density assumptions made, the SVM does not yield probability estimates for hypothesis testing (classical view) or predictive/posterior bounds (Bayesian view). Efforts to provide such estimates are available <ref type="bibr">(Platt 2000, Vapnik and</ref><ref type="bibr" target="#b87">Chapelle 2000)</ref>, but a support vector machine does not generate them naturally. Lack of easy-to-use computer software will also impede the SVM's acceptance in marketing, although better software should be forthcoming in the next few years. More fundamentally, there is no complete, working metatheory to assist with the selection of kernel transformation for an SVM. Depending on the choice of kernel family, parameter estimation can be time consuming.</p><p>Although our focus is on prediction, not structural diagnostics, having both is a plus for any model. In the area of interpretability, the SVM faces a catch-22. Its most significant advantages are in nonlinear environments, precisely the environments in which estimated parameters cannot be interpreted directly. True, in these situations numerical analysis can be performed to yield response coefficients for the SVM as outlined in Appendix C. Nevertheless, implementing these analyses requires custom programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Weaknesses of the Experimental Comparisons</head><p>Although we took considerable care to be thorough and fair in conducting the experiments reported here, our methods can be criticized on several fronts. The main experiment could be enriched by including more levels on certain factors. For example, using six product attributes and three individual characteristics may be too restrictive in some cases. Countering this, Figure <ref type="figure">5</ref> suggests that the SVM's predictive accuracy is not sensitive to the number of individual characteristics. Furthermore, when predictive tasks involve seven-plus attributes, in the kinds of "ad hoc" data collection environments we envision, data preprocessing can filter out redundant information prior to the SVM modeling step <ref type="bibr" target="#b39">(Guyon and Elisseeff 2003)</ref>. Find-ings suggest that, comparatively, the SVM would gain rather than lose ground in more complex cases.</p><p>Researchers specializing in discrete-choice modeling may be disappointed that more sophisticated RUT models were not used in this research. However, our goal was not to pit MNL against SVM in any direct way, but rather to use reasonably sophisticated MNL models to create a workable baseline to provide perspective on the predictive accuracy of the support vector machine. The MNL models performed well in this research, and using more sophisticated versions could improve MNL predictive hit-rates. We offer our data to specialists developing and testing more flexible models. However, these models require custom programming and considerable human intervention in the identification and fitting stages, steps that defeat the purpose of predictive modeling in the environments on which we focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Directions for Future Research</head><p>Our directions for future research stress two areas where SVM and random utility theory (RUT) models are complementary: structural gap identification and simultaneously improving prediction and diagnosticity when nonlinear information integration rules are in play.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">The SVM as a Structural Gap Identifier</head><p>Given a set of predictor variables x, the conditional density y x may contain irreducible uncertainty about the target variable y. However, when an SVM yields out-of-sample prediction rates that significantly exceed those from a structurally rich RUT model (with both methods using precisely the same input), this is a clear signal that the set x contains additional predictive information in higher-order effects. Under these circumstances, the support vector machine complements standard modeling in two useful ways. First, it puts the analyst on guard when interpreting estimated coefficients as measures of marginal response. Second, it provides motivation to add additional terms to the systematic component of utility (in RUT models) or, more generally, to the functional relationship between target and predictors in other types of models. For example, if a polynomial kernel is used in the SVM, the degree of the polynomial serves as an upper limit to the nested set of functions that need to be searched to better specify the model. Although the perfect structure is unlikely to be found, adding additional effects should achieve much higher prediction rates and structural accuracy than the linear model under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Predictive Power and Structural Diagnostics:</head><p>Convergent Research for Noncompensatory Information Integration Rules Because the SVM is predictively robust even with very small samples, future research using the SVM may blend emerging directions in choice-based conjoint and aggregate-level, discrete-choice modeling. (Technical Appendix B provides an overview of choice-based conjoint modeling and its relation to the present work.) For example, <ref type="bibr" target="#b36">Gilbride and Allenby (2004)</ref> have recently published models from the RUT class that stress structural accuracy by identifying and modeling noncompensatory information processing. Meanwhile, <ref type="bibr" target="#b13">Bradlow (2003)</ref> has wished for predictively accurate conjoint models for choices from noncompensatory information integration rules. These and other modelers from the "choice-based conjoint" camp rightfully want to be able to estimate models at the individual level that are both structurally accurate and highly predictive. This goal is being actively pursued by <ref type="bibr" target="#b32">Evgeniou et al. (2005)</ref>, who use an SVM-style kernel transformation for analyzing data from choicebased conjoint experiments. This is a promising blend of research streams. The idea may be extended further. The goals would be, first, to isolate the attributes involved in noncompensatory rules at the individual respondent level (because these may differ by person) and, second, to associate explicit patterns of SVM parameter estimates with specific noncompensatory processing strategies-latitude of acceptance, conjunctive, disjunctive, XOR, etc.-to identify which consumers use these strategies, and with what attributes. Results from such models would yield highly useful insights to marketing strategists in the areas of new product design and market segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Concluding Comments</head><p>In this paper, we argue that highly predictive models will play an increasingly important role in 21st century marketing applications, particularly in areas such as automated modeling, mass-produced models, intelligent software agents, and data mining. <ref type="bibr" target="#b64">Politz and Deming (1953)</ref> argued forcefully that predictive accuracy is the standard by which model quality should be measured. More than 50 years later the argument still resonates with marketing scientists <ref type="bibr" target="#b3">(Allenby et al. 2002)</ref>. The support vector machine performs well on predictive tasks where the relationship between predictors and target is complex. Although its modeling philosophy is nonstandard, the SVM and related kernel methods may provide not only accurate "pure prediction," but a unique link between structural diagnostics and predictive accuracy in a wide variety of marketing applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Optimal Margin Classifier</head><p>An optimal margin classifier finds a particular linear boundary between two perfectly separable classes in an attribute space of raw data. The idea is similar in spirit to OLS because the problem is stated and solved as a nonparametric optimization problem, not as a parametric (maximumlikelihood) problem. Suppose we have data sampled from an unknown distribution P x y where y takes on two values. (Multivalued extensions are discussed subsequently.) A linear decision function that completely separates the observations can be expressed as the inner product between a weight vector w and an input vector x, plus a constant, w 0 .</p><p>f</p><formula xml:id="formula_9">x = w • x + w 0 (A1)</formula><p>In the two-class case, the sign of f x determines the membership class of the point x. Thus, for points x i , i = 1 l, the problem is to find w w 0 such that w</p><formula xml:id="formula_10">• x i + w 0 &gt; 0 if y i = 1 (A2a) w • x i + w 0 &lt; 0 if y i = −1 (A2b)</formula><p>These inequalities can be expressed compactly as A3.</p><formula xml:id="formula_11">y i w • x i + w 0 &gt; 0 for i = 1 l (A3)</formula><p>The formulation (A3) leads to a direct solution of the classification problem without attempting to estimate the probability density P x y . However, the model is underidentified because, for linearly separable data, there are an infinite number of linear functions that can perform the separation without error. To choose one solution among many, the support vector machine defines the optimal decision function as the one that leaves the largest possible margin on both sides of the decision boundary. The margin of the ith point x i y i with respect to a particular function f is the quantity i = y i f x i , which is positive if f correctly classifies the observation, and nonpositive otherwise. Given a specific sample with linearly separable points, the support vector algorithm finds the separating function with maximum margin of the training set with respect to the class of functions under consideration <ref type="bibr" target="#b24">(Cristianini and Shawe-Taylor 2000)</ref>. The optimal classifier is called the maximum margin classifier or optimal margin classifier. <ref type="bibr" target="#b85">Vapnik (1998)</ref> shows that this classifier is unique to a given data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derivation-Binary Scenario</head><p>To derive the maximum margin classifier, one must minimize the norm of the weight vector, w, under the con- </p><formula xml:id="formula_12">x i + w 0 ≥ 1 for i = 1 l (A4b)</formula><p>Using Lagrangian multipliers, (A4) can be transformed into its dual form, where the Kuhn-Tucker conditions guarantee a unique solution. <ref type="bibr">14</ref> The dual form is</p><p>Max</p><formula xml:id="formula_13">n i=1 i − 1 2 n i j=1 i j y i y j x i • x j (A5a) s.t. n i=1 y i i = 0 i ≥ 0 i = 1 l (<label>A5b</label></formula><formula xml:id="formula_14">)</formula><p>where i is the Lagrange multiplier for inequality i in (A4b). The dual problem is a quadratic program, efficiently solvable using the recursive procedure developed by <ref type="bibr" target="#b61">Platt (1999)</ref>. Transitioning from (A4) to (A5) yields</p><formula xml:id="formula_15">w * = n i=1 * i y i x i (A6)</formula><p>where</p><formula xml:id="formula_16">* 1 * 2 *</formula><p>n are solutions to the problem (A5). The data points with nonzero i are the problem's support vectors. The optimal constant, w * 0 , can be derived using any single support vector, but more often-to achieve numerical stability-is calculated using their mean. The optimal decision function can be represented in terms of either the primal or dual optimal solution as shown in A7.</p><formula xml:id="formula_17">f x = w * • x + w * 0 = n i=1 * i y i x • x i + w * (A7)</formula><p>Two properties of the solution are important. First, the optimal boundary is a function only of the relatively few support vectors, not all data points. Although counterintuitive from a sampling perspective, these "inliers" are not as susceptible to boundary bias as are boundaries based on all data points. Second, the size of the dual problem scales directly with the sample size, not with the dimensionality of the data model (e.g., the function class with its respective parameters). Thus, solutions do not suffer from the curse of dimensionality. This property is illustrated in Appendix B, which provides a complete numerical example of an SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel-Induced Transformations</head><p>In practice, observations are rarely linearly separable in the original space, but may be linearly separable in a specially constructed higher-dimensional space. The SVM uses a kernel-induced transformation R n → to map the original input space into a higher-dimensional space, called feature space.</p><p>is chosen so that data points appear in the algorithm uniquely in the form of dot products, i.e., functions where the vector inner product, x • x i in (A7), takes the form of the inner product between the images x • x i in . This property is satisfied by a kernel function, K, such that K x x i = x • x i . Replacing x • x i everywhere with x • x i , a support vector machine finds an optimal linear boundary in , the feature space, which maps to a nonlinear decision function in the original n-dimensional attribute space (see Appendix B for details). The decision function with kernel (A7) becomes (A8). <ref type="figure">and  *</ref> i i = 1 l are solutions to the QP maximization problem (A9).</p><formula xml:id="formula_18">D x = n i=1 * i y i K x x i + w * 0 (A8) where K x • x i = x • x i ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max</head><formula xml:id="formula_19">l i=1 i − 1 2 l i j=1 i j y i y j K x x i (A9a) s.t. n i=1 y i i = 0 i ≥ 0 i = 1 l (A9b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft-Margin Classifiers</head><p>In practice, a data set normally contains nonseparable observations due to the nature of the problem domain, random error, theoretical ignorance, variable deficiency, and data mislabeling. In such situations, using the maximum margin classifier will lead to "overfitting" noisy data <ref type="bibr" target="#b22">(Cortes and</ref><ref type="bibr">Vapnik 1995, Cristianini and</ref>. Softmargin classifiers seek an optimal decision function with maximum margins for observations that can be separated accurately and, simultaneously, a minimum number of errors for nonseparable observations. To accomplish this goal, positive slack variables i i = 1 l are included in the decision function; y i w</p><formula xml:id="formula_20">• x i + w 0 ≥ 1 − i ; i = 1 l.</formula><p>The weight of the slack variables is controlled by a hyperparameter (or penalty term) C. This alters the quadratic programming problem, where (A9c) replaces (A9b).</p><formula xml:id="formula_21">n i=1 y i i = 0 0≤ i ≤ C i = 1 n (A9c)</formula><p>The soft-margin classifier (A9c) is identical to the maximum margin classifier (A9) if the penalty term C is infinite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiclass Classification Formulation</head><p>The SVM classifier described above is binary. Although direct generalization to multigroup classifiers is possible (e.g., <ref type="bibr">Watkins 1998, Vapnik 1998)</ref>, the direct approach is not necessarily efficient. SVM researchers often combine binary classifiers to handle multiclass situations <ref type="bibr" target="#b50">(Krebel 1999</ref>. Suppose we have m classes; a simple and effective procedure is to train m one-versusrest binary classifiers (say, "one" positive, "rest" negative) and assign a test observation to the class with the largest positive distance <ref type="bibr" target="#b12">(Boser et al. 1992</ref><ref type="bibr" target="#b84">, Vapnik 1995</ref>. This procedure has been shown to give excellent results and is the method we use in our multiclass tests, described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVMs and Capacity Control</head><p>A kernel-induced transformation may result in a very highdimensional feature space. For example, in classification problems the most often used kernels are polynomial kernels (A10) and Gaussian kernels (A11).</p><formula xml:id="formula_22">K x i x j = x i • x j + 1 d (A10) K x i x j = e − x i −x j 2 /2 2 (A11)</formula><p>If the data have dimensionality n, then the feature space mapped by a polynomial kernel of degree d has dimension n+d d . The dimensionality of a feature space mapped by a Gaussian kernel can be infinite <ref type="bibr" target="#b19">(Burges 1998)</ref>. A linear function in a very high-dimensional feature space will not yield a machine that generalizes well. <ref type="bibr">15</ref> However, an SVM looks for a linear separating function with maximum margin. By maximizing the margin of the training set, SVMs control the capacity of the optimal linear function in feature space. In fact, it can be shown that the margin of a training set is an effective capacity measure; i.e., the structural risk bound (1) can be expressed as a function of a measure of margin on the training set (Shawe-Taylor and Cristianini 1999a, b, and c; <ref type="bibr" target="#b24">Cristianini and Shawe-Taylor 2000)</ref>. Thus, observing a large margin is equivalent to minimizing the VCdimension, resulting in good generalization from a small sample. In fact, <ref type="bibr" target="#b85">Vapnik (1998)</ref> shows that the capacity of the support vector machine is bounded when the observations are completely separable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Kernel Transformations: An Example</head><p>We engineer a simple but complete example to explain how kernel transformations work. The support vector machine uses such transformations to map the original data in attribute space to a higher-dimensional feature space. In feature space, a linear decision function is found that corresponds to the nonlinear function in input space. Equation (B1) shows the dual form of this function, which includes the kernel mapping K</p><formula xml:id="formula_23">x • x i = x • x i explained in this appendix. D x = n i=1 i y i K x x i + w 0 (B1)</formula><p>The * i are solutions to the dual QP maximization problem, as outlined in Appendix A, Equations (A5) and (A7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>The four data points shown in Table <ref type="table" target="#tab_0">B1</ref> are members of two groups with coordinates on axes x 1 and x 2 and group membership indexed by y ∈ −1 +1 . The points are shown in Figure <ref type="figure">B1</ref> using the symbols indicated in the table.</p><p>This classification problem is known as the XOR problem in the literature of artificial intelligence. <ref type="bibr" target="#b57">Minsky and Papert (1969)</ref> were the first to note that the problem could not be solved by perceptrons. <ref type="bibr" target="#b43">Hinton et al. (1986)</ref> rediscovered the problem, and MacKay and Oldfield (1995) presented a solution using a nonlinear, hidden, layered artificial neural net. When implementing this solution, <ref type="bibr" target="#b51">Langley and Burgess (2000)</ref> found convergence to be extremely slow. <ref type="bibr">16</ref> Figure <ref type="figure">B1</ref> shows that a linear boundary cannot separate these four points. However, four points can be shattered <ref type="bibr">16</ref> The data in Table <ref type="table" target="#tab_0">B1</ref> represent a linearly transformed version of an exclusive OR truth table, where class membership is indicated by the binary sum of a point's coordinates. Points belong to class false = −1 if their scores on each dimension match. Otherwise, they belong to class true = +1 , and the outcome, y = x 1 + x 2 mod 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table B1</head><p>Data Set (Attribute Space)</p><formula xml:id="formula_24">Data point x 1 x 2 y Symbol a 0 −1 −1 Square b −1 0 +1 Circle c 0 +1 −1 Square d +1 0 −1 Circle Figure B1 Four Points in Two Classes 0 1 -1 -1 0 1 x 1 x 2 a b c d</formula><p>by a second-degree polynomial. The inner product kernel for a polynomial of degree two is given by the nonlinear function in (B2).</p><p>x</p><formula xml:id="formula_25">z = K x z = x • z + 1 2 (B2)</formula><p>Expanding (B2) for our two-dimensional attribute space, we find x z = x • z as follows:</p><formula xml:id="formula_26">K x z = x 1 x 2 z 1 z 2 + 1 2 = x 1 z 1 + x 2 z 2 + 1 2 = x 2 1 z 2 1 + x 2 2 z 2 2 + 2x 1 x 2 z 1 z 2 + 2x 1 z 1 + 2x 2 z 2 + 1 = x 2 1 x 2 2 √ 2x 1 x 2 √ 2x 1 √ 2x 2 1              z 2 1 z 2 2 √ 2z 1 z 2 √ 2z 1 √ 2z 2 1              = x • z (B3)</formula><p>In words, the kernel function is chosen so that its application to vectors in attribute space will yield the inner product of their representations in feature space. Result (B3) generalizes to (B4), which shows that the transformation x i z i + 1 n j=1</p><p>x j z j + 1</p><formula xml:id="formula_27">= n n i j= 1 1 x i x j z i z j + n i=1 √ 2x i √ 2z i +1 2 = x i • x j n n i j= 1 1 √ 2x i 1     z i • z j n n i j= 1 1 √ 2z i 1     = x • z (B4)</formula><p>(The algebra goes through for arbitrary d and c. See Cristianini and Shawe-Taylor 2000, Chapter 3). With d = 2, the terms involved in this higher-dimensional inner product are quadratic x i x i = x 2 i , bilinear x i x j , linear √ 2x i , and constant (1) terms from the attribute space. The choice of the constant (c = 1) determines the relative weights applied to these terms in feature space. The induced structure contains the types of terms-quadratic, bilinear, and higherorder interactions-that may be present in choice data or in other data-generating processes.</p><p>Inner product matrices are always square and symmetric, and their size is simply the number of data points. With four points, we expect to see the 4 × 4 matrix K shown in (B5). For example, the entry K 2 1 = 1 is found from the following inner product:</p><formula xml:id="formula_28">K 2 1 = −1 2 0 2 √ 2•−1•0 √ 2•−1 √ 2•0 1 • 0 2 −1 2 √ 2•0•−1 √ 2•0 √ 2•−1 1 T = 1 K =      4 1 0 1 1 4 1 0 0 1 4 1 1 0 1 4     </formula><p>(B5) K is known as the kernel or gram matrix corresponding to the kernel function K. Because K is an inner product matrix in n , it is a positive semidefinite, symmetric matrix that can be factored as K = P P T , where is a diagonal matrix of the eigenvalues t ≥ 0 of K with corresponding eigenvectors, p t = p t1 p tn T as the columns of P. (See <ref type="bibr" target="#b95">Young and Householder 1940;</ref><ref type="bibr">Bronson 1991, Chapter 9</ref>.) Using the mapping x i → t p ti n t=1 ∈ n i = 1 n , it follows directly that is a kernel function corresponding to the feature mapping ; e.g.,</p><formula xml:id="formula_29">x i • x j = n t=1 t p ti p tj = P P T ij = K ij = K x i x j (B6)</formula><p>With a particular choice of weightings in the kernel function (Mercer kernels), the analyst can even insure that the features will be orthogonal. <ref type="bibr" target="#b19">Burges (1998)</ref> derives this property, which is a deeper result from continuous mathematics in Hilbert spaces <ref type="bibr" target="#b23">(Courant and</ref><ref type="bibr">Hilbert 1953, Vapnik 1995)</ref>.</p><p>In summary, SVM kernels map vectors in a lowdimensional attribute space into their inner products in a high-dimensional feature space. These functions directly provide the inner product that appears in the dual form of the separating hyperplane quadratic program, avoiding the need to perform calculations in feature space. Because estimation involves only inner products of data points, the problem size scales with sample size, not the dimensionality of the data. This neutralizes the "curse of dimensionality" that afflicts parametric models (see <ref type="bibr" target="#b9">Ben-Akiva et al. 1997</ref><ref type="bibr" target="#b34">, Friedman 1997</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimal Decision Function</head><p>The optimal decision function is obtained by substituting the four data points into the decision function (B1) and expanding. This yields Expression (B7).</p><formula xml:id="formula_30">D x = 1 K x x 1 − 2 K x x 2 + 3 K x x 3 − 4 K x x 4 +w 0 = 1 x 1 x 2 1 0 + 1 2 − 2 x 1 x 2 0 1 + 1 2 + 3 x 1 x 2 −1 0 + 1 2 − 4 x 1 x 2 0 −1 + 1 2 + w 0 = 1 x 1 + 1 2 − 2 x 2 + 1 2 + 3 −x 1 + 1 2 − 4 −x 2 + 1 2 + w 0 (B7)</formula><p>The optimal decision boundary-which is linear in feature space-is found by solving the optimization problem (B8), a constrained quadratic program.</p><p>Max Q a = 1 + 2 + 3 + 4 − 1 2 4 i j=1 i j y i y j K ij (B8) s.t.</p><p>4</p><formula xml:id="formula_31">i=1 y i i = 1 − 2 + 3 − 4 = 0 0 ≤ i i= 1 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution</head><p>The solution to the QP problem is * 1 = * 2 = * 3 = * 4 = 0 5, indicating that all four points are support vectors. The functional Q reaches its maximum of 1.0 at this point. The decision function in the dual form-which is nonlinear in attribute space-is given by (B9).</p><formula xml:id="formula_32">D x = 4 i=1 * i y i K x x i + w 0 = 1 2 x 1 + 1 2 − x 2 + 1 2 + −x 1 + 1 2 − −x 2 + 1 2 + w 0 = x 2 1 − x 2 2 + w 0 (B9)</formula><p>We can use any one of the support vectors to solve for the constant w 0 ; i.e., y i • D x i = 1 or solving, we find w * 0 = 0. Therefore, the optimal decision boundary has the form x 2 1 = x 2 2 or x 1 + x 2 x 1 − x 2 = 0, which yields the nonlinear boundary function shown in Figure <ref type="figure">B2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Structural Diagnostics with the SVM</head><p>Suppose we have a data set where each observation consists of an n-dimensional vector x i ∈ R n , i = 1 l and an associated output y i . We use x k to represent the kth predictor variable, k ∈ 1 n . The sensitivity of the output D x with respect to the kth input x k can be determined from the partial derivative D x / x k of the decision function with respect to x k . This derivative is evaluated at the observed value of x k holding other variables x m , m = k at their observed values. The net effect of x k is then determined by averaging over respondents in each response category for each variable.</p><p>In a support vector machine, partial derivatives are easy to determine because the decision function is linear; e.g., D x = s j=1 * j y j K x x j + w * 0 . Thus, a given kernel, K, yields Equation (C1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D x</head><p>x k = s j=1 a * j y j K x x j x k (C1)</p><p>However, results vary by kernel type. We illustrate for three kernel families-identity, Gaussian, and polynomial-using two-dimensional data x = x 1 x 2 . If K is the identity transformation, results are carried out in attribute space and</p><formula xml:id="formula_33">D x x 1 = s j=1 a j y j x 1 j • x 1 + x 2 j • x 2 x 1 = s j=1 a j y j x 1 j (C2)</formula><p>which is a constant as expected. If a Gaussian kernel is used, we have the chain of reasoning shown as (C3).</p><formula xml:id="formula_34">D x x 1 = s j=1 a j y j exp − x 1 j • x 1 + x 2 j • x 2 /2 2 x 1 = s j=1 a j y j −x 1 j 2 2 exp − x 1 j • x 1 + x 2 j • x 2 2 2 (C3)</formula><p>This expression can be easily evaluated. If a polynomial kernel is used, we have Equation (C4). Similarly, we can derive sensitivity formulae for other types of kernels.</p><formula xml:id="formula_35">D x x 1 = s j=1 a j y j x 1 j • x 1 + x 2 j • x 2 + 1 d x 1 * = s j=1 a j y j x 1 j d x 1 j • x 1 + x 2 j • x 2 + 1 d−1 (C4)</formula><p>Posttraining when s, a j , and the support vectors are known, the average sensitivity of output D x with respect to out-put x k evaluated at a given input response category is calculated by integrating over respondents who choose category C, denoted l y i =C .</p><formula xml:id="formula_36">S x k = 1 l y i =C l y i =C i D x x k (C5)</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1Latitude of Acceptance Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2Capacity to Shatter 2 p Points (a) M1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3Sample Size, Capacity, and Generalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ik3 , and x ik4 are Person i Choice k's attribute data; z i is Person i's single individual characteristic. Utility 4 x ik = x ik1 x ik2 x ik1 x ik2 x ik2 z i1 x ik3 z i2 x 2 ik3 x 2 ik4 z i2 z i3 x ik1 , x ik2 , x ik3 , and x ik4 are Person i Choice k's attribute data; z i1 , z i2 , and z i3 are Person i's three individual characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ik5 , and x ik6 are Person i Choice k's attribute data; and z i are Person i's single individual characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ik5 , and x ik6 are Person i Choice k's attribute data; z i1 , z i2 , and z i3 are Person i's three individual characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 5Three Subexperiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure B2The SVM Decision Boundary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Sample Size, Capacity, and Generalization</cell><cell></cell><cell></cell></row><row><cell>Est.</cell><cell>Model</cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell cols="2">(d) Range</cell></row><row><cell>sample</cell><cell>type</cell><cell>Freq (Wins) a</cell><cell>Mean</cell><cell>Std</cell><cell>min</cell><cell>max</cell></row><row><cell>l = 10</cell><cell>L</cell><cell>85</cell><cell cols="4">0.1894 0.0475 0.1360 0.4120</cell></row><row><cell></cell><cell>Q</cell><cell>15</cell><cell cols="4">0.2388 0.0769 0.1300 0.5050</cell></row><row><cell>l = 50</cell><cell>L</cell><cell>69</cell><cell cols="4">0.1575 0.0123 0.1295 0.1970</cell></row><row><cell></cell><cell>Q</cell><cell>31</cell><cell cols="4">0.1611 0.0121 0.1355 0.2030</cell></row><row><cell>l = 100</cell><cell>L</cell><cell>66</cell><cell cols="4">0.1524 0.0080 0.1340 0.1710</cell></row><row><cell></cell><cell>Q</cell><cell>34</cell><cell cols="4">0.1547 0.0093 0.1240 0.1780</cell></row><row><cell>l = 200</cell><cell>L</cell><cell>60 5</cell><cell cols="4">0.1509 0.0096 0.1330 0.1800</cell></row><row><cell></cell><cell>Q</cell><cell>39 5</cell><cell cols="4">0.1522 0.0088 0.1310 0.1745</cell></row><row><cell>l = 400</cell><cell>L</cell><cell>47</cell><cell cols="4">0.1501 0.0088 0.1240 0.1725</cell></row><row><cell></cell><cell>Q</cell><cell>53</cell><cell cols="4">0.1501 0.0088 0.1245 0.1720</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Independent Variables in Simulation Study</figDesc><table><row><cell>Variable</cell><cell></cell><cell cols="3">Low (L) Med (M) High (H)</cell></row><row><cell>symbol</cell><cell>Variables</cell><cell>level</cell><cell>level</cell><cell>level</cell></row><row><cell>A</cell><cell>Number of product attributes</cell><cell>2</cell><cell>4</cell><cell>6</cell></row><row><cell>B</cell><cell>Estimation sample size</cell><cell>100</cell><cell>400</cell><cell>1 600</cell></row><row><cell>C</cell><cell>Error size (stochastic</cell><cell>0.5%</cell><cell></cell><cell>10%</cell></row><row><cell></cell><cell>component)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>D</cell><cell>Number of individual</cell><cell>1</cell><cell></cell><cell>3</cell></row><row><cell></cell><cell>characteristics</cell><cell></cell><cell></cell><cell></cell></row><row><cell>E</cell><cell>Number of choices</cell><cell>2</cell><cell></cell><cell>6</cell></row><row><cell>F</cell><cell>Type of error distribution</cell><cell>Normal</cell><cell></cell><cell>Gamma</cell></row><row><cell>G</cell><cell>Correlated error structure</cell><cell>No</cell><cell></cell><cell>Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Systematic Components of Utility Deterministic component of the utility function Utility</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Predictive Performance of the Support Vector Machine and Logit Model</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Support vector</cell><cell></cell><cell></cell><cell cols="2">Support vector</cell></row><row><cell cols="2">Logit model</cell><cell cols="2">machine</cell><cell cols="2">Logit model</cell><cell cols="2">machine</cell></row><row><cell>Training</cell><cell>Testing</cell><cell>Training</cell><cell>Testing</cell><cell>Training</cell><cell>Testing</cell><cell>Training</cell><cell>Testing</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 ANOVA</head><label>5</label><figDesc>Results (Variance Explained Normalized to 100%)    </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Logit</cell><cell></cell></row><row><cell>Symbol</cell><cell>Manipulated factor</cell><cell cols="3">SVMs models SVMs-logit</cell></row><row><cell></cell><cell>Main effects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>Number of product attributes</cell><cell>61 12</cell><cell>2 5 4</cell><cell>7 97</cell></row><row><cell>B</cell><cell>Sample size</cell><cell>6 60</cell><cell>10 63</cell><cell>14 94</cell></row><row><cell>C</cell><cell>Error size</cell><cell>11 44</cell><cell>0 5 6</cell><cell>0 76</cell></row><row><cell>D</cell><cell>Individual characteristics</cell><cell>0 24</cell><cell>3 45</cell><cell>3 15</cell></row><row><cell>E</cell><cell>Number of choices</cell><cell>6 36</cell><cell>52 84</cell><cell>48 05</cell></row><row><cell>F</cell><cell>Type of error distribution</cell><cell>0 23</cell><cell>0 25</cell><cell>0 27</cell></row><row><cell>G</cell><cell>Correlated error structure</cell><cell>1 65</cell><cell>0 00</cell><cell>0 10</cell></row><row><cell>Subtotal</cell><cell></cell><cell>87 64</cell><cell>70 26</cell><cell>75 24</cell></row><row><cell></cell><cell>Two-way interactions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A  *  B</cell><cell>No. of attributes × sample size</cell><cell>9 61</cell><cell>9 74</cell><cell>4 40</cell></row><row><cell>A  *  C</cell><cell>No. of attributes × error size</cell><cell>0 41</cell><cell>0 08</cell><cell>0 09</cell></row><row><cell>A  *  E</cell><cell>No. of attributes × number of choices</cell><cell>-ns-</cell><cell>-ns-</cell><cell>-ns-</cell></row><row><cell>B  *  C</cell><cell>Sample size × error size</cell><cell>-ns-</cell><cell>-ns-</cell><cell>-ns-</cell></row><row><cell>B  *  E</cell><cell>Sample size × number of choices</cell><cell>-ns-</cell><cell>-ns-</cell><cell>-ns-</cell></row><row><cell>C  *  E</cell><cell>Error size × number of choices</cell><cell>0 56</cell><cell>19 66</cell><cell>19 47</cell></row><row><cell>Subtotal</cell><cell></cell><cell>10 58</cell><cell>29 48</cell><cell>23 97</cell></row><row><cell>Error</cell><cell></cell><cell>1 78</cell><cell>0 25</cell><cell>0 79</cell></row><row><cell>Total</cell><cell></cell><cell>100</cell><cell>100</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Marketing Science 24(4), pp. 595-615, © 2005 INFORMS yields an inner product matrix, albeit between vectors in -space.K x z ≡ x • z + 1 2 =</figDesc><table /><note>n i=1</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The text has been altered slightly to emphasize marketing data, not just clickstream data, as is the focus of theBucklin et al. (2002)   passage.2  We employ several versions-conditional, nested-of the multinomial logit model and recognize the variety of forms that this</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A simple (OLS) version of the support vector machine has already been shown to outpredict a variety of "soft-computing" techniques, including artificial neural nets, k-nearest neighbor, decisiontree, Bayesian learning multilayer perceptron, and tree-augmented Bayes<ref type="bibr" target="#b88">(Viaene et al. 2002)</ref>.4  Kernel transformations are explained briefly in the main text and more thoroughly in Appendices A and B. 5 Today's newer estimation techniques using the Gibb's Sampler<ref type="bibr" target="#b2">(Allenby et al. 1995</ref><ref type="bibr" target="#b44">, Hofstede et al. 2002</ref>, simulated likelihood<ref type="bibr" target="#b48">(Kamakura et al. 2003)</ref>, hierarchical Bayes<ref type="bibr" target="#b66">(Rossi and</ref> Allenby 2003,  Andrews et al. 2002), and hybrid logit kernel models overcome this problem to some extent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">They tested predictions on a new sample (in the same time period), in a new time period (with the original sample), and in single stores (though pooled parameter estimates were derived from data aggregated over stores).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Class overlap in 2-d is irreducible error in x 1 x 2 , but not necessarily globally irreducible error because in higher dimensions</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We use a consumer choice prediction task because of the prominence of discrete-choice modeling in marketing and the widespread understanding of logit as opposed to most of the emerging models mentioned in §2.10 Toubia et al. (2004, p. 123) provide a detailed explanation of why Monte Carlo experiments are widely used and appropriate for testing model performance in consumer choice experiments of the type reported here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">All three subexperiments used the conditions: four attributes, normal, uncorrelated error at 10%. These levels correspond to those</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Although the fractional design in the main experiment is orthogonal, it is unbalanced due to mixing factors with two and three levels. Thus, raw marginal means do not correspond to weighted contrasts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Readers interested in a step-by-step derivation of Equations (A4) and (A5) are referred to<ref type="bibr" target="#b25">Cui and Curry (2003)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">For example, suppose that we have 200 10-dimensional data points. Mapping the 10-dimensional feature space with a polynomial kernel of degree 10 would result in a 184,756-dimensional feature space. A linear function in such a feature space has 184,757 parameters, and can memorize all 200 points.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The computer code and data used in the simulation studies are available upon request. Special thanks to Sharon McFarland for editorial comments on earlier drafts of this paper. The authors gratefully acknowledge the insightful comments of the reviewers and the area editor. The authors are listed in alphabetical order. Contributions were equal and synergistic.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Implementing a Support Vector Machine</head><p>This appendix reviews the fundamental derivations required to implement a support vector machine. Results fall into three primary areas, the optimal maximum margin classifier <ref type="bibr" target="#b83">(Vapnik 1979)</ref>, extensions to nonlinear decision functions using kernel transformations <ref type="bibr" target="#b12">(Boser et al. 1992)</ref>, and extensions to soft-margin classifiers <ref type="bibr" target="#b22">(Cortes and Vapnik 1995)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Promoter: An automated promotion evaluation system</title>
		<author>
			<persName><forename type="first">Magid</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Len</forename><surname>Lodish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An implemented system for improving promotion productivity using store scanner data</title>
		<author>
			<persName><forename type="first">Magid</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Len</forename><surname>Lodish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="248" to="269" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incorporating prior knowledge into the analysis of conjoint studies</title>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="1995-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distinguishing likelihoods, loss functions and heterogeneity in the evaluation of marketing models</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Diener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Markowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="59" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical Bayes versus finite mixture conjoint analysis models: A comparison of fit, prediction, and partworth recovery</title>
		<author>
			<persName><forename type="first">Rick</forename><forename type="middle">L</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><surname>Currim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2002-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning by collaborative and individual-based recommendation agents</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ariely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Lynch</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Manuel Aparicio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Psych</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1 &amp; 2</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The market for evaluations</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Avery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zeckhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Econom. Rev</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="564" to="584" />
			<date type="published" when="1999-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adaptive Control Processes: A Guided Tour</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Discrete Choice Analysis: Theory and Application to Travel Demand</title>
		<author>
			<persName><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">R</forename><surname>Moshe</surname></persName>
		</author>
		<author>
			<persName><surname>Lerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling methods for discrete choice analysis</title>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcfadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Makoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Bockenholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Bolduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatram</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vithala</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Revelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Steinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Data Mining Techniques: for Marketing, Sales, and Customer Support</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Linoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large-scale databases: The new marketing challenge</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Blattberg</surname></persName>
		</author>
		<author>
			<persName><surname>Byung-Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Ye</surname></persName>
		</author>
		<editor>Robert C. Blattberg, Rashi Glazer, John D. C. Little</editor>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Harvard Business School Press</publisher>
			<biblScope unit="page" from="173" to="203" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
	<note>The Marketing Information Revolution</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A trained algorithm for optimal margin classifier. Fifth Annual Workshop on Computational Learning Theory</title>
		<author>
			<persName><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="144" to="151" />
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Current issues and a &quot;wish list&quot; for conjoint analysis. Working paper, The Wharton School of the University of Pennsylvania</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">T</forename><surname>Bradlow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="10" />
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Judgment in Managerial Decision Making</title>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">H</forename><surname>Brazerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>3rd ed. J.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bronson</surname></persName>
		</author>
		<title level="m">Matrix Methods: An Introduction</title>
				<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press, Inc</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From decision support to decision automation: A 2020 vision</title>
		<author>
			<persName><forename type="first">Randolph</forename><forename type="middle">E</forename><surname>Bucklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D C</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="246" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Randolph</forename><forename type="middle">E</forename><surname>Bucklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Lattin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eloise</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><surname>Coupey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><surname>Mela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alan Marketing Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="595" to="615" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Choice and the Internet: From clickstream to research stream</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><surname>Steckel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="258" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SVM for histogram-based image classification</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hallner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1055" to="1064" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Turning datamining into a management science tool</title>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Giuffrida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Rourant</forename><surname>Courant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1953" />
			<publisher>Interscience Publishers, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines-and Other Kernel-Based Learning Methods</title>
		<author>
			<persName><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Applications of support vector machines in marketing: An exposition</title>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Curry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Working paper</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linear models in decisionmaking</title>
		<author>
			<persName><forename type="first">Robyn</forename><forename type="middle">M</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Corrigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="95" to="106" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smart agents: When lower search costs for quality information increase price sensitivity</title>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">R</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Kornish</surname></persName>
		</author>
		<author>
			<persName><surname>Lynch</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="56" to="71" />
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Urban Travel Demand: A Behavioral Analysis</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Domencich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcfadden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>North-Holland Publishing Co</publisher>
			<pubPlace>Amsterdam; The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">S</forename><surname>Edgington</surname></persName>
		</author>
		<title level="m">Randomization Tests</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker, Inc</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>3rd ed</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The use of nonlinear, non-compensatory models in decision making</title>
		<author>
			<persName><forename type="first">Hillel</forename><forename type="middle">J</forename><surname>Einhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Bull</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized robust conjoint estimation</title>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Boussios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Zacharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contributions to Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1950" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On bias, variance, 0/1-Loss, and the curse of dimensionality</title>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="55" to="77" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using a community of knowledge to build intelligent agents</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gershoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="1998-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A choice model with conjunctive, disjunctive, and compensatory screening rules</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Gilbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Resampling Methods: A Practical Guide to Data Analysis</title>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">I</forename><surname>Good</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A logit model of brand choice calibrated on scanner data</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Guadagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="238" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A catalog and computer program for the design and analysis of orthogonal symmetric and asymmetric fractional factorial experiments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Shapiro</surname></persName>
		</author>
		<idno>no. 66-C-165</idno>
	</analytic>
	<monogr>
		<title level="j">General Electric Research and Development Center</title>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Consumer decision making in online shopping environments: The effects of interactive decision aids</title>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Häubl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Trifts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Aspects of scientific explanation. Aspects of Scientific Explanation and Other Essays in the Philosophy of Science</title>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">G</forename><surname>Hempel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<publisher>The Free Press</publisher>
			<biblScope unit="page" from="331" to="496" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distributed representations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">D E L</forename><surname>Rumelhart</surname></persName>
			<persName><surname>Mcclelland</surname></persName>
			<persName><surname>Group</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="77" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bayesian prediction in hybrid conjoint analysis</title>
		<author>
			<persName><forename type="first">Frenkel</forename><surname>Hofstede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngchan</forename><surname>Ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Wedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="253" to="261" />
			<date type="published" when="2002-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Marketing Theory: The Philosophy of Marketing Science</title>
		<author>
			<persName><forename type="first">Shelby</forename><forename type="middle">D</forename><surname>Hunt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Richard D. Irwin, Inc</publisher>
			<pubPlace>Homewoods, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recommendation agents on the Internet</title>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Iacobucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Bodapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Interactive Marketing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2" to="11" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Maps of bounded rationality: A perspective on intuitive judgment and choice</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<ptr target="http://nobelprize.org/economics/laureates/2002" />
	</analytic>
	<monogr>
		<title level="m">Nobel Prize Lecture</title>
				<imprint>
			<date type="published" when="2002-12-08" />
			<biblScope unit="page" from="449" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-selling through database marketing: A mixed data factor analyzer for data augmentation and prediction</title>
		<author>
			<persName><forename type="first">Wagner</forename><forename type="middle">A</forename><surname>Kamakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Fernando De Rosa</surname></persName>
		</author>
		<author>
			<persName><surname>Afonso Mazzon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Res. Marketing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="65" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Consumer Behavior and Managerial Decision Making</title>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">R</forename><surname>Kardes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pairwise classification and support vector machines</title>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Krebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
			<persName><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="255" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Linear and nonlinear hidden units in artificial neural networks</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Burgess</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Under review</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bridging the marketing theory-practice gap with marketing engineering</title>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">L</forename><surname>Lilien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Rangaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerrit</forename><forename type="middle">H</forename><surname>Van Bruggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berend</forename><surname>Wierenga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bus. Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="111" to="121" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Marketing automation on the Internet</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D C</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Invitational Choice Symposium, U. C. Berkeley/Asilomar</title>
				<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Individual Choice Behavior: A Theoretical Analysis</title>
		<author>
			<persName><forename type="first">Duncan</forename><forename type="middle">R</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Generalization error and the number of hidden units in a multilayer perceptron</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Oldfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cambridge University</orgName>
		</respStmt>
	</monogr>
	<note>Working paper</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Support vector machines for dynamic reconstruction of a chaotic system</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mattern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Haykin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
			<persName><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="211" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Perceptrons: An Introduction to Computational Geometry</title>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seymour</forename><surname>Papert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dynamic conversion behavior at e-commerce sites</title>
		<author>
			<persName><forename type="first">Wendy</forename><forename type="middle">W</forename><surname>Moe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Fader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="335" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Using support vector machines for time series prediction</title>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">-</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Schökopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Kohlmorgen</surname></persName>
		</author>
		<author>
			<persName><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
			<persName><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="243" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Training support vector machines: An application to face detection</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vision Pattern Recognition</title>
				<meeting>Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
			<persName><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Probabilities for SV machines</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
				<editor>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Large margin DAGs for multiclass classification</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nello</forename><surname>Cristianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="547" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the necessity to present consumer preferences as predictions</title>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Politz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Edwards</forename><surname>Deming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="page" from="50" to="55" />
			<date type="published" when="1953-07" />
		</imprint>
	</monogr>
	<note>Reprinted in Marketing Res.</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Statistical Modeling and Analysis for Database Marketing: Effective Techniques for Mining Big Data</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Ratner</surname></persName>
		</author>
		<ptr target="http://dmstat1.com" />
		<imprint>
			<date type="published" when="2003" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Bayesian statistics and marketing</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName><surname>Allenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="304" to="328" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Decision Traps. Doubleday/Currency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Shoemaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Quasi-random simulation of discrete choice models</title>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Sándor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Train</forename><surname>Kenneth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Res. Part B</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">CoverStory-Automated news finding in marketing</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">O</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D C</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interfaces</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Kernel principle component analysis</title>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning, Advances in Kernel Methods</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
			<persName><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="327" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Shrinking the tube: A new support vector regression algorithm</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Barlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Kearns</surname></persName>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="330" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Prior knowledge in support vector kernels</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Michael I</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Kearns</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="640" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Margin distribution and soft margin</title>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
				<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
			<persName><forename type="first">C</forename><surname>Schuurmans</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Margin distribution bounds on generalization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Learning Theory, EuroColt&apos;99</title>
				<meeting>Eur. Conf. Comput. Learning Theory, EuroColt&apos;99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Further results on the margin distribution</title>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Learning Theory, COLT 99</title>
				<meeting>Conf. Comput. Learning Theory, COLT 99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="278" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Modeling purchase behavior at an e-commerce web site: A task completion approach</title>
		<author>
			<persName><forename type="first">Catarina</forename><surname>Sismeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randolph</forename><forename type="middle">E</forename><surname>Bucklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="page" from="306" to="323" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Consumer decision making at an Internet Shopbots brand still matters</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Brynjolfsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Indust. Econom. XLIX</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="558" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Support vector regression with ANOVA decomposition kernels</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">O</forename><surname>Stitson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
			<persName><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="285" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A law of comparative judgment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Thurstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Rev</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Polyhedral methods for adaptive choice-based conjoint analysis</title>
		<author>
			<persName><surname>Toubia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><forename type="middle">I</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><surname>Simester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2004-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Elimination by aspects: A theory of choice</title>
		<author>
			<persName><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Rev</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="281" to="299" />
			<date type="published" when="1972-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Estimation of Dependences Based on Empirical Data</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A note on one class of perceptrons</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Learning Theory</title>
				<meeting><address><addrLine>New York. Vapnik, Vladimir, A. Chervonenkis</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1964" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Probab. Its Appl</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Bounds on error expectation for SVM</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
			<persName><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="261" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A comparison of state-of-the-art classification techniques for expert automobile insurance claim fraud detection</title>
		<author>
			<persName><surname>Viaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Stijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Derrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><surname>Dedene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Risk Insurance</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="373" to="421" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A comparative analysis of neural networks and statistical methods for predicting consumer choice</title>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">M</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">L</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><surname>Golden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="370" to="391" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Agents to the rescue?</title>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">M</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ariely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Bellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><surname>Schkade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="300" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Support vector machines for multi-class pattern recognition</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Eur. Sympos. Artificial Neural Networks ESANN</title>
				<meeting>6th Eur. Sympos. Artificial Neural Networks ESANN<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Michel Verleysen</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="276" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Support vector density estimation</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">O</forename><surname>Stitson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
				<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
			<persName><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="293" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Marketing Management Support Systems: Principles, Tools and Implementation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wierenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Van Bruggen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Kluwer Academic Publishing</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The success of marketing management support systems</title>
		<author>
			<persName><surname>Wierenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerrit</forename><forename type="middle">H</forename><surname>Beremd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Van Bruggen</surname></persName>
		</author>
		<author>
			<persName><surname>Staelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="196" to="207" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Factorial invariance and significance</title>
		<author>
			<persName><forename type="first">Gale</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Householder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="1940" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
