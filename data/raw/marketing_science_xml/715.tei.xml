<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Market Share Constraints and the Loss Function in Choice-Based Conjoint Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-07-31">July 31, 2008.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Gilbride</surname></persName>
							<email>tgilbrid@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mendoza College of Business</orgName>
								<orgName type="institution" key="instit2">University of Notre Dame</orgName>
								<orgName type="institution" key="instit3">Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>Indiana</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lenk</surname></persName>
							<email>plenk@umich.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Ross</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><forename type="middle">D</forename><surname>Brazell</surname></persName>
							<email>jeff.brazell@themodellers.com</email>
							<affiliation key="aff2">
								<orgName type="department">The Modellers, LLC</orgName>
								<address>
									<postCode>84047</postCode>
									<settlement>Salt Lake City, Utah</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Market Share Constraints and the Loss Function in Choice-Based Conjoint Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-07-31">July 31, 2008.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.1080.0369</idno>
					<note type="submission">received December 1, 2006, and was with the authors 4 months for 2 revisions; processed by John Hauser.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian decision theory</term>
					<term>conjoint analysis</term>
					<term>constrained optimization</term>
					<term>cross-validation</term>
					<term>hierarchical Bayes</term>
					<term>loss function</term>
					<term>market share prediction</term>
					<term>penalized maximum likelihood</term>
					<term>posterior risk</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>C hoice-based conjoint analysis is a popular marketing research technique to learn about consumers' preferences and to make market share forecasts under various scenarios for product offerings. Managers expect these forecasts to be "realistic" in terms of being able to replicate market shares at some prespecified or "basecase" scenario. Frequently, there is a discrepancy between the recovered and base-case market share. This paper presents a Bayesian decision theoretic approach to incorporating base-case market shares into conjoint analysis via the loss function. Because defining the base-case scenario typically involves a variety of management decisions, we treat the market shares as constraints on what are acceptable answers, as opposed to informative prior information. Our approach seeks to minimize the adjustment of parameters by using additive factors from a normal distribution centered at 0, with a variance as small as possible, but such that the market share constraints are satisfied. We specify an appropriate loss function, and all estimates are formally derived via minimizing the posterior expected loss. We detail algorithms that provide posterior distributions of constrained and unconstrained parameters and quantities of interest. The methods are demonstrated using discrete choice models with simulated data and data from a commercial market research study. These studies indicate that the method recovers base-case market shares without systematically distorting the preference structure from the conjoint experiment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Discrete choice conjoint analysis has proven to be a useful tool for investigating consumer preferences in both applied and academic studies. In applied settings, results from a typical conjoint analysis are used by managers to decide on the most attractive combination of product features and price given the competitive offering, or anticipated changes to the competitive set. To facilitate "what-if" analysis, commercial market research firms frequently provide software packaged as a "market simulator." These simulators use the results from the conjoint study together with manager-supplied inputs on product features and pricing to produce market share forecasts. When a manager enters a "base-case scenario" consisting of a set of actual product features and prices, there is usually a discrepancy between the forecast from the simulator and the actual market share. Although there are many reasons why the forecasted and actual market shares may differ, when this occurs a manager may doubt the quality of the study and/or the reliability of the forecasts. The manager really has two goals from the conjoint study: to represent consumer preferences and to produce "realistic" forecasts. An interesting question is how analysts should use managers' "base-case" scenario expectations. This paper presents a Bayesian decision theoretic approach to incorporating base-case scenario projections into choice-based conjoint analysis via the loss function.</p><p>Forecasted market share from a conjoint study may differ from actual market share for a variety of reasons. In an early survey on the commercial use of conjoint analysis, <ref type="bibr" target="#b8">Cattin and Wittink (1982)</ref> list five difficulties in making market share predictions using conjoint analysis. These include the inherent difference between stated and revealed preferences, attributes present in the marketplace but excluded from the conjoint study, and the inability of conjoint studies to include the effect of mass communication,</p><note type="other">996</note><p>Marketing Science 27(6), pp. 995-1011, © 2008 INFORMS distribution, competitive reaction, and other effects. <ref type="bibr" target="#b31">Orme and Johnson (2006)</ref> reiterate many of these reasons in their summary of practitioners' methods of adjusting simulated market shares to match actual market shares. It is important to note that Orme and Johnson do not advocate adjusting the results from market simulators and instead argue that managers should be educated on the reasons why simulated and actual market shares may not agree. <ref type="bibr" target="#b2">Allenby et al. (2005)</ref> review additional reasons why choice experiments differ from actual market choices and outline data collection procedures and new types of models, which may improve the predictive accuracy of choice models. <ref type="bibr" target="#b12">Ding et al. (2005)</ref> propose incentive aligned conjoint experiments to improve out-ofsample predictions.</p><p>In a Bayesian analysis, the base-case market share can be modeled as part of the likelihood function, as informative prior information, or through the loss function. Some researchers have suggested that stated and revealed preference data be incorporated into a unified model to overcome problems with conjoint studies. <ref type="bibr" target="#b28">Louviere (2001)</ref> notes that there is a close correspondence between stated and revealed preferences. However, it may be necessary to calibrate either the location or scale parameter from analyses that only include stated preferences to account for the inherent differences between experimental and actual choice behavior. <ref type="bibr" target="#b30">Morikawa et al. (2002)</ref> detail statistical methods of combining stated preference and revealed preference data. When available, conjoint analysis data should be augmented with actual market place choices and the full set of data modeled. However, not all studies are amenable to this solution either for logistical reasons (i.e., market data are not available for conjoint participants) or due to the lack of available models for incorporating aggregate market share results with experimental studies. Further, there is no guarantee that such analyses will necessarily produce market share projections that match a manager's base-case scenario.</p><p>The base-case scenario and accompanying market share used by a manager will typically require several decisions. Market shares vary over time and geography due to changes in prices, promotion, advertising intensity or message change, fluctuations in distribution, changes in competitive offerings, etc. Managers must decide if the base-case scenario will reflect a very specific measure (e.g., one week, in one particular market) or an aggregated measure (e.g., annual market share for all markets served). If the latter, then the manager must choose whether "average values" of the product attributes will be used or if "representative values" will be selected. Selecting a point estimate to serve as the base-case scenario is not trivial.</p><p>Our experience suggests that managers use a combination of "known facts," aggregated, and "representative values" to arrive at the base-case scenario and accompanying market share estimates or expectations. These choices are unique to the manager and will largely be dictated by the specific decision or forecasting context.</p><p>From the analyst's perspective, the manager's basecase scenario is most appropriately viewed as a constraint placed on the forecasts provided by the market simulator. Bayesian decision theory implies that these constraints should be incorporated into the loss function. Essentially, our approach to making market share forecasts is to approximate a procedure that integrates over the regions of the posterior distribution of parameters that are consistent with the market share constraints.</p><p>We have several objectives in mind when developing the loss function approach. Most explicitly, we require that the market shares at the base case be sufficiently close to the managerially specified market shares such that the choice-based conjoint (CBC) analysis has face validity for the marketing manager. Clearly, this alone is not sufficient to identify the adjustment procedure, and one could imagine any number of methods to reach this objective. For example, one could adjust the preferences of a subset of subjects while leaving others untouched, or one could modify the coefficients for only a subset of attributes. The second objective is that instead of being heavy-handed with the adjustments, we want the final results to be as true as possible to the CBC data. The adjustment procedure that we develop perturbs all of the estimated parameters from the CBC data as little as possible to leave the preference structure of the CBC data relatively intact. The main purpose of most conjoint studies is to project beyond current market assumptions, and our method maintains fidelity of the CBC preferences structure as the assumptions move away from the base case. Our adjustment terms are additive factors with mean zero, and we specify the variance to be as small as possible while satisfying the constraints. By minimizing the adjustments to the CBC data we explicitly assume the conjoint study yields useful information; thus, the loss function approach will not turn a poorly designed or executed study into a "good" study. Finally, we separate the estimation of the CBC parameters from the process of perturbing them to match the constraints. This allows the analyst to judge the degree of adjustment and to maintain the integrity of the CBC results.</p><p>We contrast the loss function approach with several others including treating the base-case scenario as an informative prior. The first problem one faces with treating the constraints as data or prior information is establishing the correct weighting between the 997 CBC data and the base case. Because we lack revealed preference data for the subjects in the CBC data and because the base-case market shares are based on a different sample than the CBC study, the relative weight to place on the prior versus the likelihood is not identified. Should the analyst treat the base-case market shares for M products as M bits of information, or should he or she treat them as millions of transactions? In the former case, the CBC data will overwhelm the market information, and one will not recover the base-case market shares. In the later case, the CBC data, which are based on a few thousand hypothetical choices, will be overshadowed by the market data and will be less useful for projecting new product ideas and positioning. The relative weights of the CBC and market share data can be treated as a tuning parameter; however, a single study is not sufficient to identify it.</p><p>Although Bayesian analyses are perfectly amenable to incorporating subjective prior information, there is a difference between "prior information" and basecase market share restrictions on the analysis. Our view is that meeting the base-case scenario with the market simulator is an ancillary goal of the analysis. Contrast this with other instances in the marketing literature that use informative priors to reflect essential features of the model. <ref type="bibr" target="#b6">Boatwright et al. (1999)</ref> use truncated normal distributions to ensure price coefficients are negative in retail/market-level sales response models, and <ref type="bibr" target="#b1">Allenby et al. (1995)</ref> enforce an a priori "more is better" ordering on part-worths in conjoint analysis. In these cases, economic theory informed the choice of prior distributions.</p><p>The loss function approach has been implemented in the statistics literature to capture ancillary goals of the analyses; see, for instance, <ref type="bibr" target="#b27">Louis (1984)</ref> and <ref type="bibr" target="#b17">Ghosh (1992)</ref> who use the loss function to "match" the posterior distributions of parameters to certain empirical distributions of the data. As noted by <ref type="bibr" target="#b33">Shen and Louis (1998)</ref>, a strength of the Bayesian paradigm is its ability to "structure complicated models, inferential goals, and analyses" and that "methods should be linked to an inferential goal via the loss function." The loss function approach we outline matches the baseline market share forecast with minimal changes to the preferences as revealed in the CBC study.</p><p>Our solution dovetails nicely with hierarchical Bayes (HB) inference, which is commonly used to analyze conjoint studies to obtain individual-level estimates. However, constrained estimation also has a rich tradition in non-Bayesian statistics (cf. <ref type="bibr" target="#b0">Aitchison and Silvey 1958</ref><ref type="bibr" target="#b13">, Dupacova and Wets 1988</ref><ref type="bibr" target="#b26">, Liew 1976</ref><ref type="bibr" target="#b37">, Wang 1996</ref>). These papers not only solve the constrained optimization problem, but also describe the statistical properties of the solution. We illustrate the use of market share constraints in non-Bayesian models by adding them to the mathematical programming procedure of <ref type="bibr" target="#b15">Evgeniou et al. (2007)</ref>, which also produces individual-level estimates in CBC. The underlying HB and mathematical programming models are nearly the same, but the estimation method differs: Markov chain Monte Carlo (MCMC) versus constrained optimization. Consequently, the results of the two procedures are nearly identical.</p><p>The remainder of the paper is organized as follows. First we set up the discrete choice analysis, review the role of the loss function in Bayesian decision theory, and propose a specific loss function that incorporates market share constraints. This loss function minimizes changes in preferences as represented in the conjoint study while being within an acceptable range of the manager's base-case market share expectations. We then contrast the loss function with approaches using informative priors and a non-Bayesian method. Simulation and MCMC methods are detailed for conducting the analysis implied by the loss function and necessary for obtaining market share forecasts. These can be incorporated into the same computer program used to estimate the discrete choice model and produce posterior distributions of parameters with and without the market share constraint. We illustrate the proposed methods using simulated data for the multinomial logit (MNL) discrete choice model and examine in-sample and out-of-sample performance. We show the results from a commercial market research study and conclude with a summary and suggestions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Loss Function and Bayesian</head><p>Decision Theory 2.1. Discrete Choice Model and Market Share Constraints This section begins by stating the discrete choice model and defines various terms. We envision a commercial market research study where the data consists of a sample of respondents who have completed a CBC study and management has provided market share estimates at some base-case set of product attributes for some set of the brands or products in the CBC study.</p><p>The exact form of the model is not important to our loss function adjustment procedure, but to fix ideas the model set-up is as follows. N subjects evaluate J i choice occasions for subject i. Each choice occasion or task consists of M alternatives or product profiles. In the CBC design, there are p attribute levels, including brand intercepts. Subject i's latent random utility for product profile j is a given by</p><formula xml:id="formula_0">Y ij = x ij i + ij</formula><p>where i ∼ normal or extreme value</p><formula xml:id="formula_1">i = + i where i ∼ N p 0</formula><p>where the observed attributes x ij and the parameter vector i are p-vectors, and ij are error terms. Let W be the observed choices from the CBC. Our primary objective is to use the set of { i } in functions g i to obtain estimates of quantities of interest such as predicted market share. Let X o represent the matrix of product attributes in the base-case scenario and let S 1 S K represent the corresponding market shares. Let S 1 S K represent the estimated market shares using X o and { i }, e.g., g i X o . We define the discrepancy between the estimated and base-case market shares as</p><formula xml:id="formula_2">K k=1 S k − S k = (1)</formula><p>Our goal is to produce estimates of S k that are "close" to S k and reflect the information obtained in the conjoint study. We assume that in addition to X o and S 1 S K , management can provide t , a target level, or acceptable level of discrepancy between the predicted and base-case market shares. Let C represent the set of parameters used to estimate S k that satisfy the market share constraint ≤ t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Loss Functions</head><p>In decision analysis (cf. DeGroot 1970), the loss function L d quantifies the loss to the decision maker of taking action (or estimate) d when the state of nature (or parameter) is equal to . (Here is any arbitrary parameter.) <ref type="bibr" target="#b32">Savage (1954)</ref> derived Bayesian loss functions from the axioms of rational preferences and developed a decision-theoretic approach to inference. Loss functions are closely related to utility functions and therefore represent the individual circumstances of the decision context and the decision maker, and not necessarily universal objective functions. Because the state of nature is not known, the Bayesian decision maker wishes to minimize the posterior expected loss represented by</p><formula xml:id="formula_3">d = L d W d (2)</formula><p>where represents the current distribution of given the observed data W , in this case the posterior. The Bayes rule is the selection of d ∈ D, the set of allowable decisions, that minimizes d . The Bayes risk integrates (2) over the sample space for W . The value of d that minimizes (2) also minimizes the Bayes risk (cf. <ref type="bibr">Casella and Berger 1990, p. 474)</ref>. The overall risk to the decision maker is the weighted average of how likely an event is to occur and the loss to the decision maker for taking action d if it does occur.</p><p>The statistical problem of point estimation is one application of the loss function. In this case, d is the particular estimate of to report and use in additional analysis.  <ref type="formula">2</ref>) is minimized with respect to d, the resulting Bayes rule, or point estimate for is equal to the mean of the posterior distribution or</p><formula xml:id="formula_4">= W d = E W (3)</formula><p>where¯ represents the Bayes rule. In fact, the posterior mean is frequently reported in both applied and academic studies using Bayesian methods. However, Bayesian decision analysis can be applied to any loss function. For example, absolute error loss results in the Bayes rule being the median of the posterior distribution <ref type="bibr">(Berger 1985, p. 162</ref>), and 0/1 loss gives the posterior mode: the value of that maximizes the posterior density <ref type="bibr">(Berger 1985, p. 162)</ref>. The expected posterior loss at the Bayes rules quantifies estimation risk. Under squared error loss, this measure is the posterior variance of .</p><p>To enforce market share constraints on the CBC analysis, we will use a variant of the squared error loss function. The loss function is represented as</p><formula xml:id="formula_5">L g d = g B − d g B − d for B ∈ C (4)</formula><p>where B = i is the set of parameters from the CBC data; C is the constraint set defined by the market share at the base case; and d is the estimator of the functional g. The functional g can be of any form and will not generally involve X o . Here, the requirement that only coefficients that satisfy the market share constraints are used is represented as B ∈ C, which means that i ∈ C for i ∈ B. This can be seen as limiting the set of allowable decisions, d ∈ D. This loss function says that the decision maker gets no utility (e.g., the loss function is ) from any forecast if the analysis lacks face validity, where "face validity" is defined in terms of the base-case forecast being close to the manager's definition of the base-case market share. We use the loss function because "face validity" is defined by the individual decision maker and is specific to each decision context. In addition to meeting the market share constraints, we want to make sure that the changes to the preference structure are, in some sense, minimal and we want to explicitly separate the estimated { i } from the conjoint study from the constrained estimates. To do so, we introduce additive factors { i } for each subject where each i is a p-vector, and use the adjusted parameters { i + i } in computing market share forecasts. Clearly, the additive factors will not be unique: for a given set of { i } there are any number of { i } that will satisfy the market share constraints. To regularize the problem, we assume that i ∼ N p 0 −1 I p , where I p is a p × p identity matrix. Our procedure then forces each i to be as close to 0 as possible by making as large as possible; this will be used to help identify the { i }.</p><p>This choice of the normal distribution for the adjustment factors is not arbitrary. We set the mean to zero with the desire to keep the adjusted estimators as close as possible to the unadjusted estimators. We use a spherical variance structure to be indifferent about the set of attributes to adjust. We use a normal distribution because its tails decline rapidly, so that the estimate for any single attribute or subject will not be greatly modified relative to the adjustment for other parameters. In addition, the normal distribution maximizes entropy, a measure of uncertainty, given that the mean is zero and the precision is (cf. <ref type="bibr">Bernardo and Smith 1994, pp. 207-209)</ref>. In other words, given the mean and the variance, the normal density imposes less structure on the { i } than other choices for the distribution.</p><p>We illustrate the approach by considering a single parameter i . Panel A of Figure <ref type="figure">1</ref> illustrates a posterior distribution i W ; within i W there are regions, perhaps disjoint, that satisfy some exogenous constraints as indicated by the shaded regions. Given a draw r i from this posterior distribution, we generate adjustment factors { i } as shown in Panel B from a normal distribution N 0 −1 with density , where the precision parameter is one over the variance. We consider draws of i that map into the shaded region of the posterior via r i + r i and satisfy the base-case constraints. This is illustrated in Panel C of Figure <ref type="figure">1</ref>. The i is not a parameter in the likelihood function, but a device we use to satisfy an external constraint. Note that in this example, the region defined by ( i + i ) is disjoint, so functions such as E i + i where the expectation is with respect to i W , are not particularly meaningful. Next we develop the loss function that corresponds to this procedure. Let A = i be the set of additive factors; let B = i be the set of conjoint parameters; and define A + B = i + i . To obtain point estimates of market share forecasts or for any function h A + B , we use the loss function:</p><formula xml:id="formula_6">L h d = h A + B − d h A + B − d for A + B ∈ C (5)</formula><p>where C, the market share constraints at the base case X 0 , limits the set of allowable decisions, d ∈ D, to those that have face validity for the manager. The posterior expected loss, which includes these adjustment factors, becomes The requirement A + B ∈ C is subsumed into the area of integration. To obtain the Bayes rule d * , we differentiate ( <ref type="formula">6</ref>) with respect to d and set it equal to 0:</p><formula xml:id="formula_7">d = C h A + B − d h A + B − d • A B W</formula><formula xml:id="formula_8">d * = h A + B f A B W C dA dB where f A B W C = A B W P C W for A+B ∈ C (7) P C W = C A B W dA dB</formula><p>The support for the density f A B W C is C, and the normalizing constant is P C W . Thus Equation ( <ref type="formula">7</ref>) states that the Bayes rule for h A + B is the Marketing Science 27(6), pp. 995-1011, © 2008 INFORMS expected value of the function when the values of A and B are drawn from a specific density. The values of i + i drawn from f A B W C in ( <ref type="formula">7</ref>) are dependent on all other s and s. However, this specific distribution is a by-product of the loss function and arriving at an estimate for h A + B : the posterior distribution for and all other parameters in the model are obtained using standard procedures and without reference to or the market share constraints.</p><p>The Bayes rule is conditional on meeting the market share constraints and the value of ; determines "how close" the adjustment factors are to 0 and thus how much the CBC parameters are adjusted. If is too large, then values of i from N p 0 −1 I p may be too close to 0 to satisfy A + B ∈ C, the market shareconstraint. If is too small, then i will have a large variance and although we may satisfy the constraint, A + B will be poorly identified and we may inadvertently alter the individual s more than is necessary or desired.</p><p>Formally, we select the value of by minimizing the Bayes risk. In this case, the Bayes risk can be minimized by substituting the Bayes rule d * into posterior expected loss and minimizing it with respect to . This is represented by</p><formula xml:id="formula_9">r = C h A + B − d * h A + B − d * • A B W dA dB = P C W C h A + B − d * h A + B − d * • f A B W C dA dB = P C W M m=1 var h m A + B W C (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where M is the number of market shares we are estimating, and the variance is with respect to the density f . When the market share constraints are meaningful, e.g. they cannot be satisfied by the unadjusted parameters alone, increasing decreases the probability of meeting the constraint, P C W , because A will be closer to 0; this will reduce r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The var h m A + B W C</head><p>will also be reduced by large values of because i ∼ N p 0 −1 I p and large implies less variance. This suggests that we want to be as large as possible as long as the market share constraints are satisfied. We show numerical results with simulated data that reinforce this intuition. First, we present several alternative methods of incorporating market share constraints into CBC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Informative Priors and</head><p>Non-Bayesian Approaches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Informative Priors</head><p>An alternative to the loss function approach is to treat the base-case scenario as informative prior information about the CBC parameters. This section compares and contrasts the loss function and prior information approaches. The crux of the problem with using market share information in the prior distribution for the CBC parameters is getting the "correct" weight between the prior and CBC data. If too much weight is placed on the CBC data, the simulated market shares at the base case will not match the actual market shares. If too much weight is placed on the market share information, then the prior overwhelms the CBC data. One could fine-tune the weight, but that would violate setting the prior before analyzing the data.</p><p>A prior formulation that leads to the Bayes rule d * in Equation ( <ref type="formula">7</ref>) is to treat both A and B as parameters where A is the prior for A with the restriction that A is restricted to C given B. The joint posterior is</p><formula xml:id="formula_11">B A W ∝ l B W B A for A ∈ C B (9)</formula><p>Note that we use the conditioning argument C B so that values of B are drawn without reference to A or the market share constraint, and draws of A depend on B. An alternative and perhaps more traditional way to think about the market share constraint as an informative prior is to represent (9) as</p><formula xml:id="formula_12">B A W ∝ l B W B A for A + B ∈ C (10)</formula><p>Then draws of B are dependent on A and the market share constraints. This model confounds the CBC data with the analyst's desire to satisfy the inferential goals of the manager. Marginal posterior distributions of B from (9) will not match those from (10) and will not match the analysis of the CBC data without market share constraints. Market share constraints can also be incorporated through informative priors without using the additive factor A. For instance, one can treat the forecasted market shares at X o as a "parameter" in the model and place an informative prior on them. Consider the following transformations:</p><formula xml:id="formula_13">z k = ln S k S K and k = ln S k S K for k = 1 K − 1 (11) Let ∼ N K−1 z −1 I K−1 .</formula><p>The prior parameter determines the trade-off between the managerial constraints and the CBC data. Very large will result in posterior distributions "relatively" close to the base case, although how close will be a function of the data, which, as previously noted, violates the rules of Bayesian inference.</p><p>We favor the loss function approach because we feel the circumstances in most analyses are consistent with treating the base-case market shares as constraints on the allowable set of answers. As noted earlier, the decision on how to measure market share, the attribute values at the base-case scenario, and what constitutes a "close enough" answer are management decisions and not data nor conjoint issues. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A Non</head><formula xml:id="formula_14">w * i w * 0 D * = arg min w i w 0 D − 1 * N i=1 J i j=1 log e x ijm * w i M m=1 e x ijm w i + N i=1 w i − w 0 D −1 w i − w 0 (12)</formula><p>subject to D is a positive semi-definite matrix scaled to have trace 1</p><p>where w i and w 0 are vectors of length p, D is a p × p matrix, m * indicates the chosen alternative, and all other notation follows earlier definitions. Multiplying Equation ( <ref type="formula">12</ref>) by − * gives the following maximization problem:</p><formula xml:id="formula_15">N i=1 J i j=1 log e x ijm * w i M m=1 e x ijm w i − * N i=1 w i − w 0 D −1 w i − w 0</formula><p>This can be viewed as penalized maximum likelihood estimation (PMLE), where the first term is the standard log-likelihood function for logistic regression in CBC, and the second term is a penalty function. PMLE is a regularization technique for ill-posed problems, such as the estimation in a large dimensional parameter space. <ref type="bibr" target="#b21">Kimeldorf and Wahba (1970)</ref> and <ref type="bibr" target="#b19">Good and Gaskins (1971)</ref> introduced roughness penalties to control complexity in nonparametric regression and density estimation, respectively. PMLE balances fit to the data, measured by the loglikelihood function, and complexity, expressed by a penalty term. A critical feature of PMLE is judging the relative weight, expressed here by , between fit and complexity. Predictive cross-validation (CV) is commonly used to select by minimizing holdout prediction error. <ref type="bibr" target="#b35">Stone (1974)</ref> and <ref type="bibr" target="#b16">Geisser (1975)</ref> introduced CV in the statistical literature. <ref type="bibr" target="#b10">Craven and Wahba (1979)</ref>, <ref type="bibr" target="#b20">Good and Gaskins (1980)</ref>, <ref type="bibr" target="#b18">Golub et al. (1979), and</ref><ref type="bibr" target="#b36">Wahba (1983)</ref> used CV in PMLE. <ref type="bibr" target="#b14">Evgeniou et al. (2000)</ref> provide an excellent review of these procedures and their applications to vector support machines.</p><p>Subject-level parameter estimation in CBC is an ill-posed problem: the MLE does not exist or is instable when the number of individual-level observations is small relative to the number of individuallevel parameters, which frequently occurs in practice. Hierarchical Bayes methods shrink individual-level estimates to an aggregate estimate where the amount of shrinkage depends on the relative precisions and degree of heterogeneity in the data (cf. <ref type="bibr" target="#b25">Lenk et al. 1996)</ref>. Bayesian models provide an alternative regularization method in both nonparametric density estimation (cf. <ref type="bibr" target="#b22">Lenk 1988</ref><ref type="bibr" target="#b24">Lenk , 2003</ref> and regression (cf. <ref type="bibr" target="#b23">Lenk 1999)</ref>. In essence, Bayesian models incorporate the penalty term into the model formulation and replace the CV step by using prior distributions for and updating them.</p><p>A novel aspect of the <ref type="bibr" target="#b15">Evgeniou et al. (2007)</ref> application to conjoint analysis is using PMLE and CV to regularize the estimation of individual-level parameters. (Also, see Anderson and Blair 1982.) <ref type="bibr" target="#b15">Evgeniou et al. (2007)</ref> refer to Equation (12) as a "loss function," where the first term reflects fit and the second term reflects shrinkage to the mean (or complexity control). For sake of clarity, we will refer to Equation (12) as an "objective function" because we will reserve "loss function" for the Bayesian loss function. The trade-off between fit and complexity is governed by the value of * that is determined by CV.</p><p>As noted by many of the previously cited papers, PMLE has a Bayesian interpretation. Setting aside the cross-validation term and the restrictions on D, the convex optimization problem represents the solution to an analogous Bayesian problem. The Bayesian problem specifies a multinomial logit likelihood, w i ∼ N w 0 D for heterogeneity, and improper priors for w 0 and D such that the normalizing constants for the distribution of heterogeneity cancel out. Finally, the 0/1 Bayesian loss function results in point estimates that are the maximum of the posterior distribution; this would give identical results to minimizing a corresponding objective function (analogous to Problem 3.2.1). Hence, the underling models for HB and PMLE are nearly the same, and they should produce similar results, which we confirm with an empirical example in §5.3.</p><p>One of the computational advantages of the approached outlined in Problem 3.2.1 is that conditional on , w 0 , and D, the problem can be approached by solving N separate optimization problems, that is, one optimization problem for each respondent. for obtaining optimal values of all the variables in the objective function. However, market share constraints pose constraints across respondents that necessitate different solution procedures.</p><p>The structure of Problem 3.2.1 and the loss function approach of conditioning on the posterior distribution of the h s suggest an alternative, two-step problem. Given the set {w i * } from solving Problem 3.2.1, solve:</p><formula xml:id="formula_16">Problem 3.2.2 min a i 1 K k=1 S k − S k 2 + N i=1 a i a i subject to K k=1 S k − S k ≤ t</formula><p>where a i are the p vectors of adjustment factors, S k is the managerially supplied market share at the base case for brand k, and S k is the forecasted market share using the adjusted coefficients {w i + a i }.</p><p>Here, controls the trade-off between meeting the market share constraint and ensuring that the adjustment factors a i are close to 0. By adjusting so that K k=1 S k − S k ≤ t is minimally satisfied, this puts as much emphasis as possible on keeping the adjustment factors close to 0. We use squared error instead of absolute error of the market shares in the objective function because our optimization algorithm uses gradients, and absolute error is not differentiable everywhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MCMC Estimation</head><p>Because B is independent of A and relies only on the CBC data, standard algorithms for estimating hierarchical Bayesian conjoint models can be modified to obtain samples of B + A to use in estimating</p><formula xml:id="formula_17">E h A + B W C</formula><p>. Alternatively, current programs can be used to obtain samples of B, and in a separate program A can be sampled such that A + B ∈ C. Wellknown sampling and MCMC methods are used to obtain draws from f A B W C from Equation ( <ref type="formula">7</ref>) with the challenge that the market share constraints create dependencies among subject-level parameters.</p><p>The generation of the adjustment factors could either be performed inline with the analysis of the CBC model or it could be performed offline and after the MCMC for the CBC data. The inline MCMC algorithm goes through the following steps on each iteration:</p><p>Step 1. For i = 1 N ; (a) Draw i W X : Use standard methods for either probit or MNL models;</p><p>(b) Draw i i , { −i }, { −i }, X o , , t : Detailed in the appendix;</p><p>Step 2. i : Standard conjugate set-up;</p><p>Step 3. i : Standard conjugate set-up. For the probit model, steps are added for data augmentation and drawing the error-covariance matrix; see <ref type="bibr" target="#b29">McCulloch and Rossi (1994)</ref>. In Step 1(b), the notations { −i } and { −i } indicate the sets of parameters for all respondents other than i. Thus the draw of for person i is dependent on the current value of and for all other respondents. The offline algorithm treats the random draws { g i }, for subject i and MCMC iteration g, from the MCMC algorithm as input and draws a separate g i , as detailed in the appendix, from</p><formula xml:id="formula_18">i given g i is in C g i .</formula><p>A random walk Metropolis-Hastings <ref type="bibr" target="#b9">(Chib and Greenberg 1995)</ref> or a weighted bootstrap <ref type="bibr" target="#b34">(Smith and Gelfand 1992</ref>) is used to draw i for each i on each iteration of the algorithm. The chain converges to the stationary distribution implied by the posterior distribution f A B W C from the loss function. Note that the algorithm naturally produces draws of both B and B + A, whether performed inline or offline; this makes comparison of analyses with and without the market share constraints straightforward. As in other simulation-based methods, point estimates of E h A + B W C are obtained by computing h A + B for each draw of A + B and averaging over the draws.</p><p>There are several other practical implementation issues that are detailed with suggestions in the Technical Appendix, found at http://mktsci.pubs.informs. org. Primary among these is a method for increasing in the distribution for ∼ N p 0 −1 I p as the MCMC chain progresses, subject to A + B ∈ C. In the next section we show results from simulated data sets including an importance sampling scheme used to estimate the value of the posterior expected loss for different values of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">In-Sample Results</head><p>It is not surprising that one can use additive factors to adjust the CBC parameter estimates to recover basecase market shares. The purpose of this simulation study is to quantify the distortion of the CBC preference structure by using the proposed method. The simulation shows that relatively small additive factors are needed to meet the constraints in an MNL model. Additional results with a correlated probit model are available from the first author. The simulated data set consists of 300 respondents, 12 choice sets per respondent, with each choice set consisting of four brands. Each brand in each choice set was described by two randomly generated continuous covariates and one binary covariate intended to mimic a discrete product attribute.</p><p>A standard MNL model set-up is used. Choices are restricted to one of the four alternatives and wellknown algorithms are used to estimate the hierarchical Bayes MNL model. A base set of attributes X o was randomly generated and the market share using the actual set of i was measured. The market share constraints, e.g., S 1 S K , were then set arbitrarily, but different than the market share using the true values of the parameters. The model is identified by dropping one of the brand intercepts (Brand D). The MCMC chain was run for 30,000 iterations with a sample of every 10th from the last 10,000 used to describe posterior moments.</p><p>Table <ref type="table" target="#tab_1">1</ref> presents selected results from the simulations with the target level of discrepancy t set at 0.10, 0.05, and 0.02. In all instances the algorithm was able   <ref type="figure">----------------------------------------</ref>  <ref type="figure">----------------------------------------</ref> to increase , the precision of the additive factors A, to the maximum value of 100,000 (variance = −1 = 0 00001) while meeting the market share constraints. The posterior mean of is from the CBC data and is close to the true . If one were to average the posterior means of i (not reported in the table), they would be close to the posterior mean of . Selected individuallevel histograms of i + i were inspected and it does not appear that f A B W C is disjoint for these data sets. We therefore present the empirical average of + averaged across individuals and draws from the MCMC chain. These are provided as a point of comparison to the posterior mean of from the distribution of heterogeneity. Note that brand intercept parameters are adjusted in the expected direction given the differences between the actual and constrained market share. For example, the market share constraint 0.15 for brand C at X o is greater than the market share 0.05 using only the CBC data. Consequently, the average of + for brand C is greater than the posterior mean of for brand C, thus increasing the preference for brand C to satisfy the market share constraints. Figure <ref type="figure">2</ref> plots the individual-level average i + i compared to the individual-level average¯ i for selected parameters when t = 0 02. The top panel is the partworth for Brand C. The loss function approach tends to increase the preference for subjects with moderate partworths, while leaving subjects with extreme partworths untouched. The plots show that individual-level parameters are adjusted differently, but that adjustments are generally small and directionally consistent.</p><p>Taken together, these results suggest that the loss function approach is capable of meeting the market share constraints with minimal influence on the average preference structure from just the CBC data. However, the exact amount of change needed to meet the market share constraints will depend on each data set and the discrepancy with the base-case market share. Note also that the average values are not used in market share simulations; they would not necessarily match the base-case scenario. The market share constraints are met by using the additive factors A to coordinate the draws of B + A in each iteration of the MCMC chain.</p><p>There is a trade-off between the target level of discrepancy t and the size of the adjustment factors i . As t gets smaller (e.g., recovered market shares must be closer to the managerial base case), the adjustment factors tend to be larger. This can be seen in Table <ref type="table" target="#tab_1">1</ref> by comparing the columns "Posterior " to "Posterior average ( + )" across the different levels of t , or more directly by looking at the value of the mean absolute difference (MAD) between the two columns. At t = 0 10 the MAD is 0.189 as compared to 0.230 at t = 0 02.  </p><formula xml:id="formula_19">-3 -2 -1 1 2 3 -3 -2 -1 1 2 3 -3 -2 -1 1 2 3 -3 -2 -1 1 2 3 β i3 vs. β i3 + α i3 β i6 vs. β i6 + α i6 0 0 0 0</formula><p>The posterior expected loss was investigated for different values of with t = 0 02. Recall that the value of that minimizes the posterior expected loss also minimizes the Bayes risk. Using Equation ( <ref type="formula">7</ref>), the natural log of the posterior expected loss was calculated for the function h B + A that estimates the market share at the base-case scenario X o . Draws of B + A from the MCMC sampler were used together with an importance sampling algorithm to estimate the function along a grid of values for , presented as −1 in Table <ref type="table" target="#tab_3">2</ref>. Full details on the importance sampling algorithm are available from the authors. The posterior expected loss is dominated by ln P C W and because the market share constraints are binding, the smallest value of −1 minimizes the function. The lower half of Table <ref type="table" target="#tab_3">2</ref> shows additional quantities calculated using the importance sampler. As expected, the value of E N i−1 i i X C , which measures the dispersion of the adjustment factors from 0, reaches its minimum value at the minimum value of −1 , but the improvement is marginal beyond −1 = 0 005. The values of d * equal to E h A + B W C were the  to estimate E h A + B W C . Although it is possible to investigate the value of that minimizes the posterior expected loss, the practical benefits of doing so are unclear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Out-of-Sample Results</head><p>Conjoint is frequently used to project market shares when attributes are different than existing product offerings. Here we want to know if imposing market share constraints results in "poor" predictions as compared to the unconstrained analysis. This section investigates the effect of the market share constraints on market share simulators when the attributes are "near" and "far" from the base-case scenario. The results show that the additive factors do not greatly impact market shares for scenarios other than the base case.</p><p>The conjoint design was adapted from an actual study used in a commercial market research application. Details of the simulations are provided in Table <ref type="table" target="#tab_4">3</ref> and a full report is available from the first author. The simulation involved 300 "respondents" and 10 brands described by three binary attributes, three continuous attributes, and nine brand-specific dummies. The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distortions</head><p>• "Additive"-for each respondent, randomly select up to 5 coefficients, add random term from U(1, 3) distribution • "Scale"-for each respondent, divide all coefficients by random term from a U(0.50, 0.75) distribution</p><p>• "Missing Attributes"-include one binary and one continuous attribute in "base-Case" and "out-of-sample" choice designs that are not included in the conjoint data Base case • All 10 brands included in base case • One binary attribute set = 0 for all brands; represents "new to market" feature • All other attribute levels set at random Out-of-sample prediction tasks "Near" the base case "Far" from the base case • 30 new choice sets</p><p>• 30 new choice sets • Each choice set includes all 10 brands</p><p>• Each choice set includes all 10 brands • Randomly add "new" feature to some brands and randomly adjust price +/−50%</p><p>• All attribute levels randomly set for all brands and attributes (including "missing attributes") • All other attributes the same as base case (including "missing attributes")</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulations</head><p>• Conjoint choice data generated using the "distorted" coefficients • Market share constraints and prediction results measured for: All 10 brands; 5 "high" market share brands; 5 "low" market share brands; 2 arbitrary brands • Prediction accuracy measured using p where the "known" market share is estimated using the "undistorted" coefficients and "missing attributes"</p><p>choice data consisted of 10 choice sets per respondents, where 6 of the 10 brands were included in each choice set. We introduced several distortions between the conjoint data and the observed market shares. These represent possible reasons why forecasted and base-case or out-of-sample predictions do not match.</p><p>The "additive" distortion assumes that consumers are much more sensitive to some attributes in the conjoint exercise than they are when making actual choices.</p><p>The "scale" distortion assumes that there is more variance in actual market-place choices than revealed in the conjoint data; that is, our consumers are more consistent when making hypothetical choices. Finally, we assumed that there were attributes present in the market that were not included in the conjoint design. This condition is labeled "missing attributes." Design matrices were generated for a "base case" and for a relatively large number of out-of-sample choice sets. The out-of-sample choice sets represent the possible "what-if" scenarios of interest to managers. The base case consisted of all ten brands. Attribute levels were chosen randomly 2 except for one binary attribute that was set to 0 for all alternatives; this binary attribute represents a "new to the market" attribute. Thirty out-of-sample choice sets were generated to minimize the chance that the predictive results were being driven by one or two wellchosen (or poorly chosen) choice sets. Two separate predictive exercises were considered. "Near" the base case was intended to mimic marginal changes to the current product configuration: add the new attribute to randomly selected alternatives and change price +/−50%. In the "far" condition all attributes were randomly generated and is intended to encompass a wider array of new product configurations.</p><p>Brand managers may not have market share information on all brands and may be interested in out-of-sample predictions for only their own products and/or a subset of competitors. To investigate this situation, market share constraints were applied to different subsets of brands: all 10 brands, the 5 "high" market share brands, the 5 "low" market share brands, and 2 arbitrary brands. Separate results were estimated for each of the different market share constraints. The target level of for the base case was set to 0.02; the lowest level of −1 was set at 0.00001. The targeted level of and the lowest value of −1 were reached for all the different types of distortions and different market share constraints.</p><p>Out-of-sample predictions were calculated by integrating over a sample of 1,000 observations from the posterior distribution for each respondent, with constraints and without, for each of the 30 choice  Notes. Numbers in bold indicate that there is less than 5% posterior probability that the two methods produce identical estimates; i.e., bold numbers indicate a "significant" difference between the adjusted and unadjusted estimates. Out-of-sample predictions based on 30 choice sets with 10 brands in each choice set.</p><p>sets. The discrepancy between the predicted market share and the actual market share was measured using p from Equation ( <ref type="formula">1</ref>), again for the constrained and unconstrained model. p was calculated for each choice set and then averaged across the 30 out-ofsample choice sets included in each simulation. In the simulations, the "additive" and "scale" distortions are combined and presented in one set of results, and these are combined with the "missing attributes" distortion in a separate analysis. The results are contained in Table <ref type="table" target="#tab_6">4</ref>. The chart shows the relative performance of the loss function approach compared to the standard method. Positive numbers represent the percentage improvement in p , the absolute error between the actual and predicted market shares, for the loss function approach, whereas negative numbers represent a decrease in predictive accuracy. Numbers in bold indicate that the 95% highest posterior density of the mean difference in between the two measures does not include 0; i.e., the results are "statistically significant."</p><p>In the area around the base case, the loss function approach produces significantly more accurate predictions than the standard approach. However, the changes induced by the loss function do not result in significantly worse predictions even over a broad range of out-of-sample data. 3 Thus, the more accurate localized predictions do not come at the expense of less accurate predictions in the range of the attribute levels of interest to brand managers. In the "far" condition, the adjusted and unadjusted coefficients produced very similar market share forecasts. The only substantive difference in the predictive accuracy in the "far" condition was for the "scale" and "additive" distortions for "all 10 brands": the adjusted coefficients improved predictive accuracy by 23% over the unadjusted coefficients. However, for this condition, the mean absolute difference in market share forecasts between the two methods was only 1.35 percentage points across all brands in all choice sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">PMLE Results</head><p>In this section we investigate the mathematical programming approaches outlined in §3.2. Because published studies using mathematical programming techniques have not incorporated market share constraints, we restrict ourselves to an illustrative example. We anticipate future research will result in better and more robust mathematical programming techniques. Nonetheless, our results suggest that the mathematical programming approach is feasible.</p><p>For this illustration we return to the simulated data set used in §5.1. These data consist of 300 respondents, 12 choice sets per respondents, with four alternatives per choice set. Covariates consisted of binary brand indicators (three), two continuous covariates, and a binary covariate. We used the procedure detailed by <ref type="bibr" target="#b15">Evgeniou et al. (2007)</ref> to solve Problem 3.2.1 and obtain individual-level coefficients {w * i }. <ref type="bibr">4</ref> Table <ref type="table" target="#tab_7">5</ref> contains results. The average values of {w * i } are somewhat smaller but are comparable to the actual and the posterior estimates of from the Bayesian approach in Table <ref type="table" target="#tab_1">1</ref>. We note that using {w * i } we obtained a market share forecast at X o very close to the actual result using the true { i }.</p><p>Market share constraints were incorporated via solving Problem 3.2.2 with the squared error objective function. We used a standard numeric optimization routine. <ref type="bibr">5</ref> To obtain {w * i } we (repeatedly) solved 300 problems with six coefficients; by comparison, to obtain {w * i + a * } we (repeatedly) solved one problem with 300 × 6 = 1 800 coefficients. The algorithm starts with a relatively large , obtains a solution, decreases , and uses the values from the previous solution as a starting guess to obtain a new solution.</p><p>is decreased until the market share constraint is minimally satisfied.</p><p>We find that using mathematical programming with the squared error objective function is a viable   <ref type="figure">----------------------------------------</ref>  <ref type="figure">----------------------------------------</ref> approach. Table <ref type="table" target="#tab_7">5</ref> shows that we were able to obtain solutions for t = 0 10, t = 0 05, and t = 0 02. Consistent with the Bayesian results in Table <ref type="table" target="#tab_1">1</ref>, the mean absolute difference between the adjusted and unadjusted coefficients increases as t decreases. Strictly speaking, the MAD and adjusted coefficients are not comparable between the Bayesian and the mathematical programming approach because the adjustments are made to different bases: point estimates of {w * i } versus the posterior distribution B W . However, we note that the average adjusted coefficients w * i + a * are comparable to the posterior average of i + i from the Bayes solution but that the MAD is smaller in the mathematical programming approach. This may be a result of directly minimizing a i a i in the objective function 3.2.2; the loss function penalizes large i indirectly through the posterior expected loss (8) and the parameter .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Empirical Example</head><p>This section presents the results of a commercial market research study that involved CBC and managerially supplied market share constraints. Due to the proprietary nature of the data set, the specific product and the attributes have been disguised. The product category involved a durable consumer electronic device that is typically used in conjunction with another consumer durable. The product is currently available in the market, but management was interested in measuring demand for products that included many new features that were recently developed.</p><p>The CBC study included 425 respondents who each provided dual response data on 15 choice sets. Each choice set consisted of three alternatives that were uniquely described by 20 binary attributes and the price. As noted above, brand name was not an element in the design matrix. Price was entered as the natural log of price in the response function. Respondents were asked to indicate which one of the three alternatives they preferred, and then in a follow-up question, whether they would actually purchase the alternative if it were available in the market. An uncorrelated, dual response probit model was used to represent the likelihood function with a standard hierarchical structure to represent heterogeneity; standard conjugate, but noninformative priors were used to complete the hierarchy.</p><p>The base-case scenario provided by the study sponsors did not include market share for competing brands. Because of the nature of the product category, management was only able to provide information on the proportion of customers who chose a representative base product, versus the "none" option. The base product was described by the attributes it included and its price. Management provided the choice share for two a priori market segments that were identified by socio-demographic variables. These variables were also available from the CBC study participants. Although the loss function approach was developed assuming that competing brands would make up the base-case scenario, it is straightforward to adopt it to a situation involving a "buy/no buy" choice set with different market segments.</p><p>The loss function approach was used to obtain draws from the posterior distributions of B W and f A B W C with t = 0 01. A single analysis that produced draws from both distributions was performed. The algorithm was run for 20,000 iterations. The target t was met at about iteration 3,000 and −1 met its prespecified minimum of 0.00001  at about iteration 4,000. A sample of every 10th from the last 10,000 iterations was used to compute summary statistics. Selected individual-level histograms of i + i were inspected and it does not appear that f A B W C is disjoint for this data set. Table <ref type="table" target="#tab_9">6</ref> contains the summary statistics for the constrained and unconstrained parameters including the standard deviation for i from the distribution of heterogeneity √ pp , compared to the standard deviation + pp from the empirical distribution of ( i + i ). For attributes included in the "base-case product profile," the constrained estimates all increased in importance; for attributes not included in the "base-case product profile," parameter values generally decreased (see below for explanation). The constrained parameters exhibited somewhat larger measures of heterogeneity.</p><p>The top section of Table <ref type="table" target="#tab_10">7</ref> shows the managerial base-case market shares and estimated market shares using { i } and { i + i }. All estimates are based on a sample of 1,000 draws from the individuallevel posterior distribution of i or i + i . Table <ref type="table" target="#tab_10">7</ref> shows that there was a sizable difference between the estimate using the unconstrained parameters and the managerial base case. Using the unconstrained parameters, respondents were estimated to be much less likely to choose the base product. For market segment #1, the base case was 18.9% choosing the representative product versus a managerial expectation of 43.6%. To increase the attractiveness of the base product over the "none" option, the relative attractiveness of attributes included in the base-case product are increased, thus increasing their coefficients in Table <ref type="table" target="#tab_9">6</ref>. The loss function approach was able to match the base-case choice share to within the prespecified level of accuracy despite the relatively large discrepancy between it and the forecasts using the unadjusted parameters.</p><p>The bottom panel of Table <ref type="table" target="#tab_10">7</ref> shows the actual and estimated choice shares averaged across choice sets. Recall that in this study, respondents not only indicated which of the three options they preferred, but With the "none" option Option 1 (%) 11 7 1 2 3 1 6 5 Option 2 (%) 14 1 1 3 7 1 6 6 Option 3 (%) 17 2 1 6 9 1 9 9 None (%) 57 1 5 7 1 4 7 0</p><p>Note. Posterior mean displayed. 1009 also whether or not they would actually purchase it, e.g., a dual response format. This allows us to estimate and compare choice shares with and without the "none" option. We can see compared to the raw data and estimates based on { hi }, when "none" is included in the choice set the estimates based on { hi + hi } decrease the relative frequency of "none" and increase the relative frequency of the other options. This is consistent with the market share constraint. However, when the choice set is restricted to the relative preference between the three options, the estimated choice shares are all reasonably close. This is another example of the localized nature of the loss function approach. Figure <ref type="figure">3</ref> plots the individual-level average i + i compared to the individual-level average¯ i for selected coefficients. Coefficient #3 was selected because the difference between the posterior mean of = 1 008 and the posterior mean of + = 1 176 from Table <ref type="table" target="#tab_7">5</ref> was about average. Coefficient #5 was selected because it had the largest difference, = 0 266 and + = 0 654. Table <ref type="table" target="#tab_7">5</ref> and Figure <ref type="figure">3</ref> show that relatively small changes in the i s were sufficient to </p><formula xml:id="formula_20">-4 -4 -3 -3 -2 -2 -1 -1 1 2 3 4 0 0 1 2 3 4 -4 -4 -3 -3 -2 -2 -1 1 2 3 4 -1 β i3 vs. β i3 + α i3 β i5 vs. β i5 + α i5</formula><p>meet the choice share constraint. Although the loss function approach is designed to minimize changes to the unconstrained CBC estimates, the exact amount of change needed to individual-level parameters will depend on factors such as the number of attributes and the discrepancy between the data and the base case. Because the method produces estimates of both B and B + A, the analysis and changes necessary to meet the market share constraints are completely transparent to both analysts and decision makers.</p><p>This example shows that the loss function approach is able to meet market share constraints with relatively modest adjustments to individual-level parameters using real data, even when there is a big difference between the base case and unconstrained estimates. Further, the method is sufficiently flexible to adapt to "base-case scenarios" that differ from the "K-brands" set-up used earlier to define the loss function approach. The algorithm performed as expected, but we anticipate additional research will provide areas for improving its implementation and for further defining the proper boundaries between the likelihood function, the loss function, and prior information in the Bayesian paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we take the perspective of the marketing research analyst conducting a CBC study who is presented information from the client on current market conditions, which are external to the CBC study. In addition to representing the preferences of the study participants, the client expects the analysis will be able to replicate "base-case" market results. How should the analyst incorporate this information into his/her analysis? This paper presents a Bayesian decision theoretic approach that uses the loss function to capture the various goals of the decision maker. In addition, we illustrate a similar procedure for non-Bayesian models. The Bayesian and non-Bayesian approaches provide similar results because the underlying models are similar: the main differences are the numerical procedures.</p><p>Formally, we introduce additive factors into the loss function that map draws from the posterior distribution into the set that satisfies the market share constraints at the base-case scenario. The set of additive factors are variables from a normal distribution. These are variables in the loss function and not parameters in the likelihood or priors for parameters in the model. We derive the Bayes rule for any function of the model parameters and additive factors via the posterior expected loss and provide an algorithm for simulating the additive factors to satisfy the market share constraints. Conceptually, the loss function approach approximates a procedure that integrates Marketing Science 27(6), pp. 995-1011, © 2008 INFORMS over those regions of the posterior that are consistent with the market share constraints.</p><p>Simulated and real data sets are used to illustrate the proposed approach. These empirical studies demonstrate that the average representation of preferences changes relatively little using the loss function approach. The use of a normal distribution with mean 0 for the adjustment factors minimizes the adjustments at the individual level, while recovering the managerially supplied base-case market shares without distorting the preference structure from the CBC experiment. The algorithm tends to seek opportunities where a small adjustment in the estimated parameters will change choices for the base-case scenario. When predicting scenarios far from the base case, which is often the objective of conjoint studies, the adjusted and unadjusted market shares are similar.</p><p>This paper highlights the need for additional research in a number of areas. First, by adjusting the CBC results as little as possible we assume they accurately capture preferences under the new market conditions represented by the new products, attributes, attribute levels, etc., in the conjoint study. The basecase market share constraint serves to anchor any changes in preferences to a point considered to have face validity by management. If the discrepancy between revealed and stated preferences is purely a methodological artifact, then one would want to model this systematically. Second, and along these same lines, the loss function approach allows all of the coefficients to be adjusted. However, if one assumed that only brand awareness or distribution was responsible for the discrepancy, it is easy to set up the loss function such that only brand intercepts are adjusted; or only the price coefficient; or all coefficients are adjusted uniformly representing a change in scale; or that only certain respondent's coefficients are adjusted. Additional empirical and/or theoretical evidence may suggest that only a subset of coefficients should be adjusted via the loss function or provide insight on how to systematically model any adjustments.</p><p>In this research we assumed only one "base-case" observation was available consistent with our experience in applied settings. However, it is interesting to consider a situation in which multiple base-case observations may be available. Multiple base cases in the loss function should improve prediction. However, the more base cases that are added to the loss function, the greater the possible deviance from the CBC preference structure, in which case a more elaborate adjustment scheme than the one described in this paper may be required. For example, it may be necessary to introduce multiple adjustment factors for different base cases. We leave this topic for future research.</p><p>Finally, this paper proposed a loss function that linked the goals of the decision maker to the statistical analysis of CBC data; different loss functions for this situation are possible and should be explored. Although Bayesian methods have been used in marketing to produce posterior distributions of quantities of interest, such as parameters, expected profit, expected market share, etc., there is little published research that completes the Bayesian decision theoretic approach and formally incorporates the loss function facing the decision maker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. Drawing i</head><p>The following steps detail how to draw i i , { −i }, { −i }, X o , , t . Recall that A + B must satisfy A + B ∈ C or S = 1, the market share constraint in the loss function. In this appendix we will use the indicator function S = 1 to indicate when the constraint is satisfied and S = 0 when it is not. For each individual on each iteration, i is drawn from i W ) and that draw is used in (6) to draw i from f A B W C</p><p>. Let o i represent the draw of i from the previous iteration.</p><p>Step does not satisfy S , then the numerator is 0 and the old value of i is retained.</p><p>Step 3. If S o i = 0, then a weighted bootstrap is used to draw a new value of i . A challenge in sampling i from f A B W C is satisfying the market share constraint. With an appropriately selected proposal density, the weighted bootstrap facilitates this process. Let </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A common-loss function is squared error loss represented as L d = − d 2 for a scalar parameter and L d = − d − d for a vector of parameters. When a squared error loss function is used and (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1Mapping into Constrained Area of the Posterior Distribution Panel A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-Bayesian Approach Evgeniou et al. (2007) proposed a mathematical programming approach to individual-level estimation in CBC. 1 They use the following convex optimization program to obtain individual-level coefficients: Problem 3.2.1 * = arg min Cross-Validation and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>−1 025 (0.09) (4) −0 757 (0.06) (5) 0 785 (0.07) (5) 0 806 (0.03) (6) 1 518 (0.09) (6) 1 062 (0.05) −1 = 0 00001 MAD = 0 230 Notes. MS, market share. X o , "Base-case" design matrix. Posterior mean and (standard deviation) displayed. MAD, mean absolute difference between posterior and posterior average ( + ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 2 Simulated MNL Model: Posterior Mean of Individual-Level Parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>d</head><label></label><figDesc>* Brand A d * Brand B d * Brand C d * Brand D the third decimal place for all the different values of −1 .Because the posterior expected loss and hence the Bayes risk involves var[h m A + B ], then the minimizing value of will be dependent on the form of h A + B , or for purposes of market forecasts, the values of attributes used in the market simulator. A dogmatic Bayesian will determine the value of that minimizes the posterior expected loss for each different form of h A + B and different values of attributes. However, the above analysis suggests a more pragmatic approach of conditioning on the value of . When adjustment factors are needed, set as large as possible and use draws of h A + B from f A B W C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>, market share. MAD, mean absolute difference between adjusted and unadjusted coefficients. X o , "Base-case" design matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 3 Consumer Electronics Study: Posterior Mean of Individual-Level Parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>i , { −i }, { −i }, and X o ; (then a random walk Metropolis-Hastings (M-H) step is used to draw i . Form a candidate or new i as n i = o i + , where is drawn from N p 0 zI p and z is a scalar chosen to ensure a 50% rejection rate for the M-H step. Note that may be used instead of I p . The distribution f to Step 1. Note that if the new n i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>−i } and { −i }, n i + will satisfy the market share constraint.(i) For r = 1, to R draw p . The target density implied by (6) is f ∝ S and the proposal density for the weighted bootstrap is g ∼ N −1 I p . (ii) For each draw r, calculate the weight w r : , and Brazell: Market Share Constraints and the Loss Function in Choice-Based Conjoint AnalysisMarketing Science 27(6), pp. 995-1011, Note that if the market share constraint is not satisfied, w r = 0 and that value of r i cannot be selected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Simulation Results for MNL Bayesian Approach</cell><cell></cell></row><row><cell></cell><cell>MS at X o</cell><cell>MS constraint</cell><cell></cell></row><row><cell>Brand</cell><cell>using { i }</cell><cell>a t X o</cell><cell>Actual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 Importance</head><label>2</label><figDesc></figDesc><table /><note>Sampling Results for MNL Model: Simulated Data</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Out-of-Sample Predictions: Simulation DetailsConjoint data• 10 brands, 3 binary attributes, 3 continuous attributes (one representing price)• 10 choice sets per respondent • Each experimental choice set contained 6 brands • 300 "respondents"</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Out-of-Sample Prediction Results; Percentage Improvement in Absolute Error of the Loss Function Adjustments Compared to Standard Market Shares from Conjoint Simulators</figDesc><table><row><cell></cell><cell cols="2">Type of distortion</cell></row><row><cell></cell><cell></cell><cell>"Missing attributes"</cell></row><row><cell></cell><cell>"Scale"</cell><cell>"Scale"</cell></row><row><cell></cell><cell>"Additive" (%)</cell><cell>"Additive" (%)</cell></row><row><cell>"Near" the base case</cell><cell></cell><cell></cell></row><row><cell>All 10 brands</cell><cell>52</cell><cell>69</cell></row><row><cell>5 "high" mkt share brands</cell><cell>49</cell><cell>76</cell></row><row><cell>5 "low" mkt share brands</cell><cell>47</cell><cell>40</cell></row><row><cell>2 brands</cell><cell>49</cell><cell>20</cell></row><row><cell>"Far" from the base case</cell><cell></cell><cell></cell></row><row><cell>All 10 brands</cell><cell>23</cell><cell>7</cell></row><row><cell>5 "high" mkt share brands</cell><cell>9</cell><cell>6</cell></row><row><cell>5 "low" mkt share brands</cell><cell>19</cell><cell>−3</cell></row><row><cell>2 brands</cell><cell>3</cell><cell>−2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Simulation Results for MNL: Mathematical Programming</cell></row><row><cell></cell><cell>Approach</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MS at X o</cell><cell>MS at X o</cell><cell>MS constraint</cell><cell></cell></row><row><cell>Brand</cell><cell>using i }</cell><cell>using {w  *  i }</cell><cell>a tX o</cell><cell>Actual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>Selected Summary Statistics: Consumer Electronics Study</figDesc><table><row><cell>Posterior</cell><cell>Attribute included</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="5">Choice Share Results: Consumer Electronics Study</cell></row><row><cell></cell><cell cols="2">Base-case</cell><cell>Managerial</cell><cell>Base-case</cell></row><row><cell></cell><cell cols="2">choice share</cell><cell>base-case</cell><cell>choice share</cell></row><row><cell></cell><cell cols="2">using { i }</cell><cell cols="2">choice share using { i + i }</cell></row><row><cell>Market segment #1 (%)</cell><cell>18 9</cell><cell></cell><cell>43 6</cell><cell>43 6</cell></row><row><cell>Market segment #2 (%)</cell><cell>23 1</cell><cell></cell><cell>55 6</cell><cell>55 5</cell></row><row><cell cols="2">Compared to managerial</cell><cell></cell><cell></cell></row><row><cell>base case</cell><cell cols="2">0 573</cell><cell></cell><cell>0 001</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Estimated</cell></row><row><cell></cell><cell></cell><cell cols="2">Estimated choice</cell><cell>choice share</cell></row><row><cell cols="2">Raw data</cell><cell cols="2">share using { i }</cell><cell>using { i + i }</cell></row><row><cell>Option 1 (%)</cell><cell>29 1</cell><cell></cell><cell>3 0 0</cell><cell>3 1 5</cell></row><row><cell>Option 2 (%)</cell><cell>33 1</cell><cell></cell><cell>3 1 8</cell><cell>3 1 7</cell></row><row><cell>Option 3 (%)</cell><cell>37 8</cell><cell></cell><cell>3 8 2</cell><cell>3 6 8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Gilbride,Lenk, and Brazell: Market Share Constraints and the Loss Function in Choice-Based Conjoint Analysis    </figDesc><table><row><cell>Marketing Science 27(6), pp. 995-1011, © 2008 INFORMS</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Gilbride, Brazell: Market Share Constraints and the Loss Function in Choice-Based Conjoint Analysis   Marketing Science 27(6), pp. 995-1011, © 2008 INFORMS   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We thank the associate editor for drawing our attention to this approach.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Additional simulations showed that the results were not sensitive to the level of the attributes in the base case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We used "accurate" market share constraints. An experiment with inaccurate market share constraints resulted in less accurate predictions near the base case, but no difference in the broader range of the out-of-sample data. This is consistent with the results in Table4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To solve the optimization problem for each respondent, we used a standard IMSL routine that uses a quasi-Newton method with finite-difference gradient. The same routine was used in the crossvalidation and final steps.5  Again, we used an IMSL routine that uses a quasi-Newton method with finite-difference gradient.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum-likelihood estimation of parameters subject to restraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aitchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Silvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="828" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incorporating prior knowledge into the analysis of conjoint studies</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="1995-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adjusting choice models to better predict market behavior</title>
		<author>
			<persName><forename type="first">G</forename><surname>Allenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fennell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gilbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olfek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Orme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="197" to="208" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Penalized maximum likelihood estimation in logistic regression and discrimination</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Blair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="136" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<title level="m">Statistical Decision Theory and Bayesian Analysis</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bayesian Theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Account-level modeling for trade promotion: An application of a constrained parameter hierarchical model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boatwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">448</biblScope>
			<biblScope unit="page" from="1063" to="1073" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Statistical Inference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Duxbury Press</publisher>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Commercial use of conjoint analysis: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cattin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wittink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">Summer</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the metropolishastings algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Statist</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="327" to="335" />
			<date type="published" when="1995-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smoothing noisy data with spline functions: Estimating the correct degree of smoothing by the method of generalized cross-validation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Optimal Statistical Decision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Degroot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incentive-aligned conjoint analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liechty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="2005-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Asymptotic-behavior of statistical estimators and of optimal solutions of stochastic optimization problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dupacova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1517" to="1549" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regularization networks and support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convex optimization approach to modeling consumer heterogeneity in conjoint estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="805" to="818" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predictive sample reuse method with applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geisser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">350</biblScope>
			<biblScope unit="page" from="320" to="328" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constrained Bayes estimation with applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">418</biblScope>
			<biblScope unit="page" from="533" to="540" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized crossvalidation as a method for choosing a good ridge parameter</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="223" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonparametric roughness penalties for probability densities</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Gaskins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="271" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Density estimation and bumphunting by the penalized likelihood method exemplified by scattering and meteorite data</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Gaskins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">369</biblScope>
			<biblScope unit="page" from="42" to="56" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A correspondence between Bayesian estimation on stochastic processes and smoothing by splines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Kimeldorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="495" to="501" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The logistic normal distribution for Bayesian, nonparametric, predictive densities</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">402</biblScope>
			<biblScope unit="page" from="509" to="516" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian inference of semiparametric regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="863" to="879" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bayesian semiparametric density estimation and model verification using a logistic-gaussian process</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graphical Statist</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="565" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical Bayes conjoint analysis: Recovery of partworth heterogeneity from reduced experimental designs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="191" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inequality constrained least-squares estimation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">355</biblScope>
			<biblScope unit="page" from="746" to="751" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimating a population of parameter values using Bayes and empirical Bayes methods</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">386</biblScope>
			<biblScope unit="page" from="393" to="398" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What if consumer experiments impact variances as well as means? Response variability as a behavioral phenomenon</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Louviere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="506" to="511" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An exact likelihood analysis of the multinomial probit model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econometrics</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="207" to="240" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discrete choice models incorporating revealed preferences and psychometric data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfadden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econom. Models Marketing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="29" to="55" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">External effect adjustments in conjoint analysis. Working paper</title>
		<author>
			<persName><forename type="first">B</forename><surname>Orme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://www.sawtoothsoftware.com/download/techpop/externaleffects.pdf" />
	</analytic>
	<monogr>
		<title level="m">Sawtooth Software</title>
				<meeting><address><addrLine>Sequim, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The Foundations of Statistics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Savage</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Triple goal estimates in two-stage hierarchical models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="455" to="471" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bayesian statistics without tears: A sampling-resampling perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Statist</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="84" to="88" />
			<date type="published" when="1992-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-validatory choice and assessment of statistical prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="147" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bayesian &quot;confidence intervals&quot; for the crossvalidated smoothing spline</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="133" to="150" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Asymptotics of least-squares estimators for constrained nonlinear regression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1316" to="1326" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
