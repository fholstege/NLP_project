<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Idea Screening Using Consumers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-11-15">November 15, 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Toubia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Columbia Business School</orgName>
								<orgName type="institution">Uris Hall</orgName>
								<address>
									<addrLine>Room 522</addrLine>
									<postCode>3022, 10027-6902</postCode>
									<settlement>Broadway, New York</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Laurent</forename><surname>Florès</surname></persName>
							<email>lflores@crmmetrix.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire INSEEC and CRMMETRIX</orgName>
								<address>
									<addrLine>700 Plaza Drive, 2nd Floor</addrLine>
									<postCode>07094</postCode>
									<settlement>Secaucus</settlement>
									<region>New Jersey</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Idea Screening Using Consumers</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 e 1526-548X 07 2603 0342</idno>
						<imprint>
							<date type="published" when="2005-11-15">November 15, 2005</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.1070.0273</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>innovation</term>
					<term>marketing research</term>
					<term>marketing surveys</term>
					<term>marketing tools</term>
					<term>new product research</term>
					<term>product development History: This paper was received</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>F ollowing a successful idea generation exercise, a company might easily be left with hundreds of ideas generated by experts, employees, or consumers. The next step is to screen these ideas and identify those with the highest potential. In this paper we propose a practical approach to involving consumers in idea screening.</p><p>Although the number of ideas may potentially be very large, it would be unreasonable to ask each consumer to evaluate more than a few ideas. This raises the challenge of efficiently selecting the ideas to be evaluated by each consumer. We describe several idea-screening algorithms that perform this selection adaptively based on the evaluations made by previous consumers. We use simulations to compare and analyze the performance of the algorithms as well as to understand their behavior. The best-performing algorithm focuses on the ideas that are the most likely to have been misclassified (as "top" or "bottom" ideas) based on the previous evaluations, and avoids discarding ideas too fast by adding random perturbations to the misclassification probabilities. We demonstrate the convergent validity of this algorithm using a field experiment, which also confirms the convergence pattern predicted by simulations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Idea generation is critical to new product development. It belongs to the "fuzzy front end" of the development process, recognized as a key leverage point for a firm <ref type="bibr">Hauser 2001a, Hauser et al. 2006)</ref>. A variety of idea generation methods have been introduced since the 1950s. The most popular is probably brainstorming <ref type="bibr" target="#b31">(Osborn 1957)</ref>. Other traditional examples include lateral thinking <ref type="bibr" target="#b8">(De Bono 1970)</ref>, synectics <ref type="bibr" target="#b33">(Prince 1970</ref><ref type="bibr" target="#b23">, Gordon 1969</ref>, and six thinking hats <ref type="bibr" target="#b9">(De Bono 1985)</ref>. Recent developments include electronic brainstorming <ref type="bibr" target="#b30">(Nunamaker et al. 1987;</ref><ref type="bibr" target="#b14">Gallupe et al. 1991</ref><ref type="bibr" target="#b15">Gallupe et al. , 1992</ref><ref type="bibr" target="#b9">Dennis and Valacich 1993;</ref><ref type="bibr" target="#b38">Valacich et al. 1994)</ref>, ideation templates <ref type="bibr">(Goldenberg et al. 1999a, b;</ref><ref type="bibr" target="#b18">Goldenberg and Mazursky 2002)</ref>, and incentives-based idea generation <ref type="bibr" target="#b36">(Toubia 2006)</ref>. With the development of Internet-based tools, companies are increasingly involving their own consumers in idea generation <ref type="bibr" target="#b13">(Forbes 2005)</ref>.</p><p>Depending on the method used, a successful idea generation exercise may result in up to hundreds of ideas generated by experts, consumers, or employees. The number of ideas appears even more likely to be large when consumers are involved in the process. <ref type="bibr">1</ref> The new product development team is then left with the daunting task of screening these ideas in order to focus its limited resources on those with the highest potential. The selected ideas will be refined and translated into specific features or integrated products. In our field experiment, examples of consumergenerated ideas on how to improve cellular phones (see Table <ref type="table" target="#tab_3">2</ref>) included "There would be a way to ftp data files," "Download movies with your phone and project them on the wall so it seems like you're at the theater," etc. One traditional approach to idea screening is to ask one or a few experts to go over the transcripts of ideas and evaluate them <ref type="bibr" target="#b37">(Urban and Hauser 1993</ref>). However, experts' judgments might not always reflect consumers' needs and preferences. <ref type="bibr">2</ref> In this paper we propose a practical approach to involving consumers in idea screening. Although the number of ideas to be screened may potentially be very large (especially if a large number of consumers have been involved in the idea generation process), it would be unreasonable to ask each consumer to evaluate more than a few ideas, especially if the evaluations are to be performed online in a noncontrolled  <ref type="bibr" target="#b6">(Dahan and Hauser 2001b)</ref>. This raises the challenge of efficiently selecting the ideas to be evaluated by each consumer in order to converge to the best ideas as quickly (i.e., with only few respondents) and reliably as possible. We assume that the evaluations are done online and sequentially, allowing the selection to be performed adaptively based on the evaluations made by previous consumers.</p><p>We propose and explore several algorithms for adaptive idea screening. We assume that each idea appeals to an unknown proportion of consumers. Our estimate of this proportion follows a beta distribution with parameters depending on the previous evaluations. We assume that the team's objective is to identify the top m ideas out of a given set. We use simulations to compare and analyze the performance of the algorithms, as well as to understand their behavior and the drivers of differences in performance. We demonstrate the convergent validity of the bestperforming algorithm using a field experiment, which also confirms the convergence pattern predicted by simulations. Note that our field experiment focuses on convergence and convergent validity, and that we rely on simulations to compare the performance of the different algorithms.</p><p>A problem with some similarities to ours was studied in the educational testing literature by <ref type="bibr" target="#b2">Bradlow and Wainer (1998)</ref>. Bradlow and Wainer consider subjective tests (e.g., essays) raters by human judges (on a continuous scale), resulting in binary pass/fail decisions (such that only candidates with an average grade above a predefined cutoff pass). They consider a situation in which the rescoring of some tests is possible after all tests have been rated by a fixed number of judges and initial pass/fail decisions have been made, and study the problem of allocating judges in the rescoring phase (e.g., which essays should be graded again). They find, using a modeling setup different from ours, 3 that for tests in which the number of initial failers and passers are approximately equal, a reasonable strategy is to rescore only examinees near the cutoff score (they compare this strategy to one where only failures are rescored).</p><p>Beyond the differences in modeling approach, context, and type of evaluations, two fundamental differences between our problem and the one studied by Bradlow and Wainer are that (1) the number of previous evaluations per item is constant across items in the latter (same initial number of raters on each essay) and different in the former (different number of previous evaluations per idea) and (2) allocation decisions are made once in the latter versus many times (once for each consumer) in the former. Given these differences, Bradlow and Wainer's work is not directly applicable to our problem. However, we will use it as an initial building block for some of our algorithms.</p><p>This paper is structured as follows. We introduce the idea selection algorithms in §2. In §3 we report the results of a series of simulations designed to study the performance and behavior of these algorithms. We report the results of our field experiment in §4. We describe a managerial application of our research in §5 and conclude in §6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Algorithms for Adaptive Idea Selection Notations and Definitions</head><p>As mentioned earlier, we assume that our goal is to select a fixed number of ideas to be brought to the next stage of the new product development process, i.e., to identify the top m ideas out of a set of I previously generated ideas. In order to achieve this goal, we ask different consumers to evaluate different subsets of k ideas. For simplicity, we assume that consumers provide binary evaluations of the ideas, i.e., they indicate which ideas they believe to be "good." Note that we do not restrict the definition of a "good" idea. It can be specified by the researcher and should be explicitly given to the consumers before they start their evaluations. Note also that we show in Appendix A how our framework could be extended to nonbinary evaluations. We leave to future research the extension to other screening goals, such as identifying all ideas above a predefined threshold. Let us define p i i∈ 1 I as the probability that a randomly selected consumer will classify idea i as a good idea. We use p i as a measure of the quality of the idea, i.e., our goal is to identify the m ideas with the highest probabilities.</p><p>Let us define the following: n Si i∈ 1 I = number of respondents who have evaluated idea i and classified it as a good idea. (S stands for "success.") n F i i∈ 1 I = number of respondents who have evaluated idea i and did not classify it as a good idea. (F stands for "failure.") n S0 , n F 0 : parameters of our prior on p i , assumed to follow a beta distribution Beta(n S0 n F 0 .</p><p>p i i∈ 1 I = our estimate of p i , based on the previous evaluations. Given our beta prior and the fact that the evaluations follow a binomial likelihood, the posterior on p i follows another beta distribution: Beta(n S0 + n Si n F 0 + n F i . <ref type="bibr">4</ref> Our point estimate of p i is simply the expected value of this distribution:</p><formula xml:id="formula_0">p i = n Si + n S0 / n F i + n Si + n S0 + n F 0 .</formula><p>Marketing <ref type="bibr">Science 26(3), pp. 342-360, © 2007 INFORMS</ref> Note that our definition of quality ignores other important criteria such as cost, feasibility, or fit with the company's core competencies <ref type="bibr" target="#b32">(Ozer 2005)</ref>. This can be addressed by subjecting the subset of ideas selected using our approach to a second round of evaluations based on these other criteria (likely to be performed by a small set of experts in a traditional fashion). Consumer acceptance being a necessary condition for success, the initial screening performed by consumers would probably make this second round of evaluations easier with only a reduced risk of leaving out potentially fruitful ideas.</p><p>Note finally that this framework may be extended to account for the existence of noise in the evaluations. In the simplest case, the amount of noise may be assumed to be constant across ideas and across consumers. For example, if a consumer produces a random evaluation (positive with probability 0.5) with probability 0.2, the observed probability for idea i is 0 8 * p i + 0 2 * 0 5, where p i is the "true" probability. Such monotonic transformation would not change the identity of the top m ideas; however, it would make their identification harder by reducing the amount of variation across ideas. We leave the extension to cases in which the amount of noise is assumed to vary across ideas and/or across consumers to future research.</p><p>We assume that the sets of ideas presented to the first n − 1 consumers, as well as the corresponding evaluations, are available when selecting the subset of ideas to be presented to the nth consumer. 5 This is the case if the evaluations are done online (as in our field experiment). Consumers could be invited to participate by e-mail, or directed to the evaluation site from the company's main site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective Function and Performance Metrics</head><p>The problem of identifying the top m ideas can be viewed as that of correctly classifying ideas into two groups: the group composed of the m ideas with the highest associated probabilities, and its complement. We will refer to these two groups as the top and bottom groups, respectively. We index the m ideas with the highest probabilities as i 1 i m and the others as</p><formula xml:id="formula_1">i m+1 i I such thatp i 1 ≥ • • •p i m ≥p i m+1 ≥ • • •p i I .</formula><p>We consider two performance metrics: 1. A hit rate, defined as the number of ideas estimated to be in the top m that are correctly classified. Although it is easy to interpret, this metric is discrete, and hence does not take into account the actual quality of the ideas.</p><p>2. The average true probability (the true probabilities are known in simulations) of the estimated top m ideas: 1/m i∈ i 1 i m p i .</p><p>Maximizing performance on either of these metrics by adaptively asking N consumers to evaluate k ideas each is a dynamic program with the metric as the objective function, the number of previous positive and negative evaluations for each idea n Si i∈ 1 I n F i i∈ 1 I as the state, the ideas to be evaluated by the next consumer as decision variables, and the transition between states being given by our current estimates p i i∈ 1 I . Unfortunately, the size of the state space is such that the identification of the optimal strategy would be intractable, at least with today's computers and using traditional dynamic programming techniques <ref type="bibr" target="#b1">(Bertsekas 1995)</ref>. Hence, some approximations are necessary. We first consider the myopic approximations of the dynamic programs corresponding to each of the two performance metrics. Next, we consider additional heuristics, three of which are related to the previous work of <ref type="bibr" target="#b2">Bradlow and Wainer (1998)</ref> reviewed in the introduction. A summary of all the algorithms is provided in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common Structure of the Algorithms</head><p>All of the algorithms studied in this paper share a common structure. In particular, the following steps are performed in order to select the ideas presented to the next consumer:</p><p>Step 1. Estimate the probability associated with each idea:p i = n Si + n S0 / n F i + n Si + n S0 + n F 0 .</p><p>Step 2. Assign a score to each idea, s i .</p><p>Step 3. Select the k ideas with the highest scores.</p><p>The algorithms differ only in the method used to compute the scores in step 2. Hence, they all implicitly assume in Step 3 that the improvement in performance achieved by presenting a set of k ideas is monotonically increasing in the sum of the improvements achieved by presenting each of the k ideas independently. In other words, instead of directly assigning a score to each I k subset of k ideas, they assign one score to each idea and implicitly assign a score to each subset equal to the sum of the scores of its elements. We use such approximation because of the large number of subsets. For example, with I = 100 and k = 10, there exist 100!/90! • 10! = 1 73 • 10 13 possible subsets of ideas. Future research may investigate heuristics that do not require the enumeration of all subsets. For example, borrowing techniques from the experimental design literature <ref type="bibr" target="#b28">(Kuhfeld et al. 1994</ref><ref type="bibr" target="#b11">, Federov 1972</ref><ref type="bibr" target="#b3">, Cook and Nachtsheim 1980</ref>, it may be possible to start with the set of ideas with the highest individual scores and consider replacing each idea with an idea not currently in the set. Such an operation could be repeated until no further improvement is possible, leading to a locally optimal set of ideas.</p><p>Myopic Approximations 1. Myopic Maximization of the Hit Rate. Our first algorithm myopically maximizes hit rates. The expected hit rate (number of ideas in estimated top m that are in the true top m) is given as a function of the current state n Si i∈ 1 I n F i i∈ 1 I as follows:</p><formula xml:id="formula_2">H n S1 n SI n F 1 n F I = p i =1 p i =0 p I =1 p I =0 i∈ i 1 i m j∈ 1 I \ i 1 i m 1 p i ≥ p j • n S0 +n S1 n S0 +n F 1 p 1 • • • n S0 +n SI n S0 +n FI p I dp 1 • • • dp I</formula><p>where 1( ) is the indicator function and n S0 +n Si n S0 +n F i is the probability density function of Beta(n S0 + n Sj n S0 + n Fj . The score assigned to each idea is equal to the expected hit rate that would result from obtaining an additional evaluation on that idea:</p><formula xml:id="formula_3">s i =p i • H n S1 n S i−1 n Si + 1 n S i+1 n SI n F 1 n F I + 1 −p i H n S1 n SI n F 1 n F i−1 n F i + 1 n F i+1 n F I</formula><p>In our simulations, we estimated the above integral numerically using 1,000 random draws. Because it requires numerical integration, this algorithm is the least practical, and by far the slowest, of those considered in this paper (all other algorithms can be implemented without noticeable delays between judges).</p><p>2. Myopic Maximization of the Average True Probability of the Estimated Top m Ideas. Our second algorithm myopically maximizes the second performance metric, i.e., the average probability of the estimated top m ideas. The expected value of this objective function is given as a function of the current state n Si i∈ 1 I n F i i∈ 1 I as follows:</p><formula xml:id="formula_4">A n S1 n SI n F 1 n F I = 1 m i∈ i 1 i m p i = 1 m i∈ i 1 i m n S0 + n Si n S0 + n Si + n F 0 + n F i</formula><p>The score assigned to each idea is:</p><formula xml:id="formula_5">s i =p i • A n S1 n S i−1 n Si + 1 n S i+1 n SI n F 1 n F I + 1 −p i • A n S1 n SI n F 1 n F i−1 n F i + 1 n F i+1 n F I</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common Characteristic of the Myopic Approximations</head><p>We show the following proposition in Appendix B:</p><p>Proposition. Consider the set of ideas classified as top ideas that would remain classified as top after one positive or negative evaluation, and the set of ideas classified as bottom ideas that would remain classified as bottom after one positive or negative evaluation:</p><formula xml:id="formula_6">i ∈ i 1 i m n S 0 + n S i n S 0 + n S i + n F 0 + n F i + 1 ≥p i m+1 ∪ i i 1 i m n S 0 + n S i + 1 n S 0 + n S i + n F 0 + n F i + 1 ≤p i m (we assume, without loss of generality, thatp i 1 ≥ • • •p i m ≥ p i m+1 ≥ • • •p i I ).</formula><p>Each myopic approximation assigns the same score to all ideas in this set.</p><p>This proposition implies that each myopic approximation assigns a unique score only to ideas whose classification may change after only one evaluation and assigns the same score s 0 to all other ideas. When fewer than k ideas have a score higher than s 0 , these algorithms randomly select the remaining ones from the set with a score of s 0 . As a result, we will see in the next section that they do not behave very differently from the random benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heuristic Approaches</head><p>3. Closest to Threshold. Our third algorithm directly and naively applies Bradlow and Wainer's (1998) recommendation ("select ideas near the cutoff"). The score assigned to each idea is equal to the opposite of the distance between its estimated probability and the estimated probability of the closest idea in the other group, i.e.:</p><p>• If i is in the "top" group, s i = − p i −p i m+1 .</p><p>• If i is in the "bottom" group, s i = − p i −p i m , wherep i m+1 andp i m are, respectively, the highest probability estimate among the current bottom ideas and the smallest probability estimate among the current top ideas.</p><p>4. Misclassification Minimization. As mentioned earlier, Bradlow and Wainer (1998) study a situation in which the same number of judges is assigned to all examinees in the first step. A more general interpretation of their recommended strategy is that the Marketing Science 26(3), pp. 342-360, © 2007 INFORMS examinees that should be rescored are those who are the most likely to have been misclassified in the first round. <ref type="bibr">6</ref> In their context, this is similar to a cutoff strategy because the number of evaluations on each examinee is the same in the first step. In contrast, we consider a multistage process in which the numbers of evaluations per idea differ. As a result, the probability that an idea has been misclassified is not only driven by the distance to the cutoff, but also by the variance of the corresponding posterior distribution, which is driven by the number of previous evaluations.</p><p>Our next heuristic, which we label "Misclassification Minimization," assigns a score to each idea equal to the probability that it has been misclassified based on the previous evaluations. We approximate this probability as follows: <ref type="bibr">7</ref> • If i is in the top group,</p><formula xml:id="formula_7">s i = Prob p i ≤p i m+1 = p i m+1 0 n S0 +n Si n F 0 +n F i p dp. • If i is in the bottom group, s i = Prob p i ≥p i m = 1 p i m n S0 +n Si n F 0 +n F i p dp.</formula><p>The scores are obtained directly using the cumulative distribution function of the beta distribution.</p><p>5. Misclassification Minimization with Random Perturbations. As mentioned earlier as well, another fundamental difference between our problem and the one studied by <ref type="bibr" target="#b2">Bradlow and Wainer (1998)</ref> is that we consider a multiperiod dynamic allocation of judges. If the number of periods (i.e., judges) is reduced to two, then the myopic benchmarks are not approximations anymore and they become optimal. However, with more judges, all the algorithms described in this section incur the risk of behaving myopically, and wrongly classifying ideas as top or bottom after very few evaluations, without further investigation. Our next algorithm uses insights from the literature on genetic algorithms <ref type="bibr" target="#b17">(Goldberg 1989</ref><ref type="bibr" target="#b29">, Mitchell 1996</ref> to limit such risk for the Misclassification Minimization algorithm. (Simulations studying the impact of similar modifications on the other benchmarks are available from the authors.) Genetic algorithms ensure diversity in the searched solutions by using random mutations: At each step the proposed solution is mutated (i.e., randomly altered) with a very small probability. Mutations are used as an "insurance policy against premature loss of important notions" <ref type="bibr">(Goldberg 1989, p. 14)</ref>. We introduce mutations, or in our case rather perturbations, by setting the scores assigned to each idea to: s i = s 0 i + i where s 0 i is the score assigned by the Misclassification Minimization algorithm and i is a normal Random variable with mean 0 and variance 0 01/ n S0 + n Si + n F 0 + n F i . This specification for i was adopted based on the magnitude of the mutation rate typically used in genetic algorithms, and on recent research suggesting that decreasing the mutation rate over the number of iterations (or generations) leads to higher performance compared to a fixed mutation rate <ref type="bibr" target="#b12">(Fogarty 1989;</ref><ref type="bibr">Männer 1991, 1992;</ref><ref type="bibr" target="#b0">Bäck and Schutz 1996)</ref>. Simulations available from the authors suggest that the results are robust to small variations in this specification.</p><p>6. Maximize the Right Tail. Our next algorithm selects the ideas that have the highest current tail probabilities, i.e., that are the most likely to have a probability higher than a given threshold p 0 . The scores are given by:</p><formula xml:id="formula_8">s i = Pr p i ≥ p 0 = 1 p 0 n S0 +n Si n F 0 +n F i p dp</formula><p>We set p 0 to the 100 • 1 − m/I th percentile of the prior distribution, such that the expected number of ideas with a probability higher than p 0 is m. For example, with I = 100, m = 10 and with a uniform prior, p 0 = 0 9. 7. Random Selection. Our last algorithm simply performs a random selection of the ideas (each idea is equally likely to be shown to the next consumer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Simulations</head><p>The goals of our simulations are twofold. First, we compare the performance of the algorithms introduced in the previous section under various assumptions on the true and prior distributions of the probabilities p i i∈ 1 I . Second, we attempt to understand the source of the differences in performance by studying the behavior of each algorithm. In particular, we identify some of the drivers of high performance, as well as some of the limitations of each approach. Such understanding is crucial to allow future research to further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Simulations</head><p>Our first set of simulations uses m = 10, k = 10, I = 100, N = 200 (i.e., the goal is to identify the top 10 ideas out of 100, and 200 consumers evaluate 10 ideas each). The true probabilities p 1 p I are drawn from unif 0 1 , and uniform priors are used by all algorithms (n S0 = n F 0 = 1). Figure <ref type="figure">1</ref> (based on the average of 200 sets of simulations) reports the average performance as a function of the number of judges for all seven algorithms on both metrics (to make their reading easier, the graphs start at 50 judges). The results suggest that Misclassification Minimization with Random Perturbations performs best overall: It achieves the highest performance after 188 out of the 200 (1 to 200) possible numbers of judges on the hit rate metric, and 190 on the average probability metric. It performs significantly better (at p &lt; 0 05) than Misclassification Minimization after 174 possible numbers of judges on the hit rate metric and 53 on the average probability metric. It performs significantly better (at p &lt; 0 05) than all other algorithms after 183 possible numbers of judges on both metrics (all numbers of judges superior or equal to 18). 8 Note that in the limit all <ref type="bibr">8</ref> Adding random perturbations to the other algorithms does not change the results. Misclassification Minimization with Random Perturbations performs significantly better (at p &lt; 0 05) than all algorithms would converge to a perfect performance level. Hence, another way to analyze the results is to compare the speed of convergence of the different algorithms. The performance achieved on either metric by Misclassification Minimization with Random Perturbations with 167 judges (or any larger number) is higher than that obtained by Misclassification Minimization after 200 judges. The performance achieved by Misclassification Minimization with Random Perturbations on either metric with 85 judges (or any larger number) is higher than that achieved by any other algorithm after 200 judges.</p><p>We next attempt to understand the behavior of the different algorithms and the sources of the differences in performance. We consider the estimated probability associated with each idea after the evaluation by the last judge, as well as the corresponding number of observations: p i n Si + n F i i∈ 1 I . We characterize the behavior of an algorithm by the frequency (across ideas and across replications) with which each pair p i n Si + n F i is observed. We expect high-performing algorithms to focus on ideas that are harder to classify, i.e., to lead to final states in which the estimated probabilities close to the threshold between the top and bottom groups are scoupled with the highest numbers of observations. Figure <ref type="figure" target="#fig_1">2</ref> plots the empirical probability mass functions of p i n Si + n F i for each algorithm. The x axis corresponds top, the y axis to n S + n F , and the z axis to the frequency with which the corresponding end state p n S + n F is observed. We observe the following:</p><p>• The two best-performing algorithms (Misclassification Minimization and Misclassification Minimization with Random Perturbations) have the property that the number of observations is increasing with the final estimated probability up to a certain point (around 0.9, the average threshold between the top and bottom groups), after which it decreases. This suggests that these algorithms are able to focus on ideas that are harder to classify.</p><p>• Misclassification Minimization shows a peak at p = 1/3, n s + n F = 1 , such that 39.16% of the ideas receive one negative evaluation and are not presented to any subsequent judge. Misclassification Minimization with Random Perturbations, on the other hand, does not share this characteristic. Because the variance of the perturbations decreases with the number of observations, the perturbations mostly affect the ideas with low numbers of observations, and the two algorithms behave similarly on the other ideas. To understand the speed with which Misclassification other algorithms with random perturbations in 189 out of 200 possible numbers of judges on the hit rate metric, and 182 on the average probability metric.  Minimization discards ideas, note that if, for example, the threshold between the top and bottom group is 0.90 (the average threshold), the probability that an idea classified as a bottom idea and discarded after one negative evaluation has been misclassified is only:</p><p>1 0 9 1 2 p dp = 0 01. Indeed, in our simulations only 1.09% of the ideas discarded by Misclassification Minimization after one negative evaluation were misclassified (their average true probability is 0.3322). However, these errors could be easily corrected with only a few additional observations, and their accumulation has a substantial impact on performance. Indeed, the 39.16% of ideas discarded by Misclassification Minimization after one negative evaluation account for 14.81% of the misclassification errors. In contrast, the 39.16% of ideas with the lowest number of evaluations in Misclassification Minimization with Random Perturbation account for only 2.26% of the misclassification errors made by this algorithm. This illustrates the fact that random perturbations serve as an insurance policy against discarding ideas too quickly. <ref type="bibr">9</ref> • As predicted by the previous proposition, Myopic Hit Rate and Myopic Average behave very similarly to the random benchmark. In particular, they show only a modest increase in the number of observations for ideas with estimated probabilities around 0.90 (the average threshold between the top and bottom groups), i.e., ideas likely to change classification after only one additional evaluation.</p><p>• Closest to Threshold, because it does not take into account the variance of the beliefs on the estimated probabilities, gives rise to a bipolar distribution of the number of observations in which a large proportion (47.52%) of the ideas are not evaluated even once (peak at p = 0 5 n s + n F = 0 , and a small number of ideas are evaluated by almost all the judges. Once an idea is identified that has an estimated probability close to the threshold, it may be shown to all remaining judges even if other ideas have not been evaluated even once (because with a uniform prior these ideas have an estimated probability of 0.5, which is further away from the threshold).</p><p>• Maximize the Right Tail also underexplores a large proportion of the ideas (8.96% of the ideas are left unexplored and 43.94% are discarded after one negative and no positive evaluation), and focuses on ideas that are already known to have a large estimated probability. This algorithm does take variance <ref type="bibr">9</ref> Note that another way to prevent the algorithm from discarding ideas too fast would be to force it to collect a minimum number of observations on each idea by setting the score s i to an arbitrarily large number if idea i has been evaluated fewer than t times. However, such algorithm would perform exactly like the random benchmark on the first t × I/k judges. into account-however, in a counterproductive way. In particular, an idea on which the beliefs have a high mean and low variance is actually more likely to be investigated further, although its classification is very likely to be correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of the Prior and True Distributions</head><p>We now study how the true distribution of the idea probabilities and the prior Beta(n S0 n F 0 influence the performance and behavior of the algorithms. We use a 3 (true distribution = Beta 1 3 , Beta 3 1 , or Beta 0 1 0 3 × 2 (uniform prior versus accurate prior) simulation design, in which each cell uses a setup different from the basic setup only on the true distribution of the p i s and on the parameters of the prior. A true distribution of Beta 1 3 represents a context in which most ideas are of marginal value: The average probability is decreased to 1/4, and a greater proportion of ideas have low probabilities. Beta 3 1 characterizes a situation in which most ideas are of high value. Beta 0 1 0 3 assumes a bimodal distribution of the ideas in which most ideas are of very low quality (60% of the ideas have a probability below 0.07) and a minority of ideas are of extremely high quality (6.5% of the ideas have a probability higher than 0.99). <ref type="bibr">10</ref> The simulations reveal the following: First, Misclassification Minimization with Random Perturbations continues to perform best overall. See Figures A.1-A.3 in the appendix. It performs best in 879 out of 1,200 possible comparisons (1 to 200 judges×3 true distributions×uniform versus accurate prior) on the hit rate metric and 815 on the average probability metric. It performs significantly better (at p &lt; 0 05) than Misclassification Minimization on the hit rate metric in 335 cases and in 167 cases on the average probability metric. It performs significantly better (at p &lt; 0 05) than all other algorithms on the hit rate metric in 1,041 cases and in 955 cases on the average probability metric.</p><p>Second, this set of simulations further illustrates the role of random perturbations by showing that they improve robustness to variations in the prior. See Figure <ref type="figure">3</ref>. Whereas the difference between Misclassification Minimization under a uniform versus a correct prior is significant (at p &lt; 0 05) in 452 out of the 600 possible comparisons (200 numbers of possible judges × 3 true distributions) on the hit rate metric and 282 on the average probability metric, it is significant in only 105 on the hit rate metric and in 92 on the average probability metric for Misclassification Minimization with Random Perturbations. As seen previously, Misclassification Minimization tends to discard ideas after only a very few observations. The number of observations collected before discarding an idea is sensitive to the prior distribution and to the true distribution. Random perturbations decrease this sensitivity by preventing the algorithm from discarding ideas too fast. <ref type="bibr">11</ref> Finally, this set of simulations illustrates the fact that random perturbations serve as an insurance policy not only against classifying true top ideas as bottom ideas after too few negative evaluations, but in some cases also against classifying true bottom ideas as top ideas after too few positive evaluations. When the true distribution is Beta 1 3 and the prior is uniform, if a true bottom idea is shown to a very small set of consumers who evaluate it positively, it will have a very high estimated probability (making it likely to be classified as a top idea) and a relatively low misclassification probability (making it less likely to be further investigated). With the correct prior, such an idea would have a lower expected probability and a higher variance, and would be shown to additional consumers, who, by regression to the mean, would correct the initial evaluations. Figures <ref type="figure">4(a</ref>) and 4(b) illustrate this effect by showing the posterior distribution of p i after three positive evaluations, in the case of a uniform (Figure <ref type="figure">4</ref>(a)) and a nonuniform prior Beta 1 3 (Figure <ref type="figure">4(b)</ref>). In our simulations, under a uniform prior and a true distribution of Beta 1 3 , Misclassification Minimization discards ideas with no negative evaluations after an average of 5.50 positive evaluations. This number goes up to 6.72 when random perturbations are added and to 9.32 under a correct prior. As a result, 6.17% of the final misclassification errors are made on ideas that receive only positive and no negative evaluation under a uniform prior and without random perturbations. This proportion goes down to 2.27% if random perturbations are added, and 0.54% under the correct prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to the Parameter k</head><p>In our simulations as well as in our field experiment, we assumed that each consumer evaluates k = 10 ideas and that the objective is to identify the top m = 10 ideas. The choice of both of these parameters raises interesting issues that may be addressed in future research. For example, increasing m decreases the likelihood of screening out high-quality ideas, but reduces the expected quality of the selected ideas. If </p><p>consumers are paid a fixed fee for their participation, increasing k increases the total number of evaluations without increasing cost. On the other hand, a large value of k might lead to cognitive overload and reduce the quality of the evaluations. The number of ideas that can easily be handled by consumers is an empirical question; however, our experience suggests that consumers are comfortable with the task for values of k smaller than or equal to 12. We tested the robustness of our simulation results to the parameter k by replicating the initial simulations with k = 5 and k = 15, while keeping m constant at 10. The number of respondents was set, respectively, to 400 and 133 in order to maintain the total number of observations at a constant. See Figure A.4 in the appendix. The performance of all algorithms except Closest to Threshold and Maximize the Right Tail is not noticeably affected by variations in k. Although the total number of evaluations is held constant, the performance of Closest to Threshold and Maximize the Right Tail is improved when k is increased (albeit not to the level of Misclassification Minimization with Random Perturbations). Recall that both of these algorithms tend to focus exclusively on a subset of the ideas and leave the rest unexplored. Increasing k forces the algorithms to explore a wider range of ideas. For example, the proportion of ideas left unexplored by Closest to Threshold decreases from 47.52% to 38.87% when k increases from 10 to 15, and the proportion of ideas left unexplored by Maximize the Right Tail decreases from 8.96% to 0.18%.</p><p>In conclusion, our simulations suggest that Misclassification Minimization with Random Perturbations Marketing Science 26(3), pp. 342-360, © 2007 INFORMS provides the highest and most robust performance. Random perturbations serve as an insurance policy against discarding ideas too fast, and improve the robustness to changes and misspecifications of the prior. Based on these results, we will not further compare the relative performance of the algorithms. Instead, we will use Misclassification Minimization with Random Perturbations in our field experiment, and focus on convergence and convergent validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Field Experiment</head><p>We used simulations to compare and analyze the idea-screening algorithms introduced in §2 because simulations enable exact performance evaluations, replications and powerful significance tests, and the study of the impact of changes in some parameters or assumptions. However, they assumed that the true probabilities p i s were i.i.d. from a beta distribution, and that the judges' evaluations were i.i.d. from a binomial distribution. Violations of these assumptions may impact the validity of using simulations to study and compare idea-screening algorithms, as well as the validity of involving consumers in idea screening. Our field experiment attempted to address these concerns by (1) comparing the convergence observed empirically to that suggested by simulations, (2) testing the convergent validity of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design of the Experiment</head><p>This experiment consisted of two phases. Phase I allowed us to explore convergence, and Phase II to assess convergent validity. Both phases were run in collaboration with crmmetrix TM (www.crmmetrix. com), a marketing research company.</p><p>In Phase I, Misclassification Minimization with Random Perturbations was used to identify the top 10 out of 99 consumer-generated ideas on new cell phone features and on potential improvements to current features. 12 A random set of members of crmmetrix TM 's online panel was invited to participate by e-mail, resulting in 195 respondents. Given the market penetration of the category, respondents were not screened. Each participant evaluated one subset of 10 ideas. In order to form a prior, we asked three pretest respondents to rate all 99 ideas on a five-point scale. <ref type="bibr">13</ref> We fitted the probability of receiving a score <ref type="bibr">12</ref> These ideas were prescreened to eliminate redundant and irrelevant items. <ref type="bibr">13</ref> The question asked of the three pretest respondents was: "Assume that you are on a new products team, trying to identify new possible features that could be included in cell phones, as well as opportunities for improving current features. Please indicate which of the following ideas you would pursue." The response scale was "definitely not," "probably not," "may be," "probably yes," or "definitely yes." of four or five with a beta distribution. Fit was maximized with Beta 0 97 0 76 . We rounded these parameters and used a uniform prior of Beta 1 1 . In Phase II, using a similar interface, we asked an additional 85 respondents to rate a unique subset of 10 ideas: 5 ideas from the top 10 identified in Phase I (4 ideas randomly selected from the top 5 and the 10th-ranked idea) and 5 ideas from the bottom group (the 11th-ranked idea and 4 bottom ideas randomly selected from those with the fewest evaluations). See Table <ref type="table" target="#tab_3">2</ref> for the list of ideas tested in Phase II (the ideas were presented to the respondents in a randomized order). In a commercial application, it would be advisable to segment respondents based on demographic and usage variables, and to screen ideas at the segment level. In particular, certain ideas may be particularly popular among respondents who are the most active in the category. Hence, we do not claim that the top ideas from Phase I are the ones that would lead to the highest revenues. Recall that our goal is to study convergence and convergent validity, not to make recommendations specific to the cell phone market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results from Phase I: Convergence</head><p>Our goal is to assess the validity of using simulations for studying idea-screening algorithms. Because exact performance evaluation is impossible when the "truth" is unknown, we compare the pattern of convergence observed in the field experiment to that suggested by simulations. We fitted the empirical observations from Phase I with a beta distribution (giving rise to Beta 6 2 8 3 ) and ran 200 sets of simulations using the same parameters as in the field experiment (I = 99, k = 10, m = 10, N = 195). Convergence is obtained by assuming that the probability estimates obtained after the evaluations by the last judge represent the truth, and computing performance after each intermediate number of judges. Similarly to the simulations, we define performance as hit rate (i.e., proportion of final top 10 ideas in the top 10 after judge n) or as the average true probability of the ideas estimated to be in the top 10. We report the results in Figure <ref type="figure">5</ref>. We see that the convergence pattern suggested by simulations is very comparable to that observed in the field. The convergence graphs from the field experiment lie within the 90% confidence bounds defined by the 5th and 95th percentiles (across the 200 replications) from the simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results from Phase II: Convergent Validity</head><p>Table <ref type="table" target="#tab_3">2</ref> reports the proportion of positive evaluations obtained by each of the 10 ideas tested in Phase II.</p><p>All ideas classified in the top group based on Phase I achieved higher scores than all ideas classified in the bottom group. Of all 25 possible pairwise comparisons (5 "top" ideas × 5 "bottom" ideas), 23 are significant at the p &lt; 0 01 level. This suggests that adaptive idea screening using consumers based on Misclassification Minimization with Random Perturbations has good convergent validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Managerial Application:</head><p>BrandDelphi TM</p><p>The research reported in this paper has been applied in the brandDelphi TM product (www.branddelphi. com) offered by crmmetrix TM (www.crmmetrix.com).</p><p>BrandDelphi TM asks respondents (typically recruited from a consumer panel or a client database) to perform two tasks in sequence: (1) generate ideas on a given topic and (2) evaluate a subset of the ideas proposed by the previous participants. In the initial version of the product, ideas in the second stage were selected using a method close to random selection. The company has now adopted our Misclassification Minimization with Random Perturbations algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Opportunities for Future Research</head><p>In this paper we propose a practical tool for involving consumers in idea screening. The small set of ideas to be evaluated by each consumer is adaptively selected based on the previous evaluations. We test, analyze, and compare seven idea-screening algorithms using simulations. We study the convergence and convergent validity of the best-performing algorithm using a field experiment. This paper constitutes only a first step towards studying algorithms for adaptive idea screening using consumers. We have already mentioned several areas for future research throughout the paper. We believe there are more. First, the performance of the different algorithms could be further compared using a field experiment. Second, interpendence between ideas could be captured by a hierarchical model in which the prior p i ∼ Beta n S0 n F 0 would be replaced with p i ∼ Beta exp S • z i exp F • z i , where the z i s would be idea covariates (the priors on S and F would probably be diffuse). The covariates z i s could, for example, be dummy variables capturing the category of the idea (e.g., ideas about the size of the cell phone, about the cost of the service plan, about games and entertainment, etc.), or they could capture determinants of success identified in previous research (e.g., <ref type="bibr" target="#b19">Goldenberg et al. 2001</ref>). Third, one could consider evaluations of ideas on multiple, potentially correlated dimensions (e.g., originality and feasibility), using, for example, a Sarmonov distribution <ref type="bibr" target="#b7">(Danaher and Hardie 2005)</ref>. Fourth, more effective algorithms may be developed using other tools, such as support vector machines or machine learning <ref type="bibr" target="#b4">(Cui and</ref><ref type="bibr">Curry 2005, Evgeniou et al. 2005</ref>). Fifth, it would be interesting to gain a deeper understanding of the impact of endogeneity (due to adaptivity) of idea selection on the estimates of the probabilities p i s <ref type="bibr" target="#b24">(Hauser and Toubia 2005)</ref>. Sixth, the performance metrics as well as the algorithms could be generalized to give different weights to errors of different types (type I errors versus type II errors) or to errors on ideas with higher estimated probabilities. Finally, the approach of involving a large number of consumers in idea screening could be compared to that of involving a small number of experts. We hypothesize that the difference between the approaches may be understood within von Hippel's framework of "sticky information" (Von <ref type="bibr" target="#b39">Hippel 1994</ref><ref type="bibr" target="#b40">Hippel , 1998</ref><ref type="bibr" target="#b34">Randall et al. 2006)</ref>, and that experts are more sensitive and responsive to "solution information," whereas consumers are more sensitive and responsive to "need information." (Any idea that proposes a solution to a need may be viewed as a combination of these two types of information.)</p><p>We close by noting that the framework introduced in this paper could be applied beyond idea screening. We have assumed that the judges were consumers and that the items to be judged were ideas. However, the algorithms do not rely on these two assumptions. Another possible application, relevant to the marketing academic community, could be the screening of Ph.D. applicants or job candidates. Asking all faculty members in a department to evaluate all the applications (typically around 100) may potentially result in a low response rate, and/or in noisy evaluations. The framework proposed here could be used to adaptively select a subset of the applications to be considered by each faculty member.     Closest to Threshold </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1 Basic Simulation Results: Misclassification Minimization with Random Perturbations Performs Best Hit rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2Understanding the Behavior of the Algorithms by Studying the Probability Mass Functions of the Final States p n S + n F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3 Random Perturbations Improve Robustness to Variations in the Prior Misclassification Minimization with Random Perturbations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4 (a) Impact of Three Positive Evaluations, Uniform, Prior; (b) Impact of Three Positive Evaluations, Prior = Beta 3 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 6Convergence Pattern from Field Experiment vs. SimulationsHit rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure A.1 Influence of the Prior DistributionPrior = uniform, true distribution = Beta(3, 1), hit rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Figure A.2 Influence of the Prior DistributionPrior = uniform, true distribution = Beta(1, 3), hit rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><label></label><figDesc>Figure A.4 Influence of the Number of Ideas Evaluated by Each Judge k Myopic Hit Rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Toubia and Florès: Adaptive Idea Screening Using Consumers Marketing Science 26(3), pp. 342-360, © 2007 INFORMS 343 environment</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Convergent Validity: Top Ideas Identified in Phase I Receive Higher Scores in Phase II</figDesc><table><row><cell></cell><cell>Proportion</cell></row><row><cell>Estimated</cell><cell>of positive</cell></row><row><cell cols="2">group based evaluations in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Toubia and Florès: Adaptive Idea Screening Using Consumers</figDesc><table><row><cell>358</cell><cell>Marketing Science 26(3), pp. 342-360, © 2007 INFORMS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Total number of observations Total number of observations Hit Rate Hit Rate Hit Rate Hit rate Hit Rate Hit Rate Myopic Average</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>500</cell><cell>1,000</cell><cell>1,500</cell><cell>2,000</cell></row><row><cell>0</cell><cell>500</cell><cell>1,000</cell><cell>1,500</cell><cell>2,000</cell><cell>0</cell><cell>500</cell><cell>1,000</cell><cell>1,500</cell><cell>2,000</cell></row><row><cell></cell><cell cols="3">Total number of observations</cell><cell></cell><cell></cell><cell cols="3">Total number of observations</cell><cell></cell></row><row><cell>0</cell><cell>500</cell><cell>1,000</cell><cell>1,500</cell><cell>2,000</cell><cell>0</cell><cell>500</cell><cell>1,000</cell><cell>1,500</cell><cell>2,000</cell></row><row><cell></cell><cell cols="3">Total number of observations</cell><cell></cell><cell></cell><cell cols="3">Total number of observations</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>500</cell><cell>1,000</cell><cell>1,500</cell><cell>2,000</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Total number of observations</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">An extreme example is Staples' recent organization of a competition among consumers to generate new product ideas. Eighty-three hundred ideas were submitted (The Economist 2005).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Indeed, decisions in the subsequent stages of the development process are often supported by marketing research tools such as conjoint analysis, focus groups, or pretest market forecasting<ref type="bibr" target="#b37">(Urban and Hauser 1993)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The tth evaluation y ijt of examinee i by rater j being on a continuous scale, Bradlow and Wainer assume that y ijt = + i + j + ijt where i ∼ N 0 2 , j ∼ N 0 2 , and ijt ∼ N 0 2 . An examinee passes the test ifȳ ij ≥ c where c is a predefined cutoff.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Beta priors are conjugates for binomial likelihoods<ref type="bibr" target="#b16">(Gelman et al. 1995)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We only consider adaptation across consumers. This allows all the ideas presented to a given consumer to be displayed simultaneously.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6"><ref type="bibr" target="#b2">Bradlow and Wainer (1998)</ref> use misclassification probabilities ( § §6.3 and 6.4) to determine the number of judges who should be allocated to each item in the rescoring phase. More precisely, they set this number proportional to the distance to the cutoff score, the coefficient of proportionality being chosen in order to minimize misclassification probability.7  The exact probability depends on the probability distributions associated with the entire set of I ideas. It would be challenging to estimate this exact probability adaptively without creating noticeable delays.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">The symmetric distribution Beta 0 3 0 1 implies that 49.21% of the ideas have a true probability higher than 0.99, and hence is less relevant practically.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Note that when the true distribution is Beta 1 3 , a correct prior actually hurts the performance of Misclassification Minimization (performance is similar under a correct versus uniform prior when random perturbations are added). This sensitivity is driven by the fact that a prior indicating that most ideas have a low probability exacerbates the algorithm's propensity to discard ideas after only very few negative evaluations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Each idea would have two scores, one corresponding to the very good category and the other to the good category.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Oded Netzer for his comments and suggestions, as well as participants to the 2005 Marketing Science conference and the 2006 four-school colloquium at New York University. Finally, they would like to thank Hemen Patel and RK from crmmetrix for their help with our field experiment.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Nonbinary Evaluations of the Ideas</head><p>In this paper we consider binary evaluations by the consumers. However, finer classifications may often be useful. For example, if an idea represents an opportunity for a niche product, it is likely to be very appealing to a smaller set of consumers. It is possible to generalize the approach presented in this paper to nonbinary classifications of the ideas. For example, let us assume that we introduce three categories labeled "very good," "good," and "not good," and that our goal is to identify the 5 ideas most likely to be judged as very good and the 10 ideas most likely to be judged as good. Solving this problem could be viewed as solving two binary classification problems in parallel. A set of scores s 1 i and s 2 i could be assigned to each idea for each of the two problems. 14 Different criteria could then be used to select the ideas presented to the next consumer. For example, they could be the k ideas with the highest scores with respect to the very good category and the k − k ideas with the highest scores with respect to the good category. Alternatively, they could be the k ideas with the highest average scores across the two classifications, or with the highest maximum scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Proof of the Proposition</head><p>Proposition. Consider the set of ideas classified as top ideas that would remain classified as top after one positive or negative evaluation, and the set of ideas classified as bottom ideas that would remain classified as bottom after one positive or negative evaluation:</p><p>Each myopic approximation assigns the same score to all ideas in the set .</p><p>Proof. Let us consider an idea i ∈ . Idea i is such that the identity of the top m ideas will be unchanged after one additional evaluation on that idea. Let us first consider the myopic maximization of hit rates. We have:</p><p>Hence,</p><p>Hence, all ideas in have the same score.</p><p>Let us now consider the myopic maximization of the average true probability of the estimated top m. The expected posterior probability associated with an idea after an evaluation is (the first and second term correspond to the updating of the estimated probability, respectively, after a positive and a negative evaluation):</p><p>Hence, the expected value of the objective function obtained after collecting one additional evaluation on an idea is unchanged unless the summation 1/m p i i∈ i 1 i m is performed over a different set i 1 i m before and after the evaluation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intelligent mutation rate control in canonical genetic algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bäck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Internat. Sympos. Methodologies for Intelligent Systems</title>
				<meeting>Internat. Sympos. Methodologies for Intelligent Systems</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic Programming and Optimal Control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Some statistical and logical considerations when rescoring tests</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bradlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="713" to="728" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparison of algorithms for constructing exact D-optimal designs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Nachtsheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="315" to="324" />
			<date type="published" when="1980-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prediction in marketing using the support vector machine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Curry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="595" to="615" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Product development-Managing a dispersed process</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Marketing</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Weitz</surname></persName>
			<persName><forename type="first">R</forename><surname>Wensley</surname></persName>
		</editor>
		<meeting><address><addrLine>Thousand Oaks, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Sage Publications Inc</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The virtual customer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Product Innovation Management</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="332" to="354" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bacon with your eggs? Applications of a new bivariate beta-binomial distribution</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Danaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G S</forename><surname>Hardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. Statistician</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lateral Thinking: A Textbook of Creativity</title>
		<author>
			<persName><forename type="first">De</forename><surname>Bono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>Ward Lock Educational</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computer brainstorms: More heads are better than one</title>
		<author>
			<persName><forename type="first">De</forename><surname>Bono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Boston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Valacich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Psych</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
	<note>Six Thinking Hats</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized robust conjoint estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boussios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zacharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Federov</surname></persName>
		</author>
		<title level="m">Theory of Optimal Experiments</title>
				<editor>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Studden</surname></persName>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Klimko</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Varying the probability of mutation in the genetic algorithm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fogarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Internat. Conf. Genetic Algorithms</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</editor>
		<meeting>3rd Internat. Conf. Genetic Algorithms<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Why companies need your ideas-How they&apos;re tapping customers to develop new products</title>
		<author>
			<persName><surname>Forbes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-02-14" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unblocking Brainstorms</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Gallupe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bastianutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Psych</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="142" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Electronic brainstorming and group size</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Gallupe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Valacich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bastianutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Management J</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="350" to="369" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Genetic Algorithms in Search, Optimization, and Machine Learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Creativity in Product Innovation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazursky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The idea itself and the circumstances of its emergence as predictors of new product success</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazursky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Marketing Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="360" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward identifying the inventive templates of new products: A channeled ideation approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazursky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="200" to="210" />
			<date type="published" when="1999-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Creativity templates: Towards identifying the fundamental schemes of quality advertisements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazursky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="351" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Synectics The Development of Creative Capacity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J J</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Collier-Macmillan</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The impact of utility balance and endogeneity in conjoint analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="507" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Research on innovation: A review and agenda for Marketing Science</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards an optimal mutation probability in genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Männer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Parrallel Problem Solving from Nature</title>
				<meeting>1st Parrallel Problem Solving from Nature<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Investigation of the m-heuristic for optimal mutation probabilities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Männer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Parrallel Problem Solving from Nature</title>
				<meeting>2nd Parrallel Problem Solving from Nature<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient experimental design with marketing research applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Kuhfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="1994-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An Introduction to Genetic Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Facilitating group creativity: Experience with a group decision support system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Applegate</surname></persName>
		</author>
		<author>
			<persName><surname>Konsynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Management Inform. Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5" to="19" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Applied Imagination, rev</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Osborn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Scribner</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">What do we know about new product idea selection. Working paper, Center for Innovation in Management Studies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ozer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Hong Kong</pubPlace>
		</imprint>
		<respStmt>
			<orgName>City University of Hong Kong</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Practice of Creativity; A Manual for Dynamic Group Problem Solving</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Prince</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>Harper &amp; Row</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">User design of customized products</title>
		<author>
			<persName><forename type="first">T</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Terwiesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Ulrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="280" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The rise of the creative consumer-The future of innovation</title>
		<imprint>
			<date type="published" when="2005-03-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Idea generation, creativity, and incentives</title>
		<author>
			<persName><forename type="first">O</forename><surname>Toubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="411" to="425" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Design and Marketing of New Products</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Idea generation in computer-based groups: A new ending to an old story</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Valacich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Connolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organ. Behav. Human Decision Processes</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="448" to="467" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sticky information&quot; and the locus of problem solving: Implications for innovation</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Hippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Economics of product development by users: The impact of &quot;sticky&quot; local information</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Hippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="629" to="644" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
