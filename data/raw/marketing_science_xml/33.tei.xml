<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Listening In: Extracting Brand Image Portrayed on Social Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-10">July 10, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
							<email>liu.liu-1@colorado.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Leeds School of Business</orgName>
								<orgName type="institution">University of Colorado</orgName>
								<address>
									<postCode>80309</postCode>
									<settlement>Boulder</settlement>
									<region>Colorado</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daria</forename><surname>Dzyabura</surname></persName>
							<email>ddzyabura@nes.ru</email>
							<affiliation key="aff1">
								<orgName type="department">New Economics School</orgName>
								<address>
									<postCode>121353</postCode>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natalie</forename><surname>Mizik</surname></persName>
							<email>nmizik@uw.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Foster School of Business</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>Washington</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Listening In: Extracting Brand Image Portrayed on Social Media</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0732-2399 (print)</idno>
						<imprint>
							<date type="published" when="2020-07-10">July 10, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1287/mksc.2020.1226</idno>
					<note type="submission">Received: May 8, 2017 Revised: September 30, 2018; August 24, 2019; November 19, 2019 Accepted: November 24, 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-07-06T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>social media</term>
					<term>visual marketing</term>
					<term>brand perceptions</term>
					<term>computer vision</term>
					<term>machine learning</term>
					<term>deep learning</term>
					<term>transfer learning</term>
					<term>big data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images are close to surpassing text as the medium of choice for online conversations. They convey rich information about the consumption experience, attitudes, and feelings of the user. In this paper, we propose a "visual listening in" approach (i.e., mining visual content posted by users) to measure how brands are portrayed on social media. We develop BrandImageNet, a multi-label deep convolutional neural network model, to predict the presence of perceptual brand attributes in the images consumers post online. We validate BrandImageNet model performance using human judges and find a high degree of agreement between our model and human evaluations of images. We apply the BrandImageNet model to brand-related images posted on social media to extract brand portrayal based on model predictions for 56 national brands in the apparel and beverages categories. We find a strong link between brand portrayal in consumer-created images and consumer brand perceptions collected through traditional survey tools. Firms can use the BrandImageNet model to automatically monitor their brand portrayal in real time and better understand consumer brand perceptions and attitudes toward their and competitors' brands.</p><p>History: K. Sudhir served as the senior editor and Olivier Toubia served as associate editor for this article.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A profound shift has recently occurred in how people consume information and in the origins of the information itself. Corporations and traditional media have lost their monopoly on information creation and communication channels. Much of the brand-related content is now created and spread through social media platforms (e.g., Twitter and Instagram), user discussion forums, blogs, and so on. Firms now operate in an environment in which customers are actively participating in sharing their brand perceptions and experiences on social media and co-creating brands and brand identities. With this broader, more egalitarian distribution model, monitoring how a brand is portrayed on social media is essential for effective brand management.</p><p>In this paper, we focus on consumer-created visual content. With the proliferation of camera phones, generous data plans, and image-based social media platforms, photo taking and sharing have become an important part of consumers' social lives <ref type="bibr" target="#b9">(Diehl et al. 2016)</ref>. Images are becoming an increasingly prevalent form of online conversations. Instagram has 1 billion monthly active users, and these users share an average of 95 million photos daily. <ref type="bibr">1</ref> In these shared photos, consumers often tag brands. For example, a search of #nike on Instagram returns over 92 million photos tagged with the brand name Nike. <ref type="bibr">2</ref> Through these images, consumers communicate about brands with each other.</p><p>Consumer-created brand images on social media are different from product images on retailers' websites. In the images posted on social media, consumers often link brands with usage context, feelings, and consumption experiences. For example, consider the images in panels (a) and (b) in Figure <ref type="figure" target="#fig_0">1</ref>. The first image is hashtagged with eddiebauer and the second with prada. Panel (a) shows a person wearing Eddie Bauer and resting on a rock, facing mountains in the distance. <ref type="bibr">Panel (b)</ref> shows the user taking a picture of herself wearing Prada sunglasses and bright red lipstick. The two images differ in terms of their content (mountainous landscape versus head shot) and their visual properties <ref type="bibr">(color palate, contrast, etc.)</ref>. They also differ in terms of the mood and experience consumers may associate with the two brands. The images created and shared by consumers offer a new avenue to study brand associations and consumers' brand perceptions. Firms need to monitor how consumers are portraying their brands on social media, or risk missing a large part of consumer brand conversations.</p><p>Academic researchers and practitioners have developed a set of tools for monitoring social media and listening in on consumers' online brand conversations. The focus so far has been on textual content. However, given that images are on their way to surpassing text as the medium of choice for online conversations, and the rich information about the consumption experience and feelings conveyed through images, monitoring visual content is important to get a more complete understanding of brand-related online conversations.</p><p>We introduce a "visual listening in" approach for monitoring visual brand content created and shared by consumers on social media. We develop and validate a model, BrandImageNet, to allow firms to monitor their brand portrayal on social media and evaluate it relative to competitors' and to the firm's preferred brand positioning. The BrandImageNet model maps images to specific perceptual attributes (e.g., glamorous, rugged) and allows us to measure how brands are portrayed along these attributes. For example, is Prada portrayed as a glamorous brand and is it portrayed as more glamorous than Eddie Bauer? We focus on identifying perceptual brand attributes from images, which is different from identifying functional attributes of the product <ref type="bibr" target="#b12">(Gardner and Levy 1955</ref><ref type="bibr" target="#b29">, Park et al. 1986</ref><ref type="bibr" target="#b20">, Keller and Lehmann 2006</ref>. Positioning brands on intangible attributes is at the core of many marketing strategies because it allows firms to differentiate themselves from one another in categories with functionally similar products. In categories such as beverages and apparel (our focus categories in this paper) consisting of brands offering products with similar functionality, the feelings, perceptions, and attitudes consumers have toward the brand often make a big difference. The intangible brand attributes allow consumers to choose the brand that feels most appropriate to them and to express themselves through that brand.</p><p>We develop our BrandImageNet model using a deep-learning framework to predict whether a particular perceptual attribute is expressed in a given image. We focus on apparel and beverages, two product categories for which consumers frequently post and tag photos online, and explore four brand attributes that are highly relevant for brands in these two categories: glamorous, rugged, healthy, and fun. We selected these four attributes from Young and Rubicam's (Y&amp;R) BAV (BrandAsset Valuator) consumerbrand-perception survey <ref type="bibr" target="#b27">(Mizik and Jacobson 2008)</ref> to allow subsequent comparisons of our model-based findings with external survey-based findings. We selected these specific four brand attributes because they are meaningful differentiators in the apparel and beverage sectors (they have a higher range of scores across brands in these sectors according to the BAV survey).</p><p>One challenge with studying images is that image data are unstructured. To draw any inferences, a first and important step is to create a meaningful representation of the images. Typically, doing so would require creating a feature vector summarizing the image data. An emerging area of machine learning, called deep learning, uses "deep" neural networks to simultaneously learn both the model parameters and the feature representation needed for prediction tasks. It works with raw image data directly and automatically learns the representation of data with multiple levels of abstraction <ref type="bibr" target="#b22">(LeCun et al. 2015)</ref>. Our BrandImageNet is a multi-label convolutional neural network (ConvNet), fine-tuned from the Berkeley Vision and Learning Center (BVLC) Reference Caf-feNet model on a set of Flickr images we collected for brand attribute identification. It achieves a high average out-of-sample accuracy of 90% and an average AUC (area under the receiver operating characteristic curve) of 0.85.</p><p>We apply our BrandImageNet model to consumercreated and firm-created brand images shared on Instagram. We collect a set of images hashtagged with brand names for a total of 56 brands in two product categories: apparel and beverages. Using the BrandImageNet model, we compute the probability of attribute presence in each brand image. Then, for each brand and each of the brand attributes we study, we compute an image-based brand image (IBBI) metric as the average probability of attribute presence across all consumer-generated images of this brand. A higher score indicates more images portray the brand as having a given brand attribute. We undertake Liu, Dzyabura, and Mizik: Visual Listening In validation analyses and show that our model predictions have a high degree of agreement with human judges evaluating images and that the IBBI metric reflects consumer brand perceptions. Specifically, we find strong positive correlation between IBBI and brand perception scores from survey data (Y&amp;R BAV survey and our online brand perceptions survey of active Instagram users). These results show that consumers' brand portrayal on social media reflects and is indicative of consumer brand perceptions. Consumercreated brand images contain valuable brand-related information, and our model is able to pick it up.</p><p>Our study makes both a methodological and substantive contribution to the marketing literature. We propose a new approach to measure consumers' brand perceptions from the voluble stream of public consumer-generated brand imagery. We validate the model and demonstrate its performance through tests against human judgments and surveys of consumer brand perceptions. This study is one of the first to use a deep-learning approach in marketing. Deep-learning methods are becoming more popular and have significantly improved performance of many computer vision tasks (e.g., object detection); however, to the best of our knowledge, no extant work on predicting perceptual brand attributes from images exists. This study is also one of the first in marketing to systematically study consumer-created imagery at a large scale.</p><p>Substantively, our study establishes a link between consumer-created images and brand perceptions. Past research has examined how firms use imagery, such as logos and image advertisements, to position brands and how visual stimuli influence consumer behavior <ref type="bibr" target="#b36">(Wedel and Pieters 2007)</ref>. We show that visual content generated by consumers reflects brand perceptions. We present evidence showing that consumer-created brand images contain valuable brand information. Our approach goes beyond the practice of simply counting the frequency of brand logo presence in the images. Many firms have started to look into consumer-created visual content on social media, but most efforts are focused on brand logo detection and tracking brand mentions. We show that deeper insights can be extracted from consumer-created images. Firms can use our BrandImageNet model to monitor their brand portrayal and examine the effectiveness of their positioning strategies. Because our BrandImageNet model can be easily extended to include other attributes and is highly scalable, brands can use it to track relevant brand perceptions automatically and in real time from images posted on social media.</p><p>The rest of the paper is organized as follows. First, we discuss related literature in both marketing and computer science. Then, we introduce our Brand-ImageNet model and discuss its architecture, training process, and the data we use to train it. Next, we apply our model to brand-related images posted on Instagram to measure consumers' brand portrayal and demonstrate its performance using human judges and survey instruments. We conclude with a discussion of limitations and directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This paper is related to four streams of research in marketing and computer science: user-generated content (UGC), visual marketing, computer vision, and deeplearning applications in marketing. In this section, we review how we contribute to all four.</p><p>Mining UGC for online marketing intelligence is becoming increasingly popular in both marketing and computer science research, as well as among practitioners. In the marketing literature, <ref type="bibr" target="#b28">Netzer et al. (2012)</ref> propose an approach to help firms understand market structure and monitor the topics discussed in relation to their brand by mining brand associations from consumer-generated content on forums. <ref type="bibr" target="#b5">Culotta and Cutler (2016)</ref> measure consumers' brand perceptions by mining the brands' social connections through social networks on Twitter. Several papers investigate the relationship between consumer-generated online content (e.g., product reviews and ratings) and sales (e.g., <ref type="bibr" target="#b4">Chevalier and Mayzlin 2006</ref><ref type="bibr" target="#b24">, Liu 2006</ref><ref type="bibr" target="#b0">, Archak et al. 2011</ref>. The focus so far has been on text. Only recently have marketing researchers begun to consider visual content <ref type="bibr" target="#b43">(Zhang and Luo 2019</ref><ref type="bibr" target="#b44">, Zhang et al. 2018</ref><ref type="bibr" target="#b30">, Pavlov and Mizik 2019</ref>. We propose leveraging the ever-increasing flow of consumergenerated images available online for marketing research. We contribute to this literature by advancing a new approach to measuring consumers' brand perceptions, using images that consumers post on social media.</p><p>Images constitute a large part of consumer online conversations. With the exclusive focus on text content, a significant portion of online brand conversations is not being "heard." Indeed, many consumers, particularly millennials, prefer visual-based online communication, such as Snapchat or Instagram, to text. Further, for some consumer categories (e.g., beverages, apparel), visual content is more easily available than text content. Consider, for example, the beverages category. Few people would write a review of bottled water or orange juice, but many people tag Fiji Water, Tropicana, and Minute Maid on Instagram. Finally, text mining of UGC tends to recover mostly functional product attributes. <ref type="bibr" target="#b28">Netzer et al. (2012)</ref>, for example, recover topics in a car discussion forum related to functional attributes such as powertrain warranty, good mileage, suspension noise, and emergency brakes. Similarly, <ref type="bibr" target="#b33">Timoshenko and Hauser (2019)</ref> recover functional needs for oral care <ref type="bibr">Mizik: Visual Listening In Marketing Science, 2020, vol. 39, no. 4, pp. 669-686, © 2020 INFORMS</ref> products, such as "an oral-care product that does not affect my sense of taste" and "an oral care product that is quiet." <ref type="bibr" target="#b34">Tirunillai and Tellis (2014)</ref> recover attributes such as ease of use, reliability, portability, safety, and comfort. We demonstrate that recovering nonfunctional dimensions of brand image from visual content is possible. Images may convey information about the usage situation, the setting, mood, and feelings associated with the product. Indeed, although a consumer is unlikely to write that he/she is feeling rugged wearing Levi's jeans, a photograph tagged with Levi's may convey ruggedness.</p><p>Our work also contributes to the literature on visual marketing, which has studied how consumers perceive different visual stimuli. Firms have been using visual stimuli to shape consumer brand perceptions through brand logos, advertisements, retail-store decorations, and, more recently, social media. <ref type="bibr" target="#b31">Raghubir and Greenleaf (2006)</ref> examine how geometry, specifically, the ratio of the sides of a rectangular product or package, affect consumers' purchase intentions and preferences. <ref type="bibr" target="#b42">Zhang et al. (2009)</ref> model the effects of feature-ad design characteristics (e.g., ads' surface size and number of supporting colors) on sales of the feature product and investigate the mediating role of attention. <ref type="bibr" target="#b37">Wedel and Pieters (2014)</ref> investigate how color schemes affect rapid gist perception of an ad, product category, and brand recognition under brief and blurred exposure conditions. They find that the color of the central diagnostic object in the ad plays a key role in protecting gist perception. <ref type="bibr" target="#b38">Xiao and Ding (2014)</ref> find that faces in print ads affect ad effectiveness. Our analysis contributes to this literature by demonstrating that visual content can convey perceptual brand attributes. We find that brand images created and shared by consumers reflect their brand perceptions.</p><p>Mining visual UGC requires different tools than mining text UGC. We draw on image-classification methods developed in the computer vision and deep learning literature. Until recently, much of this literature has focused on object identification and recognition. The field of computer vision was originally developed for engineering applications, such as automatic product inspection, autonomous vehicle navigation, surveillance, and medical image processing to aid diagnostics. More recently, researchers have begun developing tools for predicting image aesthetics, image style or genre, image interestingness (e.g., <ref type="bibr" target="#b18">Jiang et al. 2013</ref>, and visual sentiment <ref type="bibr" target="#b13">(Giannakopoulos et al. 2015)</ref>. Our work is closely related to this stream of research on predicting abstract constructs from images. We study a new problem: measuring perceptual brand attributes from images. We solve the problem using deep convolutional neural network architecture, which is a type of deep neural network <ref type="bibr" target="#b22">(LeCun et al. 2015)</ref>. Deep-learning methods are well suited for marketing problems, because of their ability to handle unstructured data, scalability, and superior predictive performance. As marketing researchers begin to adopt machine-learning methods (e.g., <ref type="bibr" target="#b43">Zhang and Luo 2019</ref><ref type="bibr" target="#b11">, Gabel et al. 2019</ref><ref type="bibr" target="#b23">, Liu et al. 2019</ref>, ours is among the first applications of deep-learning methods to marketing problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Machine-Learning-Based Identification of Brand Attributes in Images</head><p>To measure how brands are portrayed along particular attributes on social media, we need a scalable method to determine whether a particular brand attribute is expressed in a given image: Does the image look rugged? Fun? Glamorous? Healthy? In fact, an image can depict multiple brand attributes. An image can, for example, look both rugged and fun. This attribute identification problem can be formulated as a multi-label image-classification task-that is, classifying images into multiple brand attributes-where each image may depict more than one attribute simultaneously. To solve this problem, we need (1) an annotated data set of images labeled with brand attributes to use for training, and (2) an algorithm that learns the underlying function mapping images to brand attributes. In this section, we describe how we created a data set of labeled images and developed our multi-label convolutional neural network model, BrandImageNet, for identifying perceptual attributes in an image. Later, we apply our brand attribute identification method to brands in the apparel and beverages categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>Training a multi-label image-classification model of brand attributes requires an annotated data set consisting of images labeled with respect to whether they express each attribute. Unlike object-detection tasks, which are typically trained on large public data sets of labeled images, no existing data set is annotated with perceptual brand attributes. Thus, we created one. We gathered an annotated set of images from Flickr, an online photo-sharing website. Users share photos on Flickr, label their uploaded photos with titles and descriptions, and provide free-form tags. Flickr has been used as a data source in previous visual and social network research (e.g., <ref type="bibr" target="#b7">Dhar et al. 2011</ref><ref type="bibr" target="#b26">, McAuley and Leskovec 2012</ref><ref type="bibr" target="#b40">, Zhang et al. 2012</ref>. Flickr lends itself well to gathering an annotated set, because unlike some other social media platforms (e.g., Instagram), it provides a search engine that returns the most relevant images for a keyword. The search is based on text labels provided by users, image content, and clickstream data <ref type="bibr" target="#b32">(Stadlen 2015</ref>). An image ranked at the top for a particular query has Liu, Dzyabura, and Mizik: Visual Listening In often been validated by tens of thousands of users who clicked on the image, reflecting a large population consensus regarding a strong association between the image and the query term. Because recognizing brand attributes from images is a more subjective task than recognizing objects from images, using annotations based on a large population consensus is important to decrease noise in the labels data. <ref type="bibr">3</ref> For each attribute, we queried the attribute term on the Flickr search engine and collected about 2,000 images in the top search results. We use these images as our positive instances (i.e., images that express the brand attribute). We also needed to collect negative instances for each attribute consisting of images that do not express that brand attribute (images that are not rugged, not glamorous, etc.). We need both the positive and negative images for the algorithm to learn how to separate them. To gather negative images, we queried the antonyms of our attributes in the Flickr search engine (drab for glamorous, gentle for rugged, unhealthy for healthy, dull for fun) and again collected about 2,000 images in the top search results. To capture a wider spectrum of the negative cases, we also used the images collected for the other attributes and their antonyms as negative examples of a given brand attribute, if they were not already included in the positive image set of that brand attribute. For example, for the attribute healthy, we used "healthy" as the query to collect positive instances. For negative instances, we used images that were returned for the query "unhealthy" and images that were returned for all other terms (glamorous, rugged, fun, drab, gentle, dull) but were not in the "healthy" set. The entire annotated set has a total of 16,360 images. Each image is associated with four labels for the four brand attributes that we study. Figure <ref type="figure" target="#fig_1">2</ref> shows one sample image returned by Flickr for each queried brand attribute and its antonym.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BrandImageNet Model</head><p>Next, we built and trained a multi-label imageclassification model that maps images to perceptual brand attributes.</p><p>Image classification is challenging because the raw input data are difficult to capture quantitatively-more difficult than other unstructured data, such as text. Text consists of words, which can be grouped by topic or classified into positive or negative valence. Images, on the other hand, are composed of pixels, and a single pixel in isolation does not lend itself to meaningful interpretation. Therefore, any modeling of image data requires creation of a meaningful representation of the images. It requires creating a set of features, or predictor variables, through some complex transformations of the raw pixel data. The process of defining features is a critical step in achieving good predictive performance. However, manual feature engineering is hard, time-consuming, and requires great domain knowledge, and there is no systematic approach that would guarantee finding features that best characterize the underlying problem. Thus, we adopt a deep-learning approach <ref type="bibr" target="#b22">(LeCun et al. 2015)</ref>.</p><p>Deep learning does not require human input to manually engineer image features to represent raw image data. It is a representation learning method, meaning it automatically optimizes the feature-extraction step for the prediction problem. It learns the features and trains the classifier over these features simultaneously, by minimizing classification error. A deep neural network does so by applying several "layers" of simple nonlinear transformations of the raw data. The result, or output, of each transformation serves as the  <ref type="bibr">Mizik: Visual Listening In Marketing Science, 2020, vol. 39, no. 4, pp. 669-686, © 2020 INFORMS</ref> input into the next transformation layer. Through multiple layers of such transformations, the network extracts higher-and higher-level representations of the data, allowing the final layer to easily classify the data. For example, working with image data, lower layers of a deep-learning model may extract edges and textures, whereas higher layers detect motifs, object parts, and complete objects <ref type="bibr" target="#b16">(Goodfellow et al. 2016)</ref>. The final layer maps the resulting features onto the target variables with a classification function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">A Multi-Label Convolutional Neural Network.</head><p>We set up a multi-label convolutional neural network to identify multiple brand attributes from images. ConvNet lends itself well to this problem. It is a stateof-the-art method most commonly used for analyzing visual imagery. It has demonstrated an outstanding performance in the computer vision field for tasks such as object detection and classification, as well as in other fields (e.g., medical image analysis). ConvNets vary in number of layers and each layer's type, parameters, and hyperparameters. A typical ConvNet involves four types of layers: convolutional, activation, pooling, and fully connected layers.</p><p>The architecture of BrandImageNet, our multi-label ConvNet, is depicted in Figure <ref type="figure" target="#fig_2">3</ref>. The input into the network is an image, which is 227 pixels wide and 227 pixels tall. Each pixel is represented in the RGB system by three integers ranging from 0 (black) to 255 (white), reflecting the contribution of each of the three (red, green, blue) color channels. <ref type="bibr">4</ref> The image is represented by a three-dimensional matrix and a total of 154,587 (227 × 227 × 3) numbers. The output of the network is an array of k numbers, with each number representing the probability of the image expressing a particular brand attribute. <ref type="bibr">5</ref> In our case, k = 4, and the output contains the probabilities of the image being glamorous, rugged, healthy, and fun. The brandattribute labels are not mutually exclusive (i.e., an image can be classified as positive on multiple attributes).</p><p>Our multi-label ConvNet model maps the input, an image (the original matrix of pixel values), to the output, the probability scores, through eight trainable layers (five convolutional layers and three fully connected layers) and 15 nontrainable layers (ReLU activation, max pooling, normalization, dropout, and sigmoid layers). A trainable layer contains model parameters to be learned and hyperparameters to be set. The nontrainable layers only contain hyperparameters to be set. We describe each type of layer in detail.</p><p>The architecture of our network is based on and is modified from the Berkeley Vision and Learning Center (BVLC) Reference CaffeNet model, which is an implementation, with minor variations, of the widely used AlexNet <ref type="bibr" target="#b21">(Krizhevsky et al. 2012</ref>). <ref type="bibr">6</ref> Both AlexNet and BVLC Reference CaffeNet models are designed to classify a given image as one of 1,000 different objects (e.g., a tree, a ship, etc.) and have been trained on the ImageNet data set of 1.2 million images <ref type="bibr" target="#b6">(Deng et al. 2009)</ref>. One key difference between our model and the underlying BVLC Reference CaffeNet model is in the final layer of the network: ours is a four-sigmoid function rather than a 1,000-way softmax function, because ours is a multi-label classification problem, whereas the BVLC Reference CaffeNet is a 1,000-class object classification model in which each image can belong to one class only.</p><p>3.2.1.1. Convolutional Layer. Convolutional layers form the core of ConvNet architecture. Each convolutional layer extracts spatial local features from input data (an image for the first layer and the output of previous layer for convolutional layers 2-5) to generate multiple feature maps as output of convolution operation. Notes. Conv stands for convolutional layer. FC stands for fully connected layer. The core architecture is based on and is modified from the BVLC Reference CaffeNet model. Our hyperparameters in Conv1-Conv5 and FC6-FC7 are set to the values specified in and the parameters to be estimated are initialized with the learned parameter estimates of the BVLC Reference CaffeNet model. Parameters in the final FC8 layer are initialized with random values.</p><p>Liu, Dzyabura, and Mizik: Visual Listening In Specifically, each convolutional layer convolves the input data with a set of kernels. A kernel is often interpreted as a filter: A kernel filters the input to extract certain kinds of features. Each kernel is convolved with the input data by sliding across the width and height of the input data and computing dot products between the kernel and local regions of the input. The output of this convolution is a two-dimensional feature map, where each unit in the feature map is the result of a dot product between the kernel and a specific local region of the input that the unit connects to. Convolutional layer l with d l different kernels outputs d l feature maps, where each feature map represents a certain type of local feature extracted. For example, the first convolutional layer in our network has 96 kernels and outputs 96 two-dimensional feature maps.</p><p>A convolutional layer l is defined by several hyperparameters: number of kernels d l , size of each kernel's weight matrix (k l , k l , d l−1 ), padding p l , and stride s l . <ref type="bibr">7</ref> The padding p l determines the number of zeros to pad around the border of the input before the convolution operation. The stride s l determines the number of pixels the kernel slides, horizontally or vertically, between applications of the kernel to the input image during convolution operation. For example, s l = 2 means that the kernel will be applied to every other pixel. We set these hyperparameters to values specified in the BVLC Reference CaffeNet model. The weight matrices {K l 1 , . . ., K l d l } and biases {b l 1 , . . ., b l d l } of the d l kernels are the parameters of the convolutional layer l we estimate.</p><p>The convolutional layer l outputs Y l , consisting of d l two-dimensional feature maps of size H l × W l , where</p><formula xml:id="formula_0">H l (H l−1 − k l + 2p l )/s l + 1andW l (W l−1 − k l + 2p l )/s l + 1.</formula><p>The ith feature map, denoted Y l i , is the result of the convolution of the ith kernel and input from previous layer Y l−1 , defined as</p><formula xml:id="formula_1">Y l i ∑ d l−1 j 1 Y l−1 j * K l ij + b l i ,</formula><p>where K l i is the weight matrix and b l i is the bias of the ith kernel, j indexes the depths of the input data and kernel matrix K l i , and * denotes convolution. The unit at position (m, n) of the ith feature map, denoted</p><formula xml:id="formula_2">Y l i (m, n), is computed as Y l i (m, n) ∑ d l−1 j 1 ∑ k l q 1 ∑ k l r 1 Y l−1 j (q + (m − 1) × s l − p l , r + (n − 1) × s l − p l ) • K l ij (q, r) + b l i .</formula><p>For example, the size of the kernels in the first convolutional layer is <ref type="bibr">(11,</ref><ref type="bibr">11,</ref><ref type="bibr">3)</ref>. A unit in a feature map is a dot product of a kernel and an 11 × 11 × 3 region in the input image it connects to. Convolutional layers utilize parameter sharing (i.e., units in the same output feature map share the same kernel weights and biases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.2.">ReLU Activation</head><p>Layer. An activation layer takes the input from the previous layer and outputs activation maps by applying an element-wise nonlinear activation function over the input volume.</p><p>A ReLU activation layer uses a rectified linear unit as its activation function, defined as</p><formula xml:id="formula_3">ReLU(x) max(0, x).</formula><p>The activation function is an element-wise operation. Thus, the dimensions of the output and the input are the same. In our model, all five convolutional layers and the first two fully connected layers are followed by a ReLU layer. A ReLU layer does not change data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.3.">Max Pooling Layer.</head><p>A pooling layer, sometimes called a subsampling layer, is often inserted between convolutional layers to progressively reduce the spatial size (height and width) of the data representation. It helps reduce the number of model parameters to be learned and control overfitting. It also helps make the extracted feature maps robust to small image distortions.</p><p>A max pooling layer applies a max pooling function to down-sample the input data. Similar to convolutional layers, it has hyperparameters defined by the size of the max filter (k l , k l ) and stride s l . For each depth slice of the input data, it slides spatially and computes a max over local neighborhoods of size (k l , k l ) and disposes of the rest, thus down-sampling the input data. For example, a max pooling layer with a max filter of size (2, 2) and a stride of 2 will downsample an input of size (H, W, D) to an output of size (H/2, W/2, D). Note that as it operates independently on every depth slice of the input, the depth of the output is the same as that of the input. In our model, the output of the first, second, and fifth ReLU layers passes a max pooling layer.</p><p>3.2.1.4. Fully Connected Layer. Following the convolutional layers are three fully connected layers. Unlike convolutional layers where each unit is connected to a local region of data from the previous layer, units in a fully connected layer are connected to the entire previous layer. The unit i in a fully connected layer l is defined as the dot product of the entire output from the previous layer Y l−1 and a unique weight matrix W i , plus a bias term b i : <ref type="bibr">Mizik: Visual Listening In Marketing Science, 2020, vol. 39, no. 4, pp. 669-686, © 2020 INFORMS</ref> Each unit in a fully connected layer is associated with a different weight matrix and bias offset. For example, the final fully connected layer FC8 contains four units. Each of these four units has a different bias term and weight vector (it is a vector because the size of the input from the previous layer is (4,096, 1)).</p><formula xml:id="formula_4">y l i Y l−1 • W i + b i .</formula><p>The resulting dot products from fully connected layers FC6 and FC7 are passed through a ReLU layer to add nonlinearity. The output of the fully connected layer FC8 is fed into the final Sigmoid8 layer for final multi-label classification.</p><p>3.2.1.5. Sigmoid Layer. The final Sigmoid layer is an activation layer. It takes the input from the previous layer and applies a sigmoid function to each element of the input to produce an output between 0 and 1:</p><formula xml:id="formula_5">Sigmoid(x) 1 1 + e −x .</formula><p>In our network, it takes the four output units from FC8 and produces final predicted probabilities for our four brand attributes.</p><p>3.2.1.6. Other Layers. We have also added normalization layers (to limit the unbounded activation from ReLU activation layers) and dropout layers (to reduce the number of model parameters) after fully connected layers FC6 and FC7 for regularization purposes to reduce overfitting, as in <ref type="bibr" target="#b21">Krizhevsky et al. (2012)</ref>. The appendix offers full details of our network architecture and hyperparameters of each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Model</head><p>Training with Fine-Tuning. Convolutional layers and fully connected layers are trainable layers with model parameters to be learned. These parameters are the kernel matrices and biases {K, B c } in the convolutional layers and the weight matrices and biases {W, B f } in the fully connected layers. We train the model by minimizing a cross-entropy loss function using stochastic gradient descent:</p><formula xml:id="formula_6">L ( K, B c , W, B f ) − 1 N ∑ n ∑ a∈A [ y a n log ( Prob ( y a n 1 )) + ( 1 − y a n ) log ( 1 − Prob ( y a n 1 ))] ,</formula><p>where N is the number of training images, A {glamorous, rugged, healthy, fun}, y a n is the label of the image n for attribute a (1 for positive training examples and 0 for negative training examples), and Prob ( y a n 1) is the probability of the nth image expressing attribute a.</p><p>A challenge with training a ConvNet model is that it requires very large sets of training data, because both the kernel and the weight matrices have high dimensionality and can have millions of parameters. Training the model from scratch (i.e., initializing all the parameters with random numbers) with a small data set like ours can result in overfitting.</p><p>We adopt a fine-tuning approach to avoid the overfitting problem. That is, instead of initializing model parameters with random numbers, we initialize model parameters using learned parameters from the BVLC Reference CaffeNet model. <ref type="bibr">8</ref> The finetuning approach is a case of transfer learning <ref type="bibr" target="#b2">(Bengio et al. 2011</ref><ref type="bibr" target="#b1">, Bengio 2012</ref>: knowledge learned in one domain is transferred to help model learning in another domain. It is similar to using an informed prior in Bayesian estimation. Previous work has shown that fine-tuning a pretrained network can significantly increase model performance and avoid overfitting <ref type="bibr" target="#b10">(Donahue et al. 2014</ref><ref type="bibr" target="#b39">, Yosinski et al. 2014</ref>.</p><p>We fine-tune our BrandImageNet model on our data as follows. We initialize parameters of the five convolutional layers and the first two fully connected layers FC6 and FC7 with parameters learned from the BVLC Reference CaffeNet model. We fine-tune the parameters with a small learning rate of 0.0001 over 10,000 iterations. We initialize parameters of the final fully connected layer (FC8) with random values and train it from scratch on our data set with a larger learning rate of 0.001. We use a higher learning rate for the final layer to allow the parameters in the final layer to change quickly with our data. We use a smaller learning rate for earlier layers to preserve the parameters learned from the BVLC Reference CaffeNet model and to transfer their extracted features to our application.</p><p>We use the Caffe deep-learning framework <ref type="bibr" target="#b17">(Jia et al. 2014)</ref> to fine-tune our BrandImageNet model on a single K80 graphics processing unit node in a university's high-performance cluster. We use 80% of the images for model training, 10% of the images as a validation set to choose the best model snapshot, and the remaining 10% as a holdout sample for evaluation of model performance. We train the model for 10,000 iterations on the training set and choose the iteration at which the model has the best performance on the validation set to select our best model snapshot. We get the best model at iteration 2,800.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Evaluation</head><p>We evaluate the performance of our multi-label BrandImageNet model using its per-attribute prediction accuracy (with a 0.5 threshold) and AUC <ref type="bibr" target="#b3">(Bradley 1997)</ref>. The prediction accuracy is computed as the number of correctly predicted instances divided by the total number of instances. We set the predicted image label for the attribute a presence equal to 1, if the predicted probability of attribute a presence is greater than 0.5 and set it to 0 otherwise. <ref type="bibr">Liu, Dzyabura, and Mizik: Visual Listening In</ref> The AUC is the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance (i.e., assign a higher probability of being positive to the positive than to the negative). It evaluates how well a classifier ranks its predictions and separates two classes. The higher the AUC, the better the classifier at separating positive from negative instances. Unlike our per-attribute prediction accuracy measure which assesses model performance at a single classification threshold (0.5), AUC is a measure of aggregated model performance across all possible classification thresholds.</p><p>Table <ref type="table" target="#tab_0">1</ref> presents our model performance for the four perceptual brand attributes we study. Our multi-label ConvNet model achieves an average accuracy of 90% and an average AUC of 0.85, demonstrating good predictive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Brand Portrayal on Social Media</head><p>We have trained BrandImageNet, a multi-label ConvNet model, to ascertain the presence of four perceptual attributes in an image. We wish to apply this model to brand images posted on social media to understand consumer brand portrayal. In this section, we first show that our model trained on Flickr images can be applied to and performs well on Instagram data. Then, we show that brand portrayal in the consumer posts on social media reflects consumers' brand perceptions. Finally, we present an illustration: a case study of underwear brands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Instagram Brand Image Data</head><p>Instagram is an image-based social media platform and a popular communication medium. It was launched in 2010 and has one billion active monthly users who, on a daily basis, share an average of 95 million photos. Instagram users often hashtag brands, creating a collection of brand-related images.</p><p>We collected data for 56 large national brands covered in the Y&amp;R BAV survey <ref type="bibr" target="#b27">(Mizik and Jacobson 2008)</ref> in two product categories for which consumers frequently post photos: apparel and beverages. For each brand, we used Instagram's application programming interface to obtain consumer-created photograph posts hashtagged with the brand name. We filtered out spam, resale, and photos posted by the official account of the brand, and retained about 2,000 photographs for each brand. The final consumergenerated-image data set contains 114,367 photographs. All data were collected between May and October 2016.</p><p>We also collected firm-created brand images from the brands' official Instagram account pages, giving us a data set containing 67,863 images. Three beverage brands included in our consumer image data are not represented in this set because they did not have an official account on Instagram at the time of data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Application to Instagram Image Data</head><p>We use our multi-label BrandImageNet model to compute the probabilities that a given brand image expresses the four brand attributes we study. Then, we compute the average probability of attribute presence across all consumer-generated images for each brand and each attribute. We use these average probabilities as our metric of how a brand is portrayed by consumers. The higher this score, the more visual content portrays the brand as having a given attribute.</p><p>That is, let <ref type="bibr">(b)</ref> images hashtagged with brand name b. We apply our BrandImageNet model to each image n of brand b to compute the probability that a perceptual attribute a is expressed in this image n: <ref type="bibr">b)</ref> , a ∈ {glamorous, rugged, healthy, fun}. We compute the average of these probabilities and use it as our imagebased brand image (IBBI) metric. IBBI for brand b, attribute a is defined as follows: <ref type="bibr">b)</ref> .</p><formula xml:id="formula_7">{X (b) 1 , . . ., X (b) N (b) } be N</formula><formula xml:id="formula_8">Pr( y (b) n (a) 1|X (b) n ), n 1, . . ., N<label>(</label></formula><formula xml:id="formula_9">IBBI ba ∑ N (b) n 1 Pr ( y (b) n (a) 1 ⃒ ⃒ ⃒X (b) n ) N<label>(</label></formula><p>We compute IBBI for each of the four attributes, for the entire set of 56 brands. We also compute IBBI for each brand, using firm-created brand images to measure firms' brand portrayal. The full list of our estimates of both consumers' and firms' brand portrayal is provided in Tables OA1-OA4 in the online appendix. Consider, for example, Prada and Eddie Bauer brands. Based on consumer-created brand images, IBBI {Prada, glamorous} is 0.22, whereas IBBI {Eddie Bauer, glamorous} is 0.13. This difference is significant (T-stat = 13.5, p &lt; 0.001): consumers portray Prada as more glamorous than Eddie Bauer on Instagram. On the ruggedness attribute, IBBI {Eddie Bauer,rugged} is 0.18, which is significantly higher (T-stat = 14.3, p &lt; 0.001) than Prada's IBBI {Prada, rugged} of 0.08. The estimates we obtain match our intuition and align with the data pattern in the BAV survey. In the next section, we present a more formal evaluation of model and IBBI metric performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Validation on Instagram Image Data</head><p>We trained our model on Flickr data because doing so is cost-effective and provides reliable labels for brand attributes. However, to study consumer portrayal of brands on social media, we intend to use Instagram data, which might potentially be different from the Flickr data. Thus, we undertake a thorough validation of our BrandImageNet model application to Instagram images. First, we conduct small-sample validation tests of model performance using human judges and show that our model is applicable and performs well on Instagram data. Then, we consider our IBBI metric against two surveys of consumer perceptions (BAV, a large-scale survey of a nationally representative sample of U.S. adults, and the survey we conducted with active Instagram users) and show that the IBBI metric of brand portrayal on social media reflects consumer brand perceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Assessing Model Performance Against Human</head><p>Judges. First, we examine the degree of agreement between our model and human judges' identification of brand attributes in Instagram images. For each of our four brand attributes (glamorous, rugged, healthy, fun), we select a set of Instagram images classified by our multi-label BrandImageNet model as (1) "attribute is present" (50 images with the highest predicted probability), ( <ref type="formula">2</ref>) "attribute is not present" (50 images with the lowest predicted probability), and (3) "ambiguous" (50 images with around 50% predicted probability of reflecting a given attribute). We investigate these three groups of images to evaluate our model performance at different ranges of probability prediction. <ref type="bibr">9</ref> We present these images (total number of images = 600, 150 images per attribute) to U.S. We use Level 3 judges-described by the platform as the "smallest group of most experienced, highest accuracy contributors"-to assure quality. We collect 20 independent judgments for each attribute and each image. Each judge is asked to evaluate up to a maximum of 50 images and is compensated based on the number of total evaluations they submitted. Table <ref type="table" target="#tab_1">2</ref>, column 1, presents the AUC of our model using the majority opinion of human judges as the "ground truth." Column 2 presents a measure of agreement between the model and human judges' labeling of images. We compute agreement as the percentage of images for which the majority of human judges evaluating an image assigns this image the same label as our model (1 = attribute is present, 0 = attribute not present). To offer a relevant benchmark for the agreement measure, we also calculate and report in column 3 this measure for each individual human judge versus the majority vote of human judges. The average percentage of images for which an individual judge agrees with the majority of judges is very close and, on average, slightly lower than the BrandImageNet model versus humans' agreement numbers in column 2. Importantly, the Instagram data reveal the difficulty of identifying brand attributes from images and the heterogeneity in human evaluations of images: we have only 70 (of 600 studied) images with 100% agreement across 20 human judgments.</p><p>Table <ref type="table" target="#tab_2">3</ref> presents our measure of agreement between the model and human judges for the three types of images we selected (attribute present/ambiguous/ attribute not present). We find high levels of agreement for unambiguous images: agreement between the model and human judgments is notably higher in groups 1 and 3, where the presence or absence of an attribute is more pronounced. For the ambiguous images in group 2, where the model-based probability score for an attribute presence is close to 50%, Notes. Data are based on 600 images (150 images for each attribute) and 12,000 total judgments (20 judgments for each image and attribute). The model-based label is equal to 1 if the model-based probability estimate for attribute presence is greater than 50%, and 0 otherwise. The humanbased label for an image is equal to 1 if the majority of the judges indicate attribute presence, and 0 otherwise. Agreement is the percentage of images for which the majority of human judges evaluating an image assign this image the same label as our model. The total cost of data collection is $288.96, with an average cost per judgment of $0.024.</p><p>Liu, Dzyabura, and Mizik: Visual Listening In we see lower agreement between model-based and human-based labels. Table <ref type="table" target="#tab_3">4</ref> presents an alternative metric to evaluate agreement between the model and human judgments. It shows the percentage of human judgments indicating the presence of an attribute in an image, by image group. As expected, we see a monotonic decline in the positive identification of attribute presence by human judges as we go from high to low model-predicted likelihood of attribute presence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Accessing IBBI Metric Against Survey-Based</head><p>Brand Perceptions Data. To gain further insights, we compare the model-predicted IBBI scores from consumer-generated and firm-created brand images on Instagram with the data from a large-scale consumer survey of brand perception conducted by the Y&amp;R BAV among U.S. adults.</p><p>The BAV survey asks a large, nationally representative sample of respondents to indicate whether they perceive a given brand as possessing a particular attribute. The BAV data are summarized as a percentage of consumers who responded affirmatively to the question of whether a particular brand represents a given attribute. We use BAV data for glamorous, rugged, fun, and healthy from the first quarter of 2016 to match our image data collection timeframe.</p><p>Table <ref type="table" target="#tab_4">5</ref> presents Pearson correlations for the brand metrics computed from consumer-created brand images, firm-created brand images, and the BAV survey. We see a high degree of correspondence between the consumer and firm image-based IBBI measures. Most, but not all, correlations are high and highly significant. We also find a high degree of correspondence between consumer image-based IBBI and BAV measures and firm image-based IBBI and BAV measures for the attributes that are more relevant for the category. For apparel, we observe higher correlations for the glamorous and rugged attributes. For beverages, we see high correlations for healthy.</p><p>Table <ref type="table" target="#tab_4">5</ref> shows high and significant correlations between IBBI measures and BAV survey data. However, a significant difference likely exists between the population of BAV respondents and the population of active Instagram users posting images online. To gain further insights, we replicate our correlation analyses using the data from a survey we conducted among active Instagram users. If our IBBI metrics of brand portrayal reflect consumer brand perceptions, then we should see a higher degree of correspondence between IBBI and brand perceptions of Instagram users.</p><p>Indeed, the BAV survey data come from a nationally representative sample of U.S. adults. Consumercreated images on Instagram are posted by active Instagram users, who tend to be relatively young and tech savvy. Moreover, many consumer brand images depict personal consumption experience. For example, in a pretest that we ran, we find that at least one of three human evaluators whom we asked to review the images flagged 81.5% of the consumer-generated Prada photos as depicting personal consumption experience, whereas only about 3.5% of the BAV survey respondents report using the brand regularly and 86.2% report that they have never used it. <ref type="bibr">11</ref> Thus, we conducted an online survey to collect brand perceptions data from a random sample of Notes. Data are based on 600 images (150 images for each attribute, 50 images per attribute in each group) and 12,000 total judgments (20 judgments for each image and attribute). Agreement is computed as the percentage of images for which the majority of human judges evaluating an image assign this image the same label as the BrandImageNet model. The model-based label is equal to 1 if the model-based probability estimate for attribute presence is greater than 50%, and 0 otherwise.  <ref type="table" target="#tab_5">6</ref> presents correlations between our IBBI metrics and the survey of Instagram users. We find notably higher correlations of firmand consumerimage-based IBBI with the Instagram users' survey data than with the BAV survey data (shaded cells in Table <ref type="table" target="#tab_5">6</ref> mark notable improvement over respective Table <ref type="table" target="#tab_4">5</ref> results), and the average increase in correlations of the survey data for the Instagram users with the consumer-based IBBI is somewhat greater than the increase with the firm-based IBBI. We observe higher correlations for glamorous, rugged, and fun in the apparel category. For beverages, we observe higher and more significant correlation for glamorous, rugged, and healthy. These results, based on a survey of a more relevant population of Instagram users, suggest that our IBBI measure indeed reflects consumer brand perceptions. Although we cannot fully eliminate the population differences (a random sample of Instagram users we obtained may be different from the Instagram users who hashtag brands, and people who hashtag Victoria's Secret may be different from people who hashtag Eddie Bauer), these results further support the point that our model is able to identify brand perceptions depicted in the brand imagery.</p><p>In summary, these results provide further support for the notion that IBBI measure reflects consumer brand perceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">A Case Study of the Underwear Brands: Firm</head><p>and Consumers' Brand Portrayal on Social Media Firms engage in branding through advertising, social media, and product packaging to create a particular brand image in consumers' minds. The imagery from a brand's official account reflects the firm's positioning efforts and the desired perceptions and associations the firm wants consumers to have.  The BrandImageNet model allows examination of consumers' and firms' portrayal of brands on social media. By comparing perceptual attributes expressed in the official brand images with those in consumer-created images, one may be able to examine whether firm portrayal matches consumers' portrayal of the brands and make inferences on whether the firms' brandpositioning objectives have been achieved.</p><p>Although we find high and significant correlations between IBBI scores from consumer-and firm-created images across all attributes that we study, we do observe few notable differences for some brands and attributes. Figure <ref type="figure" target="#fig_4">4</ref> presents a chart of IBBI scores for glamorous in the apparel category. Consider, for example, Victoria's Secret, Joe Boxer, and Hanes brands. All three brands come from the same apparel subcategory: underwear. Victoria's Secret has the highest IBBI for glamorous for firm images and a slightly lower IBBI for consumer images. Joe Boxer has a similar medium level of IBBI for both firm and consumer images. Hanes has a very large differential between the firm-based and consumer-based IBBI (firm images are much more glamorous). These numbers suggest consumer perceptions of Joe Boxer are consistent with brand positioning. Consumer perceptions of Victoria's Secret fall just short of the company's positioning on glamorous. Consumer perceptions reflected in their portrayal of Hanes, however, are highly inconsistent with how the company portrays the brand.</p><p>We have validated these patterns using human judges to rule out the possibility that they are simply an artifact of model-prediction error. We selected a random set of 500 consumer-and 500 firm-created images for each brand. For Joe Boxer and Hanes, we use all available firm-created images because less than 500 firm images are available on Instagram for these brands. A total of 2,297 images are included in this test. Ten Level 3 judges on the Figure Eight platform evaluated each of the 2,297 images. Table <ref type="table" target="#tab_6">7</ref> presents a summary of our findings showing consistency between our model and human judgments.</p><p>Consistent with our model predictions, human judges perceive firm and consumer images for Victoria's Secret as highly glamorous-more glamorous than the images of Joe Boxer and Hanes. Importantly, both the model and the judges identify firm images as more glamorous than consumer images. Joe Boxer's firm images are judged as medium on glamor for both firm and consumer images, and although the slight positive differential is not significant in model predictions,  <ref type="bibr">Mizik: Visual Listening In Marketing Science, 2020, vol. 39, no. 4, pp. 669-686, © 2020 INFORMS</ref> the judges perceive firm images as significantly more glamorous. Both the model and the human judges perceive firm images of Hanes as much more glamorous than consumer images of Hanes. Our model prediction scores are consistent with the human perceptions of the images. The observed differentials identified by our model in the consumer-created and firm-created brand images for a given brand and across brands are also present in human evaluations.</p><p>This case study considers differences in the depiction of brand attributes in consumer and firm images at one time point. Managers, however, can use the BrandImageNet model and our proposed IBBI metric to monitor and track changes in consumers' brand portrayal to detect changes in consumers' depiction of their brand. Firms can also use this approach to track how fast consumers pick up new brand positioning or how they respond to a brand crisis.</p><p>Overall, our empirical analyses suggest that the visual content that consumers post on social media reveals their brand perceptions. The method we propose can extract this information and help marketing managers monitor and identify changes in brand perceptions over time, identify discrepancies between intended brand positioning and consumers' perceptions, and better understand their brand positioning in the context of a competitive landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sensitivity Analyses</head><p>We undertook several sensitivity analyses to select the best model specification for our task of extracting perceptual brand attributes. For example, we considered different options for initializing our BrandImageNet model parameters for fine-tuning. We tested a set of models initialized with learned parameters from Karayev's Fine-Tuned Flickr-Style ConvNet model for image style recognition. 12 Karayev's ConvNet is a multiclass classification model, which itself was fine-tuned on 80,000 Flickr images from the BVLC Caffe Reference model. Karayev's ConvNet could potentially be a better model to fine-tune for our application, because an image style recognition task is presumably more relevant to our perceptual-attribute-identification task than ConvNet models developed for object identification. We do not find that to be the case. We find that initializing with the original BVLC Caffe Reference model parameters generates a model with the best performance on the validation set. It also has the best out-of-sample fit on our Flickr data and generates the highest correlations with the BAV data and with our brand perceptions survey of active Instagram users.</p><p>Further, we took steps to identify a learning rate for the best model performance on our validation data. Although the differences in model performance across alternative learning-rate specifications are somewhat minor, we find that a lower learning rate of 0.0001, rather than the BVLC's recommended learning rate of 0.001, generates the best overall model performance in both validation and hold-out samples (results available upon request).</p><p>We also evaluated our deep-learning-based Brand-ImageNet model performance against alternative machine-learning approaches. We tested the Brand-ImageNet model against a popular Support Vector Machines (SVM) approach (see the online appendix for details of the SVM application). SVM is not a deeplearning method. It requires the researcher to first define and extract a set of image features. We selected a set of typical features popular in computer vision research related to color, shape, and texture and trained a multi-label SVM classifier on these features to predict the presence of perceptual brand attributes using our sample of labeled Flickr images. Similar to our approach to training the BrandImageNet model, we trained the SVM classifier on 80% of the Flicker images, used 10% as a validation set to choose model hyperparameters, and kept the remaining 10% as a holdout sample to evaluate model performance. Our SVM classifier achieves an average accuracy of 87% and an AUC of 0.72 across the four perceptual attributes that we train it to identify (see Table <ref type="table" target="#tab_5">OA6</ref> in the online appendix). The SVM performance is lower than that of our BrandImageNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The rapidly growing volume of visual brand-related content posted on social media is a new valuable information source for marketers and brand managers to monitor, track, and better understand brand performance. As text-mining approaches have gained popularity in leveraging UGC for brand monitoring, image-mining approaches are still relatively new and underutilized. Out study takes a step toward understanding and utilizing consumer-created images on social media. It bridges the image-mining and machinelearning literature with the branding literature by proposing an approach to online brand monitoring and market intelligence gathering from consumer-generated images.</p><p>The proposed approach enables managers to monitor how their brands are portrayed on image-based social platforms. We find that the brand-portrayal metrics derived from consumer-created brand images are strongly correlated with survey-based metrics of consumer brand perceptions. That is, consumers' portrayal of brands on social media contains valuable information about brands: it reflects consumer brand perceptions.</p><p>Firms can use our BrandImageNet model to understand consumers' brand perceptions, monitor brand performance and the success of repositioning efforts, and identify gaps in positioning strategies. Firms can use our model to screen consumer-created brand images to select some to feature in their marketing communications. Firms can also apply the BrandImageNet model to the visual content created and shared by a consumer (rather than a brand) to better understand this individual consumer's personality and identity (the focal components of the beliefs about self). Marketing theory postulates that better fit between consumer identity and brand identity is important for brand acceptance and success in the marketplace. That is, matching a consumer's identity attributes with fitting firm-generated imagery or ad copy can increase clickthrough rates, engagement, liking, and the likelihood of purchase. The methods described in the manuscript can be used to analyze an individual consumer's public visual data stream to extract specific attributes (e.g., healthy, glamorous, etc.) represented in the imagery the consumer creates and/or posts on social media platforms. The learned insights can be leveraged for better ad targeting by matching consumer identity characteristics with ad creatives. This application can be useful for both search engines serving ads and for the advertisers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations and Directions for Future Research</head><p>Our study has several limitations. One important limitation is that our approach does not allow to draw inferences about the specific image properties and content elements driving consumers' perceptions of whether the image conveys a particular perceptual attribute. Perceptions may be shaped by the focal content elements of the image (persons, products, objects in the image), the background of the image, or the design elements of the image (color scheme, texture, etc.). Understanding how these different elements shape the overall perceptions of the image is interesting and important. But because our BrandImageNet model utilizes a deep-learning approach, which simultaneously identifies the features and links them to <ref type="bibr">Mizik: Visual Listening In Marketing Science, 2020, vol. 39, no. 4, pp. 669-686, © 2020 INFORMS</ref> perceptual attributes in the images, we are not able to answer these questions in the current study. While ConvNet models have better predictive performance than traditional methods with feature engineering (e.g., SVM), they do not lend themselves to interpretability that may be feasible with supervised machine learning methods where researchers explicitly define image features. As such, we leave these questions for future research. Future research investigating these questions may help improve advertising copy design and social media campaigns. Future research can also explore what drives the differences in the depiction of brands in consumer and firm images posted on social media. We find that consumers' portrayal of brands on social media reflects their brand perceptions (as measured with a traditional external survey tool). As such, the differences between the firm and consumer brand portrayal could simply reflect the differences in consumers' actual brand perceptions and the firms' intended brand positioning. There might, however, also be other reasons driving these differences. For example, differences can also be driven by the type of images that consumers are able to produce compared with firms (quality, artistry), and by the type of images consumers are willing to share online (e.g., what is socially appropriate, desirable, or consistent with the individual consumer's self-image and self-signaling objectives versus how the consumers actually perceive the brand). Consider, for example, Victoria's Secret brand. A customer might perceive Victoria's Secret brand as extremely glamorous but might not be able to create or be willing to post images on her/his Instagram account as glamorous and revealing as the images on the Victoria's Secret's Instagram account.</p><p>Another limitation of the presented approach is that we predefine which perceptual brand attributes we study. Future research in this area can explore unsupervised machine-learning methods to help uncover brand attributes that consumers care about but that brand managers have not yet recognized and considered. Visual content is very amenable to unsupervised machine-learning methods. Similar to textmining applications for extracting topics and uncovering consumer needs <ref type="bibr" target="#b33">(Timoshenko and Hauser 2019)</ref>, visual content mining can be used to uncover new product-usage situations, identify individual personality traits of the content creator or a consumer group, and help design products and communication strategies to better fit and address the needs of the individual or consumer segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Visual content is a ubiquitous part of modern life. It affects consumers' beliefs, preferences, and decisionmaking at multiple stages. As such, incorporating visual content analyses into marketing models and management processes is imperative. The method presented in this paper provides evidence that capturing and analyzing rich image data generated by consumers is possible and can generate valuable insights for marketers. We hope that our research provides a foundation and encourages more future research in this emerging area.  <ref type="bibr" target="#b25">(Mahajan et al. 2018)</ref>. Using MTurk to label the images is also not practical for our purposes. The costs of securing a sufficiently large sample of images and human judges on Mturk to get reliable labels is prohibitive. Flickr, on the other hand, is cost-effective and is also more reliable. In Section 4, we present evidence validating the appropriateness of our BrandImageNet model (trained on Flickr) for application to Instagram data.</p><p>6 See https://github.com/BVLC/caffe/tree/master/models/bvlc_reference _caffenet (accessed August 15, 2019).</p><p>7 Note that d l−1 is equal to the depth (the third dimension) of the input data from the previous layer. 8 As we discuss in Section 5, we undertook multiple tests of model fine-tuning (e.g., varying the fine-tuning learning rate and using different values to initialize parameters) to train our BrandImageNet model for best performance. We found that initializing with BVLC Reference CaffeNet parameters and using a 0.0001 learning rate in Conv1-Conv5 and FC6-FC7 and a 0.001 learning rate in FC8 generates the best model according to model performance on the validation set. Thus, we focus on this specification in our main discussion. This specification also generates best performance in Flickr out-ofsample tests and in Instagram tests against human judges and consumer-perceptions surveys.</p><p>9 Figure OA1 in the online appendix reports the distribution of the predicted probabilities of attribute presence across all Instagram photos for each attribute.</p><p>10 See https://www.figure-eight.com/ (accessed August 15, 2019). <ref type="bibr">11</ref> We ran a Figure Eight pretest task in which human judges evaluated whether an image depicted personal consumption experience or not. Each image was evaluated by three judges. Full results available upon request.</p><p>12 See https://github.com/BVLC/caffe/tree/master/models/finetune _flickr_style (accessed August 15, 2019).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Sample Images from Instagram Hashtagged with Brand Names</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Sample Images of Brand Attributes and Their Antonyms in Flickr Data Set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Architecture of Multi-Label BrandImageNet Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-based Level 3 judges on Figure Eight network and ask them to indicate whether the image is glamorous? Rugged? Healthy? Fun? Figure Eight is an on-demand data label and annotation platform specifically designed for collecting training data for machine-learning tasks. 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. IBBI Scores for Glamorous in the Apparel Category</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Out</figDesc><table><row><cell cols="2">-of-Sample Predictive Performance for</cell><cell></cell></row><row><cell cols="2">Multi-Label BrandImageNet Model on Flicker</cell><cell></cell></row><row><cell>Hold-Out Data</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>AUC</cell></row><row><cell>Glamorous</cell><cell>88.6%</cell><cell>0.846</cell></row><row><cell>Rugged</cell><cell>91.3%</cell><cell>0.853</cell></row><row><cell>Healthy</cell><cell>89.9%</cell><cell>0.859</cell></row><row><cell>Fun</cell><cell>89.4%</cell><cell>0.827</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Aggregate BrandImageNet Model Performance According to Human-Based Image Labels</figDesc><table><row><cell></cell><cell>AUC: Model vs. the majority</cell><cell>Agreement: Model vs. the majority</cell><cell>Agreement: A single human judge vs. the majority</cell></row><row><cell></cell><cell>vote of human judges</cell><cell>vote of human judges</cell><cell>vote of human judges, average</cell></row><row><cell>Glamorous</cell><cell>0.93</cell><cell>83%</cell><cell>85%</cell></row><row><cell>Rugged</cell><cell>0.96</cell><cell>85%</cell><cell>83%</cell></row><row><cell>Healthy</cell><cell>0.91</cell><cell>78%</cell><cell>80%</cell></row><row><cell>Fun</cell><cell>0.94</cell><cell>84%</cell><cell>80%</cell></row><row><cell>Average</cell><cell>0.94</cell><cell>83%</cell><cell>82%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Agreement Between BrandImageNet Model and Human Labels by Image Group</figDesc><table><row><cell></cell><cell>Group 1: "Attribute present"</cell><cell>Group 2: "Ambiguous images"</cell><cell>Group 3: "Attribute absent"</cell></row><row><cell></cell><cell>(high predicted probability)</cell><cell>(~50% predicted probability)</cell><cell>(low predicted probability)</cell></row><row><cell>Glamorous</cell><cell>96%</cell><cell>52%</cell><cell>100%</cell></row><row><cell>Rugged</cell><cell>100%</cell><cell>56%</cell><cell>100%</cell></row><row><cell>Healthy</cell><cell>84%</cell><cell>52%</cell><cell>98%</cell></row><row><cell>Fun</cell><cell>94%</cell><cell>58%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Percentage of Human Judgments Indicating the Presence of an Attribute in Instagram Images by Group</figDesc><table><row><cell></cell><cell>Group 1: "Attribute present"</cell><cell>Group 2: "Ambiguous images"</cell><cell>Group 3: "Attribute absent"</cell></row><row><cell></cell><cell>(high predicted probability)</cell><cell>(~50% predicted probability)</cell><cell>(low predicted probability)</cell></row><row><cell>Glamorous</cell><cell>83%</cell><cell>34%</cell><cell>8%</cell></row><row><cell>Rugged</cell><cell>79%</cell><cell>39%</cell><cell>6%</cell></row><row><cell>Healthy</cell><cell>76%</cell><cell>24%</cell><cell>23%</cell></row><row><cell>Fun</cell><cell>80%</cell><cell>37%</cell><cell>14%</cell></row></table><note>Note. Data are based on 600 images (150 images for each attribute, 50 images per attribute in each group) and 12,000 total judgments (20 judgments for each image and attribute).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Correlation Analyses of Model-Predicted IBBI from Consumer-and Firm-Created Images on Instagram and the BAV Survey-Based Measures of Brand Perceptions</figDesc><table><row><cell></cell><cell></cell><cell>Consumer IBBI</cell><cell>Consumer IBBI</cell><cell>Firm IBBI</cell></row><row><cell></cell><cell></cell><cell>vs. firm IBBI</cell><cell>vs. BAV</cell><cell>vs. BAV</cell></row><row><cell>Apparel</cell><cell>Glamorous</cell><cell>0.7838***</cell><cell>0.5519***</cell><cell>0.6100***</cell></row><row><cell></cell><cell>Rugged</cell><cell>0.9122***</cell><cell>0.5467**</cell><cell>0.5035**</cell></row><row><cell></cell><cell>Healthy</cell><cell>0.4680**</cell><cell>0.1794</cell><cell>0.3225*</cell></row><row><cell></cell><cell>Fun</cell><cell>0.6061***</cell><cell>0.3583*</cell><cell>0.2883 ±</cell></row><row><cell>Beverage</cell><cell>Glamorous</cell><cell>0.5518**</cell><cell>0.4568**</cell><cell>0.6582***</cell></row><row><cell></cell><cell>Rugged</cell><cell>0.8259***</cell><cell>0.3596*</cell><cell>0.4708*</cell></row><row><cell></cell><cell>Healthy</cell><cell>0.7370***</cell><cell>0.6976***</cell><cell>0.4766**</cell></row><row><cell></cell><cell>Fun</cell><cell>0.3775*</cell><cell>0.1791</cell><cell>0.2584</cell></row><row><cell cols="4">± p &lt; 0.1; *p &lt; 0.05; **p &lt; 0.01; ***p &lt; 0.001 (Pearson correlation).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Correlations Between Model-Predicted IBBI from Consumer-and Firm-Created Images on Instagram and Survey-Based Measures of Brand Perceptions from Instagram Users Notes. The shaded cells represent instances of improvement over the correlations between consumer-and firmimage-based IBBI and the BAV data reported in Table 5. Average number of respondents per brand is 148 (min = 87, max = 175). Cost of the survey data collection is $547.20.</figDesc><table><row><cell></cell><cell></cell><cell>BAV vs. Instagram</cell><cell>Consumer IBBI vs.</cell><cell>Firm IBBI vs.</cell></row><row><cell></cell><cell></cell><cell>user survey</cell><cell>Instagram user survey</cell><cell>Instagram user survey</cell></row><row><cell>Apparel</cell><cell>Glamorous</cell><cell>0.9503***</cell><cell>0.5824***</cell><cell>0.6325***</cell></row><row><cell></cell><cell>Rugged</cell><cell>0.9338***</cell><cell>0.6831***</cell><cell>0.6630***</cell></row><row><cell></cell><cell>Healthy</cell><cell>0.8600***</cell><cell>0.0842</cell><cell>0.1941</cell></row><row><cell></cell><cell>Fun</cell><cell>0.6486***</cell><cell>0.5672***</cell><cell>0.4914**</cell></row><row><cell>Beverage</cell><cell>Glamorous</cell><cell>0.9238***</cell><cell>0.5001**</cell><cell>0.5743**</cell></row><row><cell></cell><cell>Rugged</cell><cell>0.5485***</cell><cell>0.7899***</cell><cell>0.6645***</cell></row><row><cell></cell><cell>Healthy</cell><cell>0.9482***</cell><cell>0.7127***</cell><cell>0.5350**</cell></row><row><cell></cell><cell>Fun</cell><cell>0.8714***</cell><cell>0.2130</cell><cell>0.3648*</cell></row></table><note>*p &lt; 0.05; **p &lt; 0.01; ***p &lt; 0.001 (Pearson correlation).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Victoria's Secret, Joe Boxer, and Hanes Case Study</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>See https://instagram-press.com/our-story/ (accessed August 15, 2019) and https://www.socialpilot.co/blog/social-media-statistics (accessed August 15, 2019). See https://www.instagram.com/explore/tags/nike/ (accessed August 15, 2019). We considered collecting a training set of Instagram images based on hashtags as an alternative. However, a hashtag is too noisy to serve as an effective supervisory label, because users apply hashtags that are irrelevant or leave out relevant hashtags. As a result, training models using such weakly supervised data usually require extremely large data sizes</figDesc><table><row><cell>Appendix. BrandImageNet Model Architecture</cell></row><row><cell>and Hyperparameters</cell></row><row><cell>Endnotes</cell></row><row><cell>1</cell></row></table><note>2 3</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">MarketingScience, 2020, vol. 39, no. 4, pp. 669-686, © 2020 </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">RGB constructs all possible colors as a combination of the red, green, and blue channels.5  Regular lowercase letters denote scalars, bold lowercase letters denote vectors, and bold uppercase letters denote matrices and sets.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Notes. We collected human judgments using the procedure outlined in Section 4.3.1. To contain the costs, we used 10 U.S.-based Level 3 judges to evaluate each image. We obtain an AUC of 0.81 using majoritybased human labels for each image as the "ground truth." Agreement between human and model labels is 79%. Average agreement between each individual human judge and the majority of human judges is 83%. Data collection cost is $573.60. a The IBBI scores here are based on a random sample of 500 images and, as such, are different from the IBBI scores reported in Figure <ref type="figure">4</ref> and Table <ref type="table">OA1</ref> in the online appendix.</p><p>b We compute human-based IBBI as follows. For each image, we compute the probability that it expresses glamor as the number of judges who label it as glamorous divided by the number of judges evaluating the image (10 judges). Then, we compute the average of these numbers for a brand and use it as the human-based IBBI. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deriving the pricing power of product features by mining consumer reviews</title>
		<author>
			<persName><forename type="first">N</forename><surname>Archak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1485" to="1509" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>; Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML Workshop Unsupervised Transfer Learn</title>
				<meeting>ICML Workshop Unsupervised Transfer Learn<address><addrLine>Bellevue, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learners benefit more from out-of-distribution examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chherawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Internat. Conf</title>
				<editor>
			<persName><forename type="first">G</forename><surname>Gordon</surname></persName>
			<persName><forename type="first">D</forename><surname>Dunson</surname></persName>
			<persName><forename type="first">D</forename><surname>Miroslav</surname></persName>
		</editor>
		<meeting>14th Internat. Conf<address><addrLine>Fort Lauderdale, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The use of the area under the ROC curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The effect of word of mouth on sales: Online book reviews</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mayzlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="354" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining brand perceptions from Twitter social networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="362" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Comput. Vision Pattern Recognition</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><surname>Conf</surname></persName>
		</author>
		<author>
			<persName><surname>Comput</surname></persName>
		</author>
		<title level="m">Vision Pattern Recognition</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How taking photos increases enjoyment of experiences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zauberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality Soc. Psych</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="140" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="647" to="655" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">P2v-map: Mapping market structures for large retail assortments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klapper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4557" to="4580" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The product and the brand</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Bus. Rev</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="33" to="39" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual sentiment analysis for brand monitoring enhancement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Perantonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karkaletsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Internat. Sympos. Image Signal Processing Anal. (ISPA)</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m">Conf. Comput. Vision and Pattern Recognition</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Internat. Conf. Multimedia</title>
				<meeting>22nd ACM Internat. Conf. Multimedia<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding and predicting interestingness of videos</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th AAAI Conf. Artificial Intelligence</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1113" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognizing image style</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Winnemoeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
			<persName><forename type="first">A</forename><surname>French</surname></persName>
			<persName><forename type="first">T</forename><surname>Pridmore</surname></persName>
		</editor>
		<meeting>British Machine Vision Conf<address><addrLine>Durham, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Brands and branding: Research findings and future priorities</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="740" to="759" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large scale cross category analysis of consumer review content on sales conversion leveraging deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="918" to="943" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word of mouth for movies: Its dynamics and impact on box office revenue</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="74" to="89" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vision (ECCV)</title>
				<meeting>Eur. Conf. Comput. Vision (ECCV)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science and Business Media</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image labeling on a network: Using social-network metadata for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vision</title>
				<meeting><address><addrLine>Fitzgibbon A, Lazebnik S, Perona P, Sato Y, Schmid C; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="828" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The financial value impact of perceptual brand attributes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mizik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="32" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mine your own business: Market-structure surveillance through text mining</title>
		<author>
			<persName><forename type="first">O</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fresko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="543" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Strategic brand conceptimage management</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Maclnnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="135" to="145" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Increasing consumer engagement with firmgenerated social media content: The role of images and words. Working paper</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mizik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Seattle</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ratios in proportion: What should the shape of the package be?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Raghubir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Greenleaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Find every photo with Flickr&apos;s new unified search experience</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stadlen</surname></persName>
		</author>
		<ptr target="https://blog.flickr.net/en/2015/05/07/flickr-unified-search/" />
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Identifying customer needs from user-generated content</title>
		<author>
			<persName><forename type="first">A</forename><surname>Timoshenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mining marketing meaning from online chatter: Strategic brand analysis of big data using latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tirunillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Tellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="463" to="479" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Dzyabura</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mizik</forename></persName>
		</author>
		<title level="m">Visual Listening In Marketing Science</title>
				<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="669" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieters</surname></persName>
		</author>
		<title level="m">) Visual Marketing</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The buffer effect: The role of color when advertising exposures are brief and blurred</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="143" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Just the faces: Exploring the effects of facial features in print advertising</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marketing Sci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="352" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mining photo-sharing websites to study ecological phenomena</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korayem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lebuhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m">Proc. 21st Internat. Conf. World Wide Web</title>
				<meeting>21st Internat. Conf. World Wide Web<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="749" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sales effects of attention to feature advertisements: A Bayesian mediation analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Marketing Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="669" to="681" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Can user-posted photos serve as a leading indicator of restaurant survival? Evidence from Yelp. Working paper</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>University of Southern California, Los Angeles</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">How much is an image worth? Airbnb property demand estimation leveraging large scale image analytics. Working paper</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>David S. Tepper Business School</publisher>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Dzyabura</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mizik</forename></persName>
		</author>
		<title level="m">Visual Listening In</title>
				<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
