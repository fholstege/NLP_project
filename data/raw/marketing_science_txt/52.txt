http://pubsonline.informs.org/journal/mksc

MARKETING SCIENCE
Vol. 39, No. 6, November­December 2020, pp. 1033­1038 ISSN 0732-2399 (print), ISSN 1526-548X (online)

Introduction to the Special Issue on Marketing Science and Field Experiments

Leif Nelson,a Duncan Simester,b K. Sudhirc
a Haas School of Business, University of California, Berkeley, California 94720; b Sloan School of Management, Massachusetts Institute of Technology, Cambridge, Massachusetts 02142; c Yale School of Management, New Haven, Connecticut 06520 Contact: leif_nelson@haas.berkeley.edu (LN); simester@mit.edu (DS); k.sudhir@yale.edu (KS)

https://doi.org/10.1287/mksc.2020.1266 Copyright: © 2020 INFORMS

Abstract. This editorial introduces the special issue on marketing science and field experiments. We compare the characteristics of the papers that were submitted and accepted for the special issue and provide several recommendations for researchers. In general, we find field experiment research is greater in the areas of advertising and pricing with digital being the most common channel. We suggest that, beyond the estimation of effects and tests of hypotheses, field experiments can complement structural models; help train targeting policies; and also contribute to the nascent area of real-time, adaptive experimentation. We also discuss how field experiment research with a marketing science orientation can enhance and contribute in the areas of behavioral research and marketing strategy.

1. Introduction
The marketing literature has recently seen a surge of papers using field experiments. Although this has been partly catalyzed by the relative ease of field experimentation in digital settings, field experiments are increasingly feasible even in some nondigital settings. From a scientific perspective, field experiments offer some advantages over other research methods. When designed well, they can help avoid concerns about endogeneity and selection that often hinder causal inference when using historical data. They can also aid in the validation of theories in real-world settings with fewer modeling assumptions. But, in addition to the potential operational burdens, field experiments also have potential limitations for inference. Notably, it may be difficult to isolate the behavioral mechanisms underlying the findings, and it is generally difficult to answer counterfactuals that are not represented by one of the experimental conditions. In such cases, field experiments can be valuable complements to traditional laboratory experiments or model-based analysis of historical data. The goal of the special issue is to provide further impetus for research using field experiments, especially by diversifying its range of substantive questions and methodological applications.
This editorial is organized as follows. Section 2 summarizes the characteristics of the submissions and the papers accepted/rejected and provides takeaways for scholars. Section 3 presents additional suggestions on how field experiments can complement research in quantitative, behavioral, and managerial marketing. Section 4 concludes.

2. The Special Issue
We summarize the characteristics of the submissions and accepted papers and our broad takeaways from the process.
2.1. Characteristics of Submitted and Accepted Papers
As a source of guidance for future submissions, we compare the characteristics of the accepted manuscripts with those of the rejected manuscripts. We begin by describing the characteristics of the submitted papers. We received a total of 61 submissions for the special issue. As we finalize the special issue, 60 of the submissions have received final decisions with nine papers accepted (15%) and the remaining 51 papers rejected. The 61 manuscripts include a total of 167 unique authors; the accepted papers include 26 unique authors.
The submissions to the special issue cover an impressive range of topics though there was concentration in certain areas. Advertising and marketing communications are the most common substantive topics (48% of submitted papers), followed by pricing (21%) and sales and service employee incentives (8%). The growth in the use of field experiments to study advertising-related topics reinforces an earlier trend. Simester (2017) reviews the field experiments published in the major quantitative marketing journals between 1995 and 2014.1 Prior to 2010, pricing topics dominate with relatively few papers on advertising. However, this pattern is reversed in the papers published after 2010 with many of the more recent papers focused on advertising (particularly digital advertising). Within these broad areas, there are

1033

1034

Nelson, Simester, and Sudhir: Introduction to the Special Issue on Marketing Science and Field Experiments Marketing Science, 2020, vol. 39, no. 6, pp. 1033­1038, © 2020 INFORMS

Figure 1. Marketing Channel
Notes. The figure summarizes the marketing channel in which the experimental manipulations occur. The distribution is reported separately for published papers and all submitted papers. The sample sizes are 8 (published papers) and 61 (all submitted papers). The percentages in each column add to 100% (within each group).
relatively few submissions on the same topics. Some topics for which we had multiple submissions include food choices (seven papers), charitable giving (three), product recommendations (three), multitask incentives (two), retirement savings (two), popularity information (two), and restaurant menus (two).
Figure 1 summarizes the marketing channels in which the experimental manipulations occurred. Digital channels contribute approximately 50% of the submitted papers. This proportion is similar for both published and submitted papers. In-person and direct mail experiments are also well represented. Telephone and email experiments are rare and are included in the "other" category.
We also classify the papers according to their disciplinary focus. To do so, we examined the research cited in the literature review and used to motivate the research question. The dominant disciplinary focus is behavioral; 59% of the submitted papers have a behavioral focus and 45% have a quantitative focus (some papers fall into both categories, and one paper is in neither category). Notably, the proportion of papers with a behavioral focus is similar between both the submitted and published papers.
The accepted papers highlight possible misconceptions about what is required to publish field experiments. In particular, the published papers do not always satisfy criteria that some might believe necessary for publication. For example, randomization is not a requirement. Two of the accepted papers report field experiments that do not rely upon randomization. One paper (Mohan et al. 2020) rotates treatments between consecutive hours in a cafeteria, and in the other (Wolters et al. 2020), the two treatments are implemented in consecutive periods. The proportion of papers that rely upon randomization (75%) is almost identical for accepted and rejected papers.
Reporting multiple field experiments is also not a requirement for acceptance. Only one of the nine

accepted papers reports multiple field experiments, compared with 11 (22%) of the rejected papers. Perhaps surprisingly, three of the accepted papers that focus on substantive issues do not investigate the mechanism behind their results. Hershfield et al. (2020) study how framing messages to encourage savings affects savings decisions. Although the authors do not investigate the underlying mechanisms, they do discuss how their results compare with previous work on related mechanisms. Wolters et al. (2020) study the profitability of customer referral reward programs and contrast their findings with previous work that focuses solely on response rates. Goswami and Urminsky (2020) study the framing of appeals in a charitable giving setting. They creatively compare their findings with both theory- and expert-based predictions. Their results illustrate the limitations in both theory and expert opinion.
There are several characteristics of the accepted papers that distinguished them from the rejected papers. When the accepted papers do investigate the underlying mechanism, they use very robust methods. For example, two of the accepted papers supplement their field experiments with laboratory experiments (Mislavsky et al. 2020, Mohan et al. 2020), and one of the papers (Munz et al. 2020) is able to observe intermediate process measures. In addition, two of the papers study the underlying mechanism by measuring conditional average treatment effects (CATEs) and contrasting the treatment effects across different customer segments. Although many of the rejected papers also estimate CATEs, it is notable that both accepted papers that use this approach (Huang et al. 2020, Li et al. 2020) randomize the covariates that they employ for this segmentation. As a result, they rule out the possibility that variation in the covariates could reflect an unobserved intervening variable. In contrast, when CATEs are used in many of the rejected papers, they rely upon available covariates that are not randomized as part of the experiment.
The accepted papers are more likely to have preregistered experiments. Four of the nine published papers were preregistered compared with only 2 of the 51 rejected papers.
The accepted papers are also more likely to make a methodological contribution. Only 5 of the 51 rejected papers (10%) offer a methodological contribution compared with three of the nine accepted papers (33%). The three accepted papers include Mislavsky et al. (2020), who investigate when people dislike experiments, and Munz et al. (2020), who investigate how to more effectively target charitable donors by matching names. The most econometrically sophisticated of the accepted papers is Tian and Feinberg (2020), who propose a method for using experimental data to optimize price menus when a firm wants

Nelson, Simester, and Sudhir: Introduction to the Special Issue on Marketing Science and Field Experiments Marketing Science, 2020, vol. 39, no. 6, pp. 1033­1038, © 2020 INFORMS

1035

Figure 2. Primary Reasons Papers Were Rejected
Notes. The figure summarizes the primary reasons cited in the area editor reports for rejecting the 51 rejected papers. For example, 73% of the papers were rejected because of concerns about the contribution. A single paper could be rejected for multiple reasons, and so the percentages sum to more than 100%.
to offer a discount to customers who sign up for a longer subscription. The method has to adjust for selection in order to anticipate how varying prices affect both how many new customers sign up for a subscription and what duration length the new customers choose.
2.2. Primary Reasons Papers Were Rejected Unfortunately, a review of the rejected papers reveals that a lot of effort is invested into papers that would require a lot of additional work to be accepted. To identify the primary reasons for rejection, we review the area editor (AE) reports for all 51 rejected papers. We group these reasons into five broad categories, and in Figure 2, summarize the proportion of rejected papers that fall into each category. Many of the papers are rejected for more than one reason, and so a single paper may fall into more than one category.
The most common reason cited for rejection is a lack of sufficient contribution. In some cases, this can be attributed to opportunism. When a firm presents researchers with an opportunity to implement a field experiment or provides data from a past experiment, it is tempting to pursue this offer. Although some opportunities lead to substantial contributions, merely documenting a field experiment itself alone is not a sufficient contribution. Publication requires a wellmotivated research question together with an interesting set of findings. Rejection because of a lack of contribution occurs for three primary reasons:
· Either the experimental variation does not speak to a research question that is of academic interest (16% of rejections) or the research question is already addressed in the literature (37% of rejections).
· The setting is so specific and obscure that there are concerns about generalizability (29% of rejections).

· The results are small, inconsistent, unsurprising, and/or have no managerial implications (33% of rejections).
After contribution, the second most frequent reason cited for rejection, is the failure to convincingly investigate the underlying mechanism. There are often obstacles that hinder investigation of the mechanism when using field experiments. Circumstances generally limit the range and number of experimental treatments, and it is often not possible to run multiple studies to rule out alternative explanations. Collecting process measures is also often infeasible.
Authors recognize these challenges and employ a wide range of methods to shed light on the mechanism. These include laboratory experiments, reporting heterogeneity in treatment effects, collecting survey data, or using ancillary data as process measures (such as email open rates or website browsing). However, each of these approaches introduces its own challenges. Reviewers question the interpretation of heterogenous treatment effects if the covariates are not randomized and question the value of laboratory data if the experimental context is different in the laboratory than in the field. As a result, reviewers often recommend papers to be rejected because the investigations of the underlying mechanism are not convincing.
The high number of papers that are rejected (at least in part) because they do not document the underlying mechanism may seem inconsistent with our earlier statement that three of the published papers do not document the mechanisms behind their findings. The acceptance of these three papers makes it clear that explaining the mechanism is not a requirement for publishing field experiments that make a substantive contribution (it is also generally not relevant for methodological papers). However, if the paper does not establish the mechanism, then the reported effects must be of unusual interest. Authors face a trade-off: either establish the mechanism or expect to be held to a higher contribution threshold for your findings.
Errors in the design, implementation, or analysis of experiments are relatively rare. Although the AE reports often highlight concerns or opportunities to improve the analysis, this is generally not the primary reason a paper is rejected. Perhaps the only issue that is recurring (and, even then, occasional) is selection bias. This is typically introduced in one of two ways: (a) in the study design when there are criteria used to qualify customers for the study or (b) in the study analysis when customers are segmented based upon posttreatment customer behavior. When this occurs, authors are generally aware of the issue and attempt either to control for it or address it through interpretation (generally not successfully).
Perhaps surprisingly, poor exposition is raised as one of the reasons for rejection in 27% of the

1036

Nelson, Simester, and Sudhir: Introduction to the Special Issue on Marketing Science and Field Experiments Marketing Science, 2020, vol. 39, no. 6, pp. 1033­1038, © 2020 INFORMS

rejected papers. The description of the hypotheses, experiments, and/or analysis sometimes lack clarity or are incomplete. Given the effort required to design and implement a field experiment, we might expect that authors would also invest considerable effort into documenting the experiments in their papers. In many cases, it appears that many of these authors do not invest enough effort. However, to fully understand the manipulations and findings for reviewers and readers, extensive discussion of both the context and the implementation is required. Papers that are rejected for expositional reasons typically have too few details, and so the review team is unable to fully evaluate the findings.
Eight of the papers were rejected for lack of fit with the special issue. These include papers that document findings from a natural experiment. These papers typically lack a control group and instead rely upon a before versus after comparison for identification. There are also some papers rejected because the experiments are essentially laboratory experiments conducted using participants sourced in the field. For these papers, the phenomenon under investigation has essentially no specific connection to the participants sourced for the experiment.
3. Where We Expect Field Experiments Can Further Advance the Marketing Literature
We now consider how field experiments can serve as a bridge to further research in the quantitative, behavioral, and strategy literatures in marketing.
3.1. Quantitative Research We note some areas in which exogenous randomized variation with field experiments can complement existing areas of quantitative work in structural models, training targeting policies, and adaptive experimentation.
3.1.1. Structural Models. First, it is generally difficult with field experiments to answer counterfactuals that are not represented by one of the experimental conditions. Without some additional modeling, it is hard to predict outcomes in counterfactual situations or optimize the marketing mix in terms of levels for which there is no experimentation. Structural models are particularly ideal for such counterfactual analysis. Typically, exogenous identifying variation and/or instruments in naturally occurring data are necessary to identify and estimate the structural models. However, field experiments with randomization might be an alternative and complementary approach to generate exogenous variation in economic/behavioral levers. There are two benefits to this: First, it helps to estimate structural models that help answer counterfactual

questions. Second, such experimental variation coupled with structural models can also test and uncover underlying mechanisms, thus enriching findings from field experiments. For example, Dubé et al. (2017) use a structural model to show that customer response to price variation, when bundled with a charitable donation, can be attributed to self-signaling. Without a structural model, it is unlikely that standard methods would have been able to provide convincing evidence of this mechanism. Similarly, Kim et al. (2020) exploit randomization in transfers at a bank (though not a field experiment) to understand the mechanism by which a salesperson's private information affects sales outcomes through changes in acquisition and maintenance behavior. Decomposing the effects of private information on acquisition and maintenance outcomes requires a structural model. Further, they conduct counterfactuals around job design and incentive design given private information.
3.1.2. Training Targeting Models. There has been a surge of industry interest in customer personalization, in which firms design policies that take different marketing actions with different customers. Scholars typically refer to customer personalization as "targeting," and the surge in industry interest is accompanied by increased academic research on this topic. There are two places in which field experiments can play an important role when training targeting models. First, field experiments can provide training data. The machine learning methods that are used to train targeting policies are well suited to identifying correlations, but they generally cannot distinguish which correlations are causal. By training targeting models using experimental data, firms can ensure that their models are identifying causal relationships.
Field experiments can also offer a second role: providing a model-free validation of the trained policy. Conveniently, a single field experiment can be used to evaluate the performance of any trained policy as long as the experiment includes each of the marketing actions that a policy might recommend (Simester et al. 2020).
These two roles of field experiments in targeting are highlighted in Dubé and Misra (2019), who study how to personalize prices using two field experiments. They use the first experiment to provide training data for their models and use the second experiment to validate the trained model.
3.1.3. Adaptive Experimentation. The ability to perform real-time, automated experimentation in digital settings, especially in the areas of pricing, advertising, and targeting, allows firms to balance the gains from exploration relative to the cost of lost profits from experimentation. Misra et al. (2019) is an illustration of this type of research. Hansen et al. (2020) show that

Nelson, Simester, and Sudhir: Introduction to the Special Issue on Marketing Science and Field Experiments Marketing Science, 2020, vol. 39, no. 6, pp. 1033­1038, © 2020 INFORMS

1037

such adaptive and automated experimentation may lead to collusive outcomes when the effects of competitor pricing are not accounted for.
3.2. Behavioral Research As noted in Sudhir (2016), there is a large missed opportunity at the intersection of behavioral and quantitative marketing, and behavioral field studies can be an excellent means by which such a bridge may be created. Sudhir (2016) notes several challenges that arise from the way in which behavioral and quantitative scholars evaluate papers, and they make it harder to publish such papers that seek to bridge the divide. It is, therefore, gratifying that four of the papers being published in this special issue are behavioral.
In an ideal world, a field experiment leverages the simplicity of random assignment to establish the unambiguous identification of a relationship that is empirically sound, theoretically insightful, and practically consequential. As can be seen from the review of the accepted papers, that trifecta of quality is seldom achieved. However, acceptable deviations from the ideal help to highlight how field experiments can appeal to multiple groups of readers. Insufficient attention to the underlying process is frequently a kiss of death for a traditional paper submitted to a classic behavioral journal, but that is, in part, because most studies in behavioral journals are only interested in the extent to which they can identify a new understanding of consumer behavior. A field experiment runs that relationship backward: because the behavior being investigated is intrinsically interesting, there is less of a demand for delivering a perfect explanation. A laboratory experiment showing variation in hypothetical purchase behavior is interesting only if that variation can be explained and such an explanation potentially could be generalized. A field experiment showing variation in actual purchase behavior is interesting because of that variation almost regardless of whether it is readily explainable. Quantitative and behavioral researchers can feel both these trade-offs, and though it is inevitably the case that they have different value functions for different shortcomings, field experiments allow them to think about the same research in some of the same ways.
The last decade has seen a tremendous evolution in scientific practices in the behavioral sciences, and it is worth thinking about where field experimentation and marketing science both fit in with these changes. The evolution starts with a recognition that the selective reporting of treatments, measures, and analyses can make it easier for researchers to falsely conclude that they have found a new truth even when there is no true relationship to find. That recognition leads to increases in sample sizes and more transparent disclosure of materials and methods in the

posting and sharing of data and materials and in the adoption of preregistration. These changes came quickly in traditional experimental psychology but considerably more slowly in consumer research. Field experimentation accelerates that evolution. It is a telling fact that four of the nine published papers in this issue were preregistered, whereas only 2 of the 51 rejected manuscripts were. Preregistration allows a reader to recognize when analyses are confirmatory and when they are exploratory. Just as important, preregistration allows researchers to show off when their results are confirmatory and not just a product of late-stage p-hacking. Because field experiments are often time-consuming to plan, expensive to run, and slow to complete (outside of digital settings), researchers really want to make sure that they generate informative (and publishable) findings. In practice, that means that sample sizes tend to be considerably larger, measures and manipulations are more likely to be shared, and everything is more likely to be preregistered. All behavioral science benefits from open science and preregistration, but those benefits are particularly salient for the consideration of field experimentation.
Marketing science may not be a traditional outlet for the consumer researcher, but it could be an outlet for the consumer research most committed to replicable findings. Looking forward, it is worth thinking more broadly about the potential value of collaboration between quantitative and behavioral researchers. It is unrealistic to think that everyone will start reading everyone else's journals and even less realistic to think that they will all start asking the same questions or using the same tools to answer them. Nevertheless, it is more plausible to think that researchers from either discipline might start to see how some research problems could benefit from contributions from others. Perhaps that might mean a behavioral researcher recognizing how novel econometrics could allow a closer understanding of complex data. Or, perhaps, it might mean that an economic theorist might see opportunities for seeing ideas tested and refined through behavioral experimentation. Any such possibilities are necessarily speculative, but we see field experiments (both here and elsewhere) as potential vehicles for those collaborations. We would like behavioral and quantitative scholars to view marketing science as a natural outlet in which those collaborations are most likely to find a home.
3.3. Marketing Strategy Research Much of the field experimental focus in quantitative marketing research tends to be in tactical elements of the marketing mix, such as pricing, advertising, and product design, which are easily manipulated in the context of a field experiment. Marketing levers that have more organizational elements, such as marketing channels and sales force management, are

1038

Nelson, Simester, and Sudhir: Introduction to the Special Issue on Marketing Science and Field Experiments Marketing Science, 2020, vol. 39, no. 6, pp. 1033­1038, © 2020 INFORMS

often less amenable to experimentation as they are harder to vary and often require more intraorganizational cooperation and coordination than is typical. Further, questions of how marketing organizations are structured (e.g., siloed, cross-functional, information sharing, decision rights, etc.) to execute chosen marketing strategies are of interest to senior managers. Expanding the focus of field experiments to intraorganizational and cross-functional decisions relevant to senior marketing managers can considerably expand our knowledge as these are also areas in which exogenous variation and establishment of causal relationships are hardest to establish. Even though such field experiments are hard to do, there is a long history (Seashore 1964). See Bandiera et al. (2011) for a recent survey of field experiments in firms.
Among such levers, the area in which field experiments have shown greatest success is, not surprisingly, in the area of sales incentives because these are relatively easy to modify in the short run (e.g., Chung and Narayandas 2017, Chung et al. 2020). Managerial training is another area in which field experiments have succeeded (e.g., Bloom et al. 2013, Anderson et al. 2018). The promise of experimentation in asking often difficult organizational questions, such as about job and incentive design, is seen in Kim et al. (2020), who exploit a policy of random job transfers to study how a salesperson's private information interacts with job and incentive design to impact customer acquisition and retention.
With big data, it becomes easier for researchers to observe various aspects of the internal organization, and firms can easily structure levers, such as incentives and information visibility on dashboards, to sales, service, and marketing employees to motivate, incentivize, and improve performance. Such easily changeable levers are potentially amenable to field experimentation and open up new areas of research within the marketing organization, B2B marketing, and marketing channels.
4. Conclusion
This special issue includes a range of papers addressing both behavioral and quantitative marketing questions through multiple marketing channels. We summarize elements of successful submissions in the review process and shed some light on success factors and reasons for failure. We also discussed ways by which field experiment papers can augment research in the areas of quantitative, behavioral, and managerial marketing. We hope these descriptions of success factors and potential contributions to different areas of marketing help researchers choose topics as well as craft successful field experiment­based papers to expand marketing scholarship.

Endnote
1 Coincidentally, that analysis also included 61 papers.
References
Anderson SJ, Chandy R, Zia B (2018) Pathways to profits: The impact of marketing vs. finance skills on business performance. Management Sci. 64(12):5559­5583.
Bandiera O, Barankay I, Rasul I (2011) Field experiments with firms. J. Econom. Perspect. 25(3):63­82.
Bloom N, Eifert B, Mahajan A, McKenzie D, Roberts J (2013) Does management matter? Evidence from India. Quart. J. Econom. 128(1):1­51.
Chung DJ, Narayandas D (2017) Incentives vs. reciprocity: Insights from a field experiment. J. Marketing Res. 54(4):511­524.
Chung DJ, Narayandas D, Chang D (2020) The effects of quota frequency: Sales performance and product focus. Management Sci., ePub ahead of print September 16, https://doi.org/10.1287/mnsc.2020.3648.
Dubé JP, Misra S (2019) Personalized pricing and customer welfare. Preprint, submitted August 3, https://ssrn.com/abstract=2992257.
Dubé JP, Luo X, Fang Z (2017) Self-signaling and prosocial behavior: A cause marketing experiment. Marketing Sci. 36(2):161­186.
Goswami I, Urminsky O (2020) No substitute for the real thing: The importance of in-context field experiments in fundraising. Marketing Sci. 39(6):1052­1070.
Hansen K, Misra K, Pai M (2020) Algorithmic collusion: Supracompetitive prices via independent algorithms. Working paper, University of California, San Diego.
Hershfield HE, Shu S, Benartzi S (2020) Temporal reframing and participation in a savings program: A field experiment. Marketing Sci. 39(6):1039­1051.
Huang S, Aral S, Hu J, Brynjolfsson E (2020) Social advertising effectiveness across products: A large-scale field experiment. Marketing Sci. 39(6):1142­1165.
Kim M, Sudhir K, Uetake K (2020) A structural model of a multitasking salesforce: Job task allocation and incentive plan design. Working Paper, Yale University, New Haven, CT.
Li J, Lim N, Chen H (2020) Examining salesperson effort allocation in teams: A randomized field experiment. Marketing Sci. 39(6): 1122­1141.
Mislavsky R, Dietvorst B, Simonsohn U (2020) Critical condition: People don't dislike a corporate experiment more than they dislike its worst condition. Marketing Sci. 39(6):1092­1104.
Misra K, Schwartz EM, Abernethy J (2019) Dynamic online pricing with incomplete information using multiarmed bandit experiments. Marketing Sci. 38(2):226­252.
Mohan B, Buell R, John L (2020) Lifting the veil: The benefits of cost transparency. Marketing Sci. 39(6):1105­1121.
Munz KP, Jung MH, Alter AL (2020) Name similarity encourages generosity: A field experiment in email personalization. Marketing Sci. 39(6):1071­1091.
Seashore S (1964) Field experiments with formal organizations. Human Organ. 23(2):164­170.
Simester DI (2017) Field experiments in marketing. Duflo E, Banerjee A, eds. Handbook of Economic Field Experiments, vol. 1 (Elsevier, Amsterdam), 465­497.
Simester DI, Sun P, Tsitsiklis JN (2006) Dynamic catalog mailing policies. Management Sci. 52(5):683­696.
Simester DI, Timoshenko A, Zoumpoulis S (2020) Efficiently evaluating targeting policies: Improving upon champion vs. challenger experiments. Management Sci. 66(8):3412­3424.
Sudhir K (2016) The exploration-exploitation tradeoff and efficiency in knowledge production. Marketing Sci. 35(1):1­9.
Tian L, Feinberg F (2020) Optimizing price menus for duration discounts: A subscription selectivity field experiment. Marketing Sci. 39(6):1181­1198.
Wolters HM, Schulze C, Gedenk K (2020) Referral reward size and new customer profitability. Marketing Sci. 39(6):1166­1180.

