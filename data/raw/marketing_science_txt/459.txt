Vol. 31, No. 4, July­August 2012, pp. 567­586 ISSN 0732-2399 (print) ISSN 1526-548X (online)

http://dx.doi.org/10.1287/mksc.1120.0718 © 2012 INFORMS

Handling Endogenous Regressors by Joint Estimation Using Copulas
Sungho Park
W. P. Carey School of Business, Arizona State University, Tempe, Arizona 85287, spark104@asu.edu
Sachin Gupta
Samuel Curtis Johnson Graduate School of Management, Cornell University, Ithaca, New York 14853, sg248@cornell.edu
We propose a new statistical instrument-free method to tackle the endogeneity problem. The proposed method models the joint distribution of the endogenous regressor and the error term in the structural equation of interest (the structural error) using a copula method, and it makes inferences on the model parameters by maximizing the likelihood derived from the joint distribution. Similar to the "exclusion restriction" in instrumental variable methods, extant instrument-free methods require the assumption that the unobserved instruments are exogenous, a requirement that is difficult to meet. The proposed method does not require such an assumption. Other benefits of the proposed method are that it allows the modeling of discrete endogenous regressors and offers a new solution to the slope endogeneity problem. In addition to linear models, the method is applicable to the popular random coefficient logit model with either aggregate-level or individual-level data. We demonstrate the performance of the proposed method via a series of simulation studies and an empirical example.
Key words: endogeneity; copula method; instrumental variables; two-stage least squares; linear regression model; logit model; random coefficient
History: Received: December 30, 2010; accepted: March 18, 2012; Preyas Desai served as the editor-in-chief and Michel Wedel served as associate editor for this article. Published online in Articles in Advance May 22, 2012.

1. Introduction
A prominent challenge faced by empirical researchers in marketing, economics, and many other social sciences is endogenous regressors. Fundamentally, the problem arises because researchers are interested in questions of cause and effect, yet data in these disciplines are frequently obtained in nonexperimental settings. The most common form of this problem is correlation between regressors and random shocks. The canonical example arises in simultaneousequations models of supply and demand. Researchers are interested in estimating the slope of the demand curve. However, ordinary least squares (OLS) estimates are biased because both the supply and the demand schedules are influenced by common shocks. Similarly, if there are unmeasured product characteristics that influence consumer decisions but are not observed by the researcher, then while the marketer observes these and incorporates them into decision making, an endogeneity problem could arise (Berry 1994, Villas-Boas and Winer 1999). Endogeneity problems also occur when there are omitted variables in a model of a causal relationship, such as the effect of schooling on wages (in this relationship, omitted variables may be motivation, ability, and socioeconomic status, which affect both schooling and wages), the

impact of medical treatments on health, or the effect of social insurance programs on labor supply.
Three broad classes of solutions to the problem of endogenous regressors have been discussed in the literature. The textbook solution uses observed instrumental variables to correct for endogeneity (Angrist and Krueger 2001 provide a survey of the history). The ideal instrumental variable meets two requirements: it is correlated with the endogenous regressor through a relationship that the researcher can explain and validate, and it is uncorrelated with the error term. The theory of instrumental variables is well developed, but applied researchers face the critical practical problem that good instruments are very hard to find. The common prescription that good instruments come from detailed knowledge of the economic mechanism and institutions that determine the (endogenous) regressor of interest does not often help in practice. Even if an instrumental variable is available, it is often subject to potential pitfalls because it fails to meet the two requirements adequately. The most serious problem is the violation of the "exclusion restriction": the instrument should be uncorrelated with the error term in the structural equation of interest. Researchers routinely devote a great deal of effort toward convincing the reader that

567

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

568

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

their assumed exclusion restriction is justifiable, but this claim is often debated (Conley et al. 2012). Note that this claim is impossible to test directly because of unobservability of the error.
Another important problem arises as a result of "weak instruments"--instruments that are insufficiently correlated with the endogenous regressors, which may result in biased estimates. In a landmark study of the effect of schooling on wages, Angrist and Krueger (1991) used census data with more than 329,000 observations and more than 180 instruments to find significant results, but they had small F -statistics from first-stage regressions of endogenous regressors on instruments. Bound et al. (1995) reexamined Angrist and Krueger's (1991) data and showed a finite sample bias even with the huge sample sizes. Further, in a rather dramatic demonstration of the problem of weak instruments, they replicated their results using random numbers as instruments. In fact, researchers sometimes have to worry if the "cure is worse than the disease"--is the bias caused by the use of weak instrumental variables worse than the bias caused by endogenous regressors?
The second class of solutions accomplishes correction of the endogeneity problem by supplementing the analysis with a supply-side model that explains observed realizations of the endogenous variables. Because these variables are often marketing variables, the model typically reflects firms' goal-directed behavior as theorized by the researcher. A number of examples of this approach are found in the marketing literature; a very partial list includes Yang et al. (2003), Dotson and Allenby (2010), and Otter et al. (2011). A specific case in this setting is that of slope endogeneity--namely, random coefficients being correlated with the explanatory variable. Manchanda et al. (2004) provides an example of dealing with slope endogeneity via a supply-side model.
Yang et al. (2003) refer to this as a "full-information approach" because in all these cases the researcher needs to have prior knowledge or a theory of the process that generates the endogeneity. For instance, Yang et al. assume a particular game-theoretic model of price setting. The key concern with this approach is that incorrect specification of the supply-side model can lead to biased estimates of demand-side parameters (Chintagunta et al. 2006). Hartmann et al. (2011, p. 1081) note that "this approach is feasible only if full information is available to the analyst about how the firm allocates its marketing efforts. In the absence of such information, the analysis is sensitive to misspecification bias."
The third class of solutions to the endogeneity problem includes statistical approaches that require neither observed instruments nor specification of

a supply-side model.1 These are sometimes called "instrument-free" approaches. Ebbes et al. (2009) provide an excellent review and compares three extant instrument-free approaches: the higher moments (HM) approach (Erickson and Whited 2002, Lewbel 1997), the identification through heteroscedasticity (IH) estimator (Rigobon 2003, Hogan and Rigobon 2003), and the latent instrumental variables (LIV) method (Ebbes et al. 2005). In the HM approach, instruments are generated using the available data by exploiting higher-order moments. Similar to other method-ofmoments approaches, this method does not make strong assumptions about the distributions of variables. However, this method works under certain higher-order moment conditions that may or may not hold (e.g., the requirement that the second moments of the mean-centered dependent variable and a meancentered endogenous regressor are uncorrelated). The IH estimator also exploits the higher-order moments for model identification and is based on the methodof-moments approach. It requires researchers to have some information on heteroscedasticity: an observable grouping variable has to be available that describes the heteroscedastic error structure. However, such a variable may not always be available. The LIV method assumes that the endogenous regressor can be separated into an exogenous part and an endogenous part. The exogenous part is approximated using a latent discrete variable with a finite number of levels. Unlike the HM and IH methods, the LIV method is likelihood based. It does not require information on higher-order moment conditions or heteroscedasticity and thus is more generally applicable (see recent applications in Rutz and Trusov 2011, Rutz et al. 2012, and Zhang et al. 2009).
In all three extant instrument-free approaches, the endogenous regressor P can be represented as P = f Z + , where f Z is treated as a random variable and unique structures are imposed on f Z for model identification. For instance, f Z is assumed to be discrete in the LIV method (leading to a latent class model), skewed in the HM method, and heteroscedastic in the IH method. These assumptions ensure the distribution of P to be different from that of the structural error and thus provide model identification. A key assumption of all three extant instrument-free approaches is that random variable f Z is exogenous. This assumption is similar to the
1 There are in addition a few published approaches that do not neatly fit into our classification. For instance, Hartmann et al. (2011) applied a regression discontinuity approach to handle the endogeneity problem. Bronnenberg and Mahajan (2001) and van Dijk et al. (2004) used spatial econometrics approaches to tackle endogeneity. However, these methods are context-specific and often also require detailed knowledge of the process that generates the endogeneity.

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

569

"exclusion restriction" for observed instrumental variable methods.
There are two reasons for possible concern with the exogeneity assumption. First, in applications of instrument-free methods, researchers face a significant hurdle in arguing that the exclusion restriction is reasonable because they cannot observe f Z and do not know what it represents. Second, in some situations one could offer conceptual challenges to the exogeneity assumption based on context-specific knowledge. For instance, reasons for possible endogeneity of retail prices that have been discussed in the literature are manufacturer coupons, TV advertising, and retailer's strategic behavior (e.g., shelfspace allocation or special display), which are known to the retailer and consumer but unobserved by the researcher (Besanko et al. 1998); hence they are omitted variables in the demand model. Manufacturer coupons affect prices, and the latent discrete instrument variable in the LIV approach is then essentially capturing the presence versus absence of the coupon. The structural error in the demand model captures the omitted variables discussed previously. If manufacturers strategically plan both TV advertising and manufacturer coupons, or retailers allocate good shelf space for items with manufacturer coupons, then the latent discrete instrument variable might be correlated with the structural error and thus endogenous.
Our proposed method, which we describe next, falls into the third class of approaches for correcting for endogeneity; it is a statistical, instrumentfree method. We model the joint distribution of the endogenous regressor and the error term in the structural equation of interest (we refer to this as structural error) and make inferences on the model parameters by maximizing the likelihood from the joint distribution. We use a nonparametric density estimation method to build the marginal distribution function of the endogenous regressor. The suggested procedure allows the observed data to determine the marginal distribution flexibly and does not impose any restrictive assumptions on the stochastic behavior of the data. To build the joint distribution function of nonparametrically determined endogenous regressors and error terms in the structural equation, we use a copula method. Copulas are functions that join or "couple" multivariate distributions to their one-dimensional marginal distribution functions (Balakrishnan and Lai 2009). In particular, using a Gaussian copula model, we construct a multivariate distribution that effectively captures the correlation between the regressor and the structural error. Once this correlation is properly handled, the proposed model does not suffer from the endogeneity problem, and we can obtain consistent estimates for model parameters.

Next we compare and contrast our proposed approach with extant instrument-free approaches. An important advantage of our proposed method is that it does not require the assumption of exogeneity of f Z because it directly addresses the correlation between P and the structural error using a copula. It only requires the distinctiveness between the distributions of P and the structural error, which is a requirement in other instrument-free methods as well. In §3.9, we illustrate this advantage of the proposed method in a simulation study.
We believe that among extant instrument-free methods, the LIV method is most closely related to our proposed method. Both the LIV and the proposed method assume that the structural error follows a normal distribution. If the endogenous regressor is also normally distributed, both models encounter difficulty in separating variation as a result of the endogenous regressor from variation due to the structural error; hence an identification problem arises.2 An important strength of the copula model is that it can capture the correlation between discrete variables and continuous variables. As a result, the proposed model can handle discrete endogenous regressors, a noteworthy advantage over the LIV and other instrumentfree methods. Another advantage of the proposed method is that it can be used to handle the "slope endogeneity" problem that occurs when a regressor is correlated with its random coefficient. Researchers have typically used a "supply-side" model or instruments to tackle this problem (Manchanda et al. 2004, Kuksov and Villas-Boas 2008, Luan and Sudhir 2010). As noted previously, supply-side approaches are subject to possible misspecification if the researcher does not have adequate knowledge of the process generating the endogeneity. To the best of our knowledge, ours is the first statistical, instrument-free method to handle the slope endogeneity problem.
The method we propose is applicable both in linear models and in models that are nonlinear in variables and parameters. As an example of the latter, we apply the model to the random coefficient logit (RCL) model that has now become the workhorse of demand analysis in marketing and economics. We show that our proposed approach can be applied to correct for endogeneity when the RCL model is estimated using either individual-level or aggregate-level data. In simulation studies, we find that the proposed method provides unbiased and consistent estimates of model parameters, not only when the dependence structure between the endogenous regressor and the structural error is what the Gaussian copula assumes but also when this is not the case. That is, the proposed method
2 We provide more details on model identification in Online Appendices 1 and 2 (at http://dx.doi.org/10.1287/mksc.1120.0718).

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

570

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

captures the dependence structure well enough to overcome the endogeneity problem. We also show that the proposed method is robust to misspecification of the distribution of the structural error. In an empirical application to paper towel sales data, we find that the proposed instrument-free method and the classical method using instrumental variables lead to essentially the same results on the endogeneity of retail price. This provides convergent validity to the proposed method. All in all, our results suggest that the proposed method handles the endogeneity problem well.
The remainder of this paper unfolds as follows. In §2, we explain the proposed method in detail. In §3, we evaluate the performance of the proposed method in several simulation studies. In §4, we apply the proposed method to real marketing data. We conclude in §5.

2. Model and Estimation Procedure
We first present the proposed method in the context of the familiar linear regression model. We show how to handle multiple endogenous regressors and discrete endogenous regressors in §§2.2 and 2.3, respectively. We then explain how the method can be applied in RCL models, to individual-level data in §2.4, and to aggregate-level data in §2.5. Finally, we explain how the slope endogeneity problem can be handled using the proposed method in §2.6.

2.1. Linear Regression Model Consider the following linear regression model with an endogenous regressor:

Yt = Xt + Pt + t

(1)

where t = 1 T indexes either time or cross-
sectional units, Yt is a 1 × 1 dependent variable, Xt is a k × 1 vector of exogenous regressors, Pt is a 1 × 1 continuous endogenous regressor, t is a structural error term, and and are model parameters. Pt is correlated with t, and this correlation generates the endogeneity problem. To handle this problem, one can
use instrumental variables that are correlated with Pt but uncorrelated with t. The instrument equation is

Pt = Zt + t

(2)

where Zt is a l × 1 vector of instruments, is a l × 1 vector of parameters, and t is a random error term correlated with t. Two classical instrumental variables estimators are the two-stage least
squares (TSLS) estimator and the generalized method-
of-moments (GMM) estimator. To obtain the TSLS
estimator, in the first stage the endogenous regressor
Pt is regressed on Zt, and in the second stage predicted values of Pt are plugged into (1) instead of Pt.

The GMM estimator is based on the orthogonality condition between Zt and t. Neither of these methods imposes specific assumptions regarding the distribution of the error terms t and t. Ebbes et al. (2005; i.e., the LIV method) and some other researchers make the assumption that t and t are jointly bivariate normal (e.g., Chao and Phillips 1998, Kleibergen and Zivot 2003, Rossi et al. 2005).
Given "good" instrumental variables, one can obtain consistent estimates for the model parameters using TSLS, GMM, or other methods. However, even without instruments, one can obtain consistent estimates if the joint distribution of the endogenous regressor Pt and the structural error term t is known. To illustrate this, let us assume that Pt and t are generated from a certain bivariate distribution f p and researchers know this. Then, one can obtain consistent estimates of the model parameters by maximizing the log-likelihood function derived from f p . As this simple example illustrates, researchers can resort to the information in the joint distribution of the endogenous regressor and the structural error term instead of instrumental variables. The proposed method is based on this essential idea.
To implement the proposed method, researchers need to know the joint distribution of the endogenous regressors Pt and the structural error term t, which is often unknown. To address this problem, we propose a unique copula model. In traditional multivariate distributions, once the parametric joint distribution function has been selected, the marginals are derived by integration. In other words, there is no flexibility for the marginals, and they are determined entirely by their parent joint multivariate distributions (Danaher and Smith 2011). Thus, traditional multivariate distributions limit the choice of marginals for the endogenous regressor and the structural error term and can impose restrictive assumptions on the stochastic behavior of the observed data, resulting in biases. Copula models overcome these shortcomings of traditional multivariate models by starting from the marginals. In the proposed approach, using information contained in the observed data, we first select marginal distributions for the endogenous regressor and the structural error term, respectively. Then, the copula model enables us to construct a flexible multivariate joint distribution from the marginals, allowing a wide range of correlations between the two marginals.
We assume that the marginal distribution of the structural error term is normal. The normality assumption for the structural error seems reasonable and has been widely used in the literature (e.g., Villas-Boas and Winer 1999, Yang et al. 2003, Chao and Phillips 1998, Kleibergen and Zivot 2003, Rossi et al. 2005, Ebbes et al. 2005). Of course,

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

571

other assumptions are possible. For example, if a researcher believes that the actual error distribution may have thicker tails than the normal, Student's t-distribution can be used. In §3, we assess robustness of the proposed model to possible misspecification in this respect. We find that model estimates are fairly robust to violations of the distributional assumption of t.
We use nonparametric density estimation to identify the marginal distribution of the endogenous regressor Pt. Unlike the structural error term, which is unobserved, we see sample data from the true distribution of Pt. Nonparametric density estimation allows the data to speak for themselves in determining the density function, thus avoiding problems of distributional misspecification. Suppose we are given independent and identically distributed observations P1 PT with the density function h p . The simplest nonparametric density estimator is a naïve estimator (Silverman 1986):

1T

h^ p

=

T

·

b

W
t=1

p - Pt b

where W x = 0 5 · I x  1 , I · is an indicator function, and b is the bandwidth parameter that determines the smoothness of the kernel estimator. The larger the bandwidth, the smoother the density estimate. This estimator can be considered as a sum of "boxes" centered at the observations. Note that this estimator has some undesirable properties:
h^ p is a step function and weights each observation inside the box equally. Because of these shortcomings, the kernel density estimator is preferred by many researchers. The kernel density estimator replaces W x , or "boxes," by smooth "bumps," or a kernel function K x , that is continuous and symmetric and satisfies K x = 1. One of the most commonly used kernel functions is the Epanechnikov kernel, and we use this kernel function in the proposed method. So the marginal density estimator for the endogenous regressor can be written as

1T

h^ p

=

T

·b

K
t=1

p - Pt b

(3)

where K x = 0 75 · 1 - x2 · I x  1 . Bandwidth selection is of crucial importance in
density estimation, and various methods have been proposed in the literature. Silverman (1986) has suggested a frequently used data-driven bandwidth b = 0 9 · T -1/5 · min s IQR/1 34 , where s is the sample standard deviation of the data and IQR is the interquartile range (the difference between the 75th and 25th percentile). This rule is regarded as a good starting point (DiNardo and Tobias 2001), and we use

this bandwidth in the proposed method. The kernel
estimator h^ p can be shown to be consistent for h p if b goes to 0 at a suitable rate as the sample size T goes to infinity.
Now we are ready to construct the joint distribution function from the marginal distributions of the endogenous regressor and the structural error term, denoted by H p and G , respectively. Also, let F p be a joint distribution function with marginals H p and G . Sklar's theorem (Sklar 1959) states that there exists a copula function C such that for all p and ,

F p = C H p G = C Up U

(4)

where Up = H p and U = G . This theorem is central to the theory of copulas and clarifies the role that

copulas play in the relationship between multivariate

distribution functions and their univariate marginals.

Since H · and G · are marginal distribution func-

tions, Up = H p and U = G are probability integral transformations, and thus Up and U are uniform(0, 1) random variables and the copula can be viewed as

a function 0 1 2  0 1 . Now it becomes clear that

the copula provides a way to study and model the

scale-free dependence between variables after filtering

out the influence of marginal distributions. Let f p

denote the joint density function. Then, from (4), we

have

f p = c Up U h p g

(5)

where c Up U = 2C/ p and g is the marginal density function of the structural error.
In the statistics literature, various copula functions have been studied (see Nelsen 2006 or Balakrishnan and Lai 2009). In the proposed method, we use the Gaussian copula:

C Up U

=

-1 Up

-1 U

1 = 2 1 - 2 1/2

-1 Up
·
-

-1 U -

exp

- s2-2 · s · t+t2 2 1- 2

ds dt

(6)

where denotes the univariate standard normal distribution function and denotes the bivariate standard normal distribution function with the correlation coefficient . If we let P  = -1 Up and
 = -1 U , the Gaussian copula models P   as the standard bivariate normal distribution with the correlation coefficient . The Gaussian copula is a general and robust copula for most applications (Song 2000) and has many desirable properties (see Danaher and Smith 2011). There are numerous applications of Gaussian copulas, particularly in hydrology and finance. Recently, Danaher and Smith applied the

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

572

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

Gaussian copula to multiple marketing data sets and showed that it performs better than existing probability models. In §3, we assess robustness of the Gaussian copula in capturing the dependence between the endogenous regressor and the structural error. From (5) and (6), we can write the joint density function of Pt and t:

f Pt t

1

2

= 1 - 2 1/2 exp -

-1 Up t 2 +

-1 U

2 t

2 1- 2

+

-1 Up t -1 U t 1- 2

h Pt g t

(7)

We can obtain consistent estimates by maximizing the following log-likelihood function, derived from (7), with respect to the model parameters =
:

ln l Pt t

T = - 2 ln 1 -

2

T
-
t=1

+ ln 0 2 t

2

-1 UP t 2 +

-1 U

2 t

2 1- 2

-1 UP t -1 U t

-

1- 2

(8)

where 0 2 · is the normal density with mean 0 and variance 2. Note that the nonparametric density

h p does not include any parameters and thus dis-

appears from the log-likelihood function. However,

it influences the log-likelihood function via UP t. By

definition, UP t = H Pt

=

Pt -

hx

dx. To obtain UP t,

we integrate the nonparametric density estimate h^ p

using a simple numerical method.3 U t can be easily calculated from the normal distribution func-

tion. For the implementation of the proposed method

in the simulation study and empirical application,

we use the GAUSS package. Specifically, we use

the quasi-Newton numerical optimization routines

(the Broyden­Fletcher­Goldfarb­Shanno method) for

the maximization of the likelihood function.

The proposed copula model implies that P  

follows the standard bivariate normal distribution

with correlation coefficient . This can be rewritten as

follows:

Pt


=

1

t

0 1- 2

1t 2t

3 Specifically, we use the trapezoidal rule for numerical integration. This method works by constructing a grid on the integration variable and thus is easy to implement. And instead of the integration of density estimate, one can use the empirical distribution function to obtain UP t. We found that this approach provides comparable results to the proposed method.

1 t N
2t

0 0

10 01

or

 t

=

·

1 t+

1- 2·

2 t=

· Pt +

1- 2·

2 t.

Also, the normality assumption of the structural error

states that U t = G t = 0 2 t , where 0 2 · is the distribution function of the normal with mean

0 and variance 2. Then,

t=

-1 02

U

t

=

-1 02

 t

=

·

 t

= · · Pt + · 1 - 2 · 2 t

(9)

Finally, we can rewrite (1) as follows:

Yt = Xt + Pt + · · Pt + · 1 - 2 · 2 t (10)
This representation provides another way to estimate the model. Given Pt as an additional regressor, 2 t is not correlated with any other terms on the right-hand side of (10), and thus we can consistently estimate the model using the least squares estimator. Pt can be obtained by transforming UP t using numerical integration of the nonparametric density estimator h^ p . Note that Pt corrects for the endogeneity bias and in this respect is similar to the "control function" (Heckman 1978, Hausman 1978).
As we explained in §1, the nonnormality of an endogenous regressor is required for the model identification. We can easily understand the role of nonnormality in model identification using (10). If Pt follows a normal distribution, Pt is a linear transformation of Pt since Pt = -1 H Pt . Hence, we cannot separately identify and · in (10). As the true distribution of Pt approaches a normal distribution, the correlation between Pt and Pt increases, causing a multicollinearity problem in (10). As is well known, multicollinearity does not create bias, only inefficiency. It appears that there is no simple remedy to this problem. If the standard errors of estimates are too large to draw meaningful conclusions, researchers might be able to mitigate this problem by increasing the sample size.
The inference procedure of the proposed method occurs in two stages. In the first stage, the density of the endogenous regressor is estimated using a nonparametric method, and then UP t or Pt are computed from the estimated density. In the second stage, computed UP t or Pt are used to construct the likelihood function, and the parameters are estimated. Here, computed UP t and Pt are the "generated regressors" (Pagan 1984). However, the usual standard errors of parameters based on the information matrix regard the generated regressors as the given observations, and thus the standard errors are incorrect. We propose a simple bootstrap method to compute the correct

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

573

standard errors.4 The bootstrap method is a sample reuse technique designed to estimate standard errors and confidence intervals (Efron 1979). To illustrate the method, from the original data with T observations, we draw T observations with replacement. Using these resampled data, estimate the model parameters and save them. This process is repeated many times. The bootstrap standard error is the sample standard deviation of the saved estimates.5
Shih and Louis (1995) and Genest et al. (1995) examine general properties of two-step semiparametric copula estimators such as our method, and they show that the estimators are consistent and asymptotically normal under common mild regularity conditions if the first-step nonparametric estimation is consistent. In our first-step nonparametric estimation, we use the Epanechnikov kernel with Silverman's bandwidth selection rule, which has been shown to be consistent (Silverman 1986). In the simulations discussed in §3, we provide further empirical evidence of consistency.

2.2. Multiple Endogenous Regressors Given (10), we can easily extend the proposed method to handle multiple endogenous regressors. Let us consider the following linear model with two endogenous regressors (P1 t and P2 t :

Yt = Xt + P1 t 1 + P2 t 2 + t

(11)

The proposed model assumes the following relationship among variables:

 P1 t 

 0   1

12


1

 P2 t   N  0   12 1

2 

 t

0

1

21

where P1 t = -1 H1 P1 t , P2 t = -1 H2 P2 t , and H1 · and H2 · are distribution functions of P1 t
and P2 t, respectively. This can be rewritten as follows:

1

 P1 t  

 12

 

P2

t

 

=

 

 

 t

1

0

1-

2 12

2 - 12 1

1-

2 12

0



0

 

1-

2 1

-



2 - 12

2 1

1-

2 12




1t
· 2 t
3t

4 The proposed method can be classified as a semiparametric method because it combines a nonparametric part (the distribution of endogenous regressor) and a parametric part (the distribution of the structural shock). In semiparametric models, analytical derivation of asymptotic standard error can be extremely difficult, and one can use bootstrap standard error instead.
5 In the simulation study and empirical application, we use 100 replications to estimate the standard error. One exception is Simulation Case 5 (T = 1 000), where we use 50 replications because of computation time. It is reported that a total of 50­200 replications is generally adequate for estimates of standard error (Mooney and Duval 1993).


1t

 0   1 0 0 

 2 t   N 0   0 1 0 

3t

0 001

Then,

t=

·

 t

=

·

1 - 12

1-

2 12

2 · P1 t +

2 - 12 1

·

1-

2 12

· P2 t +

·

1-

2 1

-

2 - 12

2 1

1-

2 12

· 3t

Also, we can rewrite (11) as follows:

Yt = Xt + P1 t 1 + P2 t 2 +

·

1 - 12

1-

2 12

2 · P1 t

+

·

2 - 12

1-

2 12

1 · P2 t

+

·

1-

2 1

-

2 - 12

2 1

1-

2 12

· 3t

(12)

Given P1 t and P2 t as additional regressors, 3 t is not correlated with any other terms on the right-hand side of (12), and thus we can consistently estimate the model using the least squares estimator. P1 t and P2 t can be obtained from the nonparametric densities of P1 t and P2 t. Note that we are not estimating twodimensional nonparametric distribution in the case of two endogenous regressors. Copulas combine onedimensional marginals to construct a joint distribution. Thus, the proposed copula model does not suffer from the "curse of dimensionality problem," which is frequently encountered in nonparametric estimation of multivariate distribution functions. Further extension to more endogenous regressors is possible using a similar construction as in (12).

2.3. Discrete Endogenous Regressor Now assume that endogenous regressor Pt is a discrete variable. The distribution function H · is a step function, and we can define Up t to be related to the discrete variable Pt through the following inequality:
H Pt - 1 < UP t < H Pt
Up t is uniformly distributed on [0, 1] and can be considered as a latent variable that determines the discrete outcome of Pt. The above inequality implies the following relationship between Pt = -1 UP t and H Pt :
-1 H Pt - 1 < Pt < -1 H Pt
H Pt can be easily estimated from the frequencies of observed data, and thus we do not need to perform the nonparametric density estimation nor the numerical integration to obtain Pt as in the continuous

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

574

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

regressor cases. By introducing Pt as an additional regressor, we can consistently estimate the model parameters as in (10).
The proposed method decomposes the structural error into one that is correlated with the endogenous regressor and the other that is unrelated to the endogenous regressor by relying on the information contained in the distribution of the endogenous regressor. Therefore, the precision of the proposed method depends on the quality of information contained in the distribution of the endogenous regressor. Some commonly used discrete distributions in the marketing literature are the Poisson, the negative binomial, and the beta-binomial distribution. These have been employed to model, for instance, the number of exposures to a TV advertisement (Rust 1986). Although these distributions contain rich information, some discrete variables do not contain enough information to decompose the structural error into the correlated and uncorrelated parts. For example, a Bernoulli random variable has only two possible outcomes (0 and 1) and does not contain rich enough information. But as the support of the discrete variable increases, the performance of the proposed method improves quickly. We demonstrate with simulation results in §3.
2.4. Random Coefficient Logit Model Using Individual-Level Data
The RCL model has been used in numerous empirical studies in marketing and economics and has been applied to individual-level data (e.g., Chintagunta et al. 2005). We briefly describe this model next. Let us specify an individual's latent utilities for choice alternatives as follows:
uh0t = h0t if no purchase,
uhjt = Xjt h + Pjt h + jt + hjt
= Xjt ¯ + bh + Pjt ¯ + ah + jt + hjt
j = 1 J (13)
where t = 1 T denotes time, j denotes choice alternatives, and h = 1 H denotes individuals.
h0t and hjt are Type I extreme value errors, and (13) becomes a logit model. The structural error jt is correlated with Pjt, and this correlation causes the endogeneity problem as in the linear regression case. jt is also referred to as "common shock" or "unmeasured product characteristic" in the literature. Consumers' heterogeneous preference is considered via ah and bh in the model.
To estimate the model, we follow a two-step estimation method. The basic approach to handle the endogeneity problem remains the same as in the linear

regression case. The second equation in (13) can be rewritten as follows:

uhjt = Xjt ¯ + Pjt ¯ + jt + Xjtbh + Pjtah + hjt = jt + Xjtbh + Pjtah + hjt

where jt = Xjt ¯ + Pjt ¯ + jt. jt is common to all individuals. We treat jt j=1 J t=1 T as parameters in the first step and estimate these along with the
other parameters. Because we have individual-level
data, the first step estimation is straightforward. jt are regarded as time- and alternative-specific fixed
effects, and a simulation method is applied to han-
dle random deviations ah and bh. Also, note that the endogeneity problem does not arise in the first stage
because the endogenous regressor and the structural
error are both encapsulated in jt. Let us denote the
first-step estimates of jt as ^jt = jt + jt, where jt is the estimation error, which is asymptotically normal.
Now we have

^jt = jt + jt = Xjt ¯ + Pjt ¯ + jt + jt = Xjt ¯ + Pjt ¯ + ¯jt

(14)

where ¯jt = jt + jt. Equation (14) is the same as the linear regression model (1) except that an additional dimension for choice alternatives j is added in the data. At the second step, we can consistently estimate the model parameters of interest ¯ and ¯ using the copula method described in the previous section.
Note that our two-step estimation strategy is similar to the method proposed by Chintagunta et al. (2005). The first-step estimation is the same as their approach. In the second step, we apply the proposed
copula method to ^jt to handle the endogeneity problem, whereas Chintagunta et al. (2005) applies the
instrumental variable method to ^jt.

2.5. Random Coefficient Logit Model Using

Aggregate-Level Data

The RCL model has been applied to aggregate-

level data by numerous researchers (e.g., Chintagunta

2001, Sudhir 2001) beginning with Berry et al. (1995).

The utility specification for the RCL model with

aggregate-level data is the same as (13). However,

what we observe in the data is different. If individual-

level data are available, a researcher knows the actual

choice of individual h at time t. Aggregate-level data

add up individual-level outcomes to J × T market

shares Sjt = njt/H , where njt is the count of purchases for alternative j in time period t. Note that the two-

step estimation of the previous section is no longer

feasible because the number of first-step parameters

is greater than the number of observations J × T .

Let us assume that jt  N 0

2
j

and

Pjt

 jt

follows

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

575

the standard bivariate normal distribution with cor-

relation coefficient j for j = 1 J . Then, as in (9),

jt can be written as

j · j · Pjt +

j·

1-

2 j

·

2 jt.

Consequently, we can rewrite the second equation of

(13) as follows:

uhjt = Xjt h + Pjt h + j · j · Pjt

+

·
j

1-

2 j

·

2 jt + hjt

(15)

2 jt is an independent and identically distributed random shock with a standard normal distribution
and is uncorrelated with any other term in (15). There-
fore, we can treat 2 jt as an additional error component and integrate it out of the demand function. Pjt can be obtained from UP jt using the numerical inte-
gration of the nonparametric density estimator h^ j pj , as we have explained in §2.1. From (15) we can derive
the logit probability that individual h chooses alter-
native j:

J

Prh0t = 1 · 1+ exp Xit ¯ +bh +Pit ¯ +ah

i=1

-1

+

i · i · Pit +

i·

1-

2 i

·

2 it

Prhjt = exp Xjt ¯ +bh +Pjt ¯ +ah

+

j · j · Pjt +

j·

1-

2 j

·

2 jt

J

· 1+ exp Xit ¯ +bh +Pit ¯ +ah

i=1

-1

+

·
i

i · Pit +

·
i

1-

2 i

·

2 it

j=1 J

Conditional on 2 t = 2 1t

2 Jt , we can write

the likelihood of the observed aggregate-level data in

time period t:

Lt

H!

J

2 t = n0t! · · · nJt! j=0

Prhjt 2 t ah bh

nj t

· ah bh d ah bh

(16)

where ah bh is the density function of the heterogeneity distribution. Since 2 t are unknown, we integrate them out:

Lt = Lt 2 t

2d 2

where · is the standard normal density. The like-

lihood function for the sample of T weeks becomes

T t=1

Lt .

Monte

Carlo

simulation

methods

can

be

used

to calculate the likelihood (see Keane 1993), and the resulting estimator becomes a simulated maximum likelihood (SML) estimator. To be more specific, the integrals in the likelihood function can be calculated by taking averages of computed integrands using random normal draws for ah, bh, and 2 t. The main idea of this estimation scheme is in line with Park and Gupta's (2009) SML estimator, in which the authors decompose jt into an endogenous part and an exogenous part and account for the endogenous part using instrumental variables via control function and then integrate out the exogenous part. A critical differentiating advantage of our proposed method is that it does not require instruments, whereas Park and Gupta's SML does. One computational issue with the proposed SML is that the evaluation of the likelihood function may encounter numerical problems because
Prhjt 2 t ah bh ah bh d ah bh njt in (16) reaches machine 0 fairly quickly. If such a problem occurs, researchers can use a subsampling method.6

2.6. Slope Endogeneity Consider the following random coefficient model:

Yt = Xt t + Pt ~t + t = Xt t + Pt ¯ + t + t

(17)

where t = 1 T indexes cross-sectional units (i.e.,
markets or households), and t is a vector of random coefficients. Unlike the previous specifications, t is an exogenous normal shock, and thus endogeneity as
a result of correlation between the structural error and regressors is not an issue here.7 We assume that t  N 0 2 , and by doing so we capture the heterogene-
ity of cross-sectional units with respect to Pt. More importantly, we assume that t is correlated with Pt. In the presence of correlation between coefficient and
regressor, both the OLS estimator and the standard
IV estimator are inconsistent (Heckman 1976), and
this problem in estimation is referred to as "slope
endogeneity." This problem occurs commonly in price
discrimination situations in which the level of Pt is strategically set based on the value of t. For example, assume that Pt is price, ~t represents price sensitivity, and t indexes geographic markets. The marketing
manager of market t might use knowledge of price
sensitivity of the market area to set market-specific
prices. This strategic decision results in correlation
between Pt and t, and the slope endogeneity problem arises.

6 See Park and Gupta (2009) for a detailed discussion of the subsampling method. Musalem et al. (2009) reported a similar numerical problem when the number of individuals underlying the aggregate share is larger than about 500. To circumvent this numerical problem, they suggested a subsampling method that is conceptually similar to Park and Gupta's (2009) approach.
7 In the literature, this common type of endogeneity is referred to as "intercept endogeneity" in contrast to "slope endogeneity."

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

576

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

The proposed Gaussian copula implies that

Pt

 t

follows the standard bivariate normal distribution

with correlation coefficient , and from this we can

rewrite (17) as the following:

Yt = Xt t + Pt ¯ + · · Pt + 1 - 2 · 2 t + t

= Xt t + ¯ · Pt + · · Pt × Pt

+

1 - 2 · Pt × 2 t + t

(18)

The interaction term Pt × Pt handles the correlation between Pt and t. 2 t is uncorrelated with Pt, and thus we can regard 2 t as the usual random effect term and use standard methods for random effect
models in estimation.

3. Simulation Study
In this section, we describe Monte Carlo simulation experiments that are designed with the following goals: (a) to assess the performance of the proposed method, (b) to examine robustness of the proposed estimator to key distributional assumptions about the structural error, (c) to assess generalizability and robustness of the suggested Gaussian copula in capturing various dependence structures between the endogenous regressor and the structural error, and (d) to compare the performance of the proposed method with existing methods. To achieve these goals, we consider nine cases that are summarized in Table 1.
For all models, we measure bias as absolute deviation of the mean of the sampling distribution from the true parameter value expressed in terms of number of standard errors of the parameter estimate, and we denote this quantity as tbias. To infer statistical significance, we use the t-test. If the tbias is smaller

than two standard errors (corresponding to the critical t-statistic for a test at 95% confidence), we infer that this deviation could be due to chance; hence the estimate is consistent.

3.1. Case 1: Linear Regression Model The specific data-generating process (DGP) and parameter values we assign are summarized below:

 
t

 0   1 1 0 

 Pt   N  0   1 1 2 

Zt

0

0 21

 0   1 0 5 0 

= N  0   0 5 1 0 8 

(19)

0

0 08 1

t = G-1 U t = G-1

 t

=

-1

 t

(20)

Pt = H -1 UP t = H -1 Pt = Pt

(21)

Yt = · Pt + t = -1 · Pt + t

(22)

We consider two levels of sample size, T = 200 and

T = 400.

 t

and

Pt

are

correlated,

and

thus

t and Pt

are correlated, generating the endogeneity problem.

By letting Zt = Zt , we generate an instrumental variable Zt. The instrumental variable is ideal since it is uncorrelated with t but highly correlated with Pt. Note that t follows the standard normal distribution, and Pt and Zt follow the uniform distribution. We consider four different estimation methods: (i) OLS,

(ii) TSLS, (iii) the proposed method in the form of

(8) (we refer to this as Proposed I), and (iv) the pro-

posed method in the form of (10) (we refer to this as

Proposed II). One thousand data sets are generated as

replicates, and models are estimated on each data set

Table 1 Description of Simulation Studies

Case

Data-generating process

1. Linear regression model, structural error from the normal distribution, Gaussian copula
2. Linear regression model, structural error from the uniform distribution, Gaussian copula
3. Linear regression model, structural error from the normal distribution, non-Gaussian copulas
4. RCL model, individual-level data, structural error from the normal distribution, Gaussian copula
5. RCL model, aggregate-level data, structural error from the normal distribution, Gaussian copula
6. Linear regression model, two endogenous regressors

7. Linear regression model, discrete endogenous regressor

8. Linear random coefficient regression model, slope endogeneity

9. Linear regression model, endogeneity based on LIV model

Goals
Examine the performance of the proposed method in linear regression models
Assess robustness of the proposed method to misspecification of distribution of structural error
Assess robustness of the proposed method to misspecification of the copula
Examine performance of the proposed method in RCL models using individual-level data
Examine performance of the proposed method in RCL models using aggregate-level data
Examine performance of the proposed method when the model has multiple endogenous regressors
Examine performance of the proposed method when an endogenous regressor is discrete
Examine performance of the proposed method in handling slope endogeneity problem
Compare performance of the proposed method with the LIV method

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

577

Table 2 Results of the Simulation Study Case 1

True

OLS

TSLS

T

Parameters values Mean

SE

tbias

Mean

SE

200

-1

-0 589 0 112 3 656 -1 010 0 119

1

--

--

--

1

0.5

--

--

--

400

-1

-0 571 0 080 5 363 -0 997 0 093

1

--

--

--

1

0.5

--

--

--

Note. Mean and SE are the average and standard deviation of the estimates, respectively.

tbias
0 085 -- --
0 033 -- --

Proposed I

Mean

SE

tbias

-1 018 1 012 0 514
-1 000 1 007 0 508

0 139 0 059 0 063
0 098 0 039 0 049

0 130 0 205 0 217
0 002 0 175 0 156

Average bootstrap SE
0 139 0 058 0 062 0 097 0 038 0 050

to obtain the empirical sampling distribution of the

parameter estimates.

Estimation results are summarized in Table 2. We

first discuss the results in the upper panel based

on T = 200. As expected, OLS estimates of are

biased (tbias = 3 656 standard errors) toward 0 as a result of the endogeneity problem. Also as expected,

TSLS corrects for the bias by using the instrument

Zt and provides a consistent estimate of . Estimates from methods Proposed I and II are almost identical

(confirming their equivalence); hence, we only show

Proposed I. Both proposed methods provide consis-

tent estimates without the use of instruments. The

standard error of is 0.139 in the proposed meth-

ods, which is 17% larger than the standard error of

the TSLS estimate. This reduced efficiency is because

TSLS uses the "ideal" instrumental variable, whereas

the proposed methods do not use any instruments.

The estimates of 1 are tightly distributed near the true value 0.5. Also, the estimates are significantly

different from zero, implying significant dependence

between

 t

and

Pt,

or

significant

dependence

between

t and Pt subsequently. The last column of Table 2

reports the average of bootstrap standard errors,

which for all parameters are very close to the standard

errors from the proposed method, confirming that the

bootstrap method provides precise standard errors.8

The lower panel shows results for the larger sam-

ple size T = 400. Although essential conclusions are

maintained, it is noteworthy that the bias (defined

as the absolute deviation of the true parameter value

from the estimate; not shown in table) and standard

error of both TSLS and Proposed I estimates move

toward 0, supporting our claim of consistency.

3.2. Case 2: Misspecification of t The proposed methods assume that the structural
error t is normally distributed. Whereas the normality of t is a reasonable statistical assumption, the true

8 In all simulation cases, we found the same result--namely, that bootstrap standard errors are very close to the empirical standard errors of the proposed method. We provide the bootstrap standard errors for the other cases in Online Appendix 3. Note that the bootstrap standard errors are the same for Proposed I and Proposed II.

distribution of t is unknown to the researcher, thus leaving room for the possibility of misspecification.
In this simulation, we examine the robustness of the
proposed methods to violations of the normality of
t. To this end, we generate 1,000 data sets using the following DGP:



t
Pt

N

0 0

1 1

=N

0 0

1 05 05 1

(23)

t = G-1 U t = U t - 0 5 =

 t

-0 5

(24)

Pt = H -1 UP t =

-1 11

UP t

=

-1 11

Pt

(25)

Yt = · Pt + t = -1 · Pt + t

(26)

where T = 200 and

-1 11

·

is the inverse distribution

function of the normal with mean 1 and variance 1.

Note that t  U -0 5 0 5 . For estimation, we use the OLS estimator along with the proposed methods

(Proposed I and Proposed II), all of which assume

normality of t. Table 3 reports estimation results. As in Case 1,

OLS estimates are biased (tbias = 5 385). By contrast, both proposed methods successfully recover the true

parameter values despite the misspecification of t (again, we only show results for Proposed I because

these are identical to Proposed II results).9 We believe

this is because the normal distribution approximates

other distributions flexibly.

3.3. Case 3: Misspecification of Copula
In the proposed methods, we use the Gaussian cop-
ula to capture the dependence structure between Up and U . In real data, the dependence between Up and U is a consequence of an economic mechanism
(such as marketing strategy decisions); hence, the

9 We also specified the triangular distribution and the Student's t-distribution for the marginal distribution of the structural error. Simulation results using these distributions echoed what we found with the uniform marginal: the proposed methods provided consistent estimates of the model parameters. Results are available from the authors upon request.

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

578

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

Table 3 Results of the Simulation Study Case 2

True

OLS

Proposed I

Parameters values Mean SE

tbias

Mean

SE

tbias

-1 -0 93 0 013 5 385 -0 999 0 021 0 048

0 289 --

-- 0 290 0 014 0 071

05

--

-- 0 486 0 077 0 182

dependence structure might be different from what the Gaussian copula generates. In this section, we examine the robustness of the Gaussian copula in simulated data. We generate the dependence between Up and U using several copula models that are different from the Gaussian. Specifically, we consider the following copula models:
(i) Ali-Mikhail-Haq distribution (with = 1 :

C UP t U t = 1 -

UP tU t 1 - UP t 1 - U t

(ii) Placket distribution (with = 20):

C UP t U t

= 1 + - 1 UP t + U t

- 1 + - 1 UP t + U t 2 - 4UP t U t - 1

· 2 - 1 -1

(iii) Farlie-Gumbel-Morgenstern distribution (with = 1):

C UP t U t = UP tU t 1 + 1 - UP t 1 - U t (iv) Clayton copula (with = 2):

C UP t U t = UP- t + U -t - 1 -1/ (v) Frank copula (with = 2):

C UP t U t

= - 1 ln

1+

exp - UP t - 1 exp - U exp - - 1

t -1

Figure 1 shows the scatterplots of randomly generated
UP t U t pairs from the above-mentioned copulas, as well as the Gaussian copula with the correlation
coefficient 0.5. We can see that each copula generates
a unique dependence structure between Up and U . After randomly generating UP t and U t from the above-mentioned copulas, we use the following pro-
cess to generate Pt and Yt :

t = G-1 U t = -1 U t

Pt = H -1 UP t =

-1 11

UP t

(27) (28)

Yt = · Pt + t = -1 · Pt + t

(29)

We set T = 200. For each copula, we generate 100 data sets and estimate the parameters using the OLS

estimator and Proposed I (Proposed II yields the same results).
Table 4 reports estimation results. OLS estimates of are biased in all cases. By contrast, estimates from the proposed model are tightly distributed around the true values, and the bias is not significantly different from 0 in all cases. In summary, we find that the proposed method based on the Gaussian copula is robust to misspecification of the dependence structure between the endogenous regressor and the structural error. Our results echo findings about robustness of the Gaussian copula model that have been documented in the literature (see Song 2000, Danaher and Smith 2011, Balakrishnan and Lai 2009).

3.4. Case 4: RCL Model Using Individual-Level Data
We examine the performance of the proposed method in RCL models using individual-level data. The specific DGP and the parameter values we assign are as follows:

uh0t = h0t

uh1t = ¯1 + ¯3 · X1t + ¯ + ah · P1t + 1t + h1t

= 0 7 + 1 · X1t + -1 + ah · P1t + 1t + h1t

uh2t = ¯2 + ¯3 · X2t + ¯ + ah · P2t + 2t + h2t

= 0 7 + 1 · X2t + -1 + ah · P2t + 2t + h2t

Xjt = I Qjt > 0 7 Qjt  U 0 1 for j = 1 2

ah  N 0

2 a

=N 0

0 52



jt
Pjt

N

0 0

1 1

=N

0 0

1 05 05 1

for j = 1 2

jt = G-1 Pjt = H -1

 jt

=

-1 0 0 22

 jt

for j = 1 2

Pjt = 1 5 + 0 2 · ln - ln 1 - Pjt

for j = 1 2

where t = 1 200 and h = 1 100. Note that H -1 · in this case is the inverse distribution function of the extreme value distribution with mean 1.5 and scale parameter 0.2. So the marginal distribution of Pjt is the extreme value distribution. Xjt is a regressor that is uncorrelated with jt. Consumers have heterogeneous preferences with respect to Pjt only. We generate individual-level choices as described above and use them as data to estimate the model parameters. In the estimation, in addition to the proposed method, we also use a benchmark method where we ignore the endogeneity by applying the OLS estimator to the
first-step estimates ^jt.

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

579

Figure 1 Scatterplots of Randomly Generated UP t U t Pairs for Considered Copulas

1.0

1.0

1.0

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0

0

0

0.2 0.4 0.6 0.8 1.0

0

0.2 0.4 0.6 0.8 1.0

0

0.2 0.4 0.6 0.8 1.0

Ali-Mikhail-Haq ( = 1)

Placket distribution ( = 20)

Farlie-Gumbel-Morgenstern ( = 1)

1.0

1.0

1.0

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0

0

0

0.2 0.4 0.6 0.8 1.0

0

0.2 0.4 0.6 0.8 1.0

0

0.2 0.4 0.6 0.8 1.0

Clayton copula ( = 2)

Frank copula ( = 2)

Gaussian copula ( = 0.5)

Estimation results are in Table 5. Again, the OLS estimates of three of the five parameters are biased. Estimates from the proposed method are tightly distributed around the true values, and the bias is not significantly different from 0 for all six parameters.
3.5. Case 5: RCL Model Using Aggregate-Level Data
We investigate the performance of the proposed method in RCL models with aggregate-level data. The

DGP is the same as that of Case 4 except that G-1 · =

-1 0 0 52

· , H = 1 000, and T = 500 and 1,000. To gen-

erate the aggregate-level data to be used in estima-

tion, we first generate individual-level choices from

the latent utilities and then add them up. We gener-

ate 100 data sets and estimate the model using the

proposed method. Table 6 summarizes the estimation

results. As in the other cases, estimates from the pro-

posed model are closely distributed around the true

parameter values and show no significant bias. As T

Table 4 Results of the Simulation Study Case 3

DGP Ali-Mikhail-Haq
Placket distribution
Farlie-GumbelMorgenstern
Clayton copula
Frank copula

Parameters

True values

Mean

-1 1
0.478
-1 1
0.774
-1 1
0.337
-1 1
0.679
-1 1
0.317

-0 750 -- --
-0 620 -- --
-0 839 -- --
-0 658 -- --
-0 849 -- --

OLS
SE
0 046 -- --
0 043 -- --
0 046 -- --
0 041 -- --
0 047 -- --

tbias
5 435 -- --
8 837 -- --
3 500 -- --
8 341 -- --
3 213 -- --

Proposed I

Mean

SE

tbias

-1 001 1 006 0 501
-1 004 1 013 0 760
-0 997 0 999 0 322
-0 999 1 004 0 684
-0 998 0 999 0 303

0 067 0 060 0 085
0 069 0 075 0 046
0 064 0 052 0 080
0 072 0 069 0 066
0 070 0 051 0 081

0 015 0 100 0 271
0 058 0 173 0 304
0 047 0 019 0 188
0 014 0 058 0 076
0 029 0 020 0 173

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

580

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

Table 5 Results of the Simulation Study Case 4

OLS True Parameters values Mean SE

Proposed II

tbias

Mean

SE

tbias

¯1

0 7 0 178 0 084 6 214 0 727 0 323 0 084

¯2

0 7 0 181 0 083 6 253 0 728 0 324 0 086

¯3

1

1 004 0 030 0 133 1 004 0 030 0 133

¯

-1 -0 627 0 057 6 544 -1 044 0 210 0 210

a

0 5 0 527 0 088 0 307 0 527 0 088 0 307

·

01

--

-- -- 0 112 0 050 0 240

Notes. a is estimated in the first step, and thus OLS and Proposed II have the same estimates. We report results of Proposed II. We obtained the same
results from Proposed I.

increases from 500 to 1,000, the bias (defined as the absolute deviation of true parameter value from the estimate) of all estimates moves toward 0 (one exception is the bias of ¯ , which increases slightly from 0.008 to 0.010). This observation supports the consistency of the proposed method.

3.6. Case 6: Multiple Endogenous Regressors We examine the performance of the proposed method when the model has multiple endogenous regressors. To this end, we generate the data using the following DGP:

Yt = 1 · P1 t + 2 · P2 t + t = -1 · P1 t + 1 · P2 t + t



t

 0   1



1

2

 

P1

t

 



N



0





1

1

12 

P2 t

0

2 12 1

 0   1 0 5 0 5  = N  0   0 5 1 0 1 
0 05 01 1

t = G-1

 t

=

-1 0 0 52

 t

P1 t = H1-1 P1 t = 1 5 + 0 2 · ln - ln 1 - P1 t

P2 t = H2-1 P2 t = 1 0 + 0 3 · ln - ln 1 - P2 t

where T = 500. H1-1 and H2-1 are the inverse distribution functions of the extreme value distributions.

Table 6 Results of the Simulation Study Case 5

Proposed method

Parameters

True

T = 500

values Mean SE tbias

T = 1 000

Mean

SE

tbias

¯1 ¯2 ¯3 ¯
· · 1 - 2 1/2

07 07
1 -1 05 0 25 0 433

0 698 0 689 0 977 -0 992 0 489 0 261 0 443

0 328 0 328 0 054 0 235 0 108 0 077 0 021

0 006 0 702 0 034 0 702 0 426 0 990 0 034 -1 010 0 102 0 492 0 143 0 259 0 476 0 440

0 219 0 217 0 042 0 157 0 088 0 044 0 016

0 009 0 009 0 238 0 064 0 091 0 205 0 438

So the marginal distributions of P1 t and P2 t are extreme value distributions. Note that both P1 t and P2 t are correlated with t. We generate 1,000 data sets and estimate the model using the proposed method. Table 7 reports estimation results. The OLS estimates of both parameters are biased, whereas estimates of all five parameters from the proposed method are unbiased, suggesting that the proposed method provides consistent estimates when the model has two endogenous regressors.

3.7. Case 7: Discrete Endogenous Regressor We investigate the performance of the proposed method when the endogenous regressor is a discrete variable. To this end, we consider four different discrete distributions. We generated the data using the following DGP:

Yt = · Pt + t = -1 · Pt + t



t
Pt

N

0 0

1 1

=N

0 0

1 05 05 1

t = G-1

 t

=

-1 02

 t

-1 H Pt - 1 < Pt < -1 H Pt

(i) Discrete 1 (Bernoulli distribution):

H Pt = 0 = 0 5 (ii) Discrete 2:

H Pt = 1 = 1

2 = 0 25

H Pt = 0 = 0 2 H Pt = 1 = 0 4

H Pt = 2 = 0 6 H Pt = 3 = 0 8

H Pt = 4 = 1

2=2

(iii) Discrete 3 (Poisson distribution):

Pt 4i H Pt = exp -4 i=0 i!

2=4

(iv) Discrete 4 (Negative binomial distribution):

Pt
H Pt =

i+4-1 i

× 0 54 × 0 5i

i=0

2=8

We set T = 200. Note that 2 is set to be equal to the variance of Pt in all cases. For each discrete distribution, we generate 1,000 data sets and estimate the parameters using the OLS estimator and the proposed method. Table 8 summarizes the results.
When the endogenous regressor is generated from the Bernoulli distribution (Discrete 1), the average OLS estimate of is -0 801 (SE, 0.047), and the average of the proposed method is -0 895 (SE, 0.056). Both

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

Table 7 Results of the Simulation Study Case 6

Parameters

1

2

1 - 12 2

·

1-

2 12

2 - 12 1

·

1-

2 12

·

1-

2
1-

2 - 12

2 1

05

1-

2 12

True values

Mean

-1

-1 174

1

1 354

0 227

--

0 227

--

0 369

--

OLS SE
0 031 0 05
--
--
--

tbias 5 613 7 080
--

Proposed method

Mean

SE

tbias

-0 985 0 978

0 117 0 194

0 128 0 113

0 225 0 036 0 056

--

0 236 0 077 0 117

--

0 366 0 013 0 231

581

estimators are biased, although the estimates from the proposed method are distributed nearer to the true value. As noted previously, this is because the support of the Bernoulli distribution is only 0 1 , and the information contained in this variable is not rich enough for the proposed method to handle the endogeneity problem properly. In Discrete 2, the support of the endogenous regressor expands to 0 1 2 3 4 , providing richer information. We observe that the OLS estimator is still biased, but the proposed method provides unbiased estimates for all three parameters. We find similar results in Discrete 3 (Poisson) and Discrete 4 (Negative binomial). To summarize, the proposed method shows good performance with discrete endogenous regressors if the support of the regressor is reasonably large.
3.8. Case 8: Slope Endogeneity We examine the performance of the proposed method in handling the slope endogeneity problem. To this end, we generated the data using the following DGP:
Yt = Pt ¯ + t + t = Pt 1 + t + t
tN 0 2 =N 0 1



t
Pt

N

0 0

1 1

=N

0 0

1 05 05 1

t = G-1

 t

=

-1 01

 t

Pt = H -1

Pt

=

-1 11

Pt

We generate 1,000 data sets with T = 300 and estimate the parameters ignoring the slope endogeneity and using the proposed method. To handle the random coefficient, we use numerical integration using 500 random draws. Table 9 summarizes the results. When we ignore the slope endogeneity, the estimate of ¯ is biased (overstated). All estimates from the proposed method are unbiased.

3.9. Case 9: Comparison with LIV The goal of this simulation is to compare the proposed copula method with the most similar extant instrument-free method, the LIV method. To briefly describe the LIV method, we reproduce Equations (1) and (2) from §2.1:

Yt = Xt + Pt + t Pt = Zt + t

Table 8 Results of the Simulation Study Case 7

DGP Discrete 1 (Bernoulli) Discrete 2 Discrete 3 (Poisson) Discrete 4 (Negative binomial)

Parameters
· · 1- 2
· · 1- 2
· · 1- 2
· · 1- 2

True values
-1 0 25 0 433 -1 0 707 1 225 -1
1 1 732 -1 1 414 2 449

Mean
-0 801 -- --
-0 84 -- --
-0 9 -- --
-0 84 -- --

OLS
SE
0 047 -- --
0 036 -- --
0 031 -- --
0 036 -- --

tbias
4 234 -- --
4 361 -- --
3 097 -- --
4 444 -- --

Proposed method

Mean

SE

tbias

-0 895 0 117 0 468
-0 98 0 606 1 259
-1 0 958 1 738
-0 99 1 352 2 45

0 056 0 04 0 024
0 046 0 116 0 061
0 036 0 156 0 085
0 047 0 231 0 131

1 875 3 325 1 458
0 478 0 871 0 557
0 056 0 269 0 071
0 149 0 268 0 008

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

582

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

Table 9 Results of the Simulation Study Case 8

Slope endogeneity ignored True

Parameters values Mean SE

tbias

Proposed method Mean SE tbias

¯
· 1- 2

1 1 05 0 866 1

1 254 0 976
-- -- 1 027

0 097 0 102
-- -- 0 084

2 619 0 235
-- -- 0 321

1 003 0 101 ----
0 493 0 085 0 846 0 095 1 019 0 078

0 030 --
0 082 0 211 0 244

The LIV method models Zt as a discrete, exogenous random variable with an unknown number of possi-
ble outcomes; t is an endogenous error and is usually assumed to be Gaussian. The endogeneity issue
is handled by modeling the correlation between t and t. The finite mixture approach is used to estimate the model.
Note that the LIV method is based on two key
assumptions: Zt is a discrete variable and Zt is exogenous. Based on simulations, Ebbes et al. (2009)
reported that the model successfully approximated
some continuous Zt ; hence, the first assumption may not be overly restrictive. The second assumption
or the exogeneity of Zt is similar to the exclusion restriction in usual IV models. In the LIV method, we
do not observe Zt, and Zt is estimated using data. Therefore it is difficult to justify or test the exogeneity
of Zt . We generate data from an LIV model and compare
the estimation performance of the copula and LIV
methods. The specific DGP is as follows:


t

 0   1 0 5 

 t   N  0   0 5 1 0 

Mt

0

01

Zt =

0 4

if Mt < 0 if Mt  0

Pt = Zt + t

Yt = Pt + t = Pt · 1 + t

We consider two different values for

= 0 and

= 0 5. When = 0, the latent instrument Zt is exogenous. In this case, the LIV method is correctly spec-

ified but the proposed method is not because the

correlation between Pt and t is different from what the Gaussian copula assumes. When = 0 5, the LIV

method is misspecified because Zt is correlated with t. The proposed method continues to be misspecified because the correlation between Pt and t is different from what the Gaussian copula assumes.

In both cases, we generate 1,000 data sets with T =300.

We estimate the LIV model assuming that the latent

instrument has two categories. Table 10 shows the

Table 10 Results of the Simulation Study Case 9

True

DGP

Method

value

Estimate

SE

tbias

OLS

1

=0

LIV

1

Proposed

1

OLS

1

=05

LIV

1

Proposed

1

1 058 1 002 0 973
1 144 1 089 0 992

0 022 0 022 0 033
0 015 0 017 0 030

2 668 0 094 -0 819
9 596 5 396 -0 280

Note. The estimation results for other model parameters are available from the authors on request.

performance of OLS, LIV, and the proposed copula methods in estimating . When = 0, as expected, the OLS estimate is biased, and the LIV estimate is unbiased. Importantly, the proposed method yields unbiased estimates despite misspecification. When = 0 5, the OLS estimate continues to be biased. Importantly, the LIV estimate is also biased, whereas the estimate from the copula model is unbiased, supporting the flexibility of the proposed approach.

4. Empirical Application
We illustrate empirical application of the proposed method to handle price endogeneity in demand estimation using data on store-level sales of paper towels (category sales) in Eau Claire, Wisconsin over 260 weeks from 2001 to 2005.10 The argument for endogeneity of retail price is that there are unmeasured product characteristics, or demand shocks, that influence not only consumer decisions but also retailer pricing decisions. Because they are unobserved by researchers, these variables are incorporated into the structural error, leading to the endogeneity problem. We focus on category sales in the two largest independent stores in the market (we refer to them as Stores 1 and 2). Retail price is defined on a per-roll basis. Retail price, in-store display, and feature advertising at the category level are computed as market share-weighted averages of UPC-level variables.
We use the TSLS estimator as a benchmark to assess the validity of the proposed method. To implement the TSLS model, we use retail price at the other store as an instrument (that is, Store 2 price is used as instrument for Store 1 price, and vice versa). Retail prices across stores in a market are likely to be highly correlated because competing retailers are typically offered the same wholesale prices for a brand. The Pearson correlation between the retail prices (category average) of paper towels at Stores 1 and 2 is 0.83. However, unmeasured product characteristics, especially those determined at retail (e.g., shelf-space allocation, shelf

10 We obtained the data from the IRI marketing data set (Bronnenberg et al. 2008).

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

583

Figure 2 Log Sales and Log Retail Prices of Paper Towel in Store 1 8.5 ln(sales) 8.0
7.5

1.0 ln(price)
0.8 0.6 0.4

7.0

0.2

0.0

6.5 ­ 0.2

(week)

6.0

­ 0.4

50

100

150

200

250

50

Store 1 log sales

100

150

200

Store 1 log retail price

(week) 250

location), are not likely to be systematically related to wholesale prices. If this is true, the instrumental variable is exogenous and thus is valid for controlling for the endogeneity of retail price.
We estimate the following model:
log Salest = log Retail Pricet · + Xt + t
where the vector Xt includes a constant, three dummy variables to represent quarters (Q2, Q3, and Q4), feature, and display. Figure 2 shows log sales and log retail prices at store 1 (Store 2 is very similar). The log retail price shows an increasing trend over the five-year sample period. Therefore, we use detrended log retail prices for estimation and also as instrumental variables. We estimate the model using the OLS estimator and also apply the two methods that correct for price endogeneity--the TSLS estimator and the proposed method. As we explained in §2, for the proposed approach, we first estimate the marginal distributions of prices using a nonparametric method and then use these distributions to construct the likelihood functions. Figure 3 shows the nonparametric densities of detrended log retail prices

at the two stores. We use the Epanechnikov kernel for the nonparametric density estimation. Following Silverman's (1986) data-driven bandwidth selection rule, we set the bandwidth parameters to be equal to 0.036 and 0.039 for Stores 1 and 2, respectively. We can observe that both distributions are left-skewed and quite different from the normal distribution. Therefore, we expect that this difference will provide sufficient information for model identification.
Table 11 reports estimation results for the three methods. Beginning with the results from Store 1, by comparing OLS estimates with TSLS estimates, we can verify whether prices are endogenous. The price coefficients are substantially different, supporting the case for retail price endogeneity. Also, the Hausman test for endogeneity (Wooldridge 2001, p. 118) confirms that the retail price is endogenous. Instrumenting for retail price changes its coefficient estimate from -0 676 to -0 928, implying that there is a positive correlation between unmeasured product characteristics and the price. This direction of correlation is consistent with previous empirical findings (e.g., Villas-Boas and Winer 1999, Chintagunta et al. 2005). Estimates from

Figure 3

Estimated Nonparametric Densities of Detrended Log Retail Prices

8

8

Store 1

Store 2

6

6

4

4

2

2

0

­ 0.8

­ 0.6

­ 0.4

­ 0.2

0.0

0

­ 0.8

­ 0.6

­0.4 ­ 0.2

0.0

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

584

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

Table 11 Estimation Results for Three Methods: Paper Towel Sales

Store 1

Store 2

Parameters Estimate SE t-value Estimate SE t-value

Constant Q2 Q3 Q4 Price Promotion Display
Constant Q2 Q3 Q4 Price Promotion Display
Constant Q2 Q3 Q4 Price Promotion Display

6 607 0 094 0 055 -0 067 -0 676 0 407 0 173
6 613 0 078 0 042 -0 071 -0 928 0 428 0 159
6 600 0 099 0 056 -0 066 -0 934 0 395 0 209 0 197

0 031 0 033 0 033 0 033 0 149 0 041 0 080
0 030 0 033 0 033 0 032 0 170 0 041 0 079
0 035 0 036 0 033 0 032 0 240 0 043 0 080 0 110

OLS 213 9 6 549
2 8 0 089 1 7 0 116 -2 0 0 060 -4 5 -0 780 9 8 0 433 2 2 0 158
TSLS 217 0 6 553
2 3 0 081 1 3 0 110 -2 2 0 057 -5 4 -0 653 10 3 0 429 2 0 0 164 Proposed 212 4 6 549 3 0 0 090 1 7 0 115 -2 0 0 059 -4 1 -0 883 9 4 0 430 2 5 0 163 1 8 0 080

0 024 0 027 0 027 0 027 0 125 0 030 0 061
0 025 0 028 0 027 0 028 0 153 0 030 0 062
0 021 0 027 0 024 0 030 0 236 0 038 0 070 0 134

270 7 33 43 22
-6 3 14 4
26
266 1 29 40 21
-4 3 14 1
26
212 4 30 17
-2 0 -4 1
94 25 06

the proposed method are very close to those from TSLS. The estimate of is significant and positive, consistent with the TSLS finding of positive correlation between unmeasured product characteristics and the retail price. As we have observed in the simulation studies, the standard errors of the proposed method are slightly larger than those of TSLS but small enough for use to draw meaningful conclusion on model parameters. This implies that there is no problem in model identification in this case.
Unlike Store 1, the results from Store 2 provide multiple convergent indications that the retail price is exogenous. The OLS estimates are not significantly different from the TSLS estimates, the Hausman test of endogeneity confirms the conclusion, the estimates from the proposed method are not significantly different from the OLS estimates, and finally, the estimated value of is not significantly different from 0.
We interpret our results to mean that retailer pricing decisions are sensitive to changes in unmeasured product characteristics in Store 1 but less sensitive in Store 2. The convergence of results between TSLS and the proposed method in both stores lends validity to the proposed method to the extent that our assumption regarding exogeneity of the instruments is true.

of the endogenous regressor and the structural error term, and it makes inferences on model parameters by maximizing the resulting likelihood function. We use flexible nonparametric density estimation to construct the marginal distribution of the endogenous regressor. In simulation studies we find that the proposed method provides consistent estimates not only when the actual correlation structure between the regressor and structural error follows what the Gaussian copula assumes but also when the correlation structure is different from what the Gaussian copula assumes. This finding supports robustness of the proposed method. We show how the proposed method can be applied to three popular models in marketing and economics--the linear regression model, the RCL model using individual-level data, and the RCL model using aggregate-level data. Also, we show how to extend the proposed method to (1) multiple endogenous regressors, (2) discrete endogenous regressors, and (3) slope endogeneity problems. In an empirical application to paper towel sales, we again find that the proposed method successfully overcomes the endogeneity bias.
The proposed method is not without limitations. The method exploits the information contained in the endogenous regressor for identification. For the proposed method to work properly, the distribution of the endogenous regressor must contain adequate information and must be different from that of the structural error (assumed to be normal). The former condition is violated when the endogenous regressor follows the Bernoulli distribution, and the latter is violated when the endogenous regressor follows the normal distribution. However, despite these limitations, we expect that the proposed method will provide a useful alternative in many empirical problems in which instruments are not available.
Electronic Companion An electronic companion to this paper is available as part of the online version at http://dx.doi.org/10.1287/ mksc.1120.0718.
Acknowledgments The authors acknowledge helpful comments and suggestions from Peter Ebbes, Duk Bin Jun, and Chang-Jin Kim; the seminar participants at Korea University, Korea Advanced Institute of Science and Technology, and the 2011 Marketing Science Conference; and the editor, area editor, and two reviewers. The authors thank Symphony IRI for making the data available. All estimates or analyses in this paper based on Symphony IRI Group, Inc., data are by the authors and not Symphony IRI Group, Inc.

5. Conclusion
We propose a new instrument-free method to handle the problem of endogenous regressors. Our method uses a Gaussian copula to model the joint distribution

References
Angrist JD, Krueger AB (1991) Does compulsory school attendance affect schooling and earnings? Quart. J. Econom. 106(4):979­1014.

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

585

Angrist JD, Krueger AB (2001) Instrumental variables and the search for identification: From supply and demand to natural experiments. J. Econom. Perspect. 15(4):69­85.
Balakrishnan N, Lai C-D (2009) Continuous Bivariate Distributions, 2nd ed. (Springer, New York).
Berry ST (1994) Estimating discrete-choice models of product differentiation. RAND J. Econom. 25(2):242­262.
Berry S, Levinsohn J, Pakes A (1995) Automobile prices in market equilibrium. Econometrica 63(4):841­890.
Besanko D, Gupta S, Jain D (1998) Logit demand estimation under competitive pricing behavior: An equilibrium framework. Management Sci. 44(11, Part 1):1533­1547.
Bound J, Jaeger D, Baker R (1995) Problems with instrumental variables estimation when the correlation between the instruments and the endogenous explanatory variable is weak. J. Amer. Statist. Assoc. 90(430):443­450.
Bronnenberg BJ, Mahajan V (2001) Unobserved retailer behavior in multimarket data: Joint spatial dependence in market shares and promotion variables. Marketing Sci. 20(3):284­299.
Bronnenberg BJ, Kruger MW, Mela CF (2008) The IRI marketing data set. Marketing Sci. 27(4):745­748.
Chao JC, Phillips PCB (1998) Posterior distributions in limited information analysis of the simultaneous equations model using the Jeffreys prior. J. Econometrics 87(1):49­86.
Chintagunta PK (2001) Endogeneity and heterogeneity in a probit demand model: Estimation using aggregate data. Marketing Sci. 20(4):442­456.
Chintagunta PK, Dubé J-P, Goh KY (2005) Beyond the endogeneity bias: The effect of unmeasured brand characteristics on household-level brand choice model. Management Sci. 51(5):832­849.
Chintagunta PK, Erdem T, Rossi PE, Wedel M (2006) Structural modeling in marketing: Review and assessment. Marketing Sci. 25(6):604­616.
Conley T, Hansen C, Rossi PE (2012) Plausibly exogenous. Rev. Econom. Statist. 94(1):260­272.
Danaher PJ, Smith MS (2011) Modeling multivariate distributions using copulas: Applications in marketing. Marketing Sci. 30(1):4­21.
DiNardo J, Tobias JL (2001) Nonparametric density and regression estimation. J. Econom. Perspect. 15(4):11­28.
Dotson JP, Allenby GM (2010) Investigating the strategic influence of customer and employee satisfaction on firm financial performance. Marketing Sci. 29(5):895­908.
Ebbes P, Wedel M, Böckenholt U (2009) Frugal IV alternatives to identify the parameter for an endogenous regressors. J. Appl. Econometrics 24(3):446­468.
Ebbes P, Wedel M, Böckenholt U, Steerneman AGM (2005) Solving and testing for regressor-error (in)dependence when no instrumental variables are available: With new evidence for the effect of education on income. Quant. Marketing Econom. 3(4):365­392.
Efron B (1979) Bootstrap methods: Another look at the jackknife. Ann. Statist. 7(1):1­26.
Erickson T, Whited TM (2002) Two-step GMM estimation of the errors-in-variables model using high-order moments. Econometric Theory 18(3):776­799.
Genest C, Ghoudi K, Rivest LP (1995) A semiparametric estimation procedure for dependence parameters in multivariate families of distributions. Biometrika 82(3):543­552.
Hartmann W, Nair HS, Narayanan S (2011) Identifying causal marketing mix effects using a regression discontinuity design. Marketing Sci. 30(6):1079­1097.
Hausman JA (1978) Specification tests in econometrics. Econometrica 46(6):1251­1272.

Heckman JJ (1976) Common structure of statistical models of truncation, sample selection and limited dependent variables and a simple estimator for such models. Ann. Econom. Soc. Measurement 5(4):475­492.
Heckman JJ (1978) Dummy endogenous variables in a simultaneous equation system. Econometrica 46(4):931­959.
Hogan V, Rigobon R (2003) Using unobserved supply shocks to estimate the returns to education. Technical report, University College Dublin, Dublin, Ireland.
Keane MP (1993) Simulation estimation for panel data models with limited dependent variables. Rao CR, Maddala GS, Vinod D, eds. The Handbook of Statistics and Econometrics (Elsevier Science, Amsterdam), 545­572.
Kleibergen F, Zivot E (2003) Bayesian and classical approaches to instrumental variable regression. J. Econometrics 114(1): 29­72.
Kuksov D, Villas-Boas JM (2008) Endogeneity and individual consumer choice. J. Marketing Res. 45(6):702­714.
Lewbel A (1997) Constructing instruments for regressions with measurement error when no additional data are available, with an application to patents and R&D. Econometrica 65(5): 1201­1213.
Luan Y, Sudhir JK (2010) Forecasting marketing-mix responsiveness for new products. J. Marketing Res. 47(3):444­457.
Manchanda P, Rossi PE, Chintagunta PK (2004) Response modeling with nonrandom marketing-mix variables. J. Marketing Res. 41(November):467­478.
Mooney CZ, Duval RD (1993) Bootstrapping: A Nonparametric Approach to Statistical Inference (Sage, Newbury Park, CA).
Musalem A, Bradlow ET, Raju JS (2009) Bayesian estimation of random-coefficients choice models using aggregate data. J. Appl. Econometrics 24(3):490­516.
Nelsen RB (2006) An Introduction to Copulas, 2nd ed. (Springer, New York).
Otter T, Gilbride T, Allenby GM (2011) Testing models of strategic behavior characterized by conditional likelihoods. Marketing Sci. 30(4):686­701.
Pagan A (1984) Econometric issues in the analysis of regressions with generated regressors. Internat. Econom. Rev. 25(1):221­247.
Park S, Gupta S (2009) Simulated maximum likelihood estimator for the random coefficient logit model using aggregate data. J. Marketing Res. 46(August):531­542.
Rigobon R (2003) Identification through heteroskedasticity. Rev. Econom. Statist. 85(4):777­792.
Rossi PE, Allenby GM, McCulloch R (2005) Bayesian Statistics and Marketing (John Wiley & Sons, New York).
Rust RT (1986) Advertising Media Models: A Practical Guide (Lexington Books, Lexington, MA).
Rutz OJ, Trusov M (2011) Zooming in on paid search ads-- An individual-level model calibrated on aggregated data. Marketing Sci. 30(5):789­800.
Rutz OJ, Bucklin RE, Sonnier GP (2012) A latent instrumental variables approach to modeling keyword conversion in paid search advertising. J. Marketing Res. 49(3):306­319.
Shih JH, Loius TA (1995) Inferences on the association parameter in copula models for bivariate survival data. Biometrics 51(4):1384­1399.
Silverman BW (1986) Density Estimation for Statistics and Data Analysis (Chapman & Hall, London).
Sklar A (1959) Fonctions de répartition n dimensions et leurs marges. Publications de l'Institut de Statistique de L'Universit de Paris 8:229­231.
Song PX-K (2000) Multivariate dispersion models generated from gaussian copula. Scand. J. Statist. 27(2):305­320.

Park and Gupta: Handling Endogenous Regressors by Joint Estimation Using Copulas

586

Marketing Science 31(4), pp. 567­586, © 2012 INFORMS

Sudhir K (2001) Competitive pricing behavior in the US auto market: A structural analysis. Marketing Sci. 20(1):42­60.
van Dijk A, van Heerde HJ, Leeflang PSH, Wittink DR (2004) Similarity-based spatial methods to estimate shelf space elasticities. Quant. Marketing Econom. 2(3):257­277.
Villas-Boas JM, Winer R (1999) Endogeneity in brand choice models. Management Sci. 45(10):1324­1338.

Wooldridge JM (2001) Econometric Analysis of Cross Section and Panel Data (MIT Press, Cambridge, MA).
Yang S, Chen Y, Allenby GM (2003) Bayesian analysis of simultaneous demand and supply. Quant. Marketing Econom. 1(3):251­275.
Zhang J, Wedel M, Pieters R (2009) Sales effects of visual attention to feature ads: A Bayesian mediation analysis. J. Marketing Res. 46(3):669­681.

