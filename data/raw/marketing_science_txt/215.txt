http://pubsonline.informs.org/journal/mksc/

MARKETING SCIENCE
Vol. 36, No. 3, May­June 2017, pp. 329­337 ISSN 0732-2399 (print), ISSN 1526-548X (online)

Motivation of User-Generated Content: Social Connectedness Moderates the Effects of Monetary Rewards

Yacheng Sun,a, b Xiaojing Dong,c Shelby McIntyrec
a Leeds School of Business, University of Colorado at Boulder, Boulder, Colorado 80309; b Department of Marketing, School of Economics and Management, Tsinghua University, 100084 Being, China; c Leavey School of Business, Santa Clara University, Santa Clara, California 95053 Contact: yacheng.sun@gmail.com (YS); xdong1@scu.edu (XD); smcintyre@scu.edu (SM)

Received: October 9, 2014 Revised: November 2, 2015; March 26, 2016; August 15, 2016 Accepted: September 19, 2016 Published Online in Articles in Advance: February 21, 2017
https://doi.org/10.1287/mksc.2016.1022
Copyright: © 2017 INFORMS

Abstract. The creation and sharing of user-generated content such as product reviews has become increasingly "social," particularly in online communities where members are connected. While some online communities have used monetary rewards to motivate product review contributions, empirical evidence regarding the effectiveness of such rewards remains limited. We examine the possible moderating effect of social connectedness (measured as the number of friends) on publicly offered monetary rewards using field data from an online review community. This community saw an (unexpected) overall decrease in total contributions after introducing monetary rewards for posting reviews. Further examination across members finds a strong moderating effect of social connectedness. Specifically, contributions from less-connected members increased by 1,400%, while contributions from more-connected members declined by 90%. To corroborate this effect, we rule out multiple alternative explanations and conduct robustness checks. Our findings suggest that token-sized monetary rewards, when offered publicly, can undermine contribution rates among the most connected community members.

History: K. Sudhir served as the senior editor and Duncan Simester served as associate editor for this article.
Supplemental Material: Data and the online appendix are available at https://doi.org/10.1287/ mksc.2016.1022
Keywords: user-generated content · monetary rewards · social connectedness · motivation

1. Introduction
User-generated content (UGC) such as product reviews (e.g., Chen and Xie 2008) has become increasingly "social," in the sense that consumers draw content not only from the general community but also from their own online social connections. Many review sites, including Citysearch, TripAdvisor, Urbanspoon, and Yelp, have endeavored to build connected review communities, and many such sites have partnered with Facebook to allow users to share reviews with their Facebook friends. The success of these efforts1 is perhaps not surprising, since reviews by social connections tend to be more attractive than "anonymous" reviews because of the high level of trust and personal knowledge that make such recommendations more relevant (Brown and Reingen 1987, Feick et al. 1986).
The low frequency of UGC contribution, however, remains a serious concern,2 prompting review platforms to consider offering monetary rewards for consumer reviews (Clifford 2012, Streitfeld 2012). Interestingly, while some platforms (e.g., Epinions and Refer.ly) make the rewards public, as dictated by recent Federal Trade Commission (FTC) guidelines,3 others offer incentive payments sub rosa (e.g., Angie's List

and Seeking Alpha).4 Still, platforms including Yelp and TripAdvisor choose to continue using nonmonetary incentives, such as user feedback for their reviews (e.g., Yelp's "useful," "funny," or "cool" buttons) and platform recognition (e.g., Yelp Elites) to induce usergenerated reviews (McIntyre et al. 2015).
The increasingly significant social aspect of UGC and the quite divergent use of monetary rewards prompt two important, yet little-understood research questions. Should online communities use monetary rewards to incentivize contribution rates among connected consumers? If so, are monetary rewards more effective for more- versus less-connected consumers? We address these questions using novel evidence from the field.
Observe that a key feature of social UGC is that the core audience for a review usually consists of the contributor's social connections (e.g., "friends" or "followers") within the community. This social aspect of review sharing makes the decision to post a review quite a distinct one, in which the utility derived by a contributor from posting a review can be a function of her social connectedness. This observation, along with

329

330

Sun, Dong, and McIntyre: Motivation of User-Generated Content Marketing Science, 2017, vol. 36, no. 3, pp. 329­337, © 2017 INFORMS

the literature review in Section 2, leads us to hypothesize that the member's level of social connectedness moderates her willingness to contribute in the presence of monetary rewards.
In the following sections, we first review the relevant literature. We then conduct empirical analysis using data from an online social review community. Finally we corroborate our findings with robustness checks.
2. Literature Review
Monetary vs. Nonmonetary Rewards. Promotional payments to consumers are extensively used in the off-line world to induce many kinds of desired behaviors and to overcome procrastination. Firms have introduced programs to encourage existing customers to make product recommendations, particularly in social media such as YouTube "purchase hauls" by the Forever 21 brand. PayPal started a program providing a $5 payment to an existing member who refers a friend, and $10 to the friend, upon opening her own Paypal account to collect the money. Similarly, Dropbox used free gigabytes of storage as its hook. Such promotions are spilling over into online word of mouth, or eWOM, which is increasingly considered as a marketing tool to be managed (Rosen 2000). The idea of using monetary rewards to promote review contributions is a natural one. Avery et al. (1999) set up a game-theoretical model of a market for product reviews, where the contributor bears the private costs of contributing reviews (e.g., the efforts for writing reviews and the risks of trying a product early), yet others can access these reviews for free. They show that introducing monetary rewards can overcome the free-riding problem and consequently induce an efficient level of product review contributions. However, field-based empirical investigations into the effectiveness of monetary rewards remain sparse, and the results are mixed (see the review by Garnefeld et al. 2012).
Nonmonetary Rewards in a Connected Online Community. Despite the significant costs to the authors of providing product reviews, online review communities that rely only on voluntary contributions often see nontrivial levels of reviewing. Hennig-Thurau et al. (2004) address this paradox by showing various types of nonmonetary rewards that operate to motivate voluntary contributions. Specifically, their survey finds that voluntary contributions generate social benefits--e.g., to "help others with my own positive experience" and reputation benefits--e.g., "my contribution shows others that I am a clever customer." Hennig-Thurau et al. (2004) suggest that both monetary and nonmonetary rewards drive review contributions. We argue, however, that monetary rewards may actually suppress intrinsic motives, and consequently, become ineffective or even countereffective. First, monetary rewards may

transform a "social market" into a "monetary market," thereby decreasing prosocial behaviors (e.g., Heyman and Ariely 2004). Reputation utility is also at risk after monetary rewards are introduced because unfavorable inferences might be drawn regarding whether the reviewer's true motivation is altruistic.5 This has been referred to as the "crowding-out effect" of a small monetary reward (Frey and Jegen 2001), but has never been empirically investigated in the context of rewards for online reviews.
Social Connectedness Moderates Nonmonetary Rewards. Informed by the fact that social connections are the main audience of a member's product reviews, we expect social connectedness to play a key moderating role in motivating UGC contributions. In the terminology of Barasch and Berger (2014), social reviews are "broadcasted" to the social connections of the contributor. When members' contributions are driven purely by nonmonetary rewards, the social benefits from review contributions are likely to increase with the size of the audience (e.g., Toubia and Stephen 2013, Zhang and Zhu 2011). Thus, we expect that when members are driven by social benefits, their willingness to contribute would increase with the number of social connections. Furthermore, reputation benefits from voluntary contributions are also likely to be amplified by a higher level of social connectedness. In the context of online communities (e.g., Facebook and Twitter), UGC contributions from more-connected community members are projected to a larger audience and thus have higher visibility and should lead to greater reputation benefits.
Social Connectedness Moderates Monetary Rewards. Monetary rewards may also trigger a negative effect for members driven by a prosocial image, since being paid for a review might diminish their reputation. For potential contributors, this becomes a realistic concern because of the FTC's increasing enforcement of its guidelines, which puts the "exchange" between monetary rewards and review contributions under greater public scrutiny.6 Benabou and Tirole (2006) set up a two-dimensional signaling model, where the contributor's effort level sends a noisy signal regarding her true "type," i.e., her valuations for both reputation benefits and monetary rewards. They show that tension between monetary rewards and reputation benefits increases with the visibility of the action. Anticipating the potential negative inference regarding their ulterior motives, members whose actions are more visible are less likely to send an unfavorable signal about themselves. Benabou and Tirole (2006) refer to this as the "overjustification" effect of monetary rewards. Importantly, such a negative effect is most likely to arise for small monetary rewards.7

Sun, Dong, and McIntyre: Motivation of User-Generated Content Marketing Science, 2017, vol. 36, no. 3, pp. 329­337, © 2017 INFORMS

331

Within a connected online community, the visibility of an "exchange" between product review contributions and monetary rewards likely increases with social connectedness, which may decrease the effectiveness of such rewards for well-connected community members. To our knowledge, however, no existing studies have examined the possible moderating effect of social connectedness for voluntary and incentivized product review contributions. We next present empirical evidence that social connectedness can indeed be an important moderator for the impact of monetary rewards.
3. Evidence from a Field Study
We draw evidence from the field (e.g., Levitt and List 2007, Simester 2017) and our empirical research context is an online social shopping community (OSSC). An OSSC is a virtual platform that integrates online shopping and the community sharing of UGC (e.g., product reviews). Examples of OSSCs in the United States include Airbnb, Foursquare, Kaboodle, Polyvore, and trendMe. A recent example of an OSSC is Pinterest, a six-year-old online inspiration board that allows its (nearly 50 million) community members to directly place an order for attractive "pins" posted by other users. OSSCs facilitate community members' generation and sharing of various types of content such as personal shopping lists, order histories, and product reviews, which are often cited as a major benefit of such communities for their members (Tedeschi 2011). Unlike traditional online retailers (e.g., Amazon), an OSSC allows its community members to connect with one another and be "friends."
Our data was obtained from an anonymous, and now defunct, OSSC based in Being, China (henceforth the community). The community hosted an online platform where consumers could find recreational services (e.g., ceramic studios, dance schools, and do-it-yourself bakeries), write and share their views about the services, as well as connect with other members. Over the course of the observation period, the online community attracted a total of 11,430 registered consumers (i.e., members) and 2,456 sellers.
While free for members, the community charged affiliated sellers a percentage of the sales price for each order made through the community website. Each seller had a virtual storefront with standardized layouts containing product descriptions, as well as order and checkout pages. Most of the "products" were experience goods and were relatively expensive, so product reviews were an important information source for potential buyers. Sellers were strictly prohibited from providing any incentives (e.g., discounts or free services) for the product reviews.
Community members set up personal portals where they could create and update personal profiles, post

product reviews, and join "circles" with other members sharing similar interests. Members also engaged in nonpurchase discussions through a public forum by either initiating a new topic or replying to an existing one. A member could form social connections by sending an invitation to another member, and once the invitation was accepted, the two members were friends on the platform. In this study, we use number of friends as the measure of social connectedness. The distinction between friends and nonfriends is very important from the perspective of product review sharing because a product review posted by any member was automatically pushed to all of her friends; by contrast, members who were not connected with the contributor would only find the same review when shopping at the seller's website.
Overview of the Field Study. During the first year of operation (January 2009­December 2009), the community depended solely on voluntarily contributed product reviews, but became increasingly concerned about the decline in contributions. In hopes of reversing the decline in its member-generated contents, the community publicly announced that starting on January 1, 2010, it would offer a monetary reward for each product review posted. Immediately before introducing the monetary rewards, the focal community placed on the landing page of its website an announcement of its new policy. This announcement remained visible for the rest of the observation period. Thus, it is reasonable to assume that the offered monetary rewards were public knowledge to all community members. The reward was a cash-equivalent community credit worth approximately $0.25, redeemable at all affiliated sellers. The introduction of a monetary reward effectively divided the observation period into two regimes: a four-month voluntary regime from September 2009­ December 2009; and a four-month paid regime from January 2010­April 2010. This intervention provides a good opportunity to empirically assess the differential effect of monetary rewards across members in this community.
Data. The online community was launched in January 2009; however, the data collection was not systematic until September 2009. The data we use span an eight-month period, from the beginning of September 2009 to the end of April 2010. The community provided us with a random sample of 2,286 members (approximately 20% of all registered members). For each member, we have the detailed records of activities that include review contributions, orders, and logins. The community also tracked each member's friendnetwork formation over time from January 2009. To obtain the estimation sample, we took two steps to eliminate data unsuitable for this research. First, the regime change may have attracted members who are

332

Sun, Dong, and McIntyre: Motivation of User-Generated Content Marketing Science, 2017, vol. 36, no. 3, pp. 329­337, © 2017 INFORMS

Table 1. Summary Statistics and Correlations

V1

V2

V3

V4

V5

V6

V7

V8 V9

Mean Std. dev.

Correlations

V1: Review dummy (Reviewit ) V2: Cumulative reviews (CumReviewsit ) V3: Number of logins (Loginit ) V4: Number of orders (Orderit ) V5: Community discussions (Discussit ) V6: Number of friends (Friendsit ) V7: Number of circles (Circleit ) V8: Tenure (weeks) (Tenureit ) V9: Postreward dummy (PostRewardt )

0.04 0.98 6.50 0.003 0.15 2.19 1.54 14.54 0.75

0.19 2.33 25.6 0.07 1.48 7.72 5.15 9.15 0.43

Note. All correlations are significant at the p < 0.01 level.

1.00 0.267 1.00 0.240 0.610 1.00 0.117 0.119 0.147 1.00 0.355 0.186 0.128 0.089 1.00 0.271 0.678 0.860 0.136 0.115 1.00 0.299 0.676 0.689 0.132 0.271 0.716 1.00 -0.138 0.232 0.096 0.004 -0.082 0.095 0.080 1.00 -0.189 0.043 -0.053 -0.018 -0.097 -0.075 -0.080 0.617 1.00

more interested in the monetary reward. To avoid this possible bias, we included only members who joined before January 1, 2010. Second, we focus on active members, i.e., those who participated in at least one of the following community activities during the data period: logins, discussions, orders, and product review postings. Inactive members were excluded because without any activity on the website, it is not feasible to infer their responses to monetary rewards. Table 1 provides the summary statistics.
The resulting estimation sample contains 25,000 weekly observations from 878 active members. For each week t, we observe whether member i provided a review (Reviewit 1) or not (Reviewit 0), her total number of reviewing weeks up to t(CumReviewit), and her nonreviewing activities, which include logins (Loginit), orders (Orderit), and community discussions (Discussit). An average member posted reviews in approximately 4% of all weeks.8 A typical member logged in to the website 6.5 times a week and engaged in 0.15 community discussions, on average, although the large standard deviations of login frequency (25.6) and discussions (1.48) indicate substantial variation in terms of engagement level with the community. The average order rate was low (0.003). At the time of the regime change, community members had an average of less than two friends.
We first observe that at the aggregate level, the introduction of monetary rewards failed to reverse the decline in the contribution rate: compared with the four-week prereward period, the total review frequency in the four-week postmonetary reward period decreased from 0.055 to 0.038, a significant drop. To examine the possible moderating effect of social connectedness, we classify members of the community into four subgroups, based on their friend counts at the time of the regime change. Among the 878 members, 689 had 0 friends (we call them "loners"), 74 members had 1­2 friends, 47 members had 3­5 friends, and 68 members had more than 5 friends (we call them "socialites"). For each of the four subgroups, we compute the average contribution during the four weeks

prior to and after the introduction of payment for reviews. The results are presented in Figure 1, where the x-axis represents the subgroups, defined by the number of friends, and the y-axis plots the average number of reviews. This chart shows that, prior to the rewards, people with more friends tended to offer more reviews; however, that reversed after the monetary rewards started.
To quantify both the main and moderating effects of social connectedness in the members' responses to the monetary reward introduction, we develop a difference-in-differences (DID) model.
Model. The dependent variable is dit, where

1, if member i posts a review in week t,

dit 0, otherwise.

(1)

This decision is based on a latent utility

Uit 0it + 1 PostRewardt + 2 CumReviewsit

+ 3 Tenureit + 4 Friendsit

+ 5 PostRewardt × Friendsit

+ 6 dit-1 + it .

(2)

In this setup, PostRewardt takes a value of 1 if week t is after the introduction of the monetary reward.

Figure 1. Average Review Production by Number of Friends (Four Weeks Prior vs. Four Weeks Post)
0.5 4 weeks before 4 weeks after
0.4

0.3

0.2

0.1

0 0 friends

1­2 friends

3­5 friends

Note. High-low indicators are ± one standard error.

>5 friends

Sun, Dong, and McIntyre: Motivation of User-Generated Content Marketing Science, 2017, vol. 36, no. 3, pp. 329­337, © 2017 INFORMS

333

The coefficient 1 captures the average effect of the monetary reward. CumReviewsit counts the cumulative number of reviews provided by i up to week t, which captures a possible fatigue effect coming into play after members started posting reviews. Tenureit refers to the number of weeks since i joined the community, which captures the change in the contribution probability over time before a member posted the first review. The two key variables are the number of friends (Friendsit) and the interaction term (PostRewardt × Friendsit). The parameter of Friendsit captures the average main effect of the number of friends on a member's review offering probability. As discussed earlier, given that an individual's reviews will be automatically shared with her friends, the number of friends is a proxy for the size of the audience, which has been identified as an important factor influencing whether or not to offer a review (Zhang and Zhu 2011). The parameter for the interaction term captures the moderating effect of friends in influencing people's responses to the monetary reward. In addition, to capture the possible state dependence in product review behaviors over time, we incorporated a review dummy into the last period by the same member. Finally, weekly level fixed effects are included to capture any possible week-specific effect (e.g., a week with a long weekend may be a relatively popular time to write a review).
The main and moderating effects of the number of friends are the focus of our study. However, it is possible that some common factors at the individual level might drive both the decisions of "the number of friends" and "whether to offer a review," which would make the number of friends an endogenous variable. To allow for such a possibility, we use an instrumental variable approach with "the number of circles" variable as an instrument.9
Assuming that it follows a standard normal distribution, we obtain a binary probit model with an endogenous regressor, which is then estimated with the maximum likelihood method provided in Stata.

The second column of Table 2 summarizes these results. The estimate for 1, the response to the monetary reward, is -1.27, statistically significant at the 0.05 level.10 The parameter estimate for Tenureit is negative (-0.053) and statistically significant, indicating a fatigue effect. By contrast, the parameter estimate for CumReviewsit has a positive and significant estimate. Combined, these two parameters suggest that over time, a member is less likely to start posting reviews. However, the more reviews a member has written, the more likely she is to post another review. The estimated coefficient of (Friendsi) is positive, with 4 0.050; furthermore, the coefficient of PostRewardt × Friendsit is negative, with 5 -0.077; and both are statistically significant. These two parameters indicate that members with more friends have a higher baseline propensity to offer reviews, compared to those with fewer friends. However, when a monetary reward is offered, members with more friends responded more negatively in their review frequency. In other words, the estimation results show that the number of friends has a positive impact on the baseline probabilities of posting reviews; in the meantime, it moderates the effect of the monetary reward. Finally, the parameter estimate for a lagged review is positive and statistically significant, indicating positive state dependence in the review contribution tendency.
Robustness Checks. To ensure the reliability of the empirical results, we conducted a number of robustness checks.11
(a) Alternative cutoff dates. To validate the DID model, we repeated the analysis with two alternative, "placebo" cutoff dates. The first one is the week right before, and the second is the week right after the week when the monetary rewards were introduced by the focal community. We found two main results. First, the model fit based on either of these alternative cutoffs is significantly worse than that of the main model.12 These additional results are consistent with the monetary rewards, taking effect in the week prescribed by the focal community.

Table 2. Estimation Results

Main model

Robustness check (c1) Robustness check (c2) Robustness check (d) Robustness check (e)

Model based with

Model based with

Model with active Main model, with

members without friends members with friends

contributors

active friends

Intercept
Friendsit PostRewardt PostRewardt × Friendsit CumReviewsit Tenureit Reviewi,t-1

-1.565 (0.131) 0.050 (0.007)
-1.268 (0.587) -0.077 (0.008)
0.212 (0.010) -0.053 (0.005)
0.277 (0.077)

-2.957 (0.172)
-- 1.062 (0.179)
-- 0.653 (0.032) -0.044 (0.004) -0.298 (0.169)

-1.115 (0.138) 0.044 (0.009) 1.399 (0.703)
-0.051 (0.009) 0.245 (0.016)
-0.169 (0.013)
-0.040 (0.089)

-1.310 (0.140) 0.048 (0.008)
-0.696 (0.493) -0.070 (0.008)
0.155 (0.011) -0.057 (0.005)
0.107 (0.071)

-1.583 (0.131) 0.064 (0.010)
-1.224 (0.577) -0.079 (0.008)
0.211 (0.010) -0.054 (0.005)
0.265 (0.078)

Note. In robustness check (c2), we replaced Friendsit with Friendsit - 1 so that the main effect PostRewardt captures the impact of monetary rewards at zero friends.
p < 0.10 level; p < 0.05 level; p < 0.01 level.

334

Sun, Dong, and McIntyre: Motivation of User-Generated Content Marketing Science, 2017, vol. 36, no. 3, pp. 329­337, © 2017 INFORMS

(b) Individual choice model. To measure the effects more precisely, we employ an individual-level logit choice model estimated within an HB framework, accounting for individual-level heterogeneity and possible endogeneity. The top level of the HB model captures the drivers of each member's decision to post a product review, while allowing for individualspecific parameters. The lower level relates the observed characteristics, particularly social connectedness, to a community member's willingness to post reviews, both before and after the monetary reward was introduced. The HB model confirms our finding that social connectedness moderates monetary rewards for generating social contributions, as found in the model-free and DID analyses. The reader is referred to part 1 of the online appendix for the details of the model specifications and results.
(c) Separate estimation of members with versus without friends. The main analysis pooled members with friends and those with no friends. As an alternative, we split the estimation sample into those with or without friends, and estimate the model on each group separately. As presented in the third and fourth columns of Table 2, the results echo those of the main model qualitatively. In particular, for the group with no friends, the estimated response to the reward is positive and statistically significant (1.06), indicating that members with no friends respond to the monetary reward positively. For the group with friends, the estimate for the reward parameter is positive, but not statistically significant (mean 1.399, standard error 0.703). The estimate for the interaction term is negative (-0.051) and statistically significant, showing that the rewards diminished the review posting frequency for more connected members.
(d) Estimation based on active contributors. Members who posted reviews may be different from those who were "active" (e.g., placed an order), yet never posted any reviews. Thus, we estimate a model using the subsample of active review contributors, defined as members who contributed at least one review in the observation period. The results are presented in column 5 of Table 2. We find that the results qualitatively echo those of the main model.
(e) Visibility as a function of active friends. The main analysis assumes that the "visibility" of review posting is a function of the number of friends before the regime change. A possibly better proxy for visibility is the number of active friends, since review posting is less likely to be observed by members who were not active. Thus, we recompute the variable by excluding friends who were inactive during the observation period. We find that the new variable is highly correlated with the original variable (correlation is 0.99). Therefore, it is not surprising that our estimation, based on the new variable, produced almost identical results, as listed in the last column of Table 2.

The above analysis demonstrates the robustness of the moderating effect of the number of friends on the response to monetary rewards. We next examine possible alternative explanations.
Alternative Explanations. Following Remler and Van Ryzin (2010), we examine three categories of alternative explanations: (a) chance factors, (b) extraneous factors, and (c) history effect.
(a) Chance factors. Bertrand et al. (2004) show that in panel data, ignoring serially correlated outcomes with a one-shot treatment (as in our context) may lead to false significant estimates of the treatment effect. We follow the suggestion by Bertrand et al. (2004) and collapse the data into "before" and "after" periods and check the before-after differences across the friend groups. We find that across the friend groups, the contribution rates significantly decreased (increased) for socialites (loners), as has been demonstrated in Figure 1.
An additional chance factor concern is regression to the mean (RTM), i.e., the high (low) level of voluntary contributions by more- (less-)connected community members was a result of sheer chance, and these levels simply reverted to a lower (higher) level after the regime change. Typically, RTM is a threat when a pretreatment measure is used to assign experimental treatments to groups, when there is self-selection, or when there is some pretreatment difference in the groups in terms of the dependent variable (i.e., frequency of review writing). Figure 2 highlights the differential change in contributions around the reward introduction (week 0), benefitting from the panel perspective of the data. Although there was a mild decline in the review frequency for socialites before the introduction of the reward, the dramatic shifts in review

Figure 2. Average Review Frequency by Number of Friends and Week

1.0

0.9

>5 friends No friends

0.8

0.50 Switch to payment
0.45
0.40

0.7

0.35

0.6

0.30

0.5

0.25

0.4

0.20

0.3

0.15

0.2

0.10

0.1

0.05

0

0

­12 ­9 ­ 6 ­3 0

3

6

9 12

Week

Note. Left axis for the group with >5 friends; right axis for the group with no friends.

Sun, Dong, and McIntyre: Motivation of User-Generated Content Marketing Science, 2017, vol. 36, no. 3, pp. 329­337, © 2017 INFORMS

335

frequency only at week 0 are evident--downward for the socialites, but upward for the loners. This seems to rule out regression to the mean as an alternative explanation for these shifts (or else the shifts would have happened in any of the other weeks prior to the announcement, but they did not).
(b) Extraneous factors. First, we consider the possible signaling effect of monetary rewards (Gneezy et al. 2011). Specifically, the announcement of a reward itself may have suggested to community members that writing a review is a more difficult task than they may have previously thought. Second, the change in review frequencies may have been driven by the change in community engagement levels, which can be measured by the average login and order frequencies around the regime change. Third, based on social exchange theory (Gatignon and Robertson 1986), more-connected contributors may have felt more obliged to increase their efforts, which might also have reduced their willingness to post reviews in the first place. To test for these possibilities, we conducted several checks that boil down to analyzing the following empirical questions: Did the number of other activities, such as weekly logins and orders, change around the regime switch? Did the effort put into writing a review change (given that one was written)? As detailed in part 2 of the online appendix, none of these effects showed any similarity to the one observed for the number of reviews written.13
(c) History effect. One possible alternative explanation to the data patterns is some unobserved extraneous factor that happened along with the monetary rewards, leading to changes in the members' behaviors and their participation in the community. From the perspective of members' decisions on community engagement, the decision calculus for participating in discussions and offering product reviews is very similar. We exploit the fact that the monetary reward was offered only for product reviews, but not for community discussions. Before the regime change, the correlation between product reviews and discussions was positive and significant (rho 0.36, p < 0.001). However, taking the panel view of the data again, Figure 3 shows that the pattern is very different for uncompensated community discussions. Specifically, discussions slightly increased in the four weeks after payment (for reviews) started for well-connected community members (e.g., those with >5 friends), but decreased in the four weeks after the regime change for less-connected members (with 0 friends).14
Combining (a)­(c), the analyses confirm that it is the introduction of monetary rewards, rather than changes in engagement factors (e.g., logins, purchases, and community discussions), that led to significant changes in the review posting frequency.

Figure 3. Average Community Discussions Frequency by Number of Friends and Week

3.0 >5 friends No friends
2.5

0.50 Switch to payment
0.45
0.40

0.35 2.0
0.30

1.5

0.25

0.20 1.0
0.15

0.10 0.5
0.05

0

0

­12 ­ 9 ­6 ­3 0

3

6

9 12

Week

Note. Left axis for the group with >5 friends; right axis for the group with no friends.

Analysis on Review Efforts. Preceding analyses have focused on changes in review contributions after a regime change. A natural question is, to what extent does the introduction of monetary rewards affect the efforts that were spent writing the reviews? We investigate this question by measuring both (1) the length of the reviews, and (2) the perceived efforts and helpfulness of the reviews around the regime change. As detailed in part 2 of the online appendix, we examine the impact of monetary rewards on the lengths of the reviews contributed (measured by the numbers of characters in the reviews). We find that the introduction of the monetary reward had a negative and significant impact only on the contribution frequency, but not on the review length by community members once they decided to contribute. To measure (2), we hired two research assistants, both of whom are native Chinese speakers and are blind to our research questions. The research assistants independently read the texts of 1,500 product reviews in the estimation sample and rated the reviewers' efforts in writing the reviews, as well as perceived review helpfulness on 1­7 point Likert scales. We find that conditional on contributing a review, after the introduction of the monetary reward, the amount of effort put forth by members without friends significantly decreased (Mbefore 4.82, Mafter 4.46, Mdiff 0.36, p < 0.05). Similarly, the perceived helpfulness of the review also decreased (Mbefore 5.39, Mafter 4.92, Mdiff 0.47, p < 0.05). These results are interesting, but not quite surprising in retrospect. Recall that the focal community's policy is that monetary rewards are given to all contributed reviews, without stipulating any requirements for the contributed content. Such a policy may have likely induced a "transactional" mindset

336

Sun, Dong, and McIntyre: Motivation of User-Generated Content Marketing Science, 2017, vol. 36, no. 3, pp. 329­337, © 2017 INFORMS

(e.g., Heyman and Ariely 2004) for loners, who might have focused on getting a good deal for the transaction, that is, a low cost of effort per unit of reward. By contrast, among members who are socially connected, the monetary reward hardly had any effect on effort (Mbefore 4.75, Mafter 4.79, Mdiff 0.04, p > 0.60), or the perceived helpfulness of the review (Mbefore 5.08, Mafter 5.14, Mdiff 0.06, p > 0.50). These results suggest that the "transaction mindset" effect seems to have had no significant impact on the socially connected, and their contributions continued to be driven by intrinsic motivations (e.g., helping others). These results let us to conclude that there is no support for the alternative explanation that members with friends decreased their contribution because of the higher level of effort needed.
4. Summary and Limitations
To summarize, this study allowed us to examine product review contributions within an online community and the heterogeneous responses to a monetary reward. Our main finding was twofold. More-connected members contribute more often when the community relies purely on intrinsic motivation. However, the token-sized monetary rewards are motivating for members with few social connections, but demotivating for well-connected members. In other words, monetary rewards proved to be countereffective for those most active contributors! A further problem facing the platform is the possible decrease in effort put forth by the loners when they finally did write a compensated review. In retrospect, our results provide a possible explanation for why platforms paying public cash rewards (Epinions and Refer.ly) have closed down, and why few existing platforms publicly offer monetary rewards for review contributions. Platform managers would be better advised to provide a private monetary reward only to members who have few connections on the platform, as this proves to be most effective.
We note that our field study has several contextual features that are conducive to the negative effect of monetary rewards on the most connected community members. First, the "push to friends only" design of the community is a feature shared by major social networks (e.g., Facebook and Twitter), but not all social networks. Second, the public introduction of a monetary reward is more likely to trigger reputational concerns than privately offered rewards. Third, a tokensized monetary reward was offered, which is more likely to have a countereffect than a large monetary reward (Benabou and Tirole 2006).
In addition, our study has a number of limitations, providing direction for future research. First, in the absence of a direct measure of motivation from consumers, the number of friends is a surrogate for some underlying set of motivations. Second, future research

can examine the effectiveness of larger-than-token-size monetary rewards, or other types of noncash incentives, such as free products (e.g., Stephen et al. 2012). It would also be interesting to conduct controlled field experiments to examine when monetary rewards are "sufficiently large" to induce across-the-board increases in review contributions. A well-designed experiment would also be able to identify the main effect in addition to the interaction effect. Third, more sophisticated text analysis methods (e.g., Lee and Bradlow 2011) can be leveraged to understand how introducing monetary rewards may affect the content of reviews. Fourth, we focused on review contribution frequencies; and future research can examine the impact of social reviews on downstream outcomes (e.g., purchases), and to compare the differential effects of reviews by more- versus less-connected members. Finally, the online community that we studied was relatively small and arguably idiosyncratic. Thus, caution is advised about generalizing our results, and future research should investigate whether tie strengths are weaker in larger communities (e.g., Facebook), and whether tie strengths among community members further moderate the negative effect of monetary rewards.
Acknowledgments The authors thank the senior editor, the associate editor, and two anonymous reviewers for their help in improving the paper. The authors would like to thank the Marketing Science Institute for funding a related project as well as the Leavey School of Business for supporting this research and thank Junlin Du for providing the data. The authors thank John G. Lynch, Page Moreau, Atanu Sinha, Pradeep Chintagunta, Praveen Kopalle, Xiaohua Zeng, Ying Xie, Mike Norton, Dina Mayzlin, William Sundstrom, and Juanjuan Zhang for their insightful comments. All remaining errors are those of the authors.
Endnotes
1 For example, Citysearch has seen a dramatic increase in registrations since implementing Facebook Connect: the number of daily registrations has tripled since its launch, and 94% of reviewers are sharing their reviews on Facebook. TripAdvisor now draws more than a third of new reviews from Facebook-connected users. In 2012 alone, one billion "open graph share actions" took place on the site, indicating that users are tapping their friends within TripAdvisor for information regarding properties, services, and locations. 2 For example, only 1% of Yelp users are active contributors (Darnell 2011). 3 The FTC guideline states: "If there's a connection between an endorser and the marketer of the product that consumers would not expect and it would affect how consumers evaluate the endorsement, that connection should be disclosed." https:// www.ftc.gov/tips-advice/business-center/guidance/ftcs-revisedendorsement-guides-what-people-are-asking (accessed January 31, 2017). 4 Personal invitations and offers received by the authors from the two companies. 5 In the context of rewarded referrals, Verlegh et al. (2013) found empirical support that rewards lead recipient consumers to infer "ulterior" motives for the referral.

Sun, Dong, and McIntyre: Motivation of User-Generated Content Marketing Science, 2017, vol. 36, no. 3, pp. 329­337, © 2017 INFORMS

337

6 For example, Seeking Alpha recently added a disclosure section, e.g., "I wrote this article myself, and it expresses my own opinions. I am not receiving compensation for it (other than from Seeking Alpha). I have no business relationship with any company whose stock is mentioned in this article." http://seekingalpha.com/article/ 4040723-biggest-flaw-teslas-cars (accessed January 31, 2017).
7 If sufficiently large, undoubtedly a monetary reward would have a positive effect on review production. However, that may not be the case for small or token-sized monetary rewards, which would be most feasible in practice. In the rest of the paper, a monetary reward always indicates a small monetary amount.
8 We find that the pattern of review posting in our focal community is similar to that reported in larger online social networks. Specifically, among all community members in the data sample, 85.1% contributed zero reviews, 12.1% contributed 1­10 reviews, and 2.8% contributed more than 10 reviews. This pattern is in line with the "90-9-1" principle (e.g., Darnell 2011), which states that 90% of users do not actively contribute to the site, 9% of users contribute occasionally, and 1% of users are very active contributors. This implies that although the focal review network is modest in size, it is similar to those larger counterparts studied previously.
9 The validity of the instrument is discussed in detail for the hierarchical Bayes (HB) model in the online appendix.
10 While it is tempting to conclude that monetary rewards had a negative impact on a member's review contributions, this result should be interpreted carefully because (1) the "design" of the field study did not have a control group and (2) this simple DID model does not control for heterogeneity.
11 We thank the reviewers for their very helpful suggestions in conducting these robustness checks.
12 The Akaike Information Criteria (AIC) for these three models are 5,046.6 (main model), 5,071.8 (the model with the placebo cutoff date set at week "-1"), and 5,112.4 (the model with the placebo cutoff date set at week "+1").
13 We thank the associate editor's suggestion to check this.
14 Note that this is essentially a "falsification check" (e.g., Sudhir and Talukdar 2015). Granted, even this test does not rule out every possible history effect; yet, for a history effect to be a true threat, it would have to (1) interact with the number of friends for the review contributions in the hypothesized direction, (2) but not interact with the number of friends for community discussions. An alternative explanation other than the effect of a monetary reward seems to be unlikely.
References
Avery C, Resnick P, Zeckhauser R (1999) The market for evaluations. Amer. Econom. Rev. 89(3):564­584.
Barasch A, Berger J (2014) Broadcasting and narrowcasting: How audience size affects what people share. J. Marketing Res. 51(3): 286­299.
Benabou R, Tirole J (2006) Incentives and pro-social behavior. Amer. Econom. Rev. 96(5):1652­1678.
Bertrand M, Duflo E, Mullainathan S (2004) How much should we trust differences-in-differences estimates. Quart. J. Econom. 119(1):249­275.
Brown JJ, Reingen PH (1987) Social ties and word-of-mouth referral behavior. J. Consumer Res. 14(3):350­362.
Chen Y, Xie J (2008) Online consumer review: Word-of-mouth as a new element of marketing communication mix. Management Sci. 54(3):477­491.

Clifford S (2012) Sites that pay the shopper for being a seller. New York Times (October 2), http://www.nytimes.com/2012/10/03/ business/media/shopping-sites-pay-contributors-who-drive -traffic-to-retailers.html.
Darnell H (2011) Yelp and the "1/9/90 rule." Yelp official blog. (June 10), https://www.yelpblog.com/2011/06/yelp-and-the-1 -9-90-rule.
Feick LF, Price LL, Higie RA (1986) People who use people: The other side of opinion leadership. Adv. Consumer Res. 13(1):301­305.
Frey BS, Jegen R (2001) Motivational interactions: Effects on behaviour. Annales d'Economie et de Statistique, 131­153.
Garnefeld I, Iseke A, Krebs A (2012) Explicit incentives in online communities: Boon or bane? Internat. J. Electronic Commerce 17(1): 11­38.
Gatignon H, Robertson TS (1986) An exchange theory model of interpersonal communication. Lutz RJ, ed. Advances in Consumer Research, Vol. 13 (Association for Consumer Research, Provo, UT), 534­538.
Gneezy U, Meier S, Rey-Biel P (2011) When and why incentives (don't) work to modify behavior. J. Econom. Perspect. 25(4): 191­209.
Hennig-Thurau T, Gwinner KP, Walsh G, Gremler D (2004) Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the Internet? J. Interactive Marketing 18(1):38­52.
Heyman J, Ariely D (2004) Effort for payment: A tale of two markets. Psych. Sci. 15(11):787­793.
Lee TY, Bradlow ET (2011) Automated marketing research using online customer reviews. J. Marketing Res. 48(5):881­894.
Levitt SD, List JA (2007) What do laboratory experiments measuring social preferences reveal about the real world? J. Econom. Perspect. 21(2):153­174.
McIntyre S, Mcquarrie E, Shanmugam R (2015) How online reviews create social network value: The role of feedback versus individual motivation. J. Strategic Marketing 23(December):1­16.
Remler DK, Van Ryzin GG (2010) Research Methods in Practice: Strategies for Description and Causation (Sage Publications, London).
Rosen E (2000) The Anatomy of Buzz (Random House, New York). Simester D (2017) Field experiments in marketing. Duflo E, Banerjee
A, eds. Handbook of Economic Field Experiments, Vol. 1 (North Holland, Amsterdam), Forthcoming. Stephen A, Bart Y, Plessis CD, Goncalves D (2012) Does paying for online product reviews pay off? The effects of monetary incentives on consumers' product evaluations. INSEAD Working Paper 2012/96/MKT, Fontainebleau, France. Streitfeld D (2012) The best book reviews money can buy. New York Times (August 25), http://www.nytimes.com/2012/08/26/ business/book-reviewers-for-hire-meet-a-demand-for-online -raves.html. Sudhir K, Talukdar D (2015) The "Peter Pan syndrome," in emerging markets: The productivity-transparency trade-off in IT adoption. Marketing Sci. 34(4):500­521. Tedeschi B (2011) Like shopping? Social networking? Try social shopping. New York Times (September 11), http://www.nytimes .com/2006/09/11/technology/11ecom.html. Toubia O, Stephen AT (2013) Intrinsic versus image-related motivations in social media: Why do people contribute content to Twitter? Marketing Sci. 32(3):368­392. Verlegh PW, Ryu G, Tuk MA, Feick L (2013) Receiver responses to rewarded referrals: The motive inferences framework. J. Acad. Marketing Sci. 41(6):669­682. Zhang X, Zhu F (2011) Group size and incentives to contribute: A natural experiment at Chinese Wikipedia. Amer. Econom. Rev. 101(4):1601­1615.

