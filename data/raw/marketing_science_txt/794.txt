Vol. 26, No. 5, September­October 2007, pp. 642­650 issn 0732-2399 eissn 1526-548X 07 2605 0642

informs ®
doi 10.1287/mksc.1060.0256 © 2007 INFORMS

Investigating Endogeneity Bias in Marketing

Qing Liu
Department of Marketing, University of Wisconsin­Madison, Madison, Wisconsin 53706, qliu@bus.wisc.edu
Thomas Otter
Faculty of Business and Economics, Johann Wolfgang Goethe University, Frankfurt, Germany, otter@marketing.uni-frankfurt.de
Greg M. Allenby
Fisher College of Business, Ohio State University, 2100 Neil Avenue, Columbus, Ohio 43210, allenby.1@osu.edu
The use of adaptive designs in conjoint analysis has been shown to lead to an endogeneity bias in partworth estimates using sampling experiments. In this paper, we re-examine the endogeneity issue in light of the likelihood principle. The likelihood principle asserts that all relevant information in the data about model parameters is contained in the likelihood function. We show that, once the data are collected, adhering to the likelihood principle leads to analysis where endogeneity becomes ignorable for estimation. The likelihood principle is implicit to Bayesian analysis, and discussion is offered for detecting and dealing with endogeneity bias in marketing.
Key words: likelihood principle; adaptive design; Bayes theorem; directed acyclic graphs History: This paper was received January 31, 2006, and was with the authors 2 months for 2 revisions;
processed by Eric Bradlow.

1. Introduction
The recent paper by Hauser and Toubia (2005), "The Impact of Utility Balance and Endogeneity in Conjoint Analysis," raises an interesting set of issues related to a wide class of marketing models. In their paper, they present evidence that adaptive designs, where answers to early questions in a conjoint interview are used to select later questions, induce biases in the estimated part-worths. While their paper focuses on analysis associated with Sawtooth Software's popular ACA (Adaptive Conjoint Analysis) software, the implications of their analysis reach beyond conjoint analysis, sequential analysis, and utility balance, touching on important philosophical issues at the core of statistical inference.
In this paper, we reexamine the endogeneity bias identified by Hauser and Toubia (HT) and explain its presence using traditional econometric methods. Bias is an aspect of statistical inference that relies on the notion of a sampling experiment, where hypothetical data sets are used to characterize the performance of an estimator. We argue that sampling experiments are useful to study properties of estimators and other procedures when real data are not available. However, when data are available, analysis should proceed according to the likelihood principle as originally proposed by Fisher (1922).
The likelihood principle asserts that the likelihood contains all the information about model parameters

(e.g., conjoint part-worths) in the data. We show that, according to the likelihood principle, the endogeneity created by adaptive questioning is not of concern for estimation, i.e., does not alter the likelihood function of the observed data. Our discussion of the likelihood principle raises a number of philosophical issues at the core of statistical inference that highlight the difference between classical (i.e., frequentist) and Bayesian philosophies.
Our analysis of endogeneity bias in conjoint models uses HT as a springboard for discussion and is not meant to criticize their findings. In fact, our analysis covers some of the same ground as their analysis, restating their findings in terms more familiar to the statistics literature. Our examples are sometimes similar to those examined by HT and sometimes depart from theirs to provide additional insight and analysis. We applaud their interest in analyzing the issue of endogeneity created by adaptive designs because of its importance to both practitioners and academic researchers.
The remainder of the paper is organized as follows. We begin with a review of endogeneity bias in regression models caused by adaptive designs, restating many of the points made by HT. We then introduce the likelihood principle and examine its implication for data analysis. Our analysis shows that, conditional on the dependent variable y , the way the design is adaptively created is not informative

642

Liu, Otter, and Allenby: Investigating Endogeneity Bias in Marketing

Marketing Science 26(5), pp. 642­650, © 2007 INFORMS

643

about model parameters. The mechanism employed to pick the design points, and the resulting endogeneity, are "ignorable" for the purpose of likelihood-based inference (Gelman et al. 2004, p. 203; Rubin 1976).1 Thus, we believe the potential harm from endogeneity created by adaptive designs should be considered when selecting among experimental procedures (e.g., adaptive versus fixed designs), not in estimation. Discussion is provided about the importance of bias in the evaluation of procedures, and tools are offered for detecting when endogeneity created by adaptive questioning and other forms of sample selection will impact conditional inference--i.e., when it alters the likelihood function. Concluding comments are then offered, in which we advocate a Bayesian orientation to conducting analysis in marketing.

2. Endogeneity Bias
Endogeneity bias arises in regression analysis y =
X + when the regressors, X, are not independent of
the errors, . When a specific realization of the regres-
sors, xt, is selected based on the outcome of previous choices, y<t, as is the case for sequential design data, then xt is determined from within the system of study and is not independently determined. In this
case, the traditional ordinary least-squares estimator
is biased because the vector space of the regressors is
dependent on the error realizations. Regression coef-
ficient bias is defined as the difference between the
true value of the parameter and the expected value of
the estimate over hypothetical sets of data (D):

ED

= ED X X -1X y

= ED X X -1X X +

= + ED X X -1X

(1)

The presence of endogenous responses biases the regression estimate because the second term in Equation (1) is not equal to zero. To see this, consider a simple example where there is just one regressor without an intercept (y = x + ), and just two observations. The OLS estimator is given by

=

x1y1 x12

+ +

x2y2 x22

(2)

Then, if x2 is determined by the value of y1, i.e., x2 = f y1 = f x1 + 1 , we have

ED

= + ED

x1 1 x12 + x22

+

x2 2 x12 + x22

(3)

1 We would like to thank the associate editor for pointing out that the result can be viewed as an extension/application of Rubin's (1976) framework for classifying missing data mechanisms.

The expectation of the first term in the brackets is
not equal to zero because y1, and hence 1, is used to determine x2. Because 1 and x2 are not independently determined, E f x2 1 = E f x2 E 1 = 0, specifically

ED

x1 1 x12 + x22

= ED

x1 x12 + x22

ED

1 =0

(4)

Thus, for regression estimates to be unbiased it must be the case that all errors are independent of all the regressors (X).
The lack of independence between regressors and errors occurs in many models. Models with a lagged dependent variable (e.g., Koyck lag models, autoregressive models) violate the assumption that the regressors are independent of the error terms. In these models, the lagged regressors are themselves functions of the lagged error terms, and the assumption of independent regressors and errors is not valid. Endogeneity bias also occurs in models with sequential design, in which design points are selected based on previous responses. While the specific algorithm used to select design points (e.g., utility balance in ACA) may exacerbate the extent of endogeneity bias documented by frequentist analyses of small-sample situations with relatively large error variance, the bias is present whenever the vector of residuals, , is not independent of all the regressors.
Figure 1 illustrates the biasing effect of endogeneity, using a simple problem similar to that studied by HT. The analysis involves 1,000 replicates of samples, each consisting of 1,000 homogeneous respondents. Each respondent supplies three observations, where the regressor for the third observation is determined as a function of the first two observations. The

Figure 1 3.0

Expected Value of OLS Estimate Across 1,000 Replications
E [3 obs. estimate] Pooled estimate

2.5

Beta 2

2.0

1.5

0.5

1.0

1.5

2.0

Beta 1

Liu, Otter, and Allenby: Investigating Endogeneity Bias in Marketing

644

Marketing Science 26(5), pp. 642­650, © 2007 INFORMS

model is yt = 1x1t + 2x2t + t, t = 1 2 3 and t  Normal 0 25 . The value of x for the first observation

is x1 = x11 x21 = 1 0 , the value of x for the second

observation is x2 = 0 1 , and



 1 -1 if y1y2 > 0

x3 =  1 1

if y1y2  0

(5)

The design rule in Equation (5) is meant to mimic

the "utility balance" criterion used by the ACA soft-

ware. The true values of the regression coefficients

are 1 = 1 and 2 = 2. The figure shows that the OLS estimates exhibit positive bias, with E 1 = 1 198 and E 2 = 2 406. Each triangle character in the figure represents the mean of individual-level OLS estimates

computed from one sample of j = 1 1 000 homo-

geneous respondents (i.e.,

1000 j =1

j /1 000, where

j is

estimated with three observations yjt, xjt , t = 1 2 3).

Econometricians recognize the bias present in

regression models when functions of lagged depen-

dent variables are included in the model specifica-

tion, and as a result, the finite sampling properties of

these models are generally unknown (see Judge et al.

1988, p. 575). This has led to the use of asymptotics to

characterize the sampling properties of estimators in

such situations. In particular, the probability limit, or

"plim" is used to describe the behavior of estimators

as the sample size increases. The probability limit is a

formal expression for the consistency of an estimator.

An estimator is said to be consistent if the probability

limit of obtaining an estimate arbitrarily close to the

true parameter value equals one in infinite samples;

i.e., the probability that an estimate from a sample

of size T falls within the interval - + goes

to one as the sample size increases, no matter how

small :

lim P
T

T-

<

=1

(6)

A result known as Slutsky's theorem can be used to

show that the OLS estimator is asymptotically consis-

tent as defined above. Slutsky's theorem states that if

g is a continuous function and zT is some random variable that depends on T , then

plim g zT = g plim zT

(7)

Thus, from Equation (1) we can derive the plim of the regression estimate as the sample size increases:

plim = + plim X X -1X

= + plim X X /T -1plim X /T

=

(8)

where the last equality holds if plim X X /T -1

converges to a finite-valued matrix (i.e.,

-1 XX

,

and

plim X /T converges to zero. The latter condition

holds as long as ED xt t = 0 for all t. Consistency only requires independence between regressor values xt and their corresponding error term t, and not the entire vector of errors. Thus, the requirements for asymptotic consistency are easier to obtain than the requirements of unbiasedness.
To illustrate the consistency of the OLS estimator in the simulation study, we obtained pooled OLS estimates for each of the replicated data sets, i.e., estimated regression coefficients using all 3,000 observations (1,000 respondents each supplying three observations) contained in each data set. The pooled estimates are plotted in Figure 1 as black diamonds. Clearly, the pooled estimates are not affected by the bias; their mean is = 1 005 2 000 which is close to the true value of = 1 2 . While the simulation study of HT does exhibit an endogeneity bias in small samples, the estimates are asymptotically consistent. Thus, endogenously determined covariates need not lead to inconsistent inferences, as is often assumed. As we show below, inconsistency arises from model misspecification, i.e., employing the wrong likelihood function for the data.
3. The Likelihood Principle
The study by HT touches on an important philosophical point about the role of sampling experiments in statistical analysis and managerial decision making. Bias is obviously a concern in conjoint analysis, where part-worth estimates are used to set prices and guide product formulation. Moreover, managers are often confronted with choosing the best way to collect and analyze data. Bayesians and frequentists disagree, however, on how to quantify knowledge given data and on how to study properties of statistical procedures and characterize their performance.
A principle of statistical inference first introduced by Fisher (1922), and adhered to in Bayesian analysis, is that the likelihood function contains all the information in the data about the model parameters. The likelihood principle implies that if two likelihoods for a parameter are proportional, then we should make the same inference for regardless of which likelihood we use. This principle has a number of significant and far-reaching implications. Berger and Wolpert (1984) provide an excellent review of this topic and provide many interesting examples. When considering implications of the likelihood principle, it is important to remember that the likelihood is always meant to represent the true data-generating mechanism, whose parameters have substantive meaning that guide decision making. Moreover, statistical inference involves the quantification of knowledge about these parameters.
An example of the application of the likelihood principle involves sequential sampling. Suppose we

Liu, Otter, and Allenby: Investigating Endogeneity Bias in Marketing

Marketing Science 26(5), pp. 642­650, © 2007 INFORMS

645

observe the number of successes Z in n independent Bernoulli trials with success probability . The likelihood for the data is

1 Z=z

= n z 1 - n-z z

(9)

Now, suppose that instead of holding fixed the number of trials, we were to decide to sample until we obtained z successes and then observed the realization of N , the number of trials to the zth success. In this scenario, N has a negative binomial distribution, so the model is

2 N =n

=

n-1 z-1

z 1 - n-z

(10)

and we have that, conditional on any pair z n , 1 Z = z n  2 N = n z . Despite the fact
that the sampling scheme and the dependent variable are different, the likelihood principle states that we should ignore this and make the same inference about in both cases. A Bayesian analysis of these data would be equivalent because the likelihood principle implies that the stopping rule associated with a data collection is irrelevant. In contrast, a frequentist analysis of these data would incorporate the effects of the stopping rule in computing standard errors of point estimates and constructing confidence intervals. Frequentist confidence intervals are based on the idea of multiple realizations of the data, across which the number of trials n will vary in Equation (10) but not in Equation (9).
Analysis in marketing contains many instances where adherence to the likelihood principle will lead to different inference. Analysis in direct marketing, for example, involves initial testing of offer formats using many lists, and subsequent mailings of the best offers using lists with the highest response rates. Ignoring the uncertainty present in the initial test phase does not matter as long as the goal of the analysis is to simply identify the best list for a specific offer. However, if the goal is more general, involving learning about offer formats and list characteristics associated with high yields using both sets of data, then whether one conditions on the observed data or not will matter. By considering multiple hypothetical realizations of the initial test data, frequentist inference must admit the possibility of alternative best offers mailed out in the second round. Frequentist confidence intervals are misrepresented unless alternative hypothetical second-round offers are included in the analysis. In contrast, the Bayesian approach is more straightforward because it adheres to the likelihood principle and simply conditions on the observed data in both samples.
Another example involves the use of screening questions by Internet merchants, where answers are used to determine offers that are likely to be of value

to the customer. Frequentist analysis of the observed purchases of the offers needs to account for possibly different answers to the screening questions, leading to offers different from those that were observed, to properly quantify the information in the data. In contrast, adherence to the likelihood principle implies that one should simply condition on the observed responses to the screening questions in the analysis. However, as discussed in greater detail later, simply conditioning on the particular offerings presented, and ignoring the responses to the screening questions, results in a misspecified likelihood.
Consider again the stylized example above, similar to the problem studied by HT. The likelihood function should acknowledge that x3, the third design point, is determined from within the system of study and is therefore endogenous:

y1 y2 x3 y3

2 = 1 y1 y2

2

× 2 x3 y1 y2

× 3 y3 x3

2

2
(11)

where the conditioning variables x1 and x2 are not written to improve readability. The first factor on the
right side of Equation (11) corresponds to the first
two observations where the design points x1 and x2 are determined beforehand. The likelihood for these
observations corresponds to that found in a standard
regression analysis:

1 y1 y2

2

=2 1

2 exp

-1 22

y1 - x1

2 + y2 - x2

2

(12)

Similarly, the third factor, which conditions on the
realized design point x3, is the same as found in standard analysis:

3 y3 x3

2 = 1 2

-1

exp
2

2

2

y3 -x3

2

(13)

The effect of endogeneity is therefore isolated in the
second factor, 2. The likelihood for x3 is determined by Equation (5),
where a positive product of y1 and y2 leads to the selection of x3 = 1 -1 , and a negative product yields x3 = 1 1 . Given y1 and y2, the selection of x3 is deterministic, or

2 x3 y1 y2

2 = 2 x3 y1 y2 = 1

(14)

Thus, while the likelihood for design point x3 is unconditionally dependent on the model parameters and 2, the likelihood is independent of these parameters given y1 and y2.2 This makes intuitive sense

2 Adding a random component to the selection mechanism imply-
ing a nondegenerate 2 does not change the basic argument. Conditional on previous data, x3 is again independent of and 2.

Liu, Otter, and Allenby: Investigating Endogeneity Bias in Marketing

646

Marketing Science 26(5), pp. 642­650, © 2007 INFORMS

because we learn about and 2 from the respondent, not the mechanism used to select the design point. Following the likelihood principle implies that, given the data, selection mechanisms such as ACA's utility balance are ignorable; i.e., the likelihood of model parameters is unchanged if 2 is included.

Heterogeneity and Selection Bias

In §6 of their paper, HT suggest that the presence

of heterogeneity might compound the biasing effects

of utility balance by also biasing the hyperparame-

ters of the model that describe the mean part-worths

(i.e., the mean of the random-effects distribution).3

The likelihood principle again implies that, conditional

on all the data, adaptive utility balance, or any other

criterion to adaptively choose design points, is irrel-

evant to the analysis. In the presence of consumer

heterogeneity, a respondent's contribution to the like-

lihood is

l i = Di i

(15)

and, by conditional independence, the sample likeli-

hood is

l

N i i=1

=

i

Di i

(16)

Equation (14) demonstrates that adaptive questioning does not change the likelihood for any of the "i" respondents. The total sample likelihood is therefore not affected. In the setting of a Bayesian hierarchical model with heterogeneity, a random-effects distribution for i is introduced along with a prior distribution for the hyperparameters, :

N i i=1

Di

N i=1



Di i × i ×

i

(17)

This corresponds to a data-generating process where the i vectors are modeled as draws from the randomeffects distribution i , and the data is generated from the likelihood Di i . Because adaptive questioning does not change the likelihood, it is not relevant to the analysis, whether heterogeneity is present or absent. Moreover, posterior estimates of the hyperparameters will converge to the true data-generating values as the sample size increases, even if each consumer provides a limited amount of information and the individual-level parameters, i, experience shrinkage toward the hyperparameter . This result is due to exchangeability of consumers implied by the random-effects distribution--adaptive questioning does not affect the exchangeability of the response vectors obtained from each consumer.

3 The selection bias results of HT (2005) may be based on analysis that excluded the self-explicated data. We comment further on that procedure, which ACA refers to as "pairs only," below.

As a result, estimates of hyperparameters are based on relatively large samples where data are pooled across respondents and will be close to their true values. Estimates of individual-level parameters, i, are biased because their posterior distributions are based on an individual's own data Di , all other data Dj as shared through the mixing distribution i , and the prior distribution . The mixing distribution and prior distribution have a nonnegligible effect on the individual-level estimates. We comment further on sampling properties of Bayesian estimators below.
We investigate the impact of endogenous covariates in a hierarchical Bayes model by adding heterogeneity to our earlier example. That is, instead of 1,000 identical respondents, we assume that the part-worths for the respondents follow a random-effects distribution:

i  Normal

=

2 1

10 V=
01 i=1

1 000 (18)

where i indexes the respondents. Diffuse prior distributions, centered on the true values, are assumed for the hyperparameters:

2=

0s02
2

with 0 = 5 and s02 = 25

(19)

0

V = I W 0 V0 with 0 = 10 and V0 = 10I (20)

Figure 2 compares Bayesian analysis of for exogenous versus endogenous covariates. Time-series plots of the draws of for these two sets of data show little difference. The posterior mean for the plot with exogenous x3 is = 2 00 0 96 , with a posterior standard deviation of (0.12, 0.11). For x3 endogenous, the posterior mean is = 2 01 0 96 , with a posterior standard deviation of (0.12, 0.12). These results agree with the implications of the likelihood principle-- analysis that conditions on all of the observed data is not affected by endogenously determined covariates. Analysis with fewer respondents (e.g., 100) yields similar results, although the precision of the parameter estimates is decreased. Managerial decisions based on the analysis of the data are also unaffected.

4. Discussion
Our analysis shows that the presence of endogenously determined covariates leads to small-sample bias in conjoint part-worths, which diminishes as the sample size increases. The likelihood principle, on the other hand, asserts that one should ignore the fact that the covariates are endogenously determined in estimation. We note that, in general, likelihoodbased estimates are biased whenever model parameters are nonlinearly related to the observed data

Liu, Otter, and Allenby: Investigating Endogeneity Bias in Marketing

Marketing Science 26(5), pp. 642­650, © 2007 INFORMS

647

Figure 2

MCMC Draws for Exogenous and Endogenous Covariates 1,000 Respondents Exogenous x3

2.0

Mean of random effects

1.0

0.0 0
2.0

500

1,000

1,500

MCMC iteration

Endogenous x3

2,000

Mean of random effects

1.0

0.0 0

500

1,000

1,500

MCMC iteration

2,000

(e.g., variance estimates, logit coefficient estimates), including endogenously determined covariates. In this section we discuss the use of bias for evaluating the performance of estimators, and offer a method for determining when endogenously generated variables will create problems in analysis and decision making--i.e., are not ignorable.
Prior to data collection, managers might be concerned about the manner in which the data will be collected and the anticipated method used to analyze these data. While data collection (i.e., question selection, experimental design) and data analysis (i.e., parameter estimation) are related tasks, and decisions regarding them are often made jointly in light of commercially available software and user knowledge, they are not necessarily related. As discussed by HT and others, utility balance can be pursued for reasons that are difficult to quantify within a statistical model. Forcing respondents to select from among alternatives with nearly equal value might avoid scaling problems that occur when one alternative dominates the rest and might encourage respondents to more carefully evaluate the alternatives. These aspects are not reflected in the standard linear models used in traditional conjoint analysis and therefore do not enter into traditional analysis. Moreover, once the data are collected, one of a number of methods of estimation can be used in analysis (e.g.,

Bayesian, method of moments, maximum likelihood). It is therefore useful to think of data collection and data analysis separately.

Sampling Properties Bias is one measure of the performance of an estimator, and while managers should be concerned about bias prior to data collection, bias should not be used as a litmus test for selecting an estimator or, as such, a scheme to generate design points adaptively. Bias is an aspect of statistical risk, defined as the expected loss from incorrectly estimating a respondent's true part-worths ( ). For squared-error loss we have

Risk

= ED

-2

= ED = ED

- ED - ED

+ ED 2 + ED

+ 2 - ED

ED

= Var + Bias 2

-2 -2 -
(21)

where the cross-product (third) term is zero because the expectation of its first factor in parentheses is zero. Bias can be traded off against variance to obtain lower risk, as it is in ridge regression, and is not usually pursued as a goal in and of itself. Many biased estimators have excellent sampling-theory properties.

Liu, Otter, and Allenby: Investigating Endogeneity Bias in Marketing

648

Marketing Science 26(5), pp. 642­650, © 2007 INFORMS

Bayesian estimators ( ~), for example, minimize expected loss with respect to the posterior distribution (see Rossi et al. 2005, pp. 17­18):

~ = arg min E D L

=L

Dd

(22) where L is the loss (e.g., squared-error loss) associated with using to estimate . Sampling properties of ~ can be studied across multiple realizations of the data:

ED E D L ~

= L~

D D d dD

= L~

D

dD d

= E ED L ~

= E Risk ~

(23)

The last equality shows that Bayes estimators have the property of minimizing expected risk, where the expectation is taken with respect to the prior distribution. Moreover, while Bayesian estimators are biased by the presence of the prior distribution, , they often outperform other estimators by successfully trading off increased bias for lower variance. Theoretically, bias (i.e., ED ~ - ) is of diminished relevance to Bayesians because its computation requires knowledge of the unobserved true value of .
An interesting issue is why one would want to engage in an adaptive design in the first place. After all, orthogonal designs are known to be optimal for linear models, and the use of procedures like utility balance will likely lead to a nonorthogonal design. As discussed by Chaloner and Verdinelli (1995), orthogonal designs are optimal only within the context of fixed designs in the absence of prior knowledge-- i.e., designs where the questions are all selected prior to the collection of any data. Orthogonality is not necessarily optimal in the space of designs, D, that include design points that are selected sequentially. As pointed out by HT, utility balance as a criterion for adaptive design is unlikely to increase efficiency within the confines of the linear model. To the extent that the linear model is at best a local approximation, however, utility balance could help the design stay in the range of values that can be sensibly fitted by the local approximation. This is an empirical issue, the investigation of which is beyond the scope of this paper.

Determining When Endogeneity Will Matter in Estimation Directed acyclic graphs (DAGs) are useful tools for determining when conditioning on endogenously determined covariates results in a misspecified likelihood function. An introduction to DAGs for

Bayesian analysis can be found in Rossi et al. (2005, p. 67). The DAG for the utility-balance example discussed earlier is
x1, x2

y1, y2

x3

y3

,  2
The graph is read from left to right, with the model parameters ( 2 and two design points x1 x2 used to generate responses y1 and y2. The third design point, x3, is determined from the responses as per Equation (5), and used to generate the third response, y3. The likelihood comprises all arrows connected to the model parameters ( 2 . Here we see that the third design point, x3, is determined entirely from y1 and y2, and is not directly connected to the parameters. This implies that the mechanism for selecting x3 can be ignored--i.e., is not relevant for inferences about ( 2 --so long as y1 and y2 are included.
The direct-marketing example discussed earlier involved a pilot study (x1) from which initial results (y1 are obtained and used to design a subsequent study (x2 whose results (y2 are used to draw inferences about model parameters ( ). The DAG for this example is similar to the utility-balance example:
x1

y1

x2

y2



The arrow from y1 to x2 in the DAG above can represent a simple calculation as in Equation (5) above,

or more complicated calculations involving the data,

including the derivation and use of the posterior dis-

tribution

y1 x1  y1

x1

for choos-

ing x2. The posterior conditions on x1 y1 and thus

insulates x2 from --i.e., if x2 is chosen based on

y1 x1 , x2 cannot convey information about

that is not already contained in

y1 x1 . If,

however, y1 (e.g., self-explicated data, results of the

pilot study, answers to screening questions) are not

included in the analysis, or prior knowledge in the

form of

y1 x1 used to select x2 is discarded, the

DAG becomes

x1

x2

y2



Liu, Otter, and Allenby: Investigating Endogeneity Bias in Marketing

Marketing Science 26(5), pp. 642­650, © 2007 INFORMS

649

and the likelihood for the endogenously determined covariate (x2) becomes a function of :

x2 y2 =

y1 y2 x2 x1 d y1

= 1 x2 x1 × 2 y2 x2 (24)

The likelihood contribution of x2 is no longer ignorable. Nonignorable likelihoods occur whenever the data selection/creation mechanism for x is directly related to a parameter ( ) that is also related to the dependent variable y. In addition to the examples described above, marketing researchers often struggle with the issue of sample-selection bias, and various corrections have been proposed (e.g., Heckman 1976) to obtain consistent estimates. All the corrections are based on the presence of a common parameter (e.g., a correlation coefficient) related to variables x and y. Equation (24) indicates that, in principle, one should deal with an endogenously selected sample by including the likelihood for x as part of the model specification--i.e., to use the correct likelihood for all the data x y (for marketing examples, see Manchanda et al. 2004, Zanutto and Bradlow 2006).

Effects of Omitting Self-Explicated Data in Conjoint Analysis Consider what would happen if enough data were initially collected to make precise inferences about a respondent's part-worths. The utility balance algorithm would then generate candidate design points with responses close to zero for all choices. The likelihood 3 in Equation (11) would then have multiple modes--one at zero, one at the true parameter value, and at all parameters values that are proportional to the true values. The mode at zero corresponds to one solution to fitting responses that are all equal to zero. The mode at the true parameter value is present as in any analysis, and the other values proportional to the true value will also yield zero responses for utility balance. Without access to the earlier data, or the original designer's knowledge to be used as a prior, inferences about the part-worths would not be identified by the data anywhere along the true parameter vector. The likelihood for stimulus selection (e.g., 1 in Equation (24)) needs to be included in these analyses to avoid model misspecification. If past observations (in the sense that they are no longer available) were used to learn about model parameters, and design points were picked based on the acquired knowledge, then any analysis that proceeds without incorporating this fact is incomplete and the likelihood function misspecified.
Expanding on our earlier example, which mimics ACA, consider a model that has three regressors instead of two: yt = 1x1t + 2x2t + 3x3t + t and t  Normal 0 1 . Assume we have three self-explicated

observations ySE i = I i + i and that we do not ask respondents questions about the attribute corresponding to the response yt with the smallest (absolute) value. That is, the response data will be uninformative about one of the coefficients. ACA uses a similar procedure to avoiding asking questions about attributes that are unimportant to the respondent. Assume further that each respondent then provides answers to six pairwise comparisons corresponding to product profiles of the remaining attributes and that the distribution of heterogeneity is multivariate normal with mean = 6 3 1 and covariance matrix V = I4. Finally, we assume that the self-explicated data are not used in the analysis.
The resulting posterior means of based on 1,000 respondents are equal to (6 23 3 59 1 67), with posterior standard deviations equal to (0 06 0 07 0 14). Thus, the relative importances of the attributes are distorted. Moreover, the presence of nonzero heterogeneity covariance can be shown to increase the frequency of rank-order reversals of individual-level estimates by two to three times. It should be noted that the magnitude of these inconsistencies are functions of the unobserved parameters. Thus, there is no way to correct for the inconsistencies without knowing the desired parameter values beforehand. When the self-explicated data are included in the analysis (i.e., by including the likelihood for these data), posterior means are equal to (6.05, 2.93, 0.88) with posterior standard deviation equal to (0 06 0 07 0 07). These results indicate that violating the likelihood principle by ignoring the self-explicated data can easily result in economic harm to managers using conjoint analysis to inform product policy. Thus, ACA's "pairs only" option, which discards the self-explicated data when making inferences, should be avoided whenever the design in the pairs section is based on self-explicated data.
ACA offers several options for analysis. Empirically, "pairs only with constraints," where the constraints are derived from the self-explicated data, seems to perform best (Sawtooth Software 2003). This method, and the analysis option "pairs plus selfexplicated" (ACA/Hierarchical Bayes v2.0), where the self-explicated data and the pairs data are analyzed jointly, can be shown to render ACA's adaptive design mechanism ignorable. For "pairs only with constraints," the self-explicated data are assumed to provide ordinal-level information without error that leads to ordinal constraints in the analysis of the pairs data (see Allenby et al. 1995). The "pairs plus selfexplicated" analysis assumes that the data are generated from the same underlying model. Both analysis options condition on the self-explicated data, and the manner in which ACA adaptively generates questions is ignorable.

Liu, Otter, and Allenby: Investigating Endogeneity Bias in Marketing

650

Marketing Science 26(5), pp. 642­650, © 2007 INFORMS

5. Concluding Comments
The likelihood principle is implicit to the Bayesian approach to statistics, where the posterior distribution is derived from the prior distribution and the likelihood. Bayesian analysis conditions on the data to draw inferences about unobservable parameters in the analysis. In a conjoint analysis, it provides an answer to the question "Given the data at hand, what do I know about the part-worths?"
Sampling experiments are useful for understanding statistical properties such as bias when data have not yet been collected. They are not generally useful, however, once data are available for analysis. We demonstrate that the endogeneity identified by HT is irrelevant for analysis once data have been collected. A manager at a user firm who wants to make inferences about part-worths based on a specific data set should not worry about endogeneity created by adaptive questioning so long as the mechanism of choosing design points is ignorable. This will be the case if inference proceeds conditional on the data used to pick design points. Prior to data collection, however, we believe that adaptive design procedures should be evaluated based on risk.
Whether one should condition on the data and adhere to the likelihood principle, as in Bayesian analysis, or conduct sampling experiments, as in a frequentist analysis, is at the philosophical core of statistical inference. The Bayes-frequentist debate is sometimes dismissed as irrelevant because one can obtain about the same answer in some special cases (e.g., a normal likelihood in combination with diffuse priors). The issue of endogeneity bias raised by HT provides a counterexample to this view, where philosophical principles of inference play an important role in conducting analysis. We believe that analysis should condition on the data that is, strictly speaking, only possible if the mode of inference is Bayesian.

The advantage of taking on this orientation is that it greatly simplifies analysis while providing a coherent framework for inference.
Acknowledgments The authors thank John Hauser, Rich Johnson, and Olivier Toubia for helpful comments.
References
Allenby, G. M., N. Arora, J. L. Ginter. 1995. Incorporating prior knowledge into the analysis of conjoint studies. J. Marketing Res. 32 152­162.
Berger, J. O., R. L. Wolpert. 1984. The Likelihood Principle. Institute of Mathematical Statistics Lecture Notes--Monograph Series, Instititute of Mathematical Statistics, Hayward, CA.
Chaloner, K., I. Verdinelli. 1995. Bayesian experimental design: A review. Statist. Sci. 10(3) 273­304.
Fisher, R. A. 1922. On the mathematical foundations of theoretical statistics. Philos. Trans. Roy. Soc. London, Ser. A 222 309­368.
Gelman, A., J. B. Carlin, H. S. Stern, D. B. Rubin. 2004. Bayesian Data Analysis. Chapman & Hall/CRC, Boca Raton, FL.
Hauser, J. R., O. Toubia. 2005. The impact of utility balance and endogeneity in conjoint analysis. Marketing Sci. 24 498­507.
Heckman, J. J. 1976. The common structure of statistical models of truncation, sample selection and limited dependent variables, and a simple estimator for such models. Ann. Econom. Soc. Measurement 5 475­492.
Judge, G. G., R. C. Hill, W. E. Griffiths, H. Lutkepohl, T.-C. Lee. 1988. Introduction to the Theory and Practice of Econometrics, 2nd ed. John Wiley & Sons, New York.
Manchanda, P., P. E. Rossi, P. K. Chintagunta. 2004. Response modeling with nonrandom marketing-mix variables. J. Marketing Res. 41 467­478.
Rossi, P. E., G. M. Allenby, R. McCulloch. 2005. Bayesian Statistics and Marketing. John Wiley & Sons, New York.
Rubin, D. B. 1976. Inference and missing data. Biometrika 63 581­592.
Sawtooth Software. 2003. ACA/Hierarchical Bayes v2.0, Technical paper, http://www.sawtoothsoftware.com/technicaldownloads. shtml#acahbtech.
Zanutto, E. L., E. T. Bradlow. 2006. Data pruning in consumer choice. Quant. Marketing Econom. 4(3) 267­287.

