{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec tutorial\n",
    "\n",
    "In this tutorial we will go over the basic usage of a pre-trained word2vec model to obtain embeddings for a piece of text. You will learn how to download a pre-trained model, load it into python, convert the piece of text to a word embedding and how to save the embeddings. You can then use these embeddings for a classification task, analysis of the corpus and/or other types of similarity orientated tasks. \n",
    "\n",
    "The dataset used for this tutorial is equivalent to the dataset used for the LDA tutorial. We will use a collection of the prestigious [NIPS](https://nips.cc/) conference obtained from a [kaggle competition](https://www.kaggle.com/benhamner/nips-papers). You can download the data from the kaggle link, it will also be attached to this file and is placed in the folder \"Data\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string # Used to remove stopwords\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the data\n",
    "Similair to any other NLP project we first have to pre-process the data, we will do so in a similair fashion as we did during the LDA tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_to_lower(df, column):\n",
    "    return df[column].str.lower()\n",
    "\n",
    "def column_remove_punctuation(df, column):\n",
    "    return df[column].str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "def column_remove_stop_words(df, column, stopwords):\n",
    "    print(f\"Currently processing the column: {column}\")\n",
    "    return df[column].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing the column: abstract\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/papers.csv\")\n",
    "stop_words = stopwords.words('english')\n",
    "df['abstract'] = column_to_lower(df, 'abstract')\n",
    "df['abstract'] = column_remove_punctuation(df, 'abstract')\n",
    "df['abstract'] = column_remove_stop_words(df, 'abstract', stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and loading the pre-trained word2vec model.\n",
    "We will use the package gensim to work with the word2vec model. Gensim has a range of available pre-models, one of them is the GoogleNews300 model that is trained on the Google News corpus which consists of over 100 billion words. The number 300 represents the dimensionality of the vector, the dimensionality of the embedding space. In the Bag of Words setting this dimensionality would be determined by the amount of unique words, in word2vec this number is pre-determined before training the model. \n",
    "\n",
    "To download the pre-trained model I took the following steps\n",
    " - In your terminal type: brew install wget\n",
    " - In your terminal type: wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    " - Move the zip to the folder that contains your code\n",
    " - Unzip the model either manually or by navigating to your folder and then use the terminal command: gzip -d GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "import gensim\n",
    "# download the pre-trained word2vec model #https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin\n",
    "w2v_vectors = models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded the model and saved it in the python variable \"w2v_vectors\" we can now use the model to get our embeddings. First, let us inspect some functions that are defined by gensim. One famous example is finding the equivalent of man to king as is woman to ?. We can get this result by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering the word relationship (king, man) \n",
      " The model deems the pair (woman, ?) to be answered as ? = queen with score 0.7118192911148071 \n"
     ]
    }
   ],
   "source": [
    "result = w2v_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(f\"Considering the word relationship (king, man) \\n The model deems the pair (woman, ?) to be answered as ? = {result[0][0]} with score {result[0][1]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the obvious equivalent missing word is \"queen\". We found this result by asking the word2vec model what is the most similair wordvector considering the relationship between man and king and woman is ?. This is calculated by using the cosine similarity, this metric is often used to determine how similair two words are. Intuitively, a high cosine similairty score indicates that two words are very similair while a low similarity score indicates that the two words are not similair at all. The cosine similarity is calculated as\n",
    "\n",
    "<img src=\"Images/cossim.png\">\n",
    "\n",
    "This allows us to compare word vectors with each other. Consider this example\n",
    "\n",
    "<img src=\"Images/cosine_sim_example.png\">\n",
    "\n",
    "It is difficult to visualise wordvectors outside of the 3D space, however, we can assume the vectors to be 2D such that we can maka use of a 2D plot for illustrative purposes. Our word vectors are 300D but I cannot visualise 300D in my mind, I don't think anyone can, but we can think of the word embeddings as vectors inside a 2D space and consider this figure\n",
    "\n",
    "<img src=\"Images/word_e_r.png\">\n",
    "\n",
    "You can see that the relationship between words can be interpreted as vector operations. Being able to capture relationships between words as man is to king as woman is to ? is very important as now we know that the word vectors do capture these relationships and that they capture the concept of words having meaning. We cannot directly infer this relationship using e.g. BoW or tf-idf\n",
    "\n",
    "## Other functions using Gensim\n",
    "Gensim also offers some other functions that could be suitable for your project. One of the functions is given a string, which word does not match the other words in the string? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(w2v_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also ask what the distance between two words is, intuitively this implies that words that are similair lie closer to each other in the embedding space thus the distance will be smaller compared to words that have nothing to do with each other. The distance between two of the same vectors will always be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the words 'coffe' and 'tea' is: 0.43647080659866333\n",
      "The distance between a word and itself (using w2v) is always:  0.0\n"
     ]
    }
   ],
   "source": [
    "distance = w2v_vectors.distance(\"coffee\", \"tea\")\n",
    "print(f\"The distance between the words 'coffe' and 'tea' is: {distance}\")\n",
    "\n",
    "distance = w2v_vectors.distance(\"coffee\", \"coffee\")\n",
    "print(\"The distance between a word and itself (using w2v) is always: \", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert a word to its corresponding word2vec word embedding and inspect the results. Given that we use the pre-trained model called Google News 300, we expect that for each word we retreive a vector of size (300,1) or in numpy terms this can be expressed as (300,). We will obtain a word vector, print its shape and manually inspect the actual word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vector is:  (300,)\n",
      "With values:  [ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
     ]
    }
   ],
   "source": [
    "vector = w2v_vectors['computer']\n",
    "print(\"Shape of the vector is: \", vector.shape)\n",
    "print(\"With values: \", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the word vector is indeed a continuous vector with a dimensionality of 300. As you can see, we as humans cannot directly interpret these numbers, they do not hold any meaning to us. However, methods such as the cosine similarity measurement or cosine distance help us understand the relationship between these vectors. \n",
    "\n",
    "The next step would be to convert a sentence to a word embedding. Considering that a sentence consists of multiple words, we have to retrieve the word embeddings for each word and average the vectors to gain a sentence embedding. There is a try-except block in the code below as it could be that you encounter a word that the model does not know. This could have several reasons such as, the word is a stopword or it is not an english word or the model has not encountered this word during training time. Hence why we use a try/except block to capture the scenario where the model does now know the word. We can simply skip this word and not take it into consideration for our sentence embedding. If you find that the model does not know many of the words that you encounter then you have to either train/fine-tune the model using your corpus or download a different pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the academic world there are numerous interesting and prestigous journals\n",
      "['in', 'the', 'academic', 'world', 'there', 'are', 'numerous', 'interesting', 'and', 'prestigous', 'journals']\n",
      "Word 'and' is not in the vocabulary.\n",
      "(10, 300)\n",
      "[-0.00956116  0.03077087  0.01315918  0.12819824  0.07929687 -0.06503906\n",
      "  0.03223877 -0.04815968  0.07803345  0.06192627 -0.02783203 -0.12019996\n",
      "  0.02840576  0.08427735 -0.06013184  0.03520508  0.02338257  0.07210694\n",
      " -0.02193603  0.00145264 -0.03520203  0.03729858 -0.06897583 -0.02338638\n",
      " -0.03984986 -0.04063721 -0.09047394  0.08147583 -0.05607452 -0.05244141\n",
      "  0.02787743 -0.10141907  0.03948975  0.03804016  0.07524414 -0.031073\n",
      " -0.02215385  0.00544434  0.07575683  0.05131836  0.10772705  0.01655274\n",
      " -0.03143311  0.10469361 -0.00141602  0.02293854  0.01469421 -0.04886322\n",
      " -0.0708374   0.01257324 -0.02077484  0.02348023 -0.09191284 -0.02677612\n",
      "  0.02268066  0.09912644  0.08406983 -0.14236145  0.09207153 -0.08804779\n",
      " -0.0247757   0.03312988  0.05045166 -0.16357422 -0.04278488  0.00926666\n",
      " -0.03708496  0.11959839 -0.068367    0.05110474 -0.05573731  0.03768311\n",
      "  0.10628662 -0.00557861 -0.07818222 -0.01599121  0.03444824  0.09941407\n",
      "  0.06147461  0.09467773  0.00760498 -0.06904908  0.02254639 -0.06245422\n",
      " -0.02856445 -0.090625   -0.08171387  0.06086426 -0.1239151   0.09515381\n",
      "  0.09276734 -0.06781616 -0.04089966 -0.12194824 -0.03818359  0.03104248\n",
      "  0.13309631 -0.01148682  0.0887146  -0.05300598 -0.03231678  0.02041016\n",
      " -0.01342773 -0.06205139  0.04053039 -0.01958008 -0.04629974 -0.02739868\n",
      "  0.10044555 -0.08878174 -0.05181885  0.0488739  -0.04563599 -0.017453\n",
      "  0.05178223 -0.08409424  0.05626221 -0.06717529  0.16496582  0.02512398\n",
      " -0.05937805 -0.02073059 -0.02350464  0.0184906  -0.07570801 -0.10356255\n",
      " -0.04241943  0.01528931 -0.11456299  0.08358154  0.00183716 -0.11401367\n",
      " -0.08518066  0.02145996  0.00782471 -0.07423095  0.04197083  0.04307861\n",
      " -0.00185547  0.06074219 -0.01125488 -0.01432495  0.12484741  0.03187256\n",
      "  0.02197571  0.07921143  0.01077271  0.02009277 -0.06693115 -0.01187744\n",
      "  0.0424469   0.00955505 -0.11191406  0.01567345 -0.03461914 -0.0723999\n",
      " -0.04853211 -0.071344   -0.02317352 -0.0010498  -0.01576538  0.10104065\n",
      " -0.00398254  0.05784302  0.15567628 -0.02363281  0.02250976 -0.11344299\n",
      "  0.00827026 -0.02783203 -0.10805664 -0.01193848 -0.06408081 -0.14267579\n",
      " -0.00872345 -0.13596192 -0.03417969 -0.10986328  0.00490723 -0.0312149\n",
      " -0.08136597 -0.10807953 -0.0329422   0.0057373   0.01791992 -0.00291748\n",
      " -0.01225662  0.0515625   0.10791626  0.08835449  0.10286865 -0.02859421\n",
      "  0.07798996 -0.07028198 -0.06887207 -0.00973816 -0.00526123 -0.1266449\n",
      " -0.04302979 -0.11367722  0.03414307  0.12209473 -0.0205719  -0.07732544\n",
      " -0.05987549 -0.0347168  -0.01695557 -0.07280274 -0.05134583  0.07947693\n",
      "  0.01808548 -0.04129639 -0.01733704  0.02629547 -0.02763672  0.07578583\n",
      "  0.06427002  0.02346191 -0.02344971 -0.04459228 -0.02880859 -0.02493286\n",
      " -0.11830445 -0.01738281  0.03381348  0.01517181  0.05120545 -0.06673584\n",
      " -0.01582031 -0.02385864  0.03438721 -0.02169189  0.03759461 -0.02001648\n",
      "  0.10291214 -0.03338013  0.01625748 -0.0259552   0.05225525  0.01977234\n",
      "  0.06227112 -0.04294433  0.0043335  -0.04016418 -0.05634766  0.0284668\n",
      "  0.02724276 -0.01694946 -0.07680054 -0.04731445  0.04023438  0.00187378\n",
      "  0.02632446  0.01760559 -0.02494507 -0.0506958  -0.0050293  -0.02323112\n",
      " -0.0263092   0.00374146  0.08683167 -0.01773071 -0.02763748  0.04601898\n",
      "  0.03681641  0.08513184 -0.05210571 -0.0180542  -0.07260742 -0.03676758\n",
      "  0.0463623   0.03864746  0.02706299  0.06325988  0.07245483 -0.05861054\n",
      " -0.03901367 -0.14564209 -0.01233215 -0.0171875   0.03273926 -0.11033783\n",
      " -0.00709839  0.02266235  0.02177887  0.09019776 -0.10437927  0.03439941\n",
      "  0.08206482  0.10255738 -0.06304932 -0.08096389 -0.1625061   0.0512085\n",
      " -0.02440185 -0.01022949  0.00870361  0.02753906  0.03848267  0.02752686]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In the academic world there are numerous interesting and prestigous journals\"\n",
    "parsed_sentence = sentence.lower().split()\n",
    "print(sentence)\n",
    "print(parsed_sentence)\n",
    "\n",
    "word_vectors = []\n",
    "\n",
    "# For each word in the sentence\n",
    "# - Try to retrieve the corresponding word vector \n",
    "# - Append the word embedding  to a list\n",
    "# once we have all word embeddings, we can simply take the average over the first dimension to gain an average embedding for the sentence \n",
    "for word in parsed_sentence:\n",
    "    try:\n",
    "        word_vector = w2v_vectors[word]\n",
    "        word_vectors.append(word_vector)\n",
    "    except:\n",
    "        print(f\"Word '{word}' is not in the vocabulary.\")\n",
    "\n",
    "print(np.asarray(word_vectors).shape) # the first dimension here stands for the amount of words that the model has word vectors for\n",
    "sentence_w2v_embedding = np.average(np.asarray(word_vectors), axis = 0) # hence why we take the averege over the first dimension\n",
    "print(sentence_w2v_embedding) # the end results remains uninterpretable for humans!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert abstracts to word embeddings and save\n",
    "Now that we have seen how to convert words (strings) to a word vector, let us consider the real data and create an embedding for each available abstract. We will store the embeddings in a dict of format {abstract_id: sentence_embedding} and pickle the result. If we want to retreive the word embeddings we could simply use the ID that is available in the dataframe and obtain the corresponding word embeddings after we have run this script once.\n",
    "\n",
    "To let the code go over each abstract in the data remove the \"break\" at line 26. For illustrative purposes we only consider the first non-missing abstract and show how you can store and save the result. This is just an example, there many other ways you can save the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === new non-missing abstract ===\n",
      "Word 'nonnegative' is not in the vocabulary.\n",
      "Word 'nmf' is not in the vocabulary.\n",
      "Word 'plicative' is not in the vocabulary.\n",
      "Word 'nmf' is not in the vocabulary.\n",
      "Word 'kullbackleibler' is not in the vocabulary.\n",
      "Word 'onally' is not in the vocabulary.\n",
      "Out of 65 words there were 6 missing in the w2v model\n"
     ]
    }
   ],
   "source": [
    "dict_sentence_embeddings = {} # dict with mapping of {id: sentence_embedding}\n",
    "for index, row in df.iterrows():\n",
    "    sentence_embedding_list = []\n",
    "    abstract = row['abstract']\n",
    "    row_id = row['id']\n",
    "    \n",
    "    missing_words = 0\n",
    "    if abstract == \"abstract missing\":\n",
    "        # scenario where there is no abstract\n",
    "        continue\n",
    "    else:\n",
    "        print(\" === new non-missing abstract ===\")\n",
    "        for word in abstract.split():\n",
    "                try:\n",
    "                    word_vector = w2v_vectors[word]\n",
    "                    sentence_embedding_list.append(word_vector)\n",
    "                except:\n",
    "                    missing_words += 1\n",
    "                    print(f\"Word '{word}' is not in the vocabulary.\")\n",
    "        \n",
    "        \n",
    "    sentence_embedding = np.average(np.asarray(sentence_embedding_list), axis = 0)\n",
    "    dict_sentence_embeddings[row_id] = sentence_embedding\n",
    "    \n",
    "    print(f\"Out of {len(abstract.split())} words there were {missing_words} missing in the w2v model\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# dump the dictionary with mapping {id: sentence_embedding} using pickle\n",
    "with open('id_w2v_map.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_sentence_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# obtain the dictionary with mapping {id: sentence_embedding} using pickle\n",
    "with open('id_w2v_map.pickle', 'rb') as handle:\n",
    "    dict_embeddings = pickle.load(handle)\n",
    "\n",
    "#print(dict_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
